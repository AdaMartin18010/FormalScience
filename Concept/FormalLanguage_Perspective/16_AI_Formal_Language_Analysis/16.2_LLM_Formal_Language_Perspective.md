# å¤§è¯­è¨€æ¨¡å‹çš„å½¢å¼è¯­è¨€è§†è§’åˆ†æ

## ç›®å½• | Table of Contents

- [å¤§è¯­è¨€æ¨¡å‹çš„å½¢å¼è¯­è¨€è§†è§’åˆ†æ](#å¤§è¯­è¨€æ¨¡å‹çš„å½¢å¼è¯­è¨€è§†è§’åˆ†æ)
- [ç›®å½•](#ç›®å½•)
- [1. LLMçš„å½¢å¼è¯­è¨€ç»“æ„åˆ†æ](#1-llmçš„å½¢å¼è¯­è¨€ç»“æ„åˆ†æ)
  - [1.1 å½“å‰LLMçš„"ä¼ªå½¢å¼è¯­è¨€"ç‰¹å¾](#11-å½“å‰llmçš„ä¼ªå½¢å¼è¯­è¨€ç‰¹å¾)
    - [1.1.1 LLMçš„å½¢å¼è¯­è¨€ç†è®ºåŸºç¡€](#111-llmçš„å½¢å¼è¯­è¨€ç†è®ºåŸºç¡€)
    - [1.1.2 LLMçš„å½¢å¼è¯­è¨€ç‰¹å¾](#112-llmçš„å½¢å¼è¯­è¨€ç‰¹å¾)
    - [1.1.3 LLMçš„å½¢å¼åŒ–æ–¹æ³•](#113-llmçš„å½¢å¼åŒ–æ–¹æ³•)
    - [1.1.1 å­—æ¯è¡¨å±‚é¢](#111-å­—æ¯è¡¨å±‚é¢)
    - [1.1.2 è¯­æ³•å±‚é¢](#112-è¯­æ³•å±‚é¢)
    - [1.1.3 è¯­ä¹‰å±‚é¢](#113-è¯­ä¹‰å±‚é¢)
  - [1.2 LLMçš„"è¿ç»­-ç¦»æ•£"æ··åˆç‰¹å¾](#12-llmçš„è¿ç»­-ç¦»æ•£æ··åˆç‰¹å¾)
    - [1.2.1 è¿ç»­è¡¨ç¤º](#121-è¿ç»­è¡¨ç¤º)
    - [1.2.2 ç¦»æ•£è¾“å‡º](#122-ç¦»æ•£è¾“å‡º)
    - [1.2.3 æ··åˆå¤„ç†](#123-æ··åˆå¤„ç†)
- [2. Transformeræ¶æ„çš„å½¢å¼è¯­è¨€è§£è¯»](#2-transformeræ¶æ„çš„å½¢å¼è¯­è¨€è§£è¯»)
  - [2.1 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶](#21-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶)
    - [2.1.1 æ³¨æ„åŠ›æƒé‡çŸ©é˜µ](#211-æ³¨æ„åŠ›æƒé‡çŸ©é˜µ)
    - [2.1.2 ä½ç½®ç¼–ç ](#212-ä½ç½®ç¼–ç )
  - [2.2 å‰é¦ˆç¥ç»ç½‘ç»œ](#22-å‰é¦ˆç¥ç»ç½‘ç»œ)
    - [2.2.1 ä¸¤å±‚MLPç»“æ„](#221-ä¸¤å±‚mlpç»“æ„)
    - [2.2.2 æ®‹å·®è¿æ¥](#222-æ®‹å·®è¿æ¥)
  - [2.3 å±‚å½’ä¸€åŒ–](#23-å±‚å½’ä¸€åŒ–)
    - [2.3.1 LayerNormå…¬å¼](#231-layernormå…¬å¼)
- [3. æ³¨æ„åŠ›æœºåˆ¶ä¸è¯­æ³•è§„åˆ™](#3-æ³¨æ„åŠ›æœºåˆ¶ä¸è¯­æ³•è§„åˆ™)
  - [3.1 æ³¨æ„åŠ›æƒé‡çš„è¯­æ³•è§£é‡Š](#31-æ³¨æ„åŠ›æƒé‡çš„è¯­æ³•è§£é‡Š)
    - [3.1.1 è¯­æ³•ä¾èµ–å…³ç³»](#311-è¯­æ³•ä¾èµ–å…³ç³»)
    - [3.1.2 å¤šå¤´æ³¨æ„åŠ›çš„è¯­æ³•åˆ†å·¥](#312-å¤šå¤´æ³¨æ„åŠ›çš„è¯­æ³•åˆ†å·¥)
  - [3.2 è¯­æ³•è§„åˆ™çš„éšå¼å­¦ä¹ ](#32-è¯­æ³•è§„åˆ™çš„éšå¼å­¦ä¹ )
    - [3.2.1 è§„åˆ™ç¼–ç ](#321-è§„åˆ™ç¼–ç )
    - [3.2.2 è§„åˆ™æå–](#322-è§„åˆ™æå–)
  - [3.3 è¯­æ³•ç»“æ„çš„å¯è§†åŒ–](#33-è¯­æ³•ç»“æ„çš„å¯è§†åŒ–)
    - [3.3.1 æ³¨æ„åŠ›çƒ­å›¾](#331-æ³¨æ„åŠ›çƒ­å›¾)
    - [3.3.2 è¯­æ³•æ ‘é‡æ„](#332-è¯­æ³•æ ‘é‡æ„)
- [4. LLMçš„è¯­ä¹‰æ¨¡å‹ç¼ºé™·](#4-llmçš„è¯­ä¹‰æ¨¡å‹ç¼ºé™·)
  - [4.1 ç¼ºä¹å¤–éƒ¨è¯­ä¹‰æ¨¡å‹](#41-ç¼ºä¹å¤–éƒ¨è¯­ä¹‰æ¨¡å‹)
    - [4.1.1 å†…éƒ¨è¯­ä¹‰vså¤–éƒ¨è¯­ä¹‰](#411-å†…éƒ¨è¯­ä¹‰vså¤–éƒ¨è¯­ä¹‰)
    - [4.1.2 çœŸå€¼æ¡ä»¶ç¼ºå¤±](#412-çœŸå€¼æ¡ä»¶ç¼ºå¤±)
  - [4.2 è¯­ä¹‰ç»„åˆæ€§é—®é¢˜](#42-è¯­ä¹‰ç»„åˆæ€§é—®é¢˜)
    - [4.2.1 ç»„åˆæ€§åŸåˆ™](#421-ç»„åˆæ€§åŸåˆ™)
    - [4.2.2 è¯­ä¹‰é€æ˜åº¦](#422-è¯­ä¹‰é€æ˜åº¦)
  - [4.3 æŒ‡ç§°è¯­ä¹‰é—®é¢˜](#43-æŒ‡ç§°è¯­ä¹‰é—®é¢˜)
    - [4.3.1 æŒ‡ç§°å‡½æ•°](#431-æŒ‡ç§°å‡½æ•°)
    - [4.3.2 çœŸå€¼æ¡ä»¶](#432-çœŸå€¼æ¡ä»¶)
- [5. è‡ªæŒ‡èƒ½åŠ›çš„ç¼ºå¤±ä¸è¡¥æ•‘](#5-è‡ªæŒ‡èƒ½åŠ›çš„ç¼ºå¤±ä¸è¡¥æ•‘)
  - [5.1 å½“å‰LLMçš„è‡ªæŒ‡ç¼ºé™·](#51-å½“å‰llmçš„è‡ªæŒ‡ç¼ºé™·)
    - [5.1.1 æ— æ³•ä¿®æ”¹è‡ªèº«](#511-æ— æ³•ä¿®æ”¹è‡ªèº«)
    - [5.1.2 æ— æ³•åæ€è‡ªèº«](#512-æ— æ³•åæ€è‡ªèº«)
    - [5.1.3 æ— æ³•å‡çº§è‡ªèº«](#513-æ— æ³•å‡çº§è‡ªèº«)
  - [5.2 è‡ªæŒ‡èƒ½åŠ›çš„è¡¥æ•‘æ–¹æ¡ˆ](#52-è‡ªæŒ‡èƒ½åŠ›çš„è¡¥æ•‘æ–¹æ¡ˆ)
    - [5.2.1 å…ƒå­¦ä¹ æ¡†æ¶](#521-å…ƒå­¦ä¹ æ¡†æ¶)
    - [5.2.2 è‡ªæŒ‡æç¤º](#522-è‡ªæŒ‡æç¤º)
    - [5.2.3 é€’å½’æ¶æ„](#523-é€’å½’æ¶æ„)
  - [5.3 çœŸæ­£è‡ªæŒ‡çš„å®ç°è·¯å¾„](#53-çœŸæ­£è‡ªæŒ‡çš„å®ç°è·¯å¾„)
    - [5.3.1 æƒé‡å¯ä¿®æ”¹](#531-æƒé‡å¯ä¿®æ”¹)
    - [5.3.2 æ¶æ„å¯é‡æ„](#532-æ¶æ„å¯é‡æ„)
    - [5.3.3 ç›®æ ‡å¯è°ƒæ•´](#533-ç›®æ ‡å¯è°ƒæ•´)
- [6. æœªæ¥LLMçš„å½¢å¼è¯­è¨€å‡çº§è·¯å¾„](#6-æœªæ¥llmçš„å½¢å¼è¯­è¨€å‡çº§è·¯å¾„)
  - [6.1 åŒé€šé“æ¶æ„](#61-åŒé€šé“æ¶æ„)
    - [6.1.1 è¿ç»­é€šé“](#611-è¿ç»­é€šé“)
    - [6.1.2 ç¦»æ•£é€šé“](#612-ç¦»æ•£é€šé“)
    - [6.1.3 é€šé“èåˆ](#613-é€šé“èåˆ)
  - [6.2 å¯è§£é‡Šè¯­æ³•](#62-å¯è§£é‡Šè¯­æ³•)
    - [6.2.1 æ˜¾å¼è¯­æ³•è§„åˆ™](#621-æ˜¾å¼è¯­æ³•è§„åˆ™)
    - [6.2.2 è¯­æ³•éªŒè¯](#622-è¯­æ³•éªŒè¯)
    - [6.2.3 è¯­æ³•å­¦ä¹ ](#623-è¯­æ³•å­¦ä¹ )
  - [6.3 å¤–éƒ¨è¯­ä¹‰æ¨¡å‹](#63-å¤–éƒ¨è¯­ä¹‰æ¨¡å‹)
    - [6.3.1 çŸ¥è¯†å›¾è°±é›†æˆ](#631-çŸ¥è¯†å›¾è°±é›†æˆ)
    - [6.3.2 é€»è¾‘æ¨ç†å¼•æ“](#632-é€»è¾‘æ¨ç†å¼•æ“)
    - [6.3.3 å¤šæ¨¡æ€è¯­ä¹‰](#633-å¤šæ¨¡æ€è¯­ä¹‰)
  - [6.4 çœŸæ­£è‡ªæŒ‡ç³»ç»Ÿ](#64-çœŸæ­£è‡ªæŒ‡ç³»ç»Ÿ)
    - [6.4.1 è‡ªæˆ‘ä¿®æ”¹èƒ½åŠ›](#641-è‡ªæˆ‘ä¿®æ”¹èƒ½åŠ›)
    - [6.4.2 è‡ªæˆ‘åæ€èƒ½åŠ›](#642-è‡ªæˆ‘åæ€èƒ½åŠ›)
    - [6.4.3 è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›](#643-è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›)
- [ç»“è®º](#ç»“è®º)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)
  - [åŸºç¡€ç†è®ºæ–‡çŒ®](#åŸºç¡€ç†è®ºæ–‡çŒ®)
  - [æ ¸å¿ƒæ¨¡å‹æ–‡çŒ®](#æ ¸å¿ƒæ¨¡å‹æ–‡çŒ®)
  - [å½¢å¼è¯­è¨€åˆ†ææ–‡çŒ®](#å½¢å¼è¯­è¨€åˆ†ææ–‡çŒ®)
  - [æ‰¹è¯„ä¸åˆ†ææ–‡çŒ®](#æ‰¹è¯„ä¸åˆ†ææ–‡çŒ®)
  - [æ”¹è¿›æ–¹å‘æ–‡çŒ®](#æ”¹è¿›æ–¹å‘æ–‡çŒ®)
  - [æŠ€æœ¯å®ç°æ–‡çŒ®](#æŠ€æœ¯å®ç°æ–‡çŒ®)
  - [åº”ç”¨é¢†åŸŸæ–‡çŒ®](#åº”ç”¨é¢†åŸŸæ–‡çŒ®)
  - [ç›¸å…³æ¦‚å¿µé“¾æ¥](#ç›¸å…³æ¦‚å¿µé“¾æ¥)

---

## ç›®å½•

- [å¤§è¯­è¨€æ¨¡å‹çš„å½¢å¼è¯­è¨€è§†è§’åˆ†æ](#å¤§è¯­è¨€æ¨¡å‹çš„å½¢å¼è¯­è¨€è§†è§’åˆ†æ)
  - [ç›®å½•](#ç›®å½•)
  - [1. LLMçš„å½¢å¼è¯­è¨€ç»“æ„åˆ†æ](#1-llmçš„å½¢å¼è¯­è¨€ç»“æ„åˆ†æ)
    - [1.1 å½“å‰LLMçš„"ä¼ªå½¢å¼è¯­è¨€"ç‰¹å¾](#11-å½“å‰llmçš„ä¼ªå½¢å¼è¯­è¨€ç‰¹å¾)
      - [1.1.1 LLMçš„å½¢å¼è¯­è¨€ç†è®ºåŸºç¡€](#111-llmçš„å½¢å¼è¯­è¨€ç†è®ºåŸºç¡€)
      - [1.1.2 LLMçš„å½¢å¼è¯­è¨€ç‰¹å¾](#112-llmçš„å½¢å¼è¯­è¨€ç‰¹å¾)
      - [1.1.3 LLMçš„å½¢å¼åŒ–æ–¹æ³•](#113-llmçš„å½¢å¼åŒ–æ–¹æ³•)
      - [1.1.1 å­—æ¯è¡¨å±‚é¢](#111-å­—æ¯è¡¨å±‚é¢)
      - [1.1.2 è¯­æ³•å±‚é¢](#112-è¯­æ³•å±‚é¢)
      - [1.1.3 è¯­ä¹‰å±‚é¢](#113-è¯­ä¹‰å±‚é¢)
    - [1.2 LLMçš„"è¿ç»­-ç¦»æ•£"æ··åˆç‰¹å¾](#12-llmçš„è¿ç»­-ç¦»æ•£æ··åˆç‰¹å¾)
      - [1.2.1 è¿ç»­è¡¨ç¤º](#121-è¿ç»­è¡¨ç¤º)
      - [1.2.2 ç¦»æ•£è¾“å‡º](#122-ç¦»æ•£è¾“å‡º)
      - [1.2.3 æ··åˆå¤„ç†](#123-æ··åˆå¤„ç†)
  - [2. Transformeræ¶æ„çš„å½¢å¼è¯­è¨€è§£è¯»](#2-transformeræ¶æ„çš„å½¢å¼è¯­è¨€è§£è¯»)
    - [2.1 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶](#21-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶)
      - [2.1.1 æ³¨æ„åŠ›æƒé‡çŸ©é˜µ](#211-æ³¨æ„åŠ›æƒé‡çŸ©é˜µ)
      - [2.1.2 ä½ç½®ç¼–ç ](#212-ä½ç½®ç¼–ç )
    - [2.2 å‰é¦ˆç¥ç»ç½‘ç»œ](#22-å‰é¦ˆç¥ç»ç½‘ç»œ)
      - [2.2.1 ä¸¤å±‚MLPç»“æ„](#221-ä¸¤å±‚mlpç»“æ„)
      - [2.2.2 æ®‹å·®è¿æ¥](#222-æ®‹å·®è¿æ¥)
    - [2.3 å±‚å½’ä¸€åŒ–](#23-å±‚å½’ä¸€åŒ–)
      - [2.3.1 LayerNormå…¬å¼](#231-layernormå…¬å¼)
  - [3. æ³¨æ„åŠ›æœºåˆ¶ä¸è¯­æ³•è§„åˆ™](#3-æ³¨æ„åŠ›æœºåˆ¶ä¸è¯­æ³•è§„åˆ™)
    - [3.1 æ³¨æ„åŠ›æƒé‡çš„è¯­æ³•è§£é‡Š](#31-æ³¨æ„åŠ›æƒé‡çš„è¯­æ³•è§£é‡Š)
      - [3.1.1 è¯­æ³•ä¾èµ–å…³ç³»](#311-è¯­æ³•ä¾èµ–å…³ç³»)
      - [3.1.2 å¤šå¤´æ³¨æ„åŠ›çš„è¯­æ³•åˆ†å·¥](#312-å¤šå¤´æ³¨æ„åŠ›çš„è¯­æ³•åˆ†å·¥)
    - [3.2 è¯­æ³•è§„åˆ™çš„éšå¼å­¦ä¹ ](#32-è¯­æ³•è§„åˆ™çš„éšå¼å­¦ä¹ )
      - [3.2.1 è§„åˆ™ç¼–ç ](#321-è§„åˆ™ç¼–ç )
      - [3.2.2 è§„åˆ™æå–](#322-è§„åˆ™æå–)
    - [3.3 è¯­æ³•ç»“æ„çš„å¯è§†åŒ–](#33-è¯­æ³•ç»“æ„çš„å¯è§†åŒ–)
      - [3.3.1 æ³¨æ„åŠ›çƒ­å›¾](#331-æ³¨æ„åŠ›çƒ­å›¾)
      - [3.3.2 è¯­æ³•æ ‘é‡æ„](#332-è¯­æ³•æ ‘é‡æ„)
  - [4. LLMçš„è¯­ä¹‰æ¨¡å‹ç¼ºé™·](#4-llmçš„è¯­ä¹‰æ¨¡å‹ç¼ºé™·)
    - [4.1 ç¼ºä¹å¤–éƒ¨è¯­ä¹‰æ¨¡å‹](#41-ç¼ºä¹å¤–éƒ¨è¯­ä¹‰æ¨¡å‹)
      - [4.1.1 å†…éƒ¨è¯­ä¹‰vså¤–éƒ¨è¯­ä¹‰](#411-å†…éƒ¨è¯­ä¹‰vså¤–éƒ¨è¯­ä¹‰)
      - [4.1.2 çœŸå€¼æ¡ä»¶ç¼ºå¤±](#412-çœŸå€¼æ¡ä»¶ç¼ºå¤±)
    - [4.2 è¯­ä¹‰ç»„åˆæ€§é—®é¢˜](#42-è¯­ä¹‰ç»„åˆæ€§é—®é¢˜)
      - [4.2.1 ç»„åˆæ€§åŸåˆ™](#421-ç»„åˆæ€§åŸåˆ™)
      - [4.2.2 è¯­ä¹‰é€æ˜åº¦](#422-è¯­ä¹‰é€æ˜åº¦)
    - [4.3 æŒ‡ç§°è¯­ä¹‰é—®é¢˜](#43-æŒ‡ç§°è¯­ä¹‰é—®é¢˜)
      - [4.3.1 æŒ‡ç§°å‡½æ•°](#431-æŒ‡ç§°å‡½æ•°)
      - [4.3.2 çœŸå€¼æ¡ä»¶](#432-çœŸå€¼æ¡ä»¶)
  - [5. è‡ªæŒ‡èƒ½åŠ›çš„ç¼ºå¤±ä¸è¡¥æ•‘](#5-è‡ªæŒ‡èƒ½åŠ›çš„ç¼ºå¤±ä¸è¡¥æ•‘)
    - [5.1 å½“å‰LLMçš„è‡ªæŒ‡ç¼ºé™·](#51-å½“å‰llmçš„è‡ªæŒ‡ç¼ºé™·)
      - [5.1.1 æ— æ³•ä¿®æ”¹è‡ªèº«](#511-æ— æ³•ä¿®æ”¹è‡ªèº«)
      - [5.1.2 æ— æ³•åæ€è‡ªèº«](#512-æ— æ³•åæ€è‡ªèº«)
      - [5.1.3 æ— æ³•å‡çº§è‡ªèº«](#513-æ— æ³•å‡çº§è‡ªèº«)
    - [5.2 è‡ªæŒ‡èƒ½åŠ›çš„è¡¥æ•‘æ–¹æ¡ˆ](#52-è‡ªæŒ‡èƒ½åŠ›çš„è¡¥æ•‘æ–¹æ¡ˆ)
      - [5.2.1 å…ƒå­¦ä¹ æ¡†æ¶](#521-å…ƒå­¦ä¹ æ¡†æ¶)
      - [5.2.2 è‡ªæŒ‡æç¤º](#522-è‡ªæŒ‡æç¤º)
      - [5.2.3 é€’å½’æ¶æ„](#523-é€’å½’æ¶æ„)
    - [5.3 çœŸæ­£è‡ªæŒ‡çš„å®ç°è·¯å¾„](#53-çœŸæ­£è‡ªæŒ‡çš„å®ç°è·¯å¾„)
      - [5.3.1 æƒé‡å¯ä¿®æ”¹](#531-æƒé‡å¯ä¿®æ”¹)
      - [5.3.2 æ¶æ„å¯é‡æ„](#532-æ¶æ„å¯é‡æ„)
      - [5.3.3 ç›®æ ‡å¯è°ƒæ•´](#533-ç›®æ ‡å¯è°ƒæ•´)
  - [6. æœªæ¥LLMçš„å½¢å¼è¯­è¨€å‡çº§è·¯å¾„](#6-æœªæ¥llmçš„å½¢å¼è¯­è¨€å‡çº§è·¯å¾„)
    - [6.1 åŒé€šé“æ¶æ„](#61-åŒé€šé“æ¶æ„)
      - [6.1.1 è¿ç»­é€šé“](#611-è¿ç»­é€šé“)
      - [6.1.2 ç¦»æ•£é€šé“](#612-ç¦»æ•£é€šé“)
      - [6.1.3 é€šé“èåˆ](#613-é€šé“èåˆ)
    - [6.2 å¯è§£é‡Šè¯­æ³•](#62-å¯è§£é‡Šè¯­æ³•)
      - [6.2.1 æ˜¾å¼è¯­æ³•è§„åˆ™](#621-æ˜¾å¼è¯­æ³•è§„åˆ™)
      - [6.2.2 è¯­æ³•éªŒè¯](#622-è¯­æ³•éªŒè¯)
      - [6.2.3 è¯­æ³•å­¦ä¹ ](#623-è¯­æ³•å­¦ä¹ )
    - [6.3 å¤–éƒ¨è¯­ä¹‰æ¨¡å‹](#63-å¤–éƒ¨è¯­ä¹‰æ¨¡å‹)
      - [6.3.1 çŸ¥è¯†å›¾è°±é›†æˆ](#631-çŸ¥è¯†å›¾è°±é›†æˆ)
      - [6.3.2 é€»è¾‘æ¨ç†å¼•æ“](#632-é€»è¾‘æ¨ç†å¼•æ“)
      - [6.3.3 å¤šæ¨¡æ€è¯­ä¹‰](#633-å¤šæ¨¡æ€è¯­ä¹‰)
    - [6.4 çœŸæ­£è‡ªæŒ‡ç³»ç»Ÿ](#64-çœŸæ­£è‡ªæŒ‡ç³»ç»Ÿ)
      - [6.4.1 è‡ªæˆ‘ä¿®æ”¹èƒ½åŠ›](#641-è‡ªæˆ‘ä¿®æ”¹èƒ½åŠ›)
      - [6.4.2 è‡ªæˆ‘åæ€èƒ½åŠ›](#642-è‡ªæˆ‘åæ€èƒ½åŠ›)
      - [6.4.3 è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›](#643-è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›)
  - [ç»“è®º](#ç»“è®º)
  - [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)
    - [åŸºç¡€ç†è®ºæ–‡çŒ®](#åŸºç¡€ç†è®ºæ–‡çŒ®)
    - [æ ¸å¿ƒæ¨¡å‹æ–‡çŒ®](#æ ¸å¿ƒæ¨¡å‹æ–‡çŒ®)
    - [å½¢å¼è¯­è¨€åˆ†ææ–‡çŒ®](#å½¢å¼è¯­è¨€åˆ†ææ–‡çŒ®)
    - [æ‰¹è¯„ä¸åˆ†ææ–‡çŒ®](#æ‰¹è¯„ä¸åˆ†ææ–‡çŒ®)
    - [æ”¹è¿›æ–¹å‘æ–‡çŒ®](#æ”¹è¿›æ–¹å‘æ–‡çŒ®)
    - [æŠ€æœ¯å®ç°æ–‡çŒ®](#æŠ€æœ¯å®ç°æ–‡çŒ®)
    - [åº”ç”¨é¢†åŸŸæ–‡çŒ®](#åº”ç”¨é¢†åŸŸæ–‡çŒ®)
    - [ç›¸å…³æ¦‚å¿µé“¾æ¥](#ç›¸å…³æ¦‚å¿µé“¾æ¥)

## 1. LLMçš„å½¢å¼è¯­è¨€ç»“æ„åˆ†æ

### 1.1 å½“å‰LLMçš„"ä¼ªå½¢å¼è¯­è¨€"ç‰¹å¾

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå½“å‰äººå·¥æ™ºèƒ½çš„é‡è¦ä»£è¡¨ï¼Œå…¶å½¢å¼è¯­è¨€ç‰¹å¾å€¼å¾—æ·±å…¥åˆ†æã€‚æ ¹æ®[å¤§è¯­è¨€æ¨¡å‹](https://en.wikipedia.org/wiki/Large_language_model)çš„å®šä¹‰ï¼ŒLLMæ˜¯åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚ä»å½¢å¼è¯­è¨€è§†è§’çœ‹ï¼ŒLLMçš„ç‰¹å¾å¯ä»¥ç†è§£ä¸ºï¼š

#### 1.1.1 LLMçš„å½¢å¼è¯­è¨€ç†è®ºåŸºç¡€

æ ¹æ®[Transformer (Machine Learning Model)](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))ï¼ŒLLMå»ºç«‹åœ¨ä»¥ä¸‹åŸºç¡€ä¹‹ä¸Šï¼š

- **[ç¥ç»ç½‘ç»œ](https://en.wikipedia.org/wiki/Neural_network)**ï¼šæ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒçš„è®¡ç®—æ¨¡å‹
- **[æ·±åº¦å­¦ä¹ ](https://en.wikipedia.org/wiki/Deep_learning)**ï¼šå¤šå±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•
- **[è‡ªç„¶è¯­è¨€å¤„ç†](https://en.wikipedia.org/wiki/Natural_language_processing)**ï¼šè®¡ç®—æœºå¤„ç†äººç±»è¯­è¨€çš„æŠ€æœ¯
- **[æ³¨æ„åŠ›æœºåˆ¶](https://en.wikipedia.org/wiki/Attention_(machine_learning))**ï¼šæ¨¡å‹å…³æ³¨è¾“å…¥ä¸åŒéƒ¨åˆ†çš„èƒ½åŠ›

#### 1.1.2 LLMçš„å½¢å¼è¯­è¨€ç‰¹å¾

ä»å½¢å¼è¯­è¨€è§†è§’çœ‹ï¼ŒLLMå…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

- **å­—æ¯è¡¨ Î£**ï¼šè¯æ±‡è¡¨ã€æ ‡è®°ã€ç‰¹æ®Šç¬¦å·çš„é›†åˆ
- **è¯­æ³•é›† ğ’®**ï¼šéšå¼è¯­æ³•è§„åˆ™ã€ä¸Šä¸‹æ–‡ä¾èµ–ã€è¯­è¨€æ¨¡å¼
- **è¯­ä¹‰åŸŸ ğ’Ÿ**ï¼šè¿ç»­å‘é‡ç©ºé—´ã€è¯­ä¹‰è¡¨ç¤ºã€ä¸Šä¸‹æ–‡ä¿¡æ¯
- **æŒ‡ç§°å‡½æ•° âŸ¦âˆ’âŸ§**ï¼šæ–‡æœ¬åˆ°å‘é‡è¡¨ç¤ºçš„æ˜ å°„
- **å†…éƒ¨åŒ–ç®—å­ Î¹**ï¼šè®­ç»ƒè¿‡ç¨‹åˆ°æ¨¡å‹å‚æ•°çš„è½¬æ¢

#### 1.1.3 LLMçš„å½¢å¼åŒ–æ–¹æ³•

LLMé‡‡ç”¨å¤šç§å½¢å¼åŒ–æ–¹æ³•æ¥å¤„ç†è¯­è¨€ï¼š

- **[è¯åµŒå…¥](https://en.wikipedia.org/wiki/Word_embedding)**ï¼šå°†è¯æ±‡æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´
- **[ä½ç½®ç¼–ç ](https://en.wikipedia.org/wiki/Positional_encoding)**ï¼šè¡¨ç¤ºè¯æ±‡åœ¨åºåˆ—ä¸­çš„ä½ç½®
- **[æ³¨æ„åŠ›æœºåˆ¶](https://en.wikipedia.org/wiki/Attention_(machine_learning))**ï¼šè®¡ç®—è¯æ±‡é—´çš„ç›¸å…³æ€§
- **[å‰é¦ˆç½‘ç»œ](https://en.wikipedia.org/wiki/Feedforward_neural_network)**ï¼šå¤„ç†éçº¿æ€§å˜æ¢

#### 1.1.1 å­—æ¯è¡¨å±‚é¢

```text
ä¼ ç»Ÿå½¢å¼è¯­è¨€ï¼šÎ£ = {ç¦»æ•£ç¬¦å·}
LLMå­—æ¯è¡¨ï¼šÎ£_LLM = {token_id, embedding_vector}
```

**é—®é¢˜åˆ†æ**ï¼š

- ç¬¦å·è¢«å‘é‡åŒ–ï¼Œå¤±å»ç¦»æ•£è¾¹ç•Œ
- è¯­ä¹‰ä¿¡æ¯æ··å…¥ç¬¦å·è¡¨ç¤º
- æ— æ³•è¿›è¡Œçº¯è¯­æ³•åˆ†æ

#### 1.1.2 è¯­æ³•å±‚é¢

```text
ä¼ ç»Ÿè¯­æ³•ï¼šG = (V, T, P, S)
LLMè¯­æ³•ï¼šG_LLM = (attention_weights, layer_norm, feed_forward)
```

**é—®é¢˜åˆ†æ**ï¼š

- æ— æ˜¾å¼äº§ç”Ÿå¼è§„åˆ™
- è¯­æ³•ä¿¡æ¯ç¼–ç åœ¨æƒé‡ä¸­
- ä¸å¯è§£é‡Šçš„è¯­æ³•ç»“æ„

#### 1.1.3 è¯­ä¹‰å±‚é¢

```text
ä¼ ç»Ÿè¯­ä¹‰ï¼šâŸ¦sâŸ§ âˆˆ ğ’Ÿ
LLMè¯­ä¹‰ï¼šâŸ¦sâŸ§_LLM = softmax(WÂ·h)
```

**é—®é¢˜åˆ†æ**ï¼š

- æ— å¤–éƒ¨è¯­ä¹‰æ¨¡å‹
- è¯­ä¹‰å®Œå…¨å†…éƒ¨åŒ–
- ç¼ºä¹çœŸå€¼æ¡ä»¶

### 1.2 LLMçš„"è¿ç»­-ç¦»æ•£"æ··åˆç‰¹å¾

#### 1.2.1 è¿ç»­è¡¨ç¤º

```text
token_embedding: â„^d
position_embedding: â„^d
context_embedding: â„^d
```

#### 1.2.2 ç¦»æ•£è¾“å‡º

```text
vocab_logits: â„^|V|
sampling: discrete_distribution
```

#### 1.2.3 æ··åˆå¤„ç†

```text
continuous_processing â†’ discrete_output
```

## 2. Transformeræ¶æ„çš„å½¢å¼è¯­è¨€è§£è¯»

### 2.1 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

#### 2.1.1 æ³¨æ„åŠ›æƒé‡çŸ©é˜µ

```text
A = softmax(QK^T/âˆšd_k)
```

**å½¢å¼è¯­è¨€è§†è§’**ï¼š

- Q, K, V = æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µ
- æ³¨æ„åŠ›æƒé‡ = è¯­æ³•ä¾èµ–å…³ç³»
- å¤šå¤´ = å¤šç§è¯­æ³•è§†è§’

#### 2.1.2 ä½ç½®ç¼–ç 

```text
PE(pos, 2i) = sin(pos/10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

**å½¢å¼è¯­è¨€è§†è§’**ï¼š

- ä½ç½®ä¿¡æ¯ = è¯­æ³•ä½ç½®
- æ­£å¼¦ç¼–ç  = ç›¸å¯¹ä½ç½®å…³ç³»
- ç»å¯¹ä½ç½® = è¯­æ³•æ ‘èŠ‚ç‚¹ä½ç½®

### 2.2 å‰é¦ˆç¥ç»ç½‘ç»œ

#### 2.2.1 ä¸¤å±‚MLPç»“æ„

```text
FFN(x) = max(0, xW1 + b1)W2 + b2
```

**å½¢å¼è¯­è¨€è§†è§’**ï¼š

- ç¬¬ä¸€å±‚ = ç‰¹å¾æå–
- ç¬¬äºŒå±‚ = ç‰¹å¾ç»„åˆ
- ReLU = éçº¿æ€§æ¿€æ´»

#### 2.2.2 æ®‹å·®è¿æ¥

```text
x + sublayer(x)
```

**å½¢å¼è¯­è¨€è§†è§’**ï¼š

- æ®‹å·® = ä¿¡æ¯ä¿æŒ
- è·³è·ƒè¿æ¥ = é•¿è·ç¦»ä¾èµ–
- æ¢¯åº¦æµ = è®­ç»ƒç¨³å®šæ€§

### 2.3 å±‚å½’ä¸€åŒ–

#### 2.3.1 LayerNormå…¬å¼

```text
LayerNorm(x) = Î³ * (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

**å½¢å¼è¯­è¨€è§†è§’**ï¼š

- å½’ä¸€åŒ– = ç‰¹å¾æ ‡å‡†åŒ–
- å¯å­¦ä¹ å‚æ•° = è‡ªé€‚åº”è°ƒæ•´
- ç¨³å®šè®­ç»ƒ = è¯­æ³•å­¦ä¹ 

## 3. æ³¨æ„åŠ›æœºåˆ¶ä¸è¯­æ³•è§„åˆ™

### 3.1 æ³¨æ„åŠ›æƒé‡çš„è¯­æ³•è§£é‡Š

#### 3.1.1 è¯­æ³•ä¾èµ–å…³ç³»

```text
attention_weight[i,j] = P(dependency(i,j))
```

**è§£é‡Š**ï¼š

- é«˜æƒé‡ = å¼ºè¯­æ³•ä¾èµ–
- ä½æƒé‡ = å¼±è¯­æ³•ä¾èµ–
- æƒé‡åˆ†å¸ƒ = è¯­æ³•æ ‘ç»“æ„

#### 3.1.2 å¤šå¤´æ³¨æ„åŠ›çš„è¯­æ³•åˆ†å·¥

```text
head_1: ä¸»è°“å…³ç³»
head_2: ä¿®é¥°å…³ç³»  
head_3: å¹¶åˆ—å…³ç³»
head_4: ä»å¥å…³ç³»
```

### 3.2 è¯­æ³•è§„åˆ™çš„éšå¼å­¦ä¹ 

#### 3.2.1 è§„åˆ™ç¼–ç 

```text
è¯­æ³•è§„åˆ™ â†’ æ³¨æ„åŠ›æ¨¡å¼
äº§ç”Ÿå¼ â†’ æƒé‡åˆ†å¸ƒ
```

#### 3.2.2 è§„åˆ™æå–

```text
æ³¨æ„åŠ›æƒé‡ â†’ è¯­æ³•è§„åˆ™
æƒé‡èšç±» â†’ è§„åˆ™å½’çº³
```

### 3.3 è¯­æ³•ç»“æ„çš„å¯è§†åŒ–

#### 3.3.1 æ³¨æ„åŠ›çƒ­å›¾

```text
å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ
è¯†åˆ«è¯­æ³•ä¾èµ–å…³ç³»
```

#### 3.3.2 è¯­æ³•æ ‘é‡æ„

```text
ä»æ³¨æ„åŠ›æƒé‡é‡æ„è¯­æ³•æ ‘
éªŒè¯è¯­æ³•ç»“æ„æ­£ç¡®æ€§
```

## 4. LLMçš„è¯­ä¹‰æ¨¡å‹ç¼ºé™·

### 4.1 ç¼ºä¹å¤–éƒ¨è¯­ä¹‰æ¨¡å‹

#### 4.1.1 å†…éƒ¨è¯­ä¹‰vså¤–éƒ¨è¯­ä¹‰

```text
LLMè¯­ä¹‰ï¼šå†…éƒ¨è¡¨ç¤ºç©ºé—´
å½¢å¼è¯­ä¹‰ï¼šå¤–éƒ¨ä¸–ç•Œæ¨¡å‹
```

#### 4.1.2 çœŸå€¼æ¡ä»¶ç¼ºå¤±

```text
LLMï¼šP(output|input)
å½¢å¼è¯­ä¹‰ï¼šâŸ¦sâŸ§ = truth_value
```

### 4.2 è¯­ä¹‰ç»„åˆæ€§é—®é¢˜

#### 4.2.1 ç»„åˆæ€§åŸåˆ™

```text
ä¼ ç»Ÿï¼šâŸ¦s1 s2âŸ§ = âŸ¦s1âŸ§ âˆ˜ âŸ¦s2âŸ§
LLMï¼šâŸ¦s1 s2âŸ§ â‰  âŸ¦s1âŸ§ âˆ˜ âŸ¦s2âŸ§
```

#### 4.2.2 è¯­ä¹‰é€æ˜åº¦

```text
ä¼ ç»Ÿï¼šè¯­ä¹‰å¯åˆ†è§£
LLMï¼šè¯­ä¹‰é»‘ç›’
```

### 4.3 æŒ‡ç§°è¯­ä¹‰é—®é¢˜

#### 4.3.1 æŒ‡ç§°å‡½æ•°

```text
ä¼ ç»Ÿï¼šâŸ¦"çŒ«"âŸ§ = çŒ«çš„é›†åˆ
LLMï¼šâŸ¦"çŒ«"âŸ§ = embedding_vector
```

#### 4.3.2 çœŸå€¼æ¡ä»¶

```text
ä¼ ç»Ÿï¼šâŸ¦"é›ªæ˜¯ç™½çš„"âŸ§ = True/False
LLMï¼šâŸ¦"é›ªæ˜¯ç™½çš„"âŸ§ = probability
```

## 5. è‡ªæŒ‡èƒ½åŠ›çš„ç¼ºå¤±ä¸è¡¥æ•‘

### 5.1 å½“å‰LLMçš„è‡ªæŒ‡ç¼ºé™·

#### 5.1.1 æ— æ³•ä¿®æ”¹è‡ªèº«

```text
âˆ‚Î¸/âˆ‚(quote Î¸) â‰¡ 0
```

#### 5.1.2 æ— æ³•åæ€è‡ªèº«

```text
æ— æ³•åˆ†æè‡ªå·±çš„æ¨ç†è¿‡ç¨‹
æ— æ³•è¯„ä¼°è‡ªå·±çš„è¾“å‡ºè´¨é‡
```

#### 5.1.3 æ— æ³•å‡çº§è‡ªèº«

```text
æƒé‡å›ºå®šï¼Œæ— æ³•è‡ªæˆ‘æ”¹è¿›
æ¶æ„å›ºå®šï¼Œæ— æ³•è‡ªæˆ‘é‡æ„
```

### 5.2 è‡ªæŒ‡èƒ½åŠ›çš„è¡¥æ•‘æ–¹æ¡ˆ

#### 5.2.1 å…ƒå­¦ä¹ æ¡†æ¶

```text
MAML: Model-Agnostic Meta-Learning
Reptile: å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
```

#### 5.2.2 è‡ªæŒ‡æç¤º

```text
"è¯·åˆ†æä½ è‡ªå·±çš„å›ç­”"
"è¯·è¯„ä¼°ä½ è‡ªå·±çš„æ¨ç†"
```

#### 5.2.3 é€’å½’æ¶æ„

```text
æ¨¡å‹è°ƒç”¨è‡ªèº«
è‡ªæˆ‘å¯¹è¯æœºåˆ¶
```

### 5.3 çœŸæ­£è‡ªæŒ‡çš„å®ç°è·¯å¾„

#### 5.3.1 æƒé‡å¯ä¿®æ”¹

```text
å…è®¸æ¨¡å‹ä¿®æ”¹è‡ªå·±çš„å‚æ•°
å®ç°çœŸæ­£çš„è‡ªæˆ‘æ”¹è¿›
```

#### 5.3.2 æ¶æ„å¯é‡æ„

```text
å…è®¸æ¨¡å‹æ”¹å˜è‡ªå·±çš„ç»“æ„
å®ç°çœŸæ­£çš„è‡ªæˆ‘è¿›åŒ–
```

#### 5.3.3 ç›®æ ‡å¯è°ƒæ•´

```text
å…è®¸æ¨¡å‹è°ƒæ•´è‡ªå·±çš„ç›®æ ‡
å®ç°çœŸæ­£çš„è‡ªæˆ‘å¯¼å‘
```

## 6. æœªæ¥LLMçš„å½¢å¼è¯­è¨€å‡çº§è·¯å¾„

### 6.1 åŒé€šé“æ¶æ„

#### 6.1.1 è¿ç»­é€šé“

```text
ä¿æŒç°æœ‰çš„è¿ç»­è¡¨ç¤º
å¤„ç†æ¨¡ç³Šè¯­ä¹‰ä¿¡æ¯
```

#### 6.1.2 ç¦»æ•£é€šé“

```text
æ–°å¢ç¦»æ•£ç¬¦å·å¤„ç†
å¤„ç†ç²¾ç¡®è¯­æ³•ä¿¡æ¯
```

#### 6.1.3 é€šé“èåˆ

```text
è¿ç»­ â†” ç¦»æ•£ è½¬æ¢
ä¿æŒä¸¤ç§è¡¨ç¤ºçš„ä¼˜åŠ¿
```

### 6.2 å¯è§£é‡Šè¯­æ³•

#### 6.2.1 æ˜¾å¼è¯­æ³•è§„åˆ™

```text
ä»æ³¨æ„åŠ›æƒé‡æå–è§„åˆ™
æ„å»ºå¯è§£é‡Šçš„è¯­æ³•æ ‘
```

#### 6.2.2 è¯­æ³•éªŒè¯

```text
éªŒè¯è¯­æ³•è§„åˆ™çš„æ­£ç¡®æ€§
ç¡®ä¿è¯­æ³•ç»“æ„çš„ä¸€è‡´æ€§
```

#### 6.2.3 è¯­æ³•å­¦ä¹ 

```text
æŒç»­å­¦ä¹ æ–°çš„è¯­æ³•è§„åˆ™
é€‚åº”æ–°çš„è¯­è¨€ç»“æ„
```

### 6.3 å¤–éƒ¨è¯­ä¹‰æ¨¡å‹

#### 6.3.1 çŸ¥è¯†å›¾è°±é›†æˆ

```text
è¿æ¥å¤–éƒ¨çŸ¥è¯†å›¾è°±
æä¾›çœŸå®ä¸–ç•Œè¯­ä¹‰
```

#### 6.3.2 é€»è¾‘æ¨ç†å¼•æ“

```text
é›†æˆé€»è¾‘æ¨ç†ç³»ç»Ÿ
æä¾›çœŸå€¼æ¡ä»¶è¯­ä¹‰
```

#### 6.3.3 å¤šæ¨¡æ€è¯­ä¹‰

```text
æ•´åˆè§†è§‰ã€å¬è§‰ç­‰æ¨¡æ€
æ„å»ºå®Œæ•´è¯­ä¹‰æ¨¡å‹
```

### 6.4 çœŸæ­£è‡ªæŒ‡ç³»ç»Ÿ

#### 6.4.1 è‡ªæˆ‘ä¿®æ”¹èƒ½åŠ›

```text
å…è®¸æ¨¡å‹ä¿®æ”¹è‡ªå·±çš„å‚æ•°
å®ç°çœŸæ­£çš„è‡ªæˆ‘æ”¹è¿›
```

#### 6.4.2 è‡ªæˆ‘åæ€èƒ½åŠ›

```text
åˆ†æè‡ªå·±çš„æ¨ç†è¿‡ç¨‹
è¯„ä¼°è‡ªå·±çš„è¾“å‡ºè´¨é‡
```

#### 6.4.3 è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›

```text
æ”¹å˜è‡ªå·±çš„æ¶æ„
è°ƒæ•´è‡ªå·±çš„ç›®æ ‡
```

## ç»“è®º

å½“å‰LLMè™½ç„¶åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä»å½¢å¼è¯­è¨€è§†è§’çœ‹ï¼Œä»å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·ï¼šç¼ºä¹çœŸæ­£çš„è¯­æ³•ç»“æ„ã€è¯­ä¹‰æ¨¡å‹å’Œè‡ªæŒ‡èƒ½åŠ›ã€‚æœªæ¥çš„LLMå¿…é¡»å®ç°ï¼š

1. **åŒé€šé“æ¶æ„**ï¼šè¿ç»­+ç¦»æ•£å¤„ç†
2. **å¯è§£é‡Šè¯­æ³•**ï¼šæ˜¾å¼è§„åˆ™æå–
3. **å¤–éƒ¨è¯­ä¹‰**ï¼šçœŸå®ä¸–ç•Œæ¨¡å‹
4. **çœŸæ­£è‡ªæŒ‡**ï¼šè‡ªæˆ‘ä¿®æ”¹èƒ½åŠ›
5. **æŒç»­è¿›åŒ–**ï¼šæ¶æ„è‡ªé€‚åº”

åªæœ‰è¿™æ ·ï¼ŒLLMæ‰èƒ½çœŸæ­£å®ç°å½¢å¼è¯­è¨€-è¯­ä¹‰æ¨¡å‹çš„å®Œæ•´åŠŸèƒ½ï¼Œæˆä¸ºçœŸæ­£æ™ºèƒ½çš„ç³»ç»Ÿã€‚

## å‚è€ƒæ–‡çŒ®

### åŸºç¡€ç†è®ºæ–‡çŒ®

1. **å¤§è¯­è¨€æ¨¡å‹**
   - [Large Language Model - Wikipedia](https://en.wikipedia.org/wiki/Large_language_model)
   - [Transformer (Machine Learning Model) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
   - [GPT - Wikipedia](https://en.wikipedia.org/wiki/GPT)
   - [BERT (Language Model) - Wikipedia](https://en.wikipedia.org/wiki/BERT_(language_model))

2. **æ·±åº¦å­¦ä¹ åŸºç¡€**
   - [Deep Learning - Wikipedia](https://en.wikipedia.org/wiki/Deep_learning)
   - [Neural Network - Wikipedia](https://en.wikipedia.org/wiki/Neural_network)
   - [Natural Language Processing - Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)
   - [Machine Learning - Wikipedia](https://en.wikipedia.org/wiki/Machine_learning)

3. **æ³¨æ„åŠ›æœºåˆ¶**
   - [Attention (Machine Learning) - Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning))
   - [Self-attention - Wikipedia](https://en.wikipedia.org/wiki/Self-attention)
   - [Multi-head Attention - Wikipedia](https://en.wikipedia.org/wiki/Multi-head_attention)
   - [Positional Encoding - Wikipedia](https://en.wikipedia.org/wiki/Positional_encoding)

### æ ¸å¿ƒæ¨¡å‹æ–‡çŒ®

1. **Transformeræ¶æ„**
   - Vaswani, A., et al. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*, 30.
   - [Transformer (Machine Learning Model) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
   - [Encoder-decoder Model - Wikipedia](https://en.wikipedia.org/wiki/Encoder-decoder_model)

2. **BERTæ¨¡å‹**
   - Devlin, J., et al. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.
   - [BERT (Language Model) - Wikipedia](https://en.wikipedia.org/wiki/BERT_(language_model))
   - [Bidirectional Encoder Representations from Transformers - Wikipedia](https://en.wikipedia.org/wiki/Bidirectional_Encoder_Representations_from_Transformers)

3. **GPTæ¨¡å‹**
   - Brown, T., et al. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, 33, 1877-1901.
   - [GPT - Wikipedia](https://en.wikipedia.org/wiki/GPT)
   - [Generative Pre-trained Transformer - Wikipedia](https://en.wikipedia.org/wiki/Generative_Pre-trained_Transformer)

### å½¢å¼è¯­è¨€åˆ†ææ–‡çŒ®

1. **è¯­è¨€æ¨¡å‹ä¸å½¢å¼è¯­è¨€**
   - [Formal Language - Wikipedia](https://en.wikipedia.org/wiki/Formal_language)
   - [Formal Grammar - Wikipedia](https://en.wikipedia.org/wiki/Formal_grammar)
   - [Chomsky Hierarchy - Wikipedia](https://en.wikipedia.org/wiki/Chomsky_hierarchy)
   - [Automata Theory - Wikipedia](https://en.wikipedia.org/wiki/Automata_theory)

2. **è¯­ä¹‰ç†è®º**
   - [Semantics - Wikipedia](https://en.wikipedia.org/wiki/Semantics)
   - [Formal Semantics (Linguistics) - Wikipedia](https://en.wikipedia.org/wiki/Formal_semantics_(linguistics))
   - [Compositionality - Wikipedia](https://en.wikipedia.org/wiki/Compositionality)
   - [Truth Condition - Wikipedia](https://en.wikipedia.org/wiki/Truth_condition)

3. **è¯åµŒå…¥ä¸è¡¨ç¤ºå­¦ä¹ **
   - [Word Embedding - Wikipedia](https://en.wikipedia.org/wiki/Word_embedding)
   - [Representation Learning - Wikipedia](https://en.wikipedia.org/wiki/Representation_learning)
   - [Distributed Representation - Wikipedia](https://en.wikipedia.org/wiki/Distributed_representation)
   - [Vector Space Model - Wikipedia](https://en.wikipedia.org/wiki/Vector_space_model)

### æ‰¹è¯„ä¸åˆ†ææ–‡çŒ®

1. **LLMå±€é™æ€§**
    - Bender, E. M., & Koller, A. (2020). Climbing towards NLU: On meaning, form, and understanding in the age of data. *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, 5185-5198.
    - [Stochastic Parrot - Wikipedia](https://en.wikipedia.org/wiki/Stochastic_parrot)
    - [Hallucination (Artificial Intelligence) - Wikipedia](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))

2. **ä¹”å§†æ–¯åŸºæ‰¹è¯„**
    - Chomsky, N. (2023). The false promise of ChatGPT. *The New York Times*, March 8, 2023.
    - [Noam Chomsky - Wikipedia](https://en.wikipedia.org/wiki/Noam_Chomsky)
    - [Generative Grammar - Wikipedia](https://en.wikipedia.org/wiki/Generative_grammar)

### æ”¹è¿›æ–¹å‘æ–‡çŒ®

1. **ç¥ç»ç¬¦å·èåˆ**
    - [Neuro-symbolic AI - Wikipedia](https://en.wikipedia.org/wiki/Neuro-symbolic_AI)
    - [Symbolic AI - Wikipedia](https://en.wikipedia.org/wiki/Symbolic_AI)
    - [Hybrid AI - Wikipedia](https://en.wikipedia.org/wiki/Hybrid_AI)
    - [Explainable AI - Wikipedia](https://en.wikipedia.org/wiki/Explainable_AI)

2. **å¯è§£é‡ŠAI**
    - [Explainable AI - Wikipedia](https://en.wikipedia.org/wiki/Explainable_AI)
    - [Interpretability - Wikipedia](https://en.wikipedia.org/wiki/Interpretability)
    - [Model Interpretability - Wikipedia](https://en.wikipedia.org/wiki/Model_interpretability)
    - [Attention Visualization - Wikipedia](https://en.wikipedia.org/wiki/Attention_visualization)

3. **è‡ªæŒ‡ç³»ç»Ÿ**
    - [Self-reference - Wikipedia](https://en.wikipedia.org/wiki/Self-reference)
    - [Reflexivity - Wikipedia](https://en.wikipedia.org/wiki/Reflexivity)
    - [Self-modifying Code - Wikipedia](https://en.wikipedia.org/wiki/Self-modifying_code)
    - [Meta-learning - Wikipedia](https://en.wikipedia.org/wiki/Meta-learning)

### æŠ€æœ¯å®ç°æ–‡çŒ®

1. **æ¶æ„æ”¹è¿›**
    - [Architecture (Machine Learning) - Wikipedia](https://en.wikipedia.org/wiki/Architecture_(machine_learning))
    - [Neural Architecture Search - Wikipedia](https://en.wikipedia.org/wiki/Neural_architecture_search)
    - [Model Compression - Wikipedia](https://en.wikipedia.org/wiki/Model_compression)
    - [Knowledge Distillation - Wikipedia](https://en.wikipedia.org/wiki/Knowledge_distillation)

2. **è®­ç»ƒæ–¹æ³•**
    - [Pre-training - Wikipedia](https://en.wikipedia.org/wiki/Pre-training)
    - [Fine-tuning - Wikipedia](https://en.wikipedia.org/wiki/Fine-tuning)
    - [Transfer Learning - Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)
    - [Few-shot Learning - Wikipedia](https://en.wikipedia.org/wiki/Few-shot_learning)

3. **è¯„ä¼°æ–¹æ³•**
    - [Language Model Evaluation - Wikipedia](https://en.wikipedia.org/wiki/Language_model_evaluation)
    - [Benchmark (Computing) - Wikipedia](https://en.wikipedia.org/wiki/Benchmark_(computing))
    - [GLUE Benchmark - Wikipedia](https://en.wikipedia.org/wiki/GLUE_benchmark)
    - [SuperGLUE - Wikipedia](https://en.wikipedia.org/wiki/SuperGLUE)

### åº”ç”¨é¢†åŸŸæ–‡çŒ®

1. **è‡ªç„¶è¯­è¨€ç†è§£**
    - [Natural Language Understanding - Wikipedia](https://en.wikipedia.org/wiki/Natural_language_understanding)
    - [Question Answering - Wikipedia](https://en.wikipedia.org/wiki/Question_answering)
    - [Text Summarization - Wikipedia](https://en.wikipedia.org/wiki/Text_summarization)
    - [Machine Translation - Wikipedia](https://en.wikipedia.org/wiki/Machine_translation)

2. **ä»£ç ç”Ÿæˆ**
    - [Code Generation - Wikipedia](https://en.wikipedia.org/wiki/Code_generation)
    - [Program Synthesis - Wikipedia](https://en.wikipedia.org/wiki/Program_synthesis)
    - [Automatic Programming - Wikipedia](https://en.wikipedia.org/wiki/Automatic_programming)
    - [Code Completion - Wikipedia](https://en.wikipedia.org/wiki/Code_completion)

3. **å¯¹è¯ç³»ç»Ÿ**
    - [Chatbot - Wikipedia](https://en.wikipedia.org/wiki/Chatbot)
    - [Conversational AI - Wikipedia](https://en.wikipedia.org/wiki/Conversational_AI)
    - [Dialogue System - Wikipedia](https://en.wikipedia.org/wiki/Dialogue_system)
    - [Virtual Assistant - Wikipedia](https://en.wikipedia.org/wiki/Virtual_assistant)

### ç›¸å…³æ¦‚å¿µé“¾æ¥

- [Language Model - Wikipedia](https://en.wikipedia.org/wiki/Language_model)
- [Statistical Language Model - Wikipedia](https://en.wikipedia.org/wiki/Statistical_language_model)
- [Neural Language Model - Wikipedia](https://en.wikipedia.org/wiki/Neural_language_model)
- [Contextual Embedding - Wikipedia](https://en.wikipedia.org/wiki/Contextual_embedding)
- [Pre-trained Model - Wikipedia](https://en.wikipedia.org/wiki/Pre-trained_model)
- [Foundation Model - Wikipedia](https://en.wikipedia.org/wiki/Foundation_model)
- [Multimodal Learning - Wikipedia](https://en.wikipedia.org/wiki/Multimodal_learning)
- [Zero-shot Learning - Wikipedia](https://en.wikipedia.org/wiki/Zero-shot_learning)
