#!/usr/bin/env python3
"""
æ–‡æ¡£ç»“æ„å’Œç›®å½•åºå·åˆ†æå·¥å…·
åˆ†ææ‰€æœ‰Markdownæ–‡æ¡£çš„ï¼š
1. ç›®å½•ç»“æ„ï¼ˆTOCï¼‰
2. æ ‡é¢˜å±‚çº§å’Œåºå·è§„èŒƒ
3. å†…å®¹é•¿åº¦
4. å†…å®¹è´¨é‡æŒ‡æ ‡
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
import json

class DocumentAnalyzer:
    def __init__(self, root_dir: str):
        self.root_dir = Path(root_dir)
        self.results = []
        
    def analyze_file(self, file_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡æ¡£"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        
        # åŸºæœ¬ä¿¡æ¯
        info = {
            'path': str(file_path.relative_to(self.root_dir)),
            'total_lines': len(lines),
            'has_toc': False,
            'headings': [],
            'heading_issues': [],
            'content_length': len(content),
            'substantive_lines': 0,
        }
        
        # æ£€æµ‹TOC
        toc_pattern = re.compile(r'##\s*ç›®å½•\s*\|\s*Table of Contents', re.IGNORECASE)
        for line in lines:
            if toc_pattern.search(line):
                info['has_toc'] = True
                break
        
        # åˆ†ææ ‡é¢˜
        heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$')
        for i, line in enumerate(lines, 1):
            match = heading_pattern.match(line)
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()
                
                # æ£€æµ‹åºå·
                numbering = None
                # åŒ¹é… "1.", "1.1", "1.1.1" ç­‰æ ¼å¼
                num_match = re.match(r'^(\d+(?:\.\d+)*\.?)\s+(.+)$', title)
                if num_match:
                    numbering = num_match.group(1)
                    title_text = num_match.group(2)
                else:
                    title_text = title
                
                info['headings'].append({
                    'line': i,
                    'level': level,
                    'numbering': numbering,
                    'title': title_text,
                    'raw': title
                })
        
        # æ£€æµ‹æ ‡é¢˜å±‚çº§é—®é¢˜
        prev_level = 0
        for h in info['headings']:
            if h['level'] > prev_level + 1 and prev_level > 0:
                info['heading_issues'].append({
                    'line': h['line'],
                    'issue': 'level_jump',
                    'detail': f"ä» {prev_level} çº§è·³åˆ° {h['level']} çº§"
                })
            prev_level = h['level']
        
        # ç»Ÿè®¡å®è´¨å†…å®¹è¡Œæ•°ï¼ˆæ’é™¤ç©ºè¡Œã€æ ‡é¢˜ã€åˆ†éš”çº¿ã€å…ƒæ•°æ®ï¼‰
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and line != '---' and not line.startswith('**') and len(line) > 10:
                info['substantive_lines'] += 1
        
        # è¯„ä¼°å†…å®¹å……å®åº¦
        if info['substantive_lines'] < 200:
            info['content_status'] = 'insufficient'
        elif info['substantive_lines'] < 400:
            info['content_status'] = 'moderate'
        else:
            info['content_status'] = 'sufficient'
        
        return info
    
    def analyze_directory(self, perspective_dir: str) -> List[Dict]:
        """åˆ†ææ•´ä¸ªè§†è§’ç›®å½•"""
        target_dir = self.root_dir / perspective_dir
        results = []
        
        for md_file in sorted(target_dir.rglob('*.md')):
            # è·³è¿‡å…ƒæ–‡æ¡£
            if md_file.name in ['README.md', 'GLOSSARY.md', 'FAQ.md', 'LEARNING_PATHS.md']:
                continue
            
            try:
                info = self.analyze_file(md_file)
                results.append(info)
            except Exception as e:
                print(f"Error analyzing {md_file}: {e}")
        
        return results
    
    def check_numbering_consistency(self, headings: List[Dict]) -> List[str]:
        """æ£€æŸ¥åºå·ä¸€è‡´æ€§"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦æ··ç”¨æœ‰åºå·å’Œæ— åºå·
        has_numbering = any(h['numbering'] for h in headings if h['level'] == 2)
        all_numbered = all(h['numbering'] for h in headings if h['level'] == 2)
        
        if has_numbering and not all_numbered:
            issues.append("æ··ç”¨æœ‰åºå·å’Œæ— åºå·çš„äºŒçº§æ ‡é¢˜")
        
        # æ£€æŸ¥åºå·è¿ç»­æ€§
        level2_numbers = []
        for h in headings:
            if h['level'] == 2 and h['numbering']:
                try:
                    num = int(h['numbering'].rstrip('.').split('.')[0])
                    level2_numbers.append(num)
                except:
                    pass
        
        if level2_numbers:
            for i in range(len(level2_numbers) - 1):
                if level2_numbers[i+1] != level2_numbers[i] + 1:
                    issues.append(f"åºå·ä¸è¿ç»­: {level2_numbers[i]} -> {level2_numbers[i+1]}")
        
        return issues
    
    def generate_report(self, results: List[Dict], output_file: str):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# æ–‡æ¡£ç»“æ„å’Œåºå·åˆ†ææŠ¥å‘Š\n")
        report.append(f"**åˆ†ææ—¶é—´**: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"**åˆ†ææ–‡æ¡£æ•°**: {len(results)}\n\n")
        report.append("---\n\n")
        
        # ç»Ÿè®¡
        no_toc = [r for r in results if not r['has_toc']]
        insufficient = [r for r in results if r['content_status'] == 'insufficient']
        heading_issues = [r for r in results if r['heading_issues']]
        
        report.append("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
        report.append(f"- **æ€»æ–‡æ¡£æ•°**: {len(results)}\n")
        report.append(f"- **ç¼ºå°‘TOC**: {len(no_toc)} ä¸ª\n")
        report.append(f"- **å†…å®¹ä¸è¶³**: {len(insufficient)} ä¸ª\n")
        report.append(f"- **æ ‡é¢˜é—®é¢˜**: {len(heading_issues)} ä¸ª\n\n")
        
        # ç¼ºå°‘TOCçš„æ–‡æ¡£
        if no_toc:
            report.append("## âŒ ç¼ºå°‘ç›®å½•çš„æ–‡æ¡£\n\n")
            for r in no_toc:
                report.append(f"- `{r['path']}` ({r['total_lines']} è¡Œ)\n")
            report.append("\n")
        
        # å†…å®¹ä¸è¶³çš„æ–‡æ¡£
        if insufficient:
            report.append("## âš ï¸ å†…å®¹ä¸è¶³çš„æ–‡æ¡£\n\n")
            for r in insufficient:
                report.append(f"- `{r['path']}` (å®è´¨å†…å®¹: {r['substantive_lines']} è¡Œ)\n")
            report.append("\n")
        
        # æ ‡é¢˜ç»“æ„é—®é¢˜
        if heading_issues:
            report.append("## ğŸ” æ ‡é¢˜ç»“æ„é—®é¢˜\n\n")
            for r in heading_issues:
                report.append(f"### {r['path']}\n\n")
                for issue in r['heading_issues']:
                    report.append(f"- è¡Œ {issue['line']}: {issue['detail']}\n")
                report.append("\n")
        
        # è¯¦ç»†æ ‡é¢˜åˆ†æ
        report.append("## ğŸ“‹ æ ‡é¢˜ç»“æ„è¯¦æƒ…\n\n")
        for r in results[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªä½œä¸ºç¤ºä¾‹
            report.append(f"### {r['path']}\n\n")
            report.append(f"- æ€»è¡Œæ•°: {r['total_lines']}\n")
            report.append(f"- å®è´¨å†…å®¹: {r['substantive_lines']} è¡Œ\n")
            report.append(f"- æœ‰TOC: {'âœ…' if r['has_toc'] else 'âŒ'}\n")
            report.append(f"- æ ‡é¢˜æ•°: {len(r['headings'])}\n\n")
            
            if r['headings']:
                report.append("æ ‡é¢˜å±‚çº§:\n\n")
                for h in r['headings'][:15]:  # åªæ˜¾ç¤ºå‰15ä¸ªæ ‡é¢˜
                    indent = "  " * (h['level'] - 1)
                    numbering = h['numbering'] or "æ— åºå·"
                    report.append(f"{indent}- L{h['level']}: {numbering} {h['title']}\n")
                report.append("\n")
        
        # å†™å…¥æ–‡ä»¶
        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(report)
        
        print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        
        # åŒæ—¶ç”ŸæˆJSONæ•°æ®
        json_path = output_path.with_suffix('.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"JSONæ•°æ®å·²ç”Ÿæˆ: {json_path}")

def main():
    analyzer = DocumentAnalyzer('.')
    
    perspectives = [
        'AI_model_Perspective',
        'FormalLanguage_Perspective',
        'Information_Theory_Perspective'
    ]
    
    all_results = []
    
    for perspective in perspectives:
        print(f"æ­£åœ¨åˆ†æ {perspective}...")
        results = analyzer.analyze_directory(perspective)
        all_results.extend(results)
        print(f"  å®Œæˆ: {len(results)} ä¸ªæ–‡æ¡£")
    
    print(f"\næ€»è®¡åˆ†æ: {len(all_results)} ä¸ªæ–‡æ¡£")
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_report(all_results, 'DOCUMENT_STRUCTURE_ANALYSIS_REPORT.md')

if __name__ == '__main__':
    main()

