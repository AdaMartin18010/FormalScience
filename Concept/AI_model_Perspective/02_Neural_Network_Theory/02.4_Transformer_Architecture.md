# Transformer 架构与注意力机制 | Transformer Architecture and Attention Mechanism

## 概述 | Overview

Transformer架构（Vaswani et al., 2017）彻底改变了深度学习，成为现代大语言模型的基础。本文档深入分析其架构、注意力机制、理论性质及计算能力。

## 1. 历史背景与动机 | Historical Background and Motivation

### 1.1 RNN 的局限性

**顺序依赖**：

- 必须按顺序处理序列
- 无法并行化训练
- 训练速度慢

**长程依赖问题**：

- 梯度消失/爆炸
- 难以捕捉远距离依赖
- LSTM/GRU 部分缓解但未根本解决

**计算效率**：

```text
RNN 时间复杂度：O(n) 顺序步骤
无法利用现代GPU并行计算能力
```

### 1.2 注意力机制的起源

**Bahdanau 注意力 (2014)**：

- 用于机器翻译
- 允许解码器"看"编码器所有隐状态
- 动态加权组合

**关键思想**：
> 不需要将所有信息压缩到固定向量，而是让模型自己学习"注意"什么

### 1.3 "Attention Is All You Need" (2017)

**Vaswani et al. 的突破**：

- 完全去除递归和卷积
- 纯基于注意力机制
- 大规模并行化
- 性能超越所有序列模型

**影响**：

- BERT (2018)：预训练语言模型
- GPT 系列：生成式语言模型
- 扩展到视觉（ViT）、多模态等

## 2. Transformer 架构 | Transformer Architecture

### 2.1 总体架构

**编码器-解码器结构**：

```text
输入序列 → Encoder → 中间表示 → Decoder → 输出序列
```

**编码器 (Encoder)**：

- N = 6 层（原始论文）
- 每层包含：
  1. 多头自注意力（Multi-Head Self-Attention）
  2. 前馈神经网络（Feed-Forward Network）

**解码器 (Decoder)**：

- N = 6 层
- 每层包含：
  1. 掩码多头自注意力
  2. 编码器-解码器注意力
  3. 前馈神经网络

### 2.2 编码器层详解

**单个编码器层**：

```text
输入 x
↓
多头自注意力
↓
残差连接 + 层归一化
↓
前馈网络
↓
残差连接 + 层归一化
↓
输出
```

**形式化**：

```text
# 多头注意力子层
x₁ = LayerNorm(x + MultiHeadAttention(x, x, x))

# 前馈子层
x₂ = LayerNorm(x₁ + FFN(x₁))
```

其中：

```text
FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
```

### 2.3 解码器层详解

**单个解码器层**：

```text
输入 y
↓
掩码多头自注意力
↓
残差连接 + 层归一化
↓
编码器-解码器注意力（Query=y, Key/Value=encoder输出）
↓
残差连接 + 层归一化
↓
前馈网络
↓
残差连接 + 层归一化
↓
输出
```

**自回归掩码**：

- 位置 i 只能看到位置 < i 的信息
- 防止"作弊"（看到未来）

## 3. 注意力机制 | Attention Mechanism

### 3.1 缩放点积注意力 (Scaled Dot-Product Attention)

**核心公式**：

```text
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```

其中：

- **Q**（Query）：查询向量，形状 (n, d_k)
- **K**（Key）：键向量，形状 (m, d_k)
- **V**（Value）：值向量，形状 (m, d_v)
- **d_k**：键的维度
- **√d_k**：缩放因子

**计算步骤**：

1. **计算相似度**：

   ```text
   S = QK^T / √d_k  # 形状 (n, m)
   ```

2. **归一化**：

   ```text
   A = softmax(S)  # 每行和为1
   ```

3. **加权求和**：

   ```text
   Output = AV  # 形状 (n, d_v)
   ```

**为什么缩放？**

当 d_k 很大时：

```text
QK^T 的方差 ≈ d_k
```

如果不缩放：

- 点积值很大
- softmax 梯度很小（饱和）
- 训练困难

缩放后：

```text
(QK^T / √d_k) 的方差 ≈ 1
```

### 3.2 多头注意力 (Multi-Head Attention)

**动机**：

- 单个注意力头可能只关注一种模式
- 多头允许模型关注不同方面

**形式化**：

```text
MultiHead(Q, K, V) = Concat(head₁, ..., head_h) W^O
```

其中每个头：

```text
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**参数矩阵**：

- W_i^Q ∈ ℝ^(d_model × d_k)
- W_i^K ∈ ℝ^(d_model × d_k)
- W_i^V ∈ ℝ^(d_model × d_v)
- W^O ∈ ℝ^(hd_v × d_model)

**典型配置**（原始论文）：

- h = 8 个头
- d_model = 512
- d_k = d_v = d_model / h = 64

**计算复杂度**：

```text
时间：O(n² · d)
空间：O(n² + n · d)
```

其中：

- n = 序列长度
- d = 模型维度

### 3.3 三种注意力类型

**1. 编码器自注意力 (Encoder Self-Attention)**：

```text
Q = K = V = 编码器输入
```

- 每个位置关注所有位置
- 双向：可以看到整个序列

**2. 解码器自注意力 (Decoder Self-Attention)**：

```text
Q = K = V = 解码器输入
+ 掩码（防止看到未来）
```

- 位置 i 只能关注位置 ≤ i
- 自回归：保持因果性

**3. 编码器-解码器注意力 (Cross-Attention)**：

```text
Q = 解码器层输出
K = V = 编码器最终输出
```

- 解码器查询编码器信息
- 类似传统 seq2seq 的注意力

### 3.4 注意力的直观理解

**注意力权重矩阵**：

```text
A[i, j] = 位置 i 对位置 j 的注意力权重
```

**例子**（英译中："The cat sat on the mat"）：

```text
翻译"猫"时：
- 高度关注 "cat"
- 中度关注 "The"
- 低度关注其他词
```

**可视化**：

- 注意力矩阵呈现学到的语言结构
- 不同头学习不同模式：句法、语义等

## 4. 位置编码 | Positional Encoding

### 4.1 问题

**Transformer 无固有位置信息**：

- 注意力是置换不变的
- 需要显式编码位置

### 4.2 正弦位置编码

**原始论文方法**：

```text
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

其中：

- pos：位置 (0, 1, 2, ...)
- i：维度索引 (0, 1, ..., d_model/2 - 1)

**性质**：

1. **唯一性**：每个位置有唯一编码
2. **相对位置**：PE(pos+k) 可表示为 PE(pos) 的线性函数
3. **外推性**：可处理训练时未见的长度

**为什么正弦？**

```text
PE(pos+k) = M_k · PE(pos)
```

其中 M_k 是只依赖于 k 的矩阵，允许模型学习相对位置。

### 4.3 可学习位置编码

**替代方案**：

```text
PE(pos) = Embedding_table[pos]
```

- 每个位置有可学习的向量
- 更灵活但需要训练数据
- 不能外推到更长序列

**实践中**：

- BERT：可学习位置编码
- GPT：可学习位置编码
- T5：相对位置偏置

### 4.4 相对位置编码

**Transformer-XL, T5 等使用**：

不编码绝对位置，而是位置差：

```text
Attention_ij = Query_i · Key_j + Bias(i - j)
```

**优势**：

- 更好的长度外推
- 更符合直觉（相对距离更重要）

## 5. 其他关键组件 | Other Key Components

### 5.1 残差连接 (Residual Connection)

**公式**：

```text
输出 = LayerNorm(x + Sublayer(x))
```

**作用**：

- 梯度直接流动
- 缓解梯度消失
- 允许更深的网络

**与 ResNet 的关系**：

- 借鉴图像领域的成功经验
- 使训练深层 Transformer 成为可能

### 5.2 层归一化 (Layer Normalization)

**公式**：

```text
LayerNorm(x) = γ ⊙ (x - μ) / σ + β
```

其中：

- μ, σ：该层的均值和标准差
- γ, β：可学习的缩放和偏移

**为什么不用 Batch Normalization？**

- Batch Norm 对批次依赖敏感
- 序列长度不一致时有问题
- Layer Norm 对每个样本独立归一化

### 5.3 前馈网络 (Feed-Forward Network)

**结构**：

```text
FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂
```

或使用 GeLU：

```text
FFN(x) = GeLU(xW₁ + b₁)W₂ + b₂
```

**维度**：

- 输入/输出：d_model = 512
- 中间层：d_ff = 2048（扩展4倍）

**作用**：

- 非线性变换
- 位置独立（每个位置单独处理）
- 增加模型容量

### 5.4 Dropout

**应用位置**：

1. 注意力权重后
2. 残差连接前
3. 位置编码加到输入后

**典型值**：p_drop = 0.1

## 6. 计算复杂度分析 | Computational Complexity Analysis

### 6.1 自注意力的复杂度

**时间复杂度**：

```text
计算 QK^T：O(n² · d)
Softmax：O(n²)
乘以 V：O(n² · d)
总计：O(n² · d)
```

**空间复杂度**：

```text
存储注意力矩阵：O(n²)
```

**问题**：

- 序列长度 n 的平方
- 长序列（n > 10,000）时不可行

### 6.2 与 RNN 的对比

| 维度 | 自注意力 | RNN |
|------|---------|-----|
| **顺序计算** | O(1) | O(n) |
| **总计算量** | O(n² · d) | O(n · d²) |
| **最大路径长度** | O(1) | O(n) |
| **并行化** | 完全并行 | 串行 |

**权衡**：

- n < d：自注意力更快（通常）
- n > d：RNN 更快（理论，但实践中并行化优势更重要）

### 6.3 长序列优化

**问题的严重性**：

```text
序列长度 n = 10,000
注意力矩阵：n² = 100,000,000 个元素
内存：~400 MB（float32）× 层数 × 头数
```

**解决方案**：

1. **稀疏注意力**：只关注部分位置
2. **局部注意力**：只关注窗口内
3. **Linformer**：低秩近似
4. **Reformer**：LSH注意力
5. **Longformer**：滑动窗口 + 全局注意力
6. **BigBird**：随机 + 窗口 + 全局

## 7. Transformer 变体 | Transformer Variants

### 7.1 BERT (2018)

**结构**：

- 只使用编码器
- 双向上下文

**预训练任务**：

- 掩码语言模型（MLM）
- 下一句预测（NSP）

**应用**：

- 文本分类
- 问答
- 命名实体识别

### 7.2 GPT 系列 (2018-)

**结构**：

- 只使用解码器
- 自回归（单向）

**训练目标**：

- 下一token预测

**规模发展**：

- GPT-1：117M 参数
- GPT-2：1.5B 参数
- GPT-3：175B 参数
- GPT-4：推测 1.7T 参数

### 7.3 T5 (2019)

**统一框架**：

- 所有NLP任务 → 文本到文本
- 编码器-解码器架构

**创新**：

- 相对位置偏置
- 系统化超参数研究

### 7.4 Vision Transformer (ViT, 2020)

**将 Transformer 用于视觉**：

1. 图像分割为 patch
2. Patch embedding
3. 标准 Transformer 编码器
4. 分类头

**发现**：

- 在大数据集上超越 CNN
- 归纳偏置更少

## 8. 理论性质 | Theoretical Properties

### 8.1 通用逼近能力

**定理（Yun et al., 2019）**：
> 单层自注意力 + 前馈网络可以近似任何序列到序列的连续函数

**意义**：

- Transformer 理论上足够表达
- 类似神经网络的通用逼近定理

### 8.2 图灵完备性

**Pérez et al. (2019)**：
> 无限深度的 Transformer 是图灵完备的

**构造思路**：

- 将图灵机状态编码到序列
- 用注意力模拟读写头
- 用前馈网络模拟状态转移

**实际意义有限**：

- 需要无限深度
- 实际模型深度有限（通常 < 100层）

### 8.3 长程依赖能力

**理论优势**：

```text
任意两位置的路径长度 = 1（直接注意）
vs RNN 的 O(n)
```

**实践考虑**：

- 注意力 O(n²) 限制序列长度
- 位置编码的有效性
- 梯度流动性

### 8.4 归纳偏置

**Transformer 的归纳偏置更少**：

| 模型 | 归纳偏置 |
|------|---------|
| **CNN** | 局部性、平移不变性 |
| **RNN** | 顺序性、马尔可夫性 |
| **Transformer** | 几乎没有（需要数据） |

**结果**：

- 需要更多数据
- 更灵活、更通用
- 更依赖预训练

## 9. 训练技巧 | Training Techniques

### 9.1 学习率调度

**Warmup + Decay**：

```text
lr(step) = d_model^(-0.5) · min(step^(-0.5), step · warmup_steps^(-1.5))
```

- 前期线性增长（warmup）
- 后期逐渐衰减

**为什么 warmup？**

- 初期梯度不稳定
- 层归一化需要适应

### 9.2 标签平滑 (Label Smoothing)

```text
y_smooth = (1 - ε) · y_true + ε / K
```

其中 K 是类别数，ε ≈ 0.1

**作用**：

- 防止过拟合
- 提高泛化能力
- 鼓励模型不过于自信

### 9.3 混合精度训练

**使用 FP16 加速**：

- 前向/反向传播：FP16
- 参数更新：FP32
- 梯度缩放防止下溢

**加速比**：2-3x

### 9.4 梯度累积

**处理大批量**：

```text
for mini_batch in batches:
    loss = forward(mini_batch)
    loss.backward()
    if step % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## 10. 权威参考文献 | Authoritative References

### Wikipedia 条目

1. [Transformer (machine learning model)](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
2. [Attention mechanism](https://en.wikipedia.org/wiki/Attention_(machine_learning))
3. [BERT (language model)](https://en.wikipedia.org/wiki/BERT_(language_model))
4. [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
5. [Multi-head attention](https://en.wikipedia.org/wiki/Multi-head_attention)

### 学术论文

1. **Vaswani, A., et al. (2017)**. "Attention Is All You Need". *NeurIPS*.
   - Transformer 原始论文

2. **Devlin, J., et al. (2018)**. "BERT: Pre-training of Deep Bidirectional Transformers". *NAACL*.
   - BERT 模型

3. **Radford, A., et al. (2018)**. "Improving Language Understanding by Generative Pre-Training".
   - GPT-1

4. **Brown, T. B., et al. (2020)**. "Language Models are Few-Shot Learners". *NeurIPS*.
   - GPT-3

5. **Dosovitskiy, A., et al. (2020)**. "An Image is Worth 16x16 Words: Transformers for Image Recognition". *ICLR*.
   - Vision Transformer

6. **Pérez, J., et al. (2019)**. "On the Turing Completeness of Modern Neural Network Architectures". *ICLR*.
   - Transformer 的图灵完备性

7. **Yun, C., et al. (2019)**. "Are Transformers universal approximators of sequence-to-sequence functions?". *ICLR*.
   - 通用逼近定理

### 标准教材与教程

1. **The Illustrated Transformer** by Jay Alammar
   - 可视化讲解 Transformer

2. **The Annotated Transformer** by Harvard NLP
   - 逐行代码实现

3. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).
   - 第10章：Transformer 和预训练

## 11. 关键要点总结 | Key Takeaways

1. **Transformer = 纯注意力**：
   - 去除递归和卷积
   - 完全基于注意力机制
   - 大规模并行化

2. **自注意力的威力**：
   - 捕捉长程依赖
   - 任意两位置路径长度 O(1)
   - 灵活的关系建模

3. **O(n²) 的代价**：
   - 长序列时内存和计算瓶颈
   - 需要各种优化技巧
   - 大量研究关注高效 Transformer

4. **位置编码必不可少**：
   - 注意力无固有顺序
   - 正弦、可学习、相对位置等方案

5. **残差 + 层归一化**：
   - 训练深层网络的关键
   - 梯度流动通畅

6. **理论性质**：
   - 通用逼近能力
   - 图灵完备（无限深度）
   - 归纳偏置少，需要数据

7. **影响深远**：
   - NLP：BERT, GPT, T5
   - 视觉：ViT, DALL-E
   - 多模态：CLIP, Flamingo
   - 成为深度学习的基础架构

---

**下一步阅读**：

- [02.1 神经网络基础理论](02.1_Neural_Network_Foundations.md)
- [02.2 RNN与Transformer架构](02.2_RNN_Transformer_Architecture.md)
- [02.3 图灵完备性分析](02.3_Turing_Completeness_Analysis.md)
- [02.5 通用逼近定理](02.5_Universal_Approximation_Theorem.md)
- [03.3 Transformer大语言模型理论](../03_Language_Models/03.3_Transformer_LLM_Theory.md)
