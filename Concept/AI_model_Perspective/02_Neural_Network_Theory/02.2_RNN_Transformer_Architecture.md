# RNN 与 Transformer 架构 | RNN and Transformer Architectures

## 1. 循环神经网络 (RNN) | Recurrent Neural Networks

### 1.1 动机与基本原理

**为什么需要 RNN？**

前馈神经网络的局限：
- 固定输入维度
- 无法处理序列数据
- 无内部状态记忆

**序列数据的特点**：
- 可变长度：句子、时间序列
- 时间依赖性：当前输出依赖历史信息
- 位置敏感：顺序很重要

### 1.2 RNN 基本架构

**标准 RNN 公式**

```text
hₜ = σ(Wₕₕ hₜ₋₁ + Wₓₕ xₜ + bₕ)
yₜ = Wₕᵧ hₜ + bᵧ
```

其中：
- `xₜ` 是 t 时刻的输入
- `hₜ` 是 t 时刻的隐状态
- `yₜ` 是 t 时刻的输出
- `Wₕₕ, Wₓₕ, Wₕᵧ` 是权重矩阵
- `σ` 是激活函数（通常是 tanh）

**关键特性**：
- 权重共享：所有时间步使用相同的参数
- 隐状态 `hₜ` 作为"内存"累积历史信息
- 可以展开为时间步的前馈网络

**RNN 的类型**：

1. **多对一** (Many-to-One): 情感分类
   - 输入：句子（多个词）
   - 输出：单个标签

2. **一对多** (One-to-Many): 图像描述生成
   - 输入：单张图像
   - 输出：描述句子

3. **多对多** (Many-to-Many): 机器翻译
   - 输入：源语言句子
   - 输出：目标语言句子

4. **同步多对多** (Synced Many-to-Many): 视频帧标注
   - 每个时间步都有输入和输出

### 1.3 RNN 的理论能力

**定理** (Siegelmann & Sontag, 1992):
> 具有有理数权重的 RNN 可以模拟任意图灵机，因此是**图灵完备的**。

**证明思路**：
1. 隐状态 `hₜ` 可以编码图灵机的带子内容
2. 循环连接提供"无限"时间步
3. 非线性激活函数实现条件转移

**重要限制**：
- 需要**无限精度**的实数运算
- 需要**任意长度**的计算时间
- 实际 RNN 使用有限精度浮点数（不图灵完备）

### 1.4 RNN 的实践问题

**梯度消失/爆炸问题**

通过时间反向传播 (BPTT, Backpropagation Through Time)：

```text
∂L/∂hₜ = ∂L/∂hₜ₊₁ · ∂hₜ₊₁/∂hₜ
       = ∂L/∂hₜ₊₁ · Wₕₕ · diag(σ'(zₜ))
```

经过 T 个时间步：
```text
∂L/∂h₀ = ∂L/∂hₜ · (∏ₖ₌₁ᵀ Wₕₕ · diag(σ'(zₖ)))
```

如果 `||Wₕₕ|| < 1`：梯度消失 → 无法学习长程依赖  
如果 `||Wₕₕ|| > 1`：梯度爆炸 → 训练不稳定

**长程依赖问题**

理论上 RNN 可以记住任意长的历史，但实际训练中：
- 难以学习跨越 >10 个时间步的依赖
- "遗忘"早期信息

## 2. LSTM 与 GRU | Long Short-Term Memory and Gated Recurrent Units

### 2.1 LSTM 架构

**LSTM** (Hochreiter & Schmidhuber, 1997) 通过**门控机制**解决梯度消失问题。

**LSTM 公式**

```text
遗忘门: fₜ = σ(Wf · [hₜ₋₁, xₜ] + bf)
输入门: iₜ = σ(Wi · [hₜ₋₁, xₜ] + bi)
候选值: C̃ₜ = tanh(Wc · [hₜ₋₁, xₜ] + bc)
细胞状态更新: Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ
输出门: oₜ = σ(Wo · [hₜ₋₁, xₜ] + bo)
隐状态: hₜ = oₜ ⊙ tanh(Cₜ)
```

**关键组件**：

1. **细胞状态 (Cell State)** `Cₜ`：
   - 信息高速公路
   - 通过加法而非乘法更新（避免梯度消失）

2. **遗忘门 (Forget Gate)** `fₜ`：
   - 决定丢弃多少旧信息
   - 0 = 完全遗忘，1 = 完全保留

3. **输入门 (Input Gate)** `iₜ`：
   - 决定写入多少新信息

4. **输出门 (Output Gate)** `oₜ`：
   - 决定输出多少细胞状态信息

**为什么 LSTM 有效？**

梯度流动：
```text
∂Cₜ/∂Cₜ₋₁ = fₜ
```

由于 `fₜ` 是通过 sigmoid 门控的，梯度可以较好地传播，不会像标准 RNN 那样快速消失。

### 2.2 GRU 架构

**GRU** (Cho et al., 2014) 是 LSTM 的简化版本，性能相当但参数更少。

**GRU 公式**

```text
重置门: rₜ = σ(Wr · [hₜ₋₁, xₜ])
更新门: zₜ = σ(Wz · [hₜ₋₁, xₜ])
候选隐状态: h̃ₜ = tanh(W · [rₜ ⊙ hₜ₋₁, xₜ])
隐状态: hₜ = (1 - zₜ) ⊙ hₜ₋₁ + zₜ ⊙ h̃ₜ
```

**与 LSTM 的对比**：
- 只有 2 个门（vs. LSTM 的 3 个）
- 没有独立的细胞状态
- 参数数量减少约 25%
- 训练速度更快

### 2.3 双向 RNN (Bidirectional RNN)

**动机**：未来信息也可能有用（例如，句子理解）

**架构**：

```text
前向: h⃗ₜ = RNN_forward(xₜ, h⃗ₜ₋₁)
后向: h⃖ₜ = RNN_backward(xₜ, h⃖ₜ₊₁)
输出: yₜ = f([h⃗ₜ; h⃖ₜ])
```

**应用**：
- BERT 使用双向编码
- 命名实体识别 (NER)
- 词性标注 (POS Tagging)

## 3. 注意力机制 | Attention Mechanism

### 3.1 注意力的动机

**Seq2Seq 模型的瓶颈**

传统 Encoder-Decoder 架构：
```text
Encoder: h₁, h₂, ..., hₙ → c (固定长度的上下文向量)
Decoder: c → y₁, y₂, ..., yₘ
```

**问题**：
- 所有源信息压缩到固定长度向量 `c`
- 信息瓶颈：长句子性能下降
- 无法对齐源和目标的不同部分

### 3.2 Bahdanau 注意力

**Bahdanau Attention** (Bahdanau et al., 2014)

在每个解码步骤 t，计算对所有编码器隐状态的加权和：

```text
1. 计算对齐分数:
   eₜᵢ = a(sₜ₋₁, hᵢ)  # 评分函数

2. 计算注意力权重:
   αₜᵢ = exp(eₜᵢ) / ∑ⱼ exp(eₜⱼ)  # softmax

3. 计算上下文向量:
   cₜ = ∑ᵢ αₜᵢ hᵢ

4. 生成输出:
   sₜ = f(sₜ₋₁, yₜ₋₁, cₜ)
```

**评分函数** (Alignment Function):

1. **加法注意力** (Bahdanau):
   ```text
   a(s, h) = vᵃ^T tanh(Wₐs + Uₐh)
   ```

2. **点积注意力** (Luong):
   ```text
   a(s, h) = s^T h
   ```

3. **缩放点积注意力** (Scaled Dot-Product):
   ```text
   a(s, h) = (s^T h) / √dₖ
   ```

**注意力的可解释性**：
- 注意力权重 `αₜᵢ` 显示模型"关注"源序列的哪些部分
- 可视化注意力热图 (Attention Heatmap)

### 3.3 自注意力 (Self-Attention)

**自注意力**：序列对自身的注意力

对于输入序列 `X = [x₁, x₂, ..., xₙ]`：

```text
1. 生成 Query, Key, Value:
   Q = XWQ,  K = XWK,  V = XWV

2. 计算注意力得分:
   Attention(Q, K, V) = softmax(QK^T / √dₖ) V
```

**含义**：
- 每个位置可以关注所有其他位置
- 捕获序列内部的依赖关系
- 不依赖循环结构，可并行计算

## 4. Transformer 架构 | Transformer Architecture

### 4.1 Transformer 的革命性突破

**Transformer** (Vaswani et al., 2017) 彻底抛弃了循环结构。

**核心创新**：
1. **完全基于注意力**：无 RNN/CNN
2. **并行化**：所有位置同时处理
3. **位置编码**：显式注入位置信息
4. **多头注意力**：多个注意力子空间

**论文标题**："Attention Is All You Need"

### 4.2 Transformer 详细架构

**整体结构**

```text
Encoder (× N 层):
  Multi-Head Self-Attention
  → Add & Norm
  → Feed-Forward Network
  → Add & Norm

Decoder (× N 层):
  Masked Multi-Head Self-Attention
  → Add & Norm
  → Multi-Head Cross-Attention (over encoder output)
  → Add & Norm
  → Feed-Forward Network
  → Add & Norm
```

**多头注意力 (Multi-Head Attention)**

```text
MultiHead(Q, K, V) = Concat(head₁, ..., headₕ) WO

其中: headᵢ = Attention(QWᵢQ, KWᵢK, VWᵢV)
```

**参数**：
- `h` 个头（通常 h = 8 或 16）
- 每个头的维度 `dₖ = d_model / h`

**优势**：
- 不同的头可以关注不同的模式
- 提升模型表达能力

**位置编码 (Positional Encoding)**

由于 Transformer 无循环结构，需要显式编码位置信息：

```text
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**特性**：
- 相对位置信息：`PE(pos+k)` 可以表示为 `PE(pos)` 的线性函数
- 可外推到更长序列（理论上）

**前馈网络 (Feed-Forward Network)**

```text
FFN(x) = ReLU(xW₁ + b₁)W₂ + b₂
```

- 每个位置独立应用
- 通常第一层扩展维度（例如，512 → 2048）
- 第二层恢复维度（2048 → 512）

**残差连接与层归一化**

```text
x' = LayerNorm(x + Sublayer(x))
```

- 残差连接缓解梯度消失
- Layer Normalization 稳定训练

### 4.3 Transformer 的计算复杂度

**自注意力复杂度分析**

| 层类型 | 每层复杂度 | 顺序操作数 | 最大路径长度 |
|--------|-----------|-----------|-------------|
| Self-Attention | O(n²·d) | O(1) | O(1) |
| RNN | O(n·d²) | O(n) | O(n) |
| CNN | O(k·n·d²) | O(1) | O(log_k(n)) |

其中：
- `n` = 序列长度
- `d` = 表示维度
- `k` = 卷积核大小

**Transformer 的优势**：
- ✅ 最大路径长度为 O(1)（任意两位置直接相连）
- ✅ 可并行化（顺序操作数 O(1)）
- ❌ 对于长序列，复杂度为 O(n²)，内存占用大

### 4.4 Transformer 变体

为解决 O(n²) 复杂度问题，提出了多种高效 Transformer：

1. **Sparse Transformer** (Child et al., 2019)
   - 稀疏注意力模式
   - 复杂度：O(n√n)

2. **Linformer** (Wang et al., 2020)
   - 低秩分解注意力矩阵
   - 复杂度：O(n)

3. **Performer** (Choromanski et al., 2020)
   - 使用核方法近似注意力
   - 复杂度：O(n)

4. **FlashAttention** (Dao et al., 2022)
   - IO 感知的注意力算法
   - 保持 O(n²) 但大幅加速实际运行

## 5. Transformer 的理论能力 | Theoretical Capabilities

### 5.1 图灵完备性分析

**问题**：Transformer 是图灵完备的吗？

**研究结果**：

1. **Pérez et al. (2019)**: 
   - 标准 Transformer（硬注意力）**不是**图灵完备的
   - 无法实现通用图灵机

2. **Dehghani et al. (2018) - Universal Transformers**:
   - 添加循环深度（动态层数）
   - **是**图灵完备的

3. **Bhattamishra et al. (2020)**:
   - 分析了 Transformer 对形式语言的识别能力
   - PARITY, DYCK 等任务的理论界限

**关键限制**：
- 固定深度 Transformer = 固定时间计算
- 位置编码的有限性限制了序列长度
- 浮点精度限制了状态空间

### 5.2 形式语言识别能力

**实验结果** (Bhattamishra et al., 2020):

| 语言类型 | Transformer 性能 |
|---------|-----------------|
| Regular (PARITY, EVEN PAIRS) | ✅ 可学习 |
| Counter (aⁿbⁿ) | ✅ 可学习（有深度限制） |
| Context-Free (DYCK) | ⚠️ 部分成功 |
| Context-Sensitive | ❌ 失败 |

**理论解释**：
- Transformer 的自注意力机制类似于 Counter Automaton
- 深度 d 的 Transformer 可以模拟 O(d) 个计数器
- 但计数器精度受限于浮点数精度

### 5.3 通用近似能力

**定理** (Yun et al., 2020):
> Transformer 可以近似任何连续序列到序列函数（在紧集上）。

**含义**：
- Transformer 具有序列函数的通用近似能力
- 但这仍然是存在性定理，不保证可学习性

## 6. 大语言模型中的应用 | Applications in Large Language Models

### 6.1 仅解码器架构 (Decoder-Only)

**GPT 系列** (Radford et al., 2018/2019; Brown et al., 2020)

```text
架构: 
  Masked Multi-Head Self-Attention (因果掩码)
  → Add & Norm
  → Feed-Forward
  → Add & Norm

训练目标: 下一个 token 预测 (Next Token Prediction)
  P(x_t | x_1, x_2, ..., x_{t-1})
```

**特点**：
- 自回归生成
- 因果掩码确保单向注意力
- 适合文本生成任务

**规模演进**：
- GPT-1: 117M 参数
- GPT-2: 1.5B 参数
- GPT-3: 175B 参数
- GPT-4: 估计 1.76T 参数（混合专家模型）

### 6.2 仅编码器架构 (Encoder-Only)

**BERT** (Devlin et al., 2018)

```text
架构: 
  Multi-Head Self-Attention (双向)
  → Add & Norm
  → Feed-Forward
  → Add & Norm

训练目标:
  1. Masked Language Modeling (MLM)
     预测被掩码的 token
  2. Next Sentence Prediction (NSP)
     判断两个句子是否连续
```

**特点**：
- 双向编码
- 适合理解任务（分类、NER、QA）
- 需要 fine-tuning

### 6.3 编码器-解码器架构 (Encoder-Decoder)

**T5** (Raffel et al., 2020), **BART** (Lewis et al., 2020)

```text
Encoder: 双向自注意力（理解输入）
Decoder: 因果自注意力（生成输出）+ 交叉注意力（编码器-解码器注意力）
```

**统一框架**：
- 所有任务都视为 Seq2Seq
- "翻译英语到法语：[输入]" → [输出]

### 6.4 缩放定律 (Scaling Laws)

**Kaplan et al. (2020)**: OpenAI 的缩放定律

```text
Loss ~ N^(-α)
```

其中：
- `N` = 参数数量、数据集大小或计算量
- `α ≈ 0.076`（参数）

**关键发现**：
1. 更大的模型表现更好（幂律关系）
2. 最优配置：模型大小与数据量应成比例
3. 过度训练（多轮）不如增大数据集

**Hoffmann et al. (2022)**: Chinchilla 缩放定律

修正了 Kaplan 的结论：
- 对于给定计算预算，数据量应与模型大小相匹配
- Chinchilla (70B) 优于 Gopher (280B)，但训练数据 4× 更多

## 7. 权威参考文献 | Authoritative References

### 循环神经网络

1. **Elman, J. L.** (1990). "Finding structure in time." *Cognitive Science*, 14(2), 179-211.
   - 简单 RNN 的早期论文

2. **Hochreiter, S., & Schmidhuber, J.** (1997). "Long short-term memory." *Neural Computation*, 9(8), 1735-1780.
   - LSTM 的原始论文

3. **Cho, K., et al.** (2014). "Learning phrase representations using RNN encoder-decoder for statistical machine translation." *EMNLP*.
   - GRU 的提出

4. **Siegelmann, H. T., & Sontag, E. D.** (1992). "On the computational power of neural nets." *COLT*.
   - RNN 图灵完备性证明

### 注意力机制

5. **Bahdanau, D., Cho, K., & Bengio, Y.** (2014). "Neural machine translation by jointly learning to align and translate." *ICLR 2015*.
   - 第一个注意力机制

6. **Luong, M. T., Pham, H., & Manning, C. D.** (2015). "Effective approaches to attention-based neural machine translation." *EMNLP*.
   - Luong 注意力

### Transformer

7. **Vaswani, A., et al.** (2017). "Attention is all you need." *NeurIPS*.
   - Transformer 的原始论文

8. **Devlin, J., et al.** (2018). "BERT: Pre-training of deep bidirectional transformers for language understanding." *NAACL 2019*.
   - BERT 模型

9. **Radford, A., et al.** (2018). "Improving language understanding by generative pre-training."
   - GPT-1

10. **Brown, T. B., et al.** (2020). "Language models are few-shot learners." *NeurIPS*.
    - GPT-3

### 理论分析

11. **Pérez, J., et al.** (2019). "On the Turing completeness of modern neural network architectures." *ICLR 2019*.
    - Transformer 计算能力分析

12. **Bhattamishra, S., et al.** (2020). "On the ability and limitations of transformers to recognize formal languages." *EMNLP 2020*.
    - Transformer 的形式语言识别能力

13. **Yun, C., et al.** (2020). "Are transformers universal approximators of sequence-to-sequence functions?" *ICLR 2020*.
    - Transformer 的通用近似定理

### 缩放定律

14. **Kaplan, J., et al.** (2020). "Scaling laws for neural language models." *arXiv:2001.08361*.
    - OpenAI 缩放定律

15. **Hoffmann, J., et al.** (2022). "Training compute-optimal large language models." *arXiv:2203.15556*.
    - Chinchilla 缩放定律

### Wikipedia 参考

16. **Recurrent Neural Network**: https://en.wikipedia.org/wiki/Recurrent_neural_network
17. **Long Short-Term Memory**: https://en.wikipedia.org/wiki/Long_short-term_memory
18. **Attention (machine learning)**: https://en.wikipedia.org/wiki/Attention_(machine_learning)
19. **Transformer (machine learning model)**: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
20. **BERT (language model)**: https://en.wikipedia.org/wiki/BERT_(language_model)
21. **GPT-3**: https://en.wikipedia.org/wiki/GPT-3

---

**本文档建立时间**: 2025-10-23  
**版本**: 1.0  
**状态**: ✅ 完成 - 包含权威引用和概念对齐

