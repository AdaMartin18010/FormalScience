# RNN ä¸ Transformer æ¶æ„ | RNN and Transformer Architectures

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **æœ€åæ›´æ–°**: 2025-10-27  
> **æ–‡æ¡£è§„æ¨¡**: 657è¡Œ | RNNä¸Transformeræ¶æ„è¯¦è§£  
> **é˜…è¯»å»ºè®®**: æœ¬æ–‡å¯¹æ¯”åˆ†æRNNå’ŒTransformeræ¶æ„ï¼Œå»ºè®®å…ˆäº†è§£åŸºæœ¬ç¥ç»ç½‘ç»œå’Œæ³¨æ„åŠ›æœºåˆ¶

---

## æ ¸å¿ƒæ¦‚å¿µæ·±åº¦åˆ†æ

<details>
<summary><b>ğŸ”„ğŸ”€ ç‚¹å‡»å±•å¼€ï¼šRNNåˆ°Transformeré©å‘½å…¨æ™¯è§£æ</b></summary>

æœ¬èŠ‚æ·±å…¥å‰–æä»RNNåˆ°Transformerçš„æ¶æ„æ¼”è¿›ã€æ³¨æ„åŠ›æœºåˆ¶é©å‘½ã€ç†è®ºèƒ½åŠ›ä¸å®è·µæƒè¡¡ã€‚

### 1ï¸âƒ£ RNN vs Transformer æ¦‚å¿µå®šä¹‰å¡

**æ¦‚å¿µåç§°**: RNN to Transformer Evolutionï¼ˆä»å¾ªç¯åˆ°æ³¨æ„åŠ›çš„èŒƒå¼è½¬å˜ï¼‰

**å†…æ¶µï¼ˆæœ¬è´¨å±æ€§ï¼‰**:

**ğŸ”¹ æ ¸å¿ƒå®šä¹‰**:
RNNé€šè¿‡éšçŠ¶æ€çš„å¾ªç¯ä¼ é€’å¤„ç†åºåˆ—ï¼ŒTransformeré€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å¹¶è¡Œå»ºæ¨¡å…¨å±€ä¾èµ–å…³ç³»ï¼Œå®ç°äº†åºåˆ—å»ºæ¨¡çš„èŒƒå¼é©å‘½ã€‚

$$
\begin{align}
\text{RNN} &: h_t = f(h_{t-1}, x_t) \quad \text{ï¼ˆé¡ºåºã€é€’å½’ï¼‰} \\
\text{Transformer} &: \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V \quad \text{ï¼ˆå¹¶è¡Œã€å…¨å±€ï¼‰}
\end{align}
$$

**ğŸ”¹ æ¶æ„æ¼”è¿›æ—¶é—´çº¿**:

| æ—¶æœŸ | æ¶æ„ | æ ¸å¿ƒæœºåˆ¶ | ä»£è¡¨æ¨¡å‹ | çªç ´ | å±€é™ |
|------|------|---------|---------|------|------|
| **1986-1990** | ç®€å•RNN | éšçŠ¶æ€é€’å½’ | Elmanç½‘ç»œ | é¦–æ¬¡åºåˆ—å»ºæ¨¡ | æ¢¯åº¦æ¶ˆå¤± |
| **1997** | LSTM | é—¨æ§æœºåˆ¶ | LSTM | é•¿æœŸä¾èµ– | ä»ç„¶é¡ºåº |
| **2014** | GRU | ç®€åŒ–é—¨æ§ | GRU | æ›´å¿«è®­ç»ƒ | ä»ç„¶é¡ºåº |
| **2014-2015** | æ³¨æ„åŠ› | åŠ¨æ€åŠ æƒ | Seq2Seq+Attn | çªç ´ç“¶é¢ˆ | ä»ä¾èµ–RNN |
| **2017** | **Transformer** | è‡ªæ³¨æ„åŠ› | Transformer | å®Œå…¨å¹¶è¡Œ | O(nÂ²)å¤æ‚åº¦ |
| **2018-2024** | Transformerå˜ä½“ | é«˜æ•ˆæ³¨æ„åŠ› | GPT/BERT/Mamba | 100B+å‚æ•° | ä¸Šä¸‹æ–‡å—é™ |

**å¤–å»¶ï¼ˆèŒƒå›´è¾¹ç•Œï¼‰**:

| ç»´åº¦ | RNNåŒ…å« âœ… | TransformeråŒ…å« âœ… | ä¸åŒ…å« âŒ |
|------|----------|------------------|----------|
| **æ¶æ„** | RNNã€LSTMã€GRU | ç¼–ç å™¨ã€è§£ç å™¨ã€è‡ªæ³¨æ„åŠ› | CNNã€MLP |
| **æœºåˆ¶** | éšçŠ¶æ€é€’å½’ã€é—¨æ§ | å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç  | å·ç§¯ã€æ± åŒ– |
| **åº”ç”¨** | æ—¶åºã€æ–‡æœ¬ã€è¯­éŸ³ | NLPã€CVã€å¤šæ¨¡æ€ | éåºåˆ—æ•°æ® |

**å±æ€§ç»´åº¦è¡¨**:

| ç»´åº¦ | RNN/LSTM | Transformer | è¯´æ˜ |
|------|---------|------------|------|
| **å¹¶è¡Œæ€§** | âŒ é¡ºåº | âœ…âœ…âœ… å®Œå…¨å¹¶è¡Œ | Transformeré©å‘½æ€§ä¼˜åŠ¿ |
| **é•¿è·ç¦»ä¾èµ–** | âš ï¸ LSTMæ”¹è¿›ä½†ä»å—é™ | âœ…âœ…âœ… å…¨å±€æ³¨æ„åŠ› | Transformeræ— è·ç¦»è¡°å‡ |
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ï¼ˆé¡ºåºï¼‰ | å¿«ï¼ˆå¹¶è¡Œ+GPUï¼‰ | 10-100Ã— |
| **ç†è®ºèƒ½åŠ›** | å›¾çµå®Œå¤‡ï¼ˆæ— é™ç²¾åº¦ï¼‰ | å›¾çµå®Œå¤‡ï¼ˆç¼–ç ï¼‰ | ä¸¤è€…ç­‰ä»· |
| **å®é™…æ€§èƒ½** | ä¸­ç­‰ | SOTA | Transformerä¸»å¯¼ |
| **å†…å­˜** | O(1)éšçŠ¶æ€ | O(nÂ²)æ³¨æ„åŠ› | Transformerå†…å­˜ç“¶é¢ˆ |
| **å¯è§£é‡Šæ€§** | ä½ï¼ˆéšçŠ¶æ€é»‘ç›’ï¼‰ | ä¸­ï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰ | Transformerç•¥å¥½ |

---

### 2ï¸âƒ£ RNNåˆ°Transformeræ¼”è¿›å…¨æ™¯å›¾è°±

```mermaid
graph TB
    RNN[ç®€å•RNN<br/>1986-1990]
    LSTM[LSTM<br/>1997]
    GRU[GRU<br/>2014]
    Attention[æ³¨æ„åŠ›æœºåˆ¶<br/>2014-2015]
    Transformer[Transformer<br/>2017]
    Modern[ç°ä»£LLM<br/>2018-2024]
    
    RNN --> Problem1[æ¢¯åº¦æ¶ˆå¤±<br/>é•¿æœŸä¾èµ–å›°éš¾]
    Problem1 --> LSTM
    
    LSTM --> LSTM_Features[é—¨æ§æœºåˆ¶:<br/>è¾“å…¥é—¨ã€é—å¿˜é—¨ã€è¾“å‡ºé—¨]
    LSTM --> Problem2[ä»ç„¶é¡ºåºè®¡ç®—<br/>å¹¶è¡ŒåŒ–å›°éš¾]
    
    Problem2 --> GRU
    GRU --> GRU_Features[ç®€åŒ–é—¨æ§:<br/>æ›´æ–°é—¨ã€é‡ç½®é—¨]
    
    LSTM --> Problem3[ç¼–ç å™¨ç“¶é¢ˆ<br/>å›ºå®šé•¿åº¦å‘é‡]
    Problem3 --> Attention
    
    Attention --> Attn_Features[åŠ¨æ€åŠ æƒæ±‚å’Œ<br/>å…³æ³¨é‡è¦éƒ¨åˆ†]
    Attention --> Problem4[ä»ä¾èµ–RNNåŸºç¡€]
    
    Problem4 --> Transformer
    
    Transformer --> Trans_Core[è‡ªæ³¨æ„åŠ›:<br/>å®Œå…¨å¹¶è¡Œ]
    Transformer --> Trans_Feat1[å¤šå¤´æ³¨æ„åŠ›]
    Transformer --> Trans_Feat2[ä½ç½®ç¼–ç ]
    Transformer --> Trans_Feat3[å‰é¦ˆç½‘ç»œ]
    
    Transformer --> Modern
    Modern --> GPT[GPTç³»åˆ—<br/>ä»…è§£ç å™¨]
    Modern --> BERT[BERTç³»åˆ—<br/>ä»…ç¼–ç å™¨]
    Modern --> T5[T5ç³»åˆ—<br/>ç¼–ç å™¨-è§£ç å™¨]
    
    Trans_Core --> Breakthrough[é©å‘½æ€§çªç ´:<br/>O&#40;1&#41;åºåˆ—æ“ä½œ]
    
    Trans_Feat1 --> Challenge1[æŒ‘æˆ˜1:<br/>O&#40;nÂ²&#41;å¤æ‚åº¦]
    Challenge1 --> Solutions[é«˜æ•ˆTransformer:<br/>Linformer/Performer/Mamba]
    
    style RNN fill:#e74c3c,stroke:#333,stroke-width:4px
    style Transformer fill:#2ecc71,stroke:#333,stroke-width:4px,color:#fff
    style Modern fill:#9b59b6,stroke:#333,stroke-width:4px
    style Breakthrough fill:#f39c12,stroke:#333,stroke-width:4px
```

---

### 3ï¸âƒ£ RNN vs LSTM vs Transformer åäº”ç»´æ·±åº¦å¯¹æ¯”

| ç»´åº¦ | ç®€å•RNN | LSTM | Transformer | å…³é”®æ´å¯Ÿ |
|------|---------|------|------------|---------|
| **1. å¹¶è¡Œæ€§** | âŒ å®Œå…¨é¡ºåº | âŒ å®Œå…¨é¡ºåº | âœ…âœ…âœ… å®Œå…¨å¹¶è¡Œ | **èŒƒå¼é©å‘½** |
| **2. è®­ç»ƒé€Ÿåº¦** | æ…¢ | æ…¢ | **10-100Ã—å¿«** | GPUåˆ©ç”¨ç‡ |
| **3. é•¿æœŸä¾èµ–** | âŒ æ¢¯åº¦æ¶ˆå¤± | âœ… é—¨æ§ç¼“è§£ | âœ…âœ…âœ… å…¨å±€æ³¨æ„åŠ› | Transformeræ— è·ç¦»é™åˆ¶ |
| **4. å‚æ•°æ•ˆç‡** | é«˜ï¼ˆå‚æ•°å°‘ï¼‰ | ä¸­ï¼ˆ4Ã—RNNï¼‰ | ä½ï¼ˆå·¨å¤§ï¼‰ | Transformeréœ€æ›´å¤šå‚æ•° |
| **5. å†…å­˜å ç”¨** | O(1)éšçŠ¶æ€ | O(1)çŠ¶æ€ | O(nÂ²)æ³¨æ„åŠ› | **Transformerç“¶é¢ˆ** |
| **6. æ¨ç†é€Ÿåº¦** | å¿«ï¼ˆå°æ¨¡å‹ï¼‰ | å¿«ï¼ˆå°æ¨¡å‹ï¼‰ | æ…¢ï¼ˆå¤§æ¨¡å‹ï¼‰ | Transformeréƒ¨ç½²éš¾ |
| **7. å¯è§£é‡Šæ€§** | ä½ï¼ˆéšçŠ¶æ€ï¼‰ | ä½ï¼ˆé—¨æ§å¤æ‚ï¼‰ | ä¸­ï¼ˆæ³¨æ„åŠ›å›¾ï¼‰ | ç•¥æœ‰æ”¹å–„ |
| **8. å½’çº³åç½®** | âœ… æ—¶åºåç½® | âœ… æ—¶åºåç½® | âŒ æ— ï¼ˆéœ€ä½ç½®ç¼–ç ï¼‰ | Transformeræ›´é€šç”¨ |
| **9. æ³›åŒ–èƒ½åŠ›** | ä¸­ | ä¸­ | âœ…âœ… å¼ºï¼ˆå¤§è§„æ¨¡ï¼‰ | æ•°æ®+è§„æ¨¡ä¼˜åŠ¿ |
| **10. ç†è®ºèƒ½åŠ›** | å›¾çµå®Œå¤‡* | å›¾çµå®Œå¤‡* | å›¾çµå®Œå¤‡* | ç†è®ºç­‰ä»· |
| **11. å®é™…æ€§èƒ½** | âš ï¸âš ï¸ å·® | âš ï¸âš ï¸âš ï¸ ä¸­ | âœ…âœ…âœ…âœ… SOTA | å®è·µä¸»å¯¼ |
| **12. ä¸Šä¸‹æ–‡é•¿åº¦** | æ•°ç™¾ | æ•°åƒ | **æ•°ç™¾ä¸‡**ï¼ˆç°ä»£ï¼‰ | Transformerå¯æ‰©å±• |
| **13. åŒå‘å»ºæ¨¡** | éœ€ç‰¹æ®Šè®¾è®¡ | BiLSTM | âœ… å¤©ç„¶åŒå‘ | Transformerä¼˜åŠ¿ |
| **14. åº”ç”¨å¹¿åº¦** | çª„ï¼ˆæ—¶åºï¼‰ | ä¸­ï¼ˆNLPï¼‰ | **å¹¿**ï¼ˆNLP/CV/å¤šæ¨¡æ€ï¼‰ | é€šç”¨æ¶æ„ |
| **15. å·¥ä¸šé‡‡ç”¨** | å·²æ·˜æ±° | é—ç•™ç³»ç»Ÿ | **ä¸»æµ**ï¼ˆ2017-ç°åœ¨ï¼‰ | Transformerç»Ÿæ²» |

**å…³é”®å…¬å¼å¯¹æ¯”**:

$$
\begin{align}
\text{RNN} &: h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t) \quad O(1) \text{å¹¶è¡Œåº¦} \\
\text{LSTM} &: \begin{cases}
f_t = \sigma(W_f \cdot [h_{t-1}, x_t]) \\
i_t = \sigma(W_i \cdot [h_{t-1}, x_t]) \\
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t]) \\
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{cases} \quad O(1) \text{å¹¶è¡Œåº¦} \\
\text{Transformer} &: \text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \quad O(n) \text{å¹¶è¡Œåº¦}
\end{align}
$$

**æ·±åº¦åˆ†æ**:

```yaml
RNNçš„è‡´å‘½ç¼ºé™·:
  1. æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸:
     - åå‘ä¼ æ’­é€šè¿‡æ—¶é—´ï¼ˆBPTTï¼‰
     - æ¢¯åº¦è¡°å‡: âˆ(âˆ‚h_t/âˆ‚h_{t-1}) â†’ 0æˆ–âˆ
     - ç»“æœ: æ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–ï¼ˆ>20æ­¥ï¼‰
  
  2. é¡ºåºç“¶é¢ˆ:
     - h_tä¾èµ–h_{t-1} â†’ æ— æ³•å¹¶è¡Œ
     - GPUåˆ©ç”¨ç‡ä½ï¼ˆ<10%ï¼‰
     - è®­ç»ƒæ—¶é—´: å¤©â†’å‘¨
  
  3. ä¿¡æ¯ç“¶é¢ˆ:
     - æ‰€æœ‰å†å²ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šç»´åº¦h_t
     - é•¿åºåˆ—ä¿¡æ¯ä¸¢å¤±

LSTMçš„æ”¹è¿›ä¸å±€é™:
  æ”¹è¿›:
    - é—¨æ§æœºåˆ¶ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    - ç»†èƒçŠ¶æ€C_tä½œä¸º"é«˜é€Ÿå…¬è·¯"
    - å¯å­¦ä¹ 100-1000æ­¥ä¾èµ–
  
  å±€é™:
    - ä»ç„¶é¡ºåºè®¡ç®—ï¼ˆç“¶é¢ˆæœªè§£å†³ï¼‰
    - é—¨æ§æœºåˆ¶å¤æ‚ï¼ˆ4å€å‚æ•°ï¼‰
    - ä»æœ‰é•¿ç¨‹è¡°å‡ï¼ˆè™½ç„¶æ›´æ…¢ï¼‰

Transformerçš„é©å‘½:
  æ ¸å¿ƒçªç ´:
    1. è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰:
       - æ¯ä¸ªä½ç½®ç›´æ¥è¿æ¥æ‰€æœ‰ä½ç½®
       - O(1)è·¯å¾„é•¿åº¦ï¼ˆvs RNNçš„O(n)ï¼‰
       - æ— è·ç¦»è¡°å‡
    
    2. å®Œå…¨å¹¶è¡Œ:
       - æ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—
       - GPUåˆ©ç”¨ç‡>90%
       - è®­ç»ƒåŠ é€Ÿ10-100Ã—
    
    3. å¤šå¤´æ³¨æ„åŠ›:
       - ä¸åŒå­ç©ºé—´æ•è·ä¸åŒå…³ç³»
       - ç±»ä¼¼CNNçš„å¤šé€šé“
    
    4. ä½ç½®ç¼–ç :
       - è¡¥å¿æ— å½’çº³åç½®
       - æ­£å¼¦/å¯å­¦ä¹ ç¼–ç 
  
  ä»£ä»·:
    - O(nÂ²)å¤æ‚åº¦ï¼ˆå†…å­˜+è®¡ç®—ï¼‰
    - ä¸Šä¸‹æ–‡é•¿åº¦å—é™ï¼ˆ2Kâ†’128Kæ¼”è¿›ï¼‰
    - éœ€å¤§é‡æ•°æ®ï¼ˆç™¾GBâ†’TBï¼‰

å½“å‰çŠ¶æ€ï¼ˆ2024ï¼‰:
  - Transformerå®Œå…¨ä¸»å¯¼NLP
  - RNN/LSTMä»…é—ç•™ç³»ç»Ÿ
  - ç ”ç©¶ç„¦ç‚¹: é«˜æ•ˆTransformerï¼ˆMamba, Linear Attnï¼‰
```

---

### 4ï¸âƒ£ æ³¨æ„åŠ›æœºåˆ¶æ¼”è¿›ä¸æ•°å­¦åŸç†

**æ³¨æ„åŠ›ä¸‰éƒ¨æ›²**:

| é˜¶æ®µ | å¹´ä»½ | ç±»å‹ | å…¬å¼ | çªç ´ |
|------|------|------|------|------|
| **é˜¶æ®µ1** | 2014 | Bahdanauæ³¨æ„åŠ› | $\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_k \exp(e_{ik})}$ | æ‰“ç ´ç¼–ç å™¨ç“¶é¢ˆ |
| **é˜¶æ®µ2** | 2015 | Luongæ³¨æ„åŠ› | ç‚¹ç§¯/é€šç”¨/æ‹¼æ¥ | æ›´ç®€å•é«˜æ•ˆ |
| **é˜¶æ®µ3** | 2017 | è‡ªæ³¨æ„åŠ› | $\text{Attention}(Q,K,V)$ | æŠ›å¼ƒRNN |

**è‡ªæ³¨æ„åŠ›æ·±åº¦è§£æ**:

$$
\begin{align}
Q &= X W^Q, \quad K = X W^K, \quad V = X W^V \\
\text{Attention}(Q, K, V) &= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \\
\text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \\
\text{where } \text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}
$$

**ä¸ºä»€ä¹ˆç¼©æ”¾ $\sqrt{d_k}$ï¼Ÿ**

```yaml
é—®é¢˜: ç‚¹ç§¯QK^Tçš„æ–¹å·®éšç»´åº¦d_kå¢é•¿
  - Q, K ~ N(0,1) â†’ QK^T ~ N(0, d_k)
  - é«˜ç»´åº¦â†’å¤§æ–¹å·®â†’softmaxé¥±å’Œâ†’æ¢¯åº¦æ¶ˆå¤±

è§£å†³: é™¤ä»¥âˆšd_k
  - QK^T/âˆšd_k ~ N(0,1)
  - ç¨³å®šæ¢¯åº¦
```

**å¤šå¤´æ³¨æ„åŠ›çš„æ„ä¹‰**:

| ç»´åº¦ | å•å¤´æ³¨æ„åŠ› | å¤šå¤´æ³¨æ„åŠ›ï¼ˆh=8ï¼‰ |
|------|-----------|----------------|
| **å­ç©ºé—´** | 1ä¸ªè¡¨ç¤ºç©ºé—´ | 8ä¸ªä¸åŒå­ç©ºé—´ |
| **æ•è·å…³ç³»** | å•ä¸€æ¨¡å¼ | å¤šç§å…³ç³»ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€ä½ç½®...ï¼‰ |
| **ç±»æ¯”** | å•é€šé“å›¾åƒ | RGBå¤šé€šé“ |
| **é²æ£’æ€§** | ä½ | é«˜ï¼ˆå†—ä½™ï¼‰ |

---

### 5ï¸âƒ£ Transformeræ¶æ„ä¸‰å¤§å˜ä½“å¯¹æ¯”

| æ¶æ„ç±»å‹ | ç»“æ„ | å› æœæ©ç  | ä»£è¡¨æ¨¡å‹ | åº”ç”¨ | ä¼˜åŠ¿ | å±€é™ |
|---------|------|---------|---------|------|------|------|
| **ä»…ç¼–ç å™¨<br/>Encoder-Only** | NÃ—ç¼–ç å™¨å±‚ | âŒ åŒå‘ | BERT, RoBERTa | ç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERï¼‰ | åŒå‘ä¸Šä¸‹æ–‡ | ä¸èƒ½ç”Ÿæˆ |
| **ä»…è§£ç å™¨<br/>Decoder-Only** | NÃ—è§£ç å™¨å±‚ | âœ… å•å‘ | GPT, LLaMA | ç”Ÿæˆä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰ | ç®€å•ã€å¯æ‰©å±• | å•å‘é™åˆ¶ |
| **ç¼–ç å™¨-è§£ç å™¨<br/>Enc-Dec** | NÃ—ç¼–ç +NÃ—è§£ç  | ç¼–ç å™¨åŒå‘<br/>è§£ç å™¨å•å‘ | T5, BART | Seq2Seqï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰ | æœ€å¼ºå¤§ | å¤æ‚ã€æ…¢ |

**æ·±åº¦åˆ†æ**:

```yaml
ä»…ç¼–ç å™¨ï¼ˆBERTï¼‰:
  ä¼˜åŠ¿:
    - åŒå‘ä¸Šä¸‹æ–‡ï¼ˆMLMé¢„è®­ç»ƒï¼‰
    - ç†è§£ä»»åŠ¡SOTA
    - [CLS] tokenå…¨å±€è¡¨ç¤º
  
  åº”ç”¨:
    - æƒ…æ„Ÿåˆ†ç±»
    - å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
    - é—®ç­”ç³»ç»Ÿï¼ˆæŠ½å–å¼ï¼‰
  
  å±€é™:
    - ä¸èƒ½è‡ªå›å½’ç”Ÿæˆ
    - éœ€è¦fine-tune

ä»…è§£ç å™¨ï¼ˆGPTï¼‰:
  ä¼˜åŠ¿:
    - è‡ªå›å½’ç”Ÿæˆ
    - æ¶æ„ç®€å•
    - æ˜“æ‰©å±•åˆ°100B+å‚æ•°
    - Zero-shot/Few-shotèƒ½åŠ›
  
  åº”ç”¨:
    - æ–‡æœ¬ç”Ÿæˆ
    - å¯¹è¯ç³»ç»Ÿ
    - ä»£ç ç”Ÿæˆ
  
  å±€é™:
    - å•å‘ï¼ˆçœ‹ä¸åˆ°æœªæ¥ï¼‰
    - ç†è§£ä»»åŠ¡ä¸å¦‚BERT
  
  ä¸»å¯¼åœ°ä½:
    - 2020+: GPT-3/GPT-4/LLaMA
    - "ç¼©æ”¾å®šå¾‹"é©±åŠ¨
    - å½“å‰ä¸»æµ

ç¼–ç å™¨-è§£ç å™¨ï¼ˆT5ï¼‰:
  ä¼˜åŠ¿:
    - ç¼–ç å™¨åŒå‘ï¼ˆç†è§£ï¼‰
    - è§£ç å™¨å•å‘ï¼ˆç”Ÿæˆï¼‰
    - Seq2Seqä»»åŠ¡æœ€å¼º
  
  åº”ç”¨:
    - æœºå™¨ç¿»è¯‘
    - æ–‡æœ¬æ‘˜è¦
    - æ•°æ®å¢å¼º
  
  å±€é™:
    - æ¶æ„å¤æ‚ï¼ˆ2Nå±‚ï¼‰
    - è®­ç»ƒ/æ¨ç†æ…¢
    - ç¼©æ”¾å›°éš¾
  
  ç°çŠ¶: éƒ¨åˆ†åœºæ™¯ä»ç”¨ï¼ˆç¿»è¯‘ï¼‰
```

---

### ğŸ”Ÿ æ ¸å¿ƒæ´å¯Ÿä¸ç»ˆæè¯„ä¼°

**äº”å¤§æ ¸å¿ƒå®šå¾‹**:

1. **å¹¶è¡ŒåŒ–é©å‘½å®šå¾‹**
   $$
   \text{RNNå¹¶è¡Œåº¦} = O(1), \quad \text{Transformerå¹¶è¡Œåº¦} = O(n)
   $$
   - Transformerå®ç°å®Œå…¨å¹¶è¡Œï¼Œè®­ç»ƒåŠ é€Ÿ10-100Ã—

2. **æ³¨æ„åŠ›å…¨å±€æ€§å®šå¾‹**
   $$
   \text{RNNè·¯å¾„é•¿åº¦} = O(n), \quad \text{Transformerè·¯å¾„é•¿åº¦} = O(1)
   $$
   - ä»»æ„ä¸¤ä¸ªä½ç½®ç›´æ¥è¿æ¥ï¼Œæ— è·ç¦»è¡°å‡

3. **å¤æ‚åº¦æƒè¡¡å®šå¾‹**
   $$
   \text{RNN}: O(n) \text{æ—¶é—´} + O(1) \text{ç©ºé—´} \quad vs \quad \text{Transformer}: O(n^2) \text{æ—¶é—´+ç©ºé—´}
   $$
   - Transformerä»¥äºŒæ¬¡å¤æ‚åº¦æ¢å–å¹¶è¡Œæ€§å’Œå…¨å±€æ€§

4. **ç¼©æ”¾å®šå¾‹**ï¼ˆKaplan et al. 2020ï¼‰
   $$
   \text{Loss} \propto \text{Compute}^{-\alpha}, \quad \alpha \approx 0.05-0.1
   $$
   - æ€§èƒ½éšè®¡ç®—é‡å¹‚å¾‹å¢é•¿ï¼ˆä»…Transformeræœ‰æ•ˆï¼‰

5. **æ¶æ„ç®€åŒ–å®šå¾‹**
   - 2017-2024: æ¶æ„è¶‹å‘ç®€å•ï¼ˆä»…è§£ç å™¨ï¼‰
   - å¤æ‚åº¦è½¬ç§»åˆ°è§„æ¨¡ï¼ˆ100B+å‚æ•°ï¼‰

**ç»ˆææ´å¯Ÿ**:

> **"Transformerä»£è¡¨äº†æ·±åº¦å­¦ä¹ å²ä¸Šæœ€é‡å¤§çš„æ¶æ„é©å‘½ä¹‹ä¸€ã€‚2017å¹´'Attention Is All You Need'è®ºæ–‡å½»åº•æŠ›å¼ƒRNNçš„é¡ºåºè®¡ç®—ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°å®Œå…¨å¹¶è¡Œå’Œå…¨å±€å»ºæ¨¡ã€‚æ ¸å¿ƒçªç ´ï¼šâ‘ O(1)è·¯å¾„é•¿åº¦ï¼ˆvs RNNçš„O(n)ï¼‰â‘¡å®Œå…¨å¹¶è¡Œï¼ˆGPUåˆ©ç”¨ç‡>90%ï¼‰â‘¢å¤šå¤´æ³¨æ„åŠ›ï¼ˆå¤šå­ç©ºé—´è¡¨ç¤ºï¼‰ã€‚ä»£ä»·æ˜¯O(nÂ²)å¤æ‚åº¦ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡é•¿åº¦ç“¶é¢ˆï¼ˆ2Kâ†’128Kæ¼”è¿›ï¼‰ã€‚Transformerç»Ÿæ²»äº†2017-2024çš„NLP/CV/å¤šæ¨¡æ€é¢†åŸŸï¼Œå‚¬ç”Ÿäº†GPT/BERT/LLaMAç­‰é‡Œç¨‹ç¢‘æ¨¡å‹ã€‚ä»…è§£ç å™¨æ¶æ„ï¼ˆGPTï¼‰å› ç®€å•æ€§å’Œå¯æ‰©å±•æ€§æˆä¸ºä¸»æµï¼ŒéªŒè¯äº†'Scaling Laws'â€”â€”æ€§èƒ½éšè®¡ç®—é‡å¹‚å¾‹å¢é•¿ã€‚RNN/LSTMå·²è¢«å½»åº•æ·˜æ±°ï¼ˆé™¤é—ç•™ç³»ç»Ÿï¼‰ã€‚å½“å‰æŒ‘æˆ˜ï¼šâ‘ ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆnÂ²ç“¶é¢ˆï¼‰â‘¡æ¨ç†æ•ˆç‡â‘¢é«˜æ•ˆTransformerï¼ˆLinear Attention, Mambaï¼‰ã€‚é¢„æµ‹ï¼šTransformerå°†ç»§ç»­ä¸»å¯¼è‡³å°‘åˆ°2030å¹´ï¼Œä½†ä¼šå‡ºç°æ›´é«˜æ•ˆçš„å˜ä½“ã€‚è¿™ä¸ä»…æ˜¯æ¶æ„åˆ›æ–°ï¼Œæ›´æ˜¯èŒƒå¼è½¬å˜â€”â€”ä»å½’çº³åç½®ï¼ˆRNNçš„æ—¶åºå‡è®¾ï¼‰åˆ°æ•°æ®é©±åŠ¨ï¼ˆTransformerçš„é€šç”¨æ€§ï¼‰ã€‚"**

**å…ƒè®¤çŸ¥**:

- **æ ¸å¿ƒé©å‘½**: é¡ºåºâ†’å¹¶è¡Œï¼ˆèŒƒå¼è½¬å˜ï¼‰
- **å…³é”®æœºåˆ¶**: è‡ªæ³¨æ„åŠ›ï¼ˆå…¨å±€å»ºæ¨¡ï¼‰
- **æ€§èƒ½çªç ´**: 10-100Ã—è®­ç»ƒåŠ é€Ÿ
- **ç†è®ºèƒ½åŠ›**: å›¾çµå®Œå¤‡ï¼ˆç†è®ºï¼‰+ SOTAï¼ˆå®è·µï¼‰
- **ä¸»å¯¼æ¶æ„**: ä»…è§£ç å™¨ï¼ˆGPTé£æ ¼ï¼‰
- **å½“å‰æŒ‘æˆ˜**: O(nÂ²)å¤æ‚åº¦ã€ä¸Šä¸‹æ–‡é•¿åº¦
- **æœªæ¥æ–¹å‘**: é«˜æ•ˆTransformerï¼ˆLinear Attn, Mamba, RWKVï¼‰

</details>

---

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒæ¦‚å¿µæ·±åº¦åˆ†æ](#æ ¸å¿ƒæ¦‚å¿µæ·±åº¦åˆ†æ)
- [ğŸ“‹ ç›®å½•](#-ç›®å½•)
- [1. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) | Recurrent Neural Networks](#1-å¾ªç¯ç¥ç»ç½‘ç»œ-rnn--recurrent-neural-networks)
  - [1.1 åŠ¨æœºä¸åŸºæœ¬åŸç†](#11-åŠ¨æœºä¸åŸºæœ¬åŸç†)
  - [1.2 RNN åŸºæœ¬æ¶æ„](#12-rnn-åŸºæœ¬æ¶æ„)
  - [1.3 RNN çš„ç†è®ºèƒ½åŠ›](#13-rnn-çš„ç†è®ºèƒ½åŠ›)
  - [1.4 RNN çš„å®è·µé—®é¢˜](#14-rnn-çš„å®è·µé—®é¢˜)
- [2. LSTM ä¸ GRU | Long Short-Term Memory and Gated Recurrent Units](#2-lstm-ä¸-gru--long-short-term-memory-and-gated-recurrent-units)
  - [2.1 LSTM æ¶æ„](#21-lstm-æ¶æ„)
  - [2.2 GRU æ¶æ„](#22-gru-æ¶æ„)
  - [2.3 åŒå‘ RNN (Bidirectional RNN)](#23-åŒå‘-rnn-bidirectional-rnn)
- [3. æ³¨æ„åŠ›æœºåˆ¶ | Attention Mechanism](#3-æ³¨æ„åŠ›æœºåˆ¶--attention-mechanism)
  - [3.1 æ³¨æ„åŠ›çš„åŠ¨æœº](#31-æ³¨æ„åŠ›çš„åŠ¨æœº)
  - [3.2 Bahdanau æ³¨æ„åŠ›](#32-bahdanau-æ³¨æ„åŠ›)
  - [3.3 è‡ªæ³¨æ„åŠ› (Self-Attention)](#33-è‡ªæ³¨æ„åŠ›-self-attention)
- [4. Transformer æ¶æ„ | Transformer Architecture](#4-transformer-æ¶æ„--transformer-architecture)
  - [4.1 Transformer çš„é©å‘½æ€§çªç ´](#41-transformer-çš„é©å‘½æ€§çªç ´)
  - [4.2 Transformer è¯¦ç»†æ¶æ„](#42-transformer-è¯¦ç»†æ¶æ„)
  - [4.3 Transformer çš„è®¡ç®—å¤æ‚åº¦](#43-transformer-çš„è®¡ç®—å¤æ‚åº¦)
  - [4.4 Transformer å˜ä½“](#44-transformer-å˜ä½“)
- [5. Transformer çš„ç†è®ºèƒ½åŠ› | Theoretical Capabilities](#5-transformer-çš„ç†è®ºèƒ½åŠ›--theoretical-capabilities)
  - [5.1 å›¾çµå®Œå¤‡æ€§åˆ†æ](#51-å›¾çµå®Œå¤‡æ€§åˆ†æ)
  - [5.2 å½¢å¼è¯­è¨€è¯†åˆ«èƒ½åŠ›](#52-å½¢å¼è¯­è¨€è¯†åˆ«èƒ½åŠ›)
  - [5.3 é€šç”¨è¿‘ä¼¼èƒ½åŠ›](#53-é€šç”¨è¿‘ä¼¼èƒ½åŠ›)
- [6. å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ | Applications in Large Language Models](#6-å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨--applications-in-large-language-models)
  - [6.1 ä»…è§£ç å™¨æ¶æ„ (Decoder-Only)](#61-ä»…è§£ç å™¨æ¶æ„-decoder-only)
  - [6.2 ä»…ç¼–ç å™¨æ¶æ„ (Encoder-Only)](#62-ä»…ç¼–ç å™¨æ¶æ„-encoder-only)
  - [6.3 ç¼–ç å™¨-è§£ç å™¨æ¶æ„ (Encoder-Decoder)](#63-ç¼–ç å™¨-è§£ç å™¨æ¶æ„-encoder-decoder)
  - [6.4 ç¼©æ”¾å®šå¾‹ (Scaling Laws)](#64-ç¼©æ”¾å®šå¾‹-scaling-laws)
- [7. æƒå¨å‚è€ƒæ–‡çŒ® | Authoritative References](#7-æƒå¨å‚è€ƒæ–‡çŒ®--authoritative-references)
  - [å¾ªç¯ç¥ç»ç½‘ç»œ](#å¾ªç¯ç¥ç»ç½‘ç»œ)
  - [æ³¨æ„åŠ›æœºåˆ¶](#æ³¨æ„åŠ›æœºåˆ¶)
  - [Transformer](#transformer)
  - [ç†è®ºåˆ†æ](#ç†è®ºåˆ†æ)
  - [ç¼©æ”¾å®šå¾‹](#ç¼©æ”¾å®šå¾‹)
  - [Wikipedia å‚è€ƒ](#wikipedia-å‚è€ƒ)
- [å¯¼èˆª | Navigation](#å¯¼èˆª--navigation)
- [ç›¸å…³ä¸»é¢˜ | Related Topics](#ç›¸å…³ä¸»é¢˜--related-topics)
  - [æœ¬ç« èŠ‚](#æœ¬ç« èŠ‚)
  - [ç›¸å…³ç« èŠ‚](#ç›¸å…³ç« èŠ‚)
  - [è·¨è§†è§’é“¾æ¥](#è·¨è§†è§’é“¾æ¥)

---

## 1. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) | Recurrent Neural Networks

### 1.1 åŠ¨æœºä¸åŸºæœ¬åŸç†

**ä¸ºä»€ä¹ˆéœ€è¦ RNNï¼Ÿ**

å‰é¦ˆç¥ç»ç½‘ç»œçš„å±€é™ï¼š

- å›ºå®šè¾“å…¥ç»´åº¦
- æ— æ³•å¤„ç†åºåˆ—æ•°æ®
- æ— å†…éƒ¨çŠ¶æ€è®°å¿†

**åºåˆ—æ•°æ®çš„ç‰¹ç‚¹**ï¼š

- å¯å˜é•¿åº¦ï¼šå¥å­ã€æ—¶é—´åºåˆ—
- æ—¶é—´ä¾èµ–æ€§ï¼šå½“å‰è¾“å‡ºä¾èµ–å†å²ä¿¡æ¯
- ä½ç½®æ•æ„Ÿï¼šé¡ºåºå¾ˆé‡è¦

### 1.2 RNN åŸºæœ¬æ¶æ„

**æ ‡å‡† RNN å…¬å¼**:

```text
hâ‚œ = Ïƒ(Wâ‚•â‚• hâ‚œâ‚‹â‚ + Wâ‚“â‚• xâ‚œ + bâ‚•)
yâ‚œ = Wâ‚•áµ§ hâ‚œ + báµ§
```

å…¶ä¸­ï¼š

- `xâ‚œ` æ˜¯ t æ—¶åˆ»çš„è¾“å…¥
- `hâ‚œ` æ˜¯ t æ—¶åˆ»çš„éšçŠ¶æ€
- `yâ‚œ` æ˜¯ t æ—¶åˆ»çš„è¾“å‡º
- `Wâ‚•â‚•, Wâ‚“â‚•, Wâ‚•áµ§` æ˜¯æƒé‡çŸ©é˜µ
- `Ïƒ` æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆé€šå¸¸æ˜¯ tanhï¼‰

**å…³é”®ç‰¹æ€§**ï¼š

- æƒé‡å…±äº«ï¼šæ‰€æœ‰æ—¶é—´æ­¥ä½¿ç”¨ç›¸åŒçš„å‚æ•°
- éšçŠ¶æ€ `hâ‚œ` ä½œä¸º"å†…å­˜"ç´¯ç§¯å†å²ä¿¡æ¯
- å¯ä»¥å±•å¼€ä¸ºæ—¶é—´æ­¥çš„å‰é¦ˆç½‘ç»œ

**RNN çš„ç±»å‹**ï¼š

1. **å¤šå¯¹ä¸€** (Many-to-One): æƒ…æ„Ÿåˆ†ç±»
   - è¾“å…¥ï¼šå¥å­ï¼ˆå¤šä¸ªè¯ï¼‰
   - è¾“å‡ºï¼šå•ä¸ªæ ‡ç­¾

2. **ä¸€å¯¹å¤š** (One-to-Many): å›¾åƒæè¿°ç”Ÿæˆ
   - è¾“å…¥ï¼šå•å¼ å›¾åƒ
   - è¾“å‡ºï¼šæè¿°å¥å­

3. **å¤šå¯¹å¤š** (Many-to-Many): æœºå™¨ç¿»è¯‘
   - è¾“å…¥ï¼šæºè¯­è¨€å¥å­
   - è¾“å‡ºï¼šç›®æ ‡è¯­è¨€å¥å­

4. **åŒæ­¥å¤šå¯¹å¤š** (Synced Many-to-Many): è§†é¢‘å¸§æ ‡æ³¨
   - æ¯ä¸ªæ—¶é—´æ­¥éƒ½æœ‰è¾“å…¥å’Œè¾“å‡º

### 1.3 RNN çš„ç†è®ºèƒ½åŠ›

**å®šç†** (Siegelmann & Sontag, 1992):
> å…·æœ‰æœ‰ç†æ•°æƒé‡çš„ RNN å¯ä»¥æ¨¡æ‹Ÿä»»æ„å›¾çµæœºï¼Œå› æ­¤æ˜¯**å›¾çµå®Œå¤‡çš„**ã€‚

**è¯æ˜æ€è·¯**ï¼š

1. éšçŠ¶æ€ `hâ‚œ` å¯ä»¥ç¼–ç å›¾çµæœºçš„å¸¦å­å†…å®¹
2. å¾ªç¯è¿æ¥æä¾›"æ— é™"æ—¶é—´æ­¥
3. éçº¿æ€§æ¿€æ´»å‡½æ•°å®ç°æ¡ä»¶è½¬ç§»

**é‡è¦é™åˆ¶**ï¼š

- éœ€è¦**æ— é™ç²¾åº¦**çš„å®æ•°è¿ç®—
- éœ€è¦**ä»»æ„é•¿åº¦**çš„è®¡ç®—æ—¶é—´
- å®é™… RNN ä½¿ç”¨æœ‰é™ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆä¸å›¾çµå®Œå¤‡ï¼‰

### 1.4 RNN çš„å®è·µé—®é¢˜

**æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜**:

é€šè¿‡æ—¶é—´åå‘ä¼ æ’­ (BPTT, Backpropagation Through Time)ï¼š

```text
âˆ‚L/âˆ‚hâ‚œ = âˆ‚L/âˆ‚hâ‚œâ‚Šâ‚ Â· âˆ‚hâ‚œâ‚Šâ‚/âˆ‚hâ‚œ
       = âˆ‚L/âˆ‚hâ‚œâ‚Šâ‚ Â· Wâ‚•â‚• Â· diag(Ïƒ'(zâ‚œ))
```

ç»è¿‡ T ä¸ªæ—¶é—´æ­¥ï¼š

```text
âˆ‚L/âˆ‚hâ‚€ = âˆ‚L/âˆ‚hâ‚œ Â· (âˆâ‚–â‚Œâ‚áµ€ Wâ‚•â‚• Â· diag(Ïƒ'(zâ‚–)))
```

å¦‚æœ `||Wâ‚•â‚•|| < 1`ï¼šæ¢¯åº¦æ¶ˆå¤± â†’ æ— æ³•å­¦ä¹ é•¿ç¨‹ä¾èµ–  
å¦‚æœ `||Wâ‚•â‚•|| > 1`ï¼šæ¢¯åº¦çˆ†ç‚¸ â†’ è®­ç»ƒä¸ç¨³å®š

**é•¿ç¨‹ä¾èµ–é—®é¢˜**:

ç†è®ºä¸Š RNN å¯ä»¥è®°ä½ä»»æ„é•¿çš„å†å²ï¼Œä½†å®é™…è®­ç»ƒä¸­ï¼š

- éš¾ä»¥å­¦ä¹ è·¨è¶Š >10 ä¸ªæ—¶é—´æ­¥çš„ä¾èµ–
- "é—å¿˜"æ—©æœŸä¿¡æ¯

## 2. LSTM ä¸ GRU | Long Short-Term Memory and Gated Recurrent Units

### 2.1 LSTM æ¶æ„

**LSTM** (Hochreiter & Schmidhuber, 1997) é€šè¿‡**é—¨æ§æœºåˆ¶**è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

**LSTM å…¬å¼**:

```text
é—å¿˜é—¨: fâ‚œ = Ïƒ(Wf Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bf)
è¾“å…¥é—¨: iâ‚œ = Ïƒ(Wi Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bi)
å€™é€‰å€¼: CÌƒâ‚œ = tanh(Wc Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bc)
ç»†èƒçŠ¶æ€æ›´æ–°: Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
è¾“å‡ºé—¨: oâ‚œ = Ïƒ(Wo Â· [hâ‚œâ‚‹â‚, xâ‚œ] + bo)
éšçŠ¶æ€: hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)
```

**å…³é”®ç»„ä»¶**ï¼š

1. **ç»†èƒçŠ¶æ€ (Cell State)** `Câ‚œ`ï¼š
   - ä¿¡æ¯é«˜é€Ÿå…¬è·¯
   - é€šè¿‡åŠ æ³•è€Œéä¹˜æ³•æ›´æ–°ï¼ˆé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰

2. **é—å¿˜é—¨ (Forget Gate)** `fâ‚œ`ï¼š
   - å†³å®šä¸¢å¼ƒå¤šå°‘æ—§ä¿¡æ¯
   - 0 = å®Œå…¨é—å¿˜ï¼Œ1 = å®Œå…¨ä¿ç•™

3. **è¾“å…¥é—¨ (Input Gate)** `iâ‚œ`ï¼š
   - å†³å®šå†™å…¥å¤šå°‘æ–°ä¿¡æ¯

4. **è¾“å‡ºé—¨ (Output Gate)** `oâ‚œ`ï¼š
   - å†³å®šè¾“å‡ºå¤šå°‘ç»†èƒçŠ¶æ€ä¿¡æ¯

**ä¸ºä»€ä¹ˆ LSTM æœ‰æ•ˆï¼Ÿ**

æ¢¯åº¦æµåŠ¨ï¼š

```text
âˆ‚Câ‚œ/âˆ‚Câ‚œâ‚‹â‚ = fâ‚œ
```

ç”±äº `fâ‚œ` æ˜¯é€šè¿‡ sigmoid é—¨æ§çš„ï¼Œæ¢¯åº¦å¯ä»¥è¾ƒå¥½åœ°ä¼ æ’­ï¼Œä¸ä¼šåƒæ ‡å‡† RNN é‚£æ ·å¿«é€Ÿæ¶ˆå¤±ã€‚

### 2.2 GRU æ¶æ„

**GRU** (Cho et al., 2014) æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œæ€§èƒ½ç›¸å½“ä½†å‚æ•°æ›´å°‘ã€‚

**GRU å…¬å¼**:

```text
é‡ç½®é—¨: râ‚œ = Ïƒ(Wr Â· [hâ‚œâ‚‹â‚, xâ‚œ])
æ›´æ–°é—¨: zâ‚œ = Ïƒ(Wz Â· [hâ‚œâ‚‹â‚, xâ‚œ])
å€™é€‰éšçŠ¶æ€: hÌƒâ‚œ = tanh(W Â· [râ‚œ âŠ™ hâ‚œâ‚‹â‚, xâ‚œ])
éšçŠ¶æ€: hâ‚œ = (1 - zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ
```

**ä¸ LSTM çš„å¯¹æ¯”**ï¼š

- åªæœ‰ 2 ä¸ªé—¨ï¼ˆvs. LSTM çš„ 3 ä¸ªï¼‰
- æ²¡æœ‰ç‹¬ç«‹çš„ç»†èƒçŠ¶æ€
- å‚æ•°æ•°é‡å‡å°‘çº¦ 25%
- è®­ç»ƒé€Ÿåº¦æ›´å¿«

### 2.3 åŒå‘ RNN (Bidirectional RNN)

**åŠ¨æœº**ï¼šæœªæ¥ä¿¡æ¯ä¹Ÿå¯èƒ½æœ‰ç”¨ï¼ˆä¾‹å¦‚ï¼Œå¥å­ç†è§£ï¼‰

**æ¶æ„**ï¼š

```text
å‰å‘: hâƒ—â‚œ = RNN_forward(xâ‚œ, hâƒ—â‚œâ‚‹â‚)
åå‘: hâƒ–â‚œ = RNN_backward(xâ‚œ, hâƒ–â‚œâ‚Šâ‚)
è¾“å‡º: yâ‚œ = f([hâƒ—â‚œ; hâƒ–â‚œ])
```

**åº”ç”¨**ï¼š

- BERT ä½¿ç”¨åŒå‘ç¼–ç 
- å‘½åå®ä½“è¯†åˆ« (NER)
- è¯æ€§æ ‡æ³¨ (POS Tagging)

## 3. æ³¨æ„åŠ›æœºåˆ¶ | Attention Mechanism

### 3.1 æ³¨æ„åŠ›çš„åŠ¨æœº

**Seq2Seq æ¨¡å‹çš„ç“¶é¢ˆ**:

ä¼ ç»Ÿ Encoder-Decoder æ¶æ„ï¼š

```text
Encoder: hâ‚, hâ‚‚, ..., hâ‚™ â†’ c (å›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡å‘é‡)
Decoder: c â†’ yâ‚, yâ‚‚, ..., yâ‚˜
```

**é—®é¢˜**ï¼š

- æ‰€æœ‰æºä¿¡æ¯å‹ç¼©åˆ°å›ºå®šé•¿åº¦å‘é‡ `c`
- ä¿¡æ¯ç“¶é¢ˆï¼šé•¿å¥å­æ€§èƒ½ä¸‹é™
- æ— æ³•å¯¹é½æºå’Œç›®æ ‡çš„ä¸åŒéƒ¨åˆ†

### 3.2 Bahdanau æ³¨æ„åŠ›

**Bahdanau Attention** (Bahdanau et al., 2014)

åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ tï¼Œè®¡ç®—å¯¹æ‰€æœ‰ç¼–ç å™¨éšçŠ¶æ€çš„åŠ æƒå’Œï¼š

```text
1. è®¡ç®—å¯¹é½åˆ†æ•°:
   eâ‚œáµ¢ = a(sâ‚œâ‚‹â‚, háµ¢)  # è¯„åˆ†å‡½æ•°

2. è®¡ç®—æ³¨æ„åŠ›æƒé‡:
   Î±â‚œáµ¢ = exp(eâ‚œáµ¢) / âˆ‘â±¼ exp(eâ‚œâ±¼)  # softmax

3. è®¡ç®—ä¸Šä¸‹æ–‡å‘é‡:
   câ‚œ = âˆ‘áµ¢ Î±â‚œáµ¢ háµ¢

4. ç”Ÿæˆè¾“å‡º:
   sâ‚œ = f(sâ‚œâ‚‹â‚, yâ‚œâ‚‹â‚, câ‚œ)
```

**è¯„åˆ†å‡½æ•°** (Alignment Function):

1. **åŠ æ³•æ³¨æ„åŠ›** (Bahdanau):

   ```text
   a(s, h) = váµƒ^T tanh(Wâ‚s + Uâ‚h)
   ```

2. **ç‚¹ç§¯æ³¨æ„åŠ›** (Luong):

   ```text
   a(s, h) = s^T h
   ```

3. **ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›** (Scaled Dot-Product):

   ```text
   a(s, h) = (s^T h) / âˆšdâ‚–
   ```

**æ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§**ï¼š

- æ³¨æ„åŠ›æƒé‡ `Î±â‚œáµ¢` æ˜¾ç¤ºæ¨¡å‹"å…³æ³¨"æºåºåˆ—çš„å“ªäº›éƒ¨åˆ†
- å¯è§†åŒ–æ³¨æ„åŠ›çƒ­å›¾ (Attention Heatmap)

### 3.3 è‡ªæ³¨æ„åŠ› (Self-Attention)

**è‡ªæ³¨æ„åŠ›**ï¼šåºåˆ—å¯¹è‡ªèº«çš„æ³¨æ„åŠ›

å¯¹äºè¾“å…¥åºåˆ— `X = [xâ‚, xâ‚‚, ..., xâ‚™]`ï¼š

```text
1. ç”Ÿæˆ Query, Key, Value:
   Q = XWQ,  K = XWK,  V = XWV

2. è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†:
   Attention(Q, K, V) = softmax(QK^T / âˆšdâ‚–) V
```

**å«ä¹‰**ï¼š

- æ¯ä¸ªä½ç½®å¯ä»¥å…³æ³¨æ‰€æœ‰å…¶ä»–ä½ç½®
- æ•è·åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»
- ä¸ä¾èµ–å¾ªç¯ç»“æ„ï¼Œå¯å¹¶è¡Œè®¡ç®—

## 4. Transformer æ¶æ„ | Transformer Architecture

### 4.1 Transformer çš„é©å‘½æ€§çªç ´

**Transformer** (Vaswani et al., 2017) å½»åº•æŠ›å¼ƒäº†å¾ªç¯ç»“æ„ã€‚

**æ ¸å¿ƒåˆ›æ–°**ï¼š

1. **å®Œå…¨åŸºäºæ³¨æ„åŠ›**ï¼šæ—  RNN/CNN
2. **å¹¶è¡ŒåŒ–**ï¼šæ‰€æœ‰ä½ç½®åŒæ—¶å¤„ç†
3. **ä½ç½®ç¼–ç **ï¼šæ˜¾å¼æ³¨å…¥ä½ç½®ä¿¡æ¯
4. **å¤šå¤´æ³¨æ„åŠ›**ï¼šå¤šä¸ªæ³¨æ„åŠ›å­ç©ºé—´

**è®ºæ–‡æ ‡é¢˜**ï¼š"Attention Is All You Need"

### 4.2 Transformer è¯¦ç»†æ¶æ„

**æ•´ä½“ç»“æ„**:

```text
Encoder (Ã— N å±‚):
  Multi-Head Self-Attention
  â†’ Add & Norm
  â†’ Feed-Forward Network
  â†’ Add & Norm

Decoder (Ã— N å±‚):
  Masked Multi-Head Self-Attention
  â†’ Add & Norm
  â†’ Multi-Head Cross-Attention (over encoder output)
  â†’ Add & Norm
  â†’ Feed-Forward Network
  â†’ Add & Norm
```

**å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)**:

```text
MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•) WO

å…¶ä¸­: headáµ¢ = Attention(QWáµ¢Q, KWáµ¢K, VWáµ¢V)
```

**å‚æ•°**ï¼š

- `h` ä¸ªå¤´ï¼ˆé€šå¸¸ h = 8 æˆ– 16ï¼‰
- æ¯ä¸ªå¤´çš„ç»´åº¦ `dâ‚– = d_model / h`

**ä¼˜åŠ¿**ï¼š

- ä¸åŒçš„å¤´å¯ä»¥å…³æ³¨ä¸åŒçš„æ¨¡å¼
- æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›

**ä½ç½®ç¼–ç  (Positional Encoding)**:

ç”±äº Transformer æ— å¾ªç¯ç»“æ„ï¼Œéœ€è¦æ˜¾å¼ç¼–ç ä½ç½®ä¿¡æ¯ï¼š

```text
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**ç‰¹æ€§**ï¼š

- ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼š`PE(pos+k)` å¯ä»¥è¡¨ç¤ºä¸º `PE(pos)` çš„çº¿æ€§å‡½æ•°
- å¯å¤–æ¨åˆ°æ›´é•¿åºåˆ—ï¼ˆç†è®ºä¸Šï¼‰

**å‰é¦ˆç½‘ç»œ (Feed-Forward Network)**:

```text
FFN(x) = ReLU(xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```

- æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨
- é€šå¸¸ç¬¬ä¸€å±‚æ‰©å±•ç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œ512 â†’ 2048ï¼‰
- ç¬¬äºŒå±‚æ¢å¤ç»´åº¦ï¼ˆ2048 â†’ 512ï¼‰

**æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–**:

```text
x' = LayerNorm(x + Sublayer(x))
```

- æ®‹å·®è¿æ¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- Layer Normalization ç¨³å®šè®­ç»ƒ

### 4.3 Transformer çš„è®¡ç®—å¤æ‚åº¦

**è‡ªæ³¨æ„åŠ›å¤æ‚åº¦åˆ†æ**:

| å±‚ç±»å‹ | æ¯å±‚å¤æ‚åº¦ | é¡ºåºæ“ä½œæ•° | æœ€å¤§è·¯å¾„é•¿åº¦ |
|--------|-----------|-----------|-------------|
| Self-Attention | O(nÂ²Â·d) | O(1) | O(1) |
| RNN | O(nÂ·dÂ²) | O(n) | O(n) |
| CNN | O(kÂ·nÂ·dÂ²) | O(1) | O(log_k(n)) |

å…¶ä¸­ï¼š

- `n` = åºåˆ—é•¿åº¦
- `d` = è¡¨ç¤ºç»´åº¦
- `k` = å·ç§¯æ ¸å¤§å°

**Transformer çš„ä¼˜åŠ¿**ï¼š

- âœ… æœ€å¤§è·¯å¾„é•¿åº¦ä¸º O(1)ï¼ˆä»»æ„ä¸¤ä½ç½®ç›´æ¥ç›¸è¿ï¼‰
- âœ… å¯å¹¶è¡ŒåŒ–ï¼ˆé¡ºåºæ“ä½œæ•° O(1)ï¼‰
- âŒ å¯¹äºé•¿åºåˆ—ï¼Œå¤æ‚åº¦ä¸º O(nÂ²)ï¼Œå†…å­˜å ç”¨å¤§

### 4.4 Transformer å˜ä½“

ä¸ºè§£å†³ O(nÂ²) å¤æ‚åº¦é—®é¢˜ï¼Œæå‡ºäº†å¤šç§é«˜æ•ˆ Transformerï¼š

1. **Sparse Transformer** (Child et al., 2019)
   - ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
   - å¤æ‚åº¦ï¼šO(nâˆšn)

2. **Linformer** (Wang et al., 2020)
   - ä½ç§©åˆ†è§£æ³¨æ„åŠ›çŸ©é˜µ
   - å¤æ‚åº¦ï¼šO(n)

3. **Performer** (Choromanski et al., 2020)
   - ä½¿ç”¨æ ¸æ–¹æ³•è¿‘ä¼¼æ³¨æ„åŠ›
   - å¤æ‚åº¦ï¼šO(n)

4. **FlashAttention** (Dao et al., 2022)
   - IO æ„ŸçŸ¥çš„æ³¨æ„åŠ›ç®—æ³•
   - ä¿æŒ O(nÂ²) ä½†å¤§å¹…åŠ é€Ÿå®é™…è¿è¡Œ

## 5. Transformer çš„ç†è®ºèƒ½åŠ› | Theoretical Capabilities

### 5.1 å›¾çµå®Œå¤‡æ€§åˆ†æ

**é—®é¢˜**ï¼šTransformer æ˜¯å›¾çµå®Œå¤‡çš„å—ï¼Ÿ

**ç ”ç©¶ç»“æœ**ï¼š

1. **PÃ©rez et al. (2019)**:
   - æ ‡å‡† Transformerï¼ˆç¡¬æ³¨æ„åŠ›ï¼‰**ä¸æ˜¯**å›¾çµå®Œå¤‡çš„
   - æ— æ³•å®ç°é€šç”¨å›¾çµæœº

2. **Dehghani et al. (2018) - Universal Transformers**:
   - æ·»åŠ å¾ªç¯æ·±åº¦ï¼ˆåŠ¨æ€å±‚æ•°ï¼‰
   - **æ˜¯**å›¾çµå®Œå¤‡çš„

3. **Bhattamishra et al. (2020)**:
   - åˆ†æäº† Transformer å¯¹å½¢å¼è¯­è¨€çš„è¯†åˆ«èƒ½åŠ›
   - PARITY, DYCK ç­‰ä»»åŠ¡çš„ç†è®ºç•Œé™

**å…³é”®é™åˆ¶**ï¼š

- å›ºå®šæ·±åº¦ Transformer = å›ºå®šæ—¶é—´è®¡ç®—
- ä½ç½®ç¼–ç çš„æœ‰é™æ€§é™åˆ¶äº†åºåˆ—é•¿åº¦
- æµ®ç‚¹ç²¾åº¦é™åˆ¶äº†çŠ¶æ€ç©ºé—´

### 5.2 å½¢å¼è¯­è¨€è¯†åˆ«èƒ½åŠ›

**å®éªŒç»“æœ** (Bhattamishra et al., 2020):

| è¯­è¨€ç±»å‹ | Transformer æ€§èƒ½ |
|---------|-----------------|
| Regular (PARITY, EVEN PAIRS) | âœ… å¯å­¦ä¹  |
| Counter (aâ¿bâ¿) | âœ… å¯å­¦ä¹ ï¼ˆæœ‰æ·±åº¦é™åˆ¶ï¼‰ |
| Context-Free (DYCK) | âš ï¸ éƒ¨åˆ†æˆåŠŸ |
| Context-Sensitive | âŒ å¤±è´¥ |

**ç†è®ºè§£é‡Š**ï¼š

- Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ç±»ä¼¼äº Counter Automaton
- æ·±åº¦ d çš„ Transformer å¯ä»¥æ¨¡æ‹Ÿ O(d) ä¸ªè®¡æ•°å™¨
- ä½†è®¡æ•°å™¨ç²¾åº¦å—é™äºæµ®ç‚¹æ•°ç²¾åº¦

### 5.3 é€šç”¨è¿‘ä¼¼èƒ½åŠ›

**å®šç†** (Yun et al., 2020):
> Transformer å¯ä»¥è¿‘ä¼¼ä»»ä½•è¿ç»­åºåˆ—åˆ°åºåˆ—å‡½æ•°ï¼ˆåœ¨ç´§é›†ä¸Šï¼‰ã€‚

**å«ä¹‰**ï¼š

- Transformer å…·æœ‰åºåˆ—å‡½æ•°çš„é€šç”¨è¿‘ä¼¼èƒ½åŠ›
- ä½†è¿™ä»ç„¶æ˜¯å­˜åœ¨æ€§å®šç†ï¼Œä¸ä¿è¯å¯å­¦ä¹ æ€§

## 6. å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ | Applications in Large Language Models

### 6.1 ä»…è§£ç å™¨æ¶æ„ (Decoder-Only)

**GPT ç³»åˆ—** (Radford et al., 2018/2019; Brown et al., 2020)

```text
æ¶æ„: 
  Masked Multi-Head Self-Attention (å› æœæ©ç )
  â†’ Add & Norm
  â†’ Feed-Forward
  â†’ Add & Norm

è®­ç»ƒç›®æ ‡: ä¸‹ä¸€ä¸ª token é¢„æµ‹ (Next Token Prediction)
  P(x_t | x_1, x_2, ..., x_{t-1})
```

**ç‰¹ç‚¹**ï¼š

- è‡ªå›å½’ç”Ÿæˆ
- å› æœæ©ç ç¡®ä¿å•å‘æ³¨æ„åŠ›
- é€‚åˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡

**è§„æ¨¡æ¼”è¿›**ï¼š

- GPT-1: 117M å‚æ•°
- GPT-2: 1.5B å‚æ•°
- GPT-3: 175B å‚æ•°
- GPT-4: ä¼°è®¡ 1.76T å‚æ•°ï¼ˆæ··åˆä¸“å®¶æ¨¡å‹ï¼‰

### 6.2 ä»…ç¼–ç å™¨æ¶æ„ (Encoder-Only)

**BERT** (Devlin et al., 2018)

```text
æ¶æ„: 
  Multi-Head Self-Attention (åŒå‘)
  â†’ Add & Norm
  â†’ Feed-Forward
  â†’ Add & Norm

è®­ç»ƒç›®æ ‡:
  1. Masked Language Modeling (MLM)
     é¢„æµ‹è¢«æ©ç çš„ token
  2. Next Sentence Prediction (NSP)
     åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­
```

**ç‰¹ç‚¹**ï¼š

- åŒå‘ç¼–ç 
- é€‚åˆç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERã€QAï¼‰
- éœ€è¦ fine-tuning

### 6.3 ç¼–ç å™¨-è§£ç å™¨æ¶æ„ (Encoder-Decoder)

**T5** (Raffel et al., 2020), **BART** (Lewis et al., 2020)

```text
Encoder: åŒå‘è‡ªæ³¨æ„åŠ›ï¼ˆç†è§£è¾“å…¥ï¼‰
Decoder: å› æœè‡ªæ³¨æ„åŠ›ï¼ˆç”Ÿæˆè¾“å‡ºï¼‰+ äº¤å‰æ³¨æ„åŠ›ï¼ˆç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼‰
```

**ç»Ÿä¸€æ¡†æ¶**ï¼š

- æ‰€æœ‰ä»»åŠ¡éƒ½è§†ä¸º Seq2Seq
- "ç¿»è¯‘è‹±è¯­åˆ°æ³•è¯­ï¼š[è¾“å…¥]" â†’ [è¾“å‡º]

### 6.4 ç¼©æ”¾å®šå¾‹ (Scaling Laws)

**Kaplan et al. (2020)**: OpenAI çš„ç¼©æ”¾å®šå¾‹

```text
Loss ~ N^(-Î±)
```

å…¶ä¸­ï¼š

- `N` = å‚æ•°æ•°é‡ã€æ•°æ®é›†å¤§å°æˆ–è®¡ç®—é‡
- `Î± â‰ˆ 0.076`ï¼ˆå‚æ•°ï¼‰

**å…³é”®å‘ç°**ï¼š

1. æ›´å¤§çš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼ˆå¹‚å¾‹å…³ç³»ï¼‰
2. æœ€ä¼˜é…ç½®ï¼šæ¨¡å‹å¤§å°ä¸æ•°æ®é‡åº”æˆæ¯”ä¾‹
3. è¿‡åº¦è®­ç»ƒï¼ˆå¤šè½®ï¼‰ä¸å¦‚å¢å¤§æ•°æ®é›†

**Hoffmann et al. (2022)**: Chinchilla ç¼©æ”¾å®šå¾‹

ä¿®æ­£äº† Kaplan çš„ç»“è®ºï¼š

- å¯¹äºç»™å®šè®¡ç®—é¢„ç®—ï¼Œæ•°æ®é‡åº”ä¸æ¨¡å‹å¤§å°ç›¸åŒ¹é…
- Chinchilla (70B) ä¼˜äº Gopher (280B)ï¼Œä½†è®­ç»ƒæ•°æ® 4Ã— æ›´å¤š

## 7. æƒå¨å‚è€ƒæ–‡çŒ® | Authoritative References

### å¾ªç¯ç¥ç»ç½‘ç»œ

1. **Elman, J. L.** (1990). "Finding structure in time." *Cognitive Science*, 14(2), 179-211.
   - ç®€å• RNN çš„æ—©æœŸè®ºæ–‡

2. **Hochreiter, S., & Schmidhuber, J.** (1997). "Long short-term memory." *Neural Computation*, 9(8), 1735-1780.
   - LSTM çš„åŸå§‹è®ºæ–‡

3. **Cho, K., et al.** (2014). "Learning phrase representations using RNN encoder-decoder for statistical machine translation." *EMNLP*.
   - GRU çš„æå‡º

4. **Siegelmann, H. T., & Sontag, E. D.** (1992). "On the computational power of neural nets." *COLT*.
   - RNN å›¾çµå®Œå¤‡æ€§è¯æ˜

### æ³¨æ„åŠ›æœºåˆ¶

1. **Bahdanau, D., Cho, K., & Bengio, Y.** (2014). "Neural machine translation by jointly learning to align and translate." *ICLR 2015*.
   - ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶

2. **Luong, M. T., Pham, H., & Manning, C. D.** (2015). "Effective approaches to attention-based neural machine translation." *EMNLP*.
   - Luong æ³¨æ„åŠ›

### Transformer

1. **Vaswani, A., et al.** (2017). "Attention is all you need." *NeurIPS*.
   - Transformer çš„åŸå§‹è®ºæ–‡

2. **Devlin, J., et al.** (2018). "BERT: Pre-training of deep bidirectional transformers for language understanding." *NAACL 2019*.
   - BERT æ¨¡å‹

3. **Radford, A., et al.** (2018). "Improving language understanding by generative pre-training."
   - GPT-1

4. **Brown, T. B., et al.** (2020). "Language models are few-shot learners." *NeurIPS*.
    - GPT-3

### ç†è®ºåˆ†æ

1. **PÃ©rez, J., et al.** (2019). "On the Turing completeness of modern neural network architectures." *ICLR 2019*.
    - Transformer è®¡ç®—èƒ½åŠ›åˆ†æ

2. **Bhattamishra, S., et al.** (2020). "On the ability and limitations of transformers to recognize formal languages." *EMNLP 2020*.
    - Transformer çš„å½¢å¼è¯­è¨€è¯†åˆ«èƒ½åŠ›

3. **Yun, C., et al.** (2020). "Are transformers universal approximators of sequence-to-sequence functions?" *ICLR 2020*.
    - Transformer çš„é€šç”¨è¿‘ä¼¼å®šç†

### ç¼©æ”¾å®šå¾‹

1. **Kaplan, J., et al.** (2020). "Scaling laws for neural language models." *arXiv:2001.08361*.
    - OpenAI ç¼©æ”¾å®šå¾‹

2. **Hoffmann, J., et al.** (2022). "Training compute-optimal large language models." *arXiv:2203.15556*.
    - Chinchilla ç¼©æ”¾å®šå¾‹

### Wikipedia å‚è€ƒ

1. **Recurrent Neural Network**: <https://en.wikipedia.org/wiki/Recurrent_neural_network>
2. **Long Short-Term Memory**: <https://en.wikipedia.org/wiki/Long_short-term_memory>
3. **Attention (machine learning)**: <https://en.wikipedia.org/wiki/Attention_(machine_learning)>
4. **Transformer (machine learning model)**: <https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)>
5. **BERT (language model)**: <https://en.wikipedia.org/wiki/BERT_(language_model)>
6. **GPT-3**: <https://en.wikipedia.org/wiki/GPT-3>

---

**æœ¬æ–‡æ¡£å»ºç«‹æ—¶é—´**: 2025-10-23  
**ç‰ˆæœ¬**: 1.0  
**çŠ¶æ€**: âœ… å®Œæˆ - åŒ…å«æƒå¨å¼•ç”¨å’Œæ¦‚å¿µå¯¹é½

---

## å¯¼èˆª | Navigation

**ä¸Šä¸€ç¯‡**: [â† 02.1 ç¥ç»ç½‘ç»œåŸºç¡€](./02.1_Neural_Network_Foundations.md)  
**ä¸‹ä¸€ç¯‡**: [02.3 å›¾çµå®Œå¤‡æ€§åˆ†æ â†’](./02.3_Turing_Completeness_Analysis.md)  
**è¿”å›ç›®å½•**: [â†‘ AIæ¨¡å‹è§†è§’æ€»è§ˆ](../README.md)

---

## ç›¸å…³ä¸»é¢˜ | Related Topics

### æœ¬ç« èŠ‚

- [02.1 ç¥ç»ç½‘ç»œåŸºç¡€](./02.1_Neural_Network_Foundations.md)
- [02.3 å›¾çµå®Œå¤‡æ€§åˆ†æ](./02.3_Turing_Completeness_Analysis.md)
- [02.4 Transformeræ¶æ„](./02.4_Transformer_Architecture.md)
- [02.5 é€šç”¨é€¼è¿‘å®šç†](./02.5_Universal_Approximation_Theorem.md)

### ç›¸å…³ç« èŠ‚

- [03.3 Transformer LLMç†è®º](../03_Language_Models/03.3_Transformer_LLM_Theory.md)
- [04.2 è¿ç»­è¡¨ç¤ºç†è®º](../04_Semantic_Models/04.2_Continuous_Representation_Theory.md)

### è·¨è§†è§’é“¾æ¥

- [Software_Perspective: æ¶æ„æ¼”è¿›](../../Software_Perspective/02_Architecture_Sink/02.5_Sink_Stage_Model.md)
- [FormalLanguage_Perspective](../../FormalLanguage_Perspective/README.md)
- [Information_Theory_Perspective](../../Information_Theory_Perspective/README.md)
