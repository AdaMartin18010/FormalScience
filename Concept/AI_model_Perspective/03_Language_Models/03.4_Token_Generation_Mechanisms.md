# Token 生成机制 | Token Generation Mechanisms

## 目录 | Table of Contents

- [Token 生成机制 | Token Generation Mechanisms](#token-生成机制--token-generation-mechanisms)
  - [目录 | Table of Contents](#目录--table-of-contents)
  - [概述 | Overview](#概述--overview)
  - [1. Token化基础 | Tokenization Fundamentals](#1-token化基础--tokenization-fundamentals)
    - [1.1 什么是Token](#11-什么是token)
    - [1.2 BPE (Byte Pair Encoding)](#12-bpe-byte-pair-encoding)
    - [1.3 WordPiece (BERT使用)](#13-wordpiece-bert使用)
    - [1.4 SentencePiece](#14-sentencepiece)
  - [2. 从模型输出到Token | From Model Output to Tokens](#2-从模型输出到token--from-model-output-to-tokens)
    - [2.1 模型输出](#21-模型输出)
    - [2.2 Softmax归一化](#22-softmax归一化)
    - [2.3 Top-k/Top-p过滤](#23-top-ktop-p过滤)
    - [2.4 温度缩放](#24-温度缩放)
  - [3. 解码策略 | Decoding Strategies](#3-解码策略--decoding-strategies)
    - [3.1 贪心解码 (Greedy Decoding)](#31-贪心解码-greedy-decoding)
    - [3.2 Beam Search](#32-beam-search)
    - [3.3 采样 (Sampling)](#33-采样-sampling)
    - [3.4 Top-k 采样](#34-top-k-采样)
    - [3.5 Top-p (Nucleus) 采样](#35-top-p-nucleus-采样)
    - [3.6 对比矩阵](#36-对比矩阵)
  - [4. 高级技术 | Advanced Techniques](#4-高级技术--advanced-techniques)
    - [4.1 对比搜索 (Contrastive Search)](#41-对比搜索-contrastive-search)
    - [4.2 重复惩罚 (Repetition Penalty)](#42-重复惩罚-repetition-penalty)
    - [4.3 长度惩罚 (Length Penalty)](#43-长度惩罚-length-penalty)
    - [4.4 强制解码 (Constrained Decoding)](#44-强制解码-constrained-decoding)
  - [5. 评估指标 | Evaluation Metrics](#5-评估指标--evaluation-metrics)
    - [5.1 困惑度 (Perplexity)](#51-困惑度-perplexity)
    - [5.2 BLEU Score](#52-bleu-score)
    - [5.3 人类评估](#53-人类评估)
    - [5.4 自动评估的挑战](#54-自动评估的挑战)
  - [6. 实践建议 | Practical Recommendations](#6-实践建议--practical-recommendations)
    - [6.1 默认配置](#61-默认配置)
    - [6.2 调参指南](#62-调参指南)
    - [6.3 常见问题排查](#63-常见问题排查)
  - [7. 权威参考文献 | Authoritative References](#7-权威参考文献--authoritative-references)
    - [学术论文](#学术论文)
    - [实践资源](#实践资源)
  - [8. 关键要点总结 | Key Takeaways](#8-关键要点总结--key-takeaways)

---

## 概述 | Overview

Token生成是大语言模型的核心输出机制。本文档系统分析从概率分布采样到各种解码策略的技术细节。

## 1. Token化基础 | Tokenization Fundamentals

### 1.1 什么是Token

**Token**：文本的基本单位

**粒度选择**：

- **字符级**：`"hello"` → `['h', 'e', 'l', 'l', 'o']`
- **词级**：`"New York"` → `['New', 'York']`
- **子词级**：`"unhappiness"` → `['un', 'happiness']`

**现代标准**：子词级（BPE, WordPiece, SentencePiece）

### 1.2 BPE (Byte Pair Encoding)

**算法**（训练）：

```text
1. 初始化：所有字符为token
2. 重复：
   a. 找最频繁的token对
   b. 合并为新token
   c. 直到达到词汇表大小
```

**例子**：

```text
初始："low", "lowest", "newer"
迭代1：合并 'lo' → token
迭代2：合并 'low' → token
...
结果词汇表：['l', 'o', 'w', 'lo', 'low', 'e', 'r', ...]
```

**推理**（编码）：

```text
贪心地应用学到的合并规则
```

**优势**：

- 平衡词汇表大小与序列长度
- 无OOV（out-of-vocabulary）
- 学习常见词缀

### 1.3 WordPiece (BERT使用)

**与BPE的区别**：

```text
BPE：基于频率
WordPiece：基于似然
```

选择使训练数据似然最大的合并

### 1.4 SentencePiece

**特点**：

- 直接从原始文本训练
- 不需要预分词
- 支持多种语言

**使用**：T5, mT5, XLM-R

## 2. 从模型输出到Token | From Model Output to Tokens

### 2.1 模型输出

**Transformer最后一层**：

```text
logits = W_output · h_final + b
```

其中：

- `logits ∈ ℝ^|V|`（V是词汇表）
- 每个维度对应一个token

### 2.2 Softmax归一化

**转换为概率分布**：

```text
P(token_i) = exp(logits_i) / ∑_j exp(logits_j)
```

**性质**：

- `∑_i P(token_i) = 1`
- `P(token_i) > 0`
- 归一化的概率分布

### 2.3 Top-k/Top-p过滤

**Top-k采样**：

```text
1. 选择概率最高的k个token
2. 重新归一化
3. 从这k个中采样
```

**Top-p (Nucleus) 采样**：

```text
1. 按概率降序排列token
2. 选择累积概率≥p的最小集合
3. 重新归一化并采样
```

**例子**（p=0.9）：

```text
原始分布：
P(the) = 0.7
P(a) = 0.15
P(an) = 0.10
P(this) = 0.04
P(that) = 0.01

累积：0.7, 0.85, 0.95, ...
选择：{"the", "a", "an"}（累积>0.9）
```

### 2.4 温度缩放

**调整分布锐度**：

```text
P_temp(token_i) = exp(logits_i / T) / ∑_j exp(logits_j / T)
```

**效果**：

- `T < 1`：更确定（峰更尖）
- `T = 1`：原始分布
- `T > 1`：更随机（更平坦）
- `T → 0`：贪心（argmax）
- `T → ∞`：均匀分布

**例子**：

```text
原始logits：[3.0, 2.0, 1.0]

T=0.5（确定）：
P = [0.84, 0.14, 0.02]

T=1.0（原始）：
P = [0.67, 0.24, 0.09]

T=2.0（随机）：
P = [0.49, 0.31, 0.19]
```

## 3. 解码策略 | Decoding Strategies

### 3.1 贪心解码 (Greedy Decoding)

**算法**：

```text
for t in 1..max_length:
    token_t = argmax P(token | token_1, ..., token_{t-1})
    output token_t
```

**优点**：

- ✅ 快速
- ✅ 确定性
- ✅ 简单

**缺点**：

- ❌ 局部最优
- ❌ 重复问题
- ❌ 无法纠正早期错误

**适用**：

- 翻译（有明确答案）
- 代码生成

### 3.2 Beam Search

**核心思想**：维护top-k假设

**算法**：

```text
初始化：beam = [(<start>, 0)]  # (序列, 分数)

for t in 1..max_length:
    candidates = []
    for each sequence in beam:
        for each token in vocabulary:
            new_seq = sequence + token
            new_score = score + log P(token | sequence)
            candidates.append((new_seq, new_score))
    
    beam = top_k(candidates, k=beam_size)
    
返回：best sequence in beam
```

**例子**（beam_size=2）：

```text
Step 0:
["<s>"]

Step 1:
["<s> The"] (score=-0.5)
["<s> A"] (score=-1.2)

Step 2:
["<s> The cat"] (score=-1.0)
["<s> The dog"] (score=-1.3)

...
```

**分数归一化**：

```text
按长度归一化：score / length
避免偏向短序列
```

**优点**：

- ✅ 比贪心更好
- ✅ 适中的计算成本
- ✅ 翻译等任务的标准

**缺点**：

- ❌ 仍可能重复
- ❌ 生成缺乏多样性
- ❌ beam_size需要调优

### 3.3 采样 (Sampling)

**纯随机采样**：

```text
token_t ~ P(· | token_1, ..., token_{t-1})
```

**优点**：

- ✅ 多样性
- ✅ 创造性
- ✅ 避免重复

**缺点**：

- ❌ 可能不连贯
- ❌ 质量不稳定
- ❌ 可能采样到低概率token

**改进**：Top-k, Top-p, 温度

### 3.4 Top-k 采样

**Radford et al. (2019)**:

**算法**：

```text
1. 选择概率最高的k个token
2. 重新归一化
3. 从中采样
```

**典型k值**：40-50

**优点**：

- ✅ 过滤低质量token
- ✅ 保持多样性

**缺点**：

- ❌ k固定不灵活
- ❌ 概率分布形状变化大

### 3.5 Top-p (Nucleus) 采样

**Holtzman et al. (2019)**:

**动态选择候选集**：

```text
选择最小的token集合，累积概率≥p
```

**典型p值**：0.9-0.95

**优点**：

- ✅ 自适应候选集大小
- ✅ 平衡质量与多样性
- ✅ 现代LLM的默认

**对比Top-k**：

```text
情况1：分布尖锐
P = [0.95, 0.02, 0.01, ...]
Top-k (k=5)：包含很多低概率token
Top-p (p=0.95)：只选第一个✅

情况2：分布平坦
P = [0.15, 0.14, 0.13, 0.12, ...]
Top-k (k=5)：可能太少
Top-p (p=0.9)：动态选7个✅
```

### 3.6 对比矩阵

| 策略 | 确定性 | 质量 | 多样性 | 速度 | 适用场景 |
|------|-------|------|-------|------|---------|
| **贪心** | 完全 | 中 | 低 | 快 | 翻译 |
| **Beam Search** | 部分 | 高 | 低 | 中 | 摘要、翻译 |
| **纯采样** | 随机 | 不稳定 | 高 | 快 | 创意写作 |
| **Top-k** | 随机 | 较好 | 中高 | 快 | 对话 |
| **Top-p** | 随机 | 好 | 中高 | 快 | **通用首选** |

## 4. 高级技术 | Advanced Techniques

### 4.1 对比搜索 (Contrastive Search)

**Su et al. (2022)**:

**目标**：平衡流畅性与多样性

**公式**：

```text
token_t = argmax [α · P(token | 前文) - (1-α) · max_similarity(token, 前文tokens)]
```

**思想**：

- 高概率（流畅）
- 低相似度（避免重复）

### 4.2 重复惩罚 (Repetition Penalty)

**调整已出现token的logit**：

```text
if token in previous_tokens:
    logit_token = logit_token / penalty
```

`penalty > 1`：降低重复概率

**问题**：

- 可能过度惩罚（合理重复也被惩罚）
- 需要调参

### 4.3 长度惩罚 (Length Penalty)

**Beam Search中**：

```text
score = log_prob / length^α
```

- `α < 1`：鼓励更长
- `α > 1`：鼓励更短
- `α = 1`：中性

### 4.4 强制解码 (Constrained Decoding)

**应用场景**：

- 格式控制（JSON输出）
- 关键词包含
- 语法约束

**实现**：

- 修改logit：不合法token设为-∞
- 有限状态机引导

## 5. 评估指标 | Evaluation Metrics

### 5.1 困惑度 (Perplexity)

**定义**：

```text
PPL = exp(-1/N ∑ log P(token_i | 前文))
```

**意义**：

- 模型对测试数据的"惊讶"程度
- 越低越好
- 但与生成质量不完全对应

### 5.2 BLEU Score

**用于翻译等任务**：

```text
BLEU = BP · exp(∑ w_n log p_n)
```

其中 p_n 是 n-gram 精确率

**问题**：

- 只看n-gram重叠
- 不考虑语义

### 5.3 人类评估

**维度**：

1. **流畅性**：语法正确、自然
2. **连贯性**：逻辑一致
3. **相关性**：切题
4. **信息量**：内容丰富
5. **真实性**：事实正确

**方法**：

- 绝对评分（1-5）
- 相对比较（A vs B）

### 5.4 自动评估的挑战

**问题**：

- 指标与人类判断相关性弱
- 多样性难以评估
- 创造性无法量化

**趋势**：

- 用LLM评估LLM输出
- 多维度评估
- 任务特定指标

## 6. 实践建议 | Practical Recommendations

### 6.1 默认配置

**通用对话/生成**：

```python
temperature = 0.7
top_p = 0.9
max_tokens = 2048
repetition_penalty = 1.1
```

**事实性任务**（QA）：

```python
temperature = 0.2  # 更确定
top_p = 0.95
max_tokens = 512
```

**创意写作**：

```python
temperature = 1.0  # 更随机
top_p = 0.95
max_tokens = 4096
```

### 6.2 调参指南

**温度调整**：

- 输出太确定/重复 → 提高温度
- 输出太随机/不连贯 → 降低温度

**Top-p调整**：

- 需要更多创造性 → 降低p（如0.85）
- 需要更稳定输出 → 提高p（如0.98）

**长度控制**：

- 设置`max_tokens`
- 使用`stop_sequences`提前终止

### 6.3 常见问题排查

**问题：重复输出**:

- 解决：增加重复惩罚、提高温度

**问题：输出不相关**:

- 解决：降低温度、改进提示

**问题：输出太短**:

- 解决：调整长度惩罚、增加`min_tokens`

**问题：输出格式错误**:

- 解决：使用约束解码、改进提示示例

## 7. 权威参考文献 | Authoritative References

### 学术论文

1. **Holtzman, A., et al. (2019)**. "The Curious Case of Neural Text Degeneration". *ICLR*.
   - Top-p (Nucleus) 采样

2. **Su, Y., et al. (2022)**. "A Contrastive Framework for Neural Text Generation". *NeurIPS*.
   - 对比搜索

3. **Sennrich, R., et al. (2016)**. "Neural Machine Translation of Rare Words with Subword Units". *ACL*.
   - BPE

4. **Kudo, T., & Richardson, J. (2018)**. "SentencePiece: A simple and language independent approach to subword tokenization". *EMNLP*.
   - SentencePiece

### 实践资源

1. **Hugging Face Transformers**：标准实现
2. **OpenAI API Documentation**：参数说明
3. **LangChain**：高级生成控制

## 8. 关键要点总结 | Key Takeaways

1. **Token化**：子词级（BPE等）是现代标准
2. **Softmax**：模型输出转为概率分布
3. **温度**：控制随机性的关键参数
4. **贪心vs采样**：确定性vs多样性权衡
5. **Top-p**：现代LLM的默认，自适应候选集
6. **Beam Search**：适合有明确答案的任务
7. **评估难题**：自动指标与人类判断不完全一致
8. **实践为王**：没有万能配置，需要针对任务调优

---

**下一步阅读**：

- [03.3 Transformer大语言模型理论](03.3_Transformer_LLM_Theory.md)
- [03.5 嵌入向量空间理论](03.5_Embedding_Vector_Spaces.md)
