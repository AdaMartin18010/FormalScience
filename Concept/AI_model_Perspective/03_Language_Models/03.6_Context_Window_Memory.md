# 上下文窗口与记忆机制 | Context Window and Memory Mechanisms

## 概述 | Overview

上下文窗口是语言模型处理信息的范围，记忆机制决定如何利用历史信息。本文档分析从固定窗口到无限记忆的技术演进。

## 1. 上下文窗口基础 | Context Window Fundamentals

### 1.1 定义

**上下文窗口**：模型一次能"看到"的token数量

```text
窗口大小 n：模型输入最多 n 个 token
```

**例子**：

```text
GPT-3：2048 tokens
GPT-4：8K/32K tokens
Claude-2：100K tokens
GPT-4-Turbo：128K tokens
```

### 1.2 为什么有限制？

**计算复杂度**：

自注意力：O(n²)

```text
序列长度 n = 100K
注意力矩阵：100K × 100K = 10B 元素
内存：~40 GB（float32）
```

**内存限制**：

```text
存储中间激活、梯度
线性增长于序列长度
```

**训练难度**：

```text
更长序列 → 更难优化
梯度消失/爆炸
```

### 1.3 Token vs 字符vs词

**Token（现代标准）**：

```text
"Hello world!" → ["Hello", " world", "!"] (3 tokens)
```

**粗略对应**：

- 1 token ≈ 0.75 词（英文）
- 1 token ≈ 4 字符（英文）

**例子**：

```text
2048 tokens ≈ 1500 words ≈ 3-4 页文本
```

## 2. 不同架构的上下文处理 | Context Handling in Different Architectures

### 2.1 前馈神经网络

**固定n-gram窗口**：

```text
P(wₜ | wₜ₋ₙ₊₁, ..., wₜ₋₁)
```

**典型**：n = 5

**局限**：

- ❌ 无法超越n个词
- ❌ 长程依赖完全无法捕捉

### 2.2 RNN/LSTM

**理论上无限上下文**：

```text
hₜ = f(hₜ₋₁, xₜ)
```

隐状态hₜ累积所有历史

**实际限制**：

1. **梯度消失**：
   - 简单RNN：~10步
   - LSTM：~100步
   - 但不是硬限制

2. **信息容量**：
   - 隐状态维度有限（通常512-2048）
   - 压缩所有历史到固定向量
   - 信息损失不可避免

### 2.3 Transformer

**固定窗口（硬限制）**：

```text
输入：n个token
输出：n个表示
```

**原因**：

1. **位置编码**：
   - 预定义的最大长度
   - 超出则无定义或外推

2. **注意力矩阵**：
   - O(n²)空间
   - 超过限制则OOM

**优势**：

- 窗口内任意两位置路径长度=1
- 无梯度消失

**劣势**：

- 硬截断
- 超出窗口则完全遗忘

## 3. 扩展上下文窗口的技术 | Techniques for Extending Context

### 3.1 稀疏注意力

**问题**：O(n²)太昂贵

**解决**：只计算部分注意力

**1. 局部注意力**：

```text
只关注窗口内的k个邻居
复杂度：O(nk)
```

**2. 跨步注意力**：

```text
每隔s个位置关注一次
```

**3. 全局+局部**：

```text
少数全局token + 局部窗口
```

**代表模型**：

- Longformer：滑动窗口+全局注意力
- BigBird：随机+窗口+全局
- Reformer：LSH注意力

### 3.2 线性注意力

**目标**：O(n)复杂度

**方法**：

**核技巧近似**：

```text
Attention(Q, K, V) = softmax(QKᵀ)V
≈ φ(Q)φ(K)ᵀV
= φ(Q)(φ(K)ᵀV)
```

右结合：O(n)

**代表**：

- Performer：FAVOR+
- Linear Transformer
- AFT (Attention Free Transformer)

**权衡**：

- ✅ 线性复杂度
- ❌ 性能下降（近似误差）

### 3.3 压缩记忆

**Transformer-XL (Dai et al., 2019)**:

**核心思想**：重用前一段的表示

```text
段1：[token 1-512] → 计算表示 → 缓存
段2：[token 513-1024] → 使用段1缓存 + 计算
```

**优势**：

- 有效上下文倍增
- 相对位置编码

**Compressive Transformer (Rae et al., 2019)**:

**扩展**：

- 近期记忆：原始隐状态
- 远期记忆：压缩后的隐状态

**压缩**：卷积、平均池化、学习压缩器

### 3.4 外推位置编码

**问题**：训练n=2048，推理时n=4096？

**ALiBi (Press et al., 2021)**：

```text
不用位置编码，而是注意力偏置：
Attention + bias(i - j)
```

偏置只依赖相对距离

**RoPE (Su et al., 2021)**：

```text
旋转位置编码
在复平面上旋转
```

**优势**：

- 更好外推
- GPT-NeoX, PaLM, LLaMA使用

## 4. 超长上下文的挑战 | Challenges of Ultra-Long Context

### 4.1 Lost in the Middle

**Liu et al. (2023)**:

**发现**：

```text
信息在开头：性能好
信息在中间：性能显著下降
信息在结尾：性能好
```

**U型性能曲线**:

**原因假说**：

- 注意力稀释
- 位置编码偏见
- 训练数据分布

### 4.2 针干草堆 (Needle in Haystack)

**测试**：

```text
在长文档中隐藏一个事实
模型能否检索？
```

**结果**：

- 短文档：90%+准确率
- 长文档（100K+）：显著下降

**改进方向**：

- 更好的注意力机制
- 显式检索增强

### 4.3 计算成本

**资源需求**：

```text
GPT-3 (2K上下文) → GPT-4 (32K)
计算增长：~16x
内存增长：~16x
```

**推理延迟**：

```text
生成100 token：
2K上下文：~1秒
32K上下文：~5秒（粗略）
```

## 5. 检索增强生成 (RAG) | Retrieval-Augmented Generation

### 5.1 动机

**问题**：

- 上下文窗口有限
- 知识储存在参数中（固定）
- 无法访问外部信息

**解决**：结合检索与生成

### 5.2 RAG架构

**流程**：

```text
1. 查询
   ↓
2. 检索相关文档（from 大规模语料库）
   ↓
3. 拼接：查询 + 检索到的文档
   ↓
4. 生成答案
```

**检索方法**：

1. **稠密检索**：

   ```text
   编码查询和文档为向量
   向量相似度检索
   ```

2. **稀疏检索**：

   ```text
   BM25、TF-IDF
   ```

3. **混合**：结合两者

### 5.3 代表模型

**REALM (Guu et al., 2020)**：

联合训练检索和生成

**RAG (Lewis et al., 2020)**：

```text
P(y | x) = ∑_{z∈top-k(x)} P(z | x) P(y | x, z)
```

其中z是检索到的文档

**Atlas (Izacard et al., 2022)**：

微调检索器和LM

### 5.4 优势与局限

**优势**：

- ✅ 有效上下文远超窗口大小
- ✅ 可更新知识库（无需重训练）
- ✅ 可追溯来源

**局限**：

- ❌ 检索质量关键
- ❌ 增加延迟
- ❌ 检索与生成的不匹配

## 6. 记忆机制 | Memory Mechanisms

### 6.1 记忆类型

**工作记忆**：

- 当前上下文窗口
- 短期、易失

**长期记忆**：

- 模型参数
- 预训练知识
- 固定

**外部记忆**：

- 向量数据库
- 知识图谱
- 动态、可更新

### 6.2 记忆网络

**MemNN (Weston et al., 2014)**:

**组件**：

1. **存储**：记忆槽
2. **检索**：注意力机制
3. **读取**：加权和
4. **更新**：写入新记忆

### 6.3 神经图灵机 (NTM)

**Graves et al. (2014)**:

**可微分外部记忆**：

```text
读：r = ∑ w(i) M[i]
写：M[i] ← M[i] + w(i) add_vector
擦除：M[i] ← M[i] ⊙ (1 - w(i) erase_vector)
```

其中w是注意力权重

**理论**：图灵完备

**实践**：训练困难

### 6.4 现代实现

**对话系统的记忆**：

1. **会话历史**：

   ```text
   存储前N轮对话
   超出则截断或总结
   ```

2. **用户画像**：

   ```text
   长期存储用户偏好
   检索式增强
   ```

3. **知识更新**：

   ```text
   定期微调或PEFT
   检索增强
   ```

## 7. 实践策略 | Practical Strategies

### 7.1 处理超长输入

**1. 截断**：

```text
取前n个token（简单但有损）
```

**2. 滑动窗口**：

```text
多次推理，每次窗口
聚合结果
```

**3. 总结**：

```text
先总结长文档为简短版
再输入模型
```

**4. 分层处理**：

```text
文档 → 段落 → 句子
先处理段落，再全局整合
```

### 7.2 提示工程技巧

**相关信息靠前**：

```text
最重要信息放开头和结尾
避免"中间丢失"
```

**结构化输入**：

```text
用标记分隔：
<document>...</document>
<question>...</question>
```

**指令明确**：

```text
"根据上述文档回答..."
"忽略无关信息"
```

### 7.3 监控与调试

**检查输入长度**：

```python
if len(tokens) > max_length:
    print("Warning: input truncated")
```

**可视化注意力**：

确认模型关注正确位置

**分段测试**：

找出哪一段导致问题

## 8. 未来方向 | Future Directions

### 8.1 无限上下文？

**目标**：模型处理任意长度

**挑战**：

- 计算复杂度
- 训练数据（长文档少）
- 评估困难

**可能路径**：

- 更好的稀疏注意力
- 分层表示
- 检索增强

### 8.2 持续学习

**在线更新知识**：

无需重新预训练

**方法**：

- 参数高效微调（LoRA）
- 元学习
- 检索式记忆

### 8.3 个性化记忆

**每个用户的长期记忆**：

```text
存储交互历史
学习偏好
个性化响应
```

**挑战**：

- 隐私
- 存储成本
- 记忆管理

## 9. 权威参考文献 | Authoritative References

### 学术论文

1. **Dai, Z., et al. (2019)**. "Transformer-XL: Attentive language models beyond a fixed-length context". *ACL*.
2. **Beltagy, I., et al. (2020)**. "Longformer: The long-document transformer". *arXiv*.
3. **Press, O., et al. (2021)**. "Train short, test long: Attention with linear biases enables input length extrapolation". *ICLR*.
4. **Su, J., et al. (2021)**. "RoFormer: Enhanced transformer with rotary position embedding". *arXiv*.
5. **Lewis, P., et al. (2020)**. "Retrieval-augmented generation for knowledge-intensive NLP tasks". *NeurIPS*.
6. **Liu, N. F., et al. (2023)**. "Lost in the middle: How language models use long contexts". *arXiv*.

### 标准教材

1. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).

## 10. 关键要点总结 | Key Takeaways

1. **上下文窗口**：模型能"看到"的范围，关键限制
2. **O(n²)困境**：自注意力的计算瓶颈
3. **技术演进**：固定窗口 → 稀疏注意力 → 线性注意力
4. **长上下文挑战**："中间丢失"、计算成本
5. **RAG范式**：检索+生成，有效扩展上下文
6. **记忆机制**：工作记忆、长期记忆、外部记忆
7. **实践策略**：截断、总结、结构化输入
8. **位置编码关键**：外推能力决定上下文扩展
9. **未来方向**：无限上下文、持续学习、个性化
10. **权衡永恒**：上下文长度 vs 计算成本 vs 性能

---

**下一步阅读**：

- [03.3 Transformer大语言模型理论](03.3_Transformer_LLM_Theory.md)
- [03.5 嵌入向量空间理论](03.5_Embedding_Vector_Spaces.md)
- [02.4 Transformer架构](../02_Neural_Network_Theory/02.4_Transformer_Architecture.md)
