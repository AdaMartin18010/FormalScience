# 嵌入向量空间理论 | Embedding Vector Spaces Theory

## 概述 | Overview

词嵌入将离散符号映射到连续向量空间，是现代NLP的基础。本文档系统分析从Word2Vec到上下文嵌入的理论与实践。

## 1. 从符号到向量 | From Symbols to Vectors

### 1.1 符号表示的局限

**One-Hot编码**：

```text
词汇表V = {cat, dog, apple, orange, ...}
"cat" → [1, 0, 0, 0, ...]
"dog" → [0, 1, 0, 0, ...]
```

**问题**：

- ❌ 维度=|V|（通常10K-100K）
- ❌ 稀疏向量
- ❌ 所有词等距：d(cat, dog) = d(cat, apple)
- ❌ 无语义信息

### 1.2 分布式表示

**核心思想**：

```text
"cat" → [0.2, -0.5, 0.8, 0.1, -0.3, ...]  (d=100-1000)
"dog" → [0.3, -0.4, 0.7, 0.2, -0.2, ...]  (相近！)
```

**优势**：

- ✅ 低维稠密（d << |V|）
- ✅ 相似词有相似向量
- ✅ 编码语义信息
- ✅ 参数共享

### 1.3 分布假设的理论基础

**Harris (1954)**：
> 出现在相似上下文中的词有相似含义

**Firth (1957)**：
> "You shall know a word by the company it keeps"

**数学表达**：

```text
sim(w₁, w₂) ∝ 上下文相似度(w₁, w₂)
```

## 2. Word2Vec | Word2Vec

### 2.1 CBOW (Continuous Bag-of-Words)

**Mikolov et al. (2013)**:

**目标**：从上下文预测中心词

```text
上下文："The cat sits on the ___"
目标：预测 "mat"
```

**模型**：

```text
输入：上下文词的嵌入 {w₋ₙ, ..., w₋₁, w₊₁, ..., w₊ₙ}
平均：h = (v(w₋ₙ) + ... + v(w₊ₙ)) / (2n)
输出：P(w | context) = softmax(v'(w)ᵀ h)
```

**训练目标**：

```text
max ∑ log P(wₜ | wₜ₋ₙ, ..., wₜ₋₁, wₜ₊₁, ..., wₜ₊ₙ)
```

### 2.2 Skip-Gram

**目标**：从中心词预测上下文

```text
中心词："cat"
目标：预测上下文 "The", "sits", "on", ...
```

**模型**：

```text
输入：中心词嵌入 v(wₜ)
输出：P(wc | wₜ) = softmax(v'(wc)ᵀ v(wₜ))
```

**训练目标**：

```text
max ∑ ∑ log P(wₜ₊ⱼ | wₜ)
    t  j∈[-n,n], j≠0
```

**直觉**：

- CBOW：预测中心
- Skip-Gram：预测周围

### 2.3 负采样 (Negative Sampling)

**Softmax问题**：

```text
P(wₒ | wᵢ) = exp(v'(wₒ)ᵀ v(wᵢ)) / ∑_{w∈V} exp(v'(w)ᵀ v(wᵢ))
```

分母需要遍历整个词汇表 O(|V|)

**负采样解决**：

```text
不计算完整Softmax，转为二分类：
目标：区分真实上下文 vs 随机噪声
```

**目标函数**：

```text
log σ(v'(wₒ)ᵀ v(wᵢ)) + ∑_{k=1}^K 𝔼_{wₖ~P_noise} [log σ(-v'(wₖ)ᵀ v(wᵢ))]
```

其中：

- σ(x) = 1/(1+e⁻ˣ)
- K：负样本数（通常5-20）
- P_noise：噪声分布（通常∝词频^0.75）

**效果**：

- 复杂度从 O(|V|) 降到 O(K)
- 质量与完整Softmax相当

### 2.4 子采样 (Subsampling)

**问题**：高频词（"the", "a"）信息少但占多数

**解决**：以概率丢弃高频词

```text
P(丢弃wᵢ) = 1 - √(t / f(wᵢ))
```

其中：

- f(wᵢ)：词频
- t：阈值（通常10⁻⁵）

**效果**：

- 加速训练
- 提高质量（更关注内容词）

### 2.5 Word2Vec的性质

**几何性质**：

```text
v(king) - v(man) + v(woman) ≈ v(queen)
v(Paris) - v(France) + v(Germany) ≈ v(Berlin)
```

**为什么有效？**

**理论解释 (Levy & Goldberg, 2014)**：

Skip-Gram with negative sampling隐式分解矩阵：

```text
v(wᵢ)ᵀ v'(wⱼ) = PMI(wᵢ, wⱼ) - log k
```

其中 PMI = 点互信息

**意义**：Word2Vec学习共现统计的低秩分解

## 3. GloVe | Global Vectors

### 3.1 动机

**Pennington et al. (2014)**:

**观察**：

- Word2Vec：局部上下文窗口
- LSA：全局共现矩阵
- 能否结合两者优势？

### 3.2 模型

**目标**：拟合词共现统计

**损失函数**：

```text
J = ∑ f(Xᵢⱼ) (wᵢᵀ w̃ⱼ + bᵢ + b̃ⱼ - log Xᵢⱼ)²
```

其中：

- Xᵢⱼ：词i和词j的共现次数
- wᵢ, w̃ⱼ：词向量
- f(x)：权重函数

**权重函数**：

```text
f(x) = {
  (x/xₘₐₓ)^α  if x < xₘₐₓ
  1           otherwise
}
```

防止高频词主导损失

### 3.3 vs Word2Vec

| 维度 | Word2Vec | GloVe |
|------|---------|-------|
| **方法** | 预测式 | 计数式 |
| **信息** | 局部上下文 | 全局共现 |
| **目标** | 最大似然 | 最小二乘 |
| **训练** | 在线 | 批量 |
| **性能** | 相当 | 相当 |

**实践中**：

- 两者性能相近
- Word2Vec更流行（更早、更简单）

## 4. 上下文嵌入 | Contextual Embeddings

### 4.1 静态 vs 上下文嵌入

**静态嵌入 (Word2Vec, GloVe)**：

```text
"bank" → 固定向量（不管上下文）
"river bank" → 同一向量
"investment bank" → 同一向量
```

**问题**：一词多义

**上下文嵌入**：

```text
"river bank" → 向量₁
"investment bank" → 向量₂（不同！）
```

### 4.2 ELMo (Embeddings from Language Models)

**Peters et al. (2018)**:

**方法**：双向LSTM语言模型

**架构**：

```text
前向LSTM：→→→
输入：token序列
输出：每层的隐状态 h⃗ⁱⱼ

后向LSTM：←←←
输出：每层的隐状态 h⃖ⁱⱼ

ELMo表示：
ELMo(wⱼ) = γ ∑ᵢ sᵢ [h⃗ⁱⱼ; h⃖ⁱⱼ]
```

其中 sᵢ 是可学习的权重

**使用方式**：

```text
固定ELMo嵌入 + 任务特定模型
```

**突破**：

- 上下文敏感
- 多个NLP任务SOTA
- 预训练范式的开端

### 4.3 BERT的嵌入

**Transformer编码器**:

**每层的表示**：

```text
Layer 0：token + position embedding
Layer 1-12：Transformer层
```

**不同层捕捉不同信息**：

- **浅层**：句法、词性
- **中层**：语义、共指
- **深层**：任务特定

**使用**：

- 通常取最后4层的平均
- 或微调整个模型

### 4.4 GPT的嵌入

**自回归Transformer**:

**特点**：

- 单向（只看左侧）
- 预训练目标：下一词预测

**使用**：

- 通常取最后一层
- 或微调

## 5. 嵌入空间的几何结构 | Geometry of Embedding Spaces

### 5.1 相似度度量

**余弦相似度**：

```text
sim(u, v) = (u · v) / (‖u‖ ‖v‖)
```

范围：[-1, 1]

**欧氏距离**：

```text
d(u, v) = ‖u - v‖₂
```

**为什么余弦更常用？**

- 规范化（只看方向）
- 与向量模长无关

### 5.2 类比关系

**向量运算**：

```text
v(queen) ≈ v(king) - v(man) + v(woman)
```

**为什么有效？**

**线性子空间假说**：

- 语义关系对应向量空间的线性子空间
- 例如：性别关系形成一个方向

### 5.3 多义词的表示

**问题**："bank"有多个含义

**解决方案**：

1. **静态嵌入**：平均所有含义
   - 简单但有损

2. **多原型嵌入**：每个含义一个向量
   - 复杂，需要聚类

3. **上下文嵌入**：动态生成
   - 最自然，现代标准

### 5.4 偏见 (Bias)

**发现 (Bolukbasi et al., 2016)**：

```text
v(he) - v(she) ≈ v(doctor) - v(nurse)
v(man) - v(woman) ≈ v(programmer) - v(homemaker)
```

**性别偏见方向**：

```text
g = v(he) - v(she)
```

**去偏见**：

1. **识别偏见方向**
2. **中性化**：移除该方向的分量
3. **均等化**：对应词对等距

**局限**：

- 只能缓解，难以完全消除
- 可能影响性能
- 偏见来自训练数据

## 6. 高级技术 | Advanced Techniques

### 6.1 FastText

**Bojanowski et al. (2017)**:

**创新**：子词信息

```text
"apple" → {"<ap", "app", "ppl", "ple", "le>"}
v(apple) = ∑ v(n-gram)
```

**优势**：

- 处理OOV词
- 学习词缀
- 形态丰富语言表现好

### 6.2 Sentence-BERT

**Reimers & Gurevych (2019)**:

**目标**：句子级嵌入

**方法**：

```text
1. BERT编码两个句子
2. 池化（mean/CLS）
3. 用对比学习训练
```

**损失函数**：

```text
L = max(0, ‖u - v⁺‖² - ‖u - v⁻‖² + ε)
```

**应用**：

- 语义搜索
- 聚类
- 重复检测

### 6.3 对比学习

**SimCSE (Gao et al., 2021)**:

**无监督版本**：

```text
同一句子通过Dropout两次 → 正样本对
不同句子 → 负样本对
```

**监督版本**：

```text
NLI数据集：
蕴含句 → 正样本
矛盾句 → 负样本
```

**效果**：

- 显著提升句子表示质量
- 简单但有效

## 7. 评估嵌入质量 | Evaluating Embeddings

### 7.1 内在评估

**词相似度任务**：

与人类相似度判断的相关性

数据集：WordSim-353, SimLex-999

**词类比任务**：

```text
man : woman :: king : ?
```

准确率：答案在top-k中

### 7.2 外在评估

**下游任务性能**：

- 文本分类
- 命名实体识别
- 情感分析
- 问答

**标准**：

- 固定嵌入 + 浅层模型
- 或微调整个模型

### 7.3 评估的挑战

**问题**：

- 内在vs外在评估相关性弱
- 任务依赖性
- 没有单一"最佳"嵌入

**实践**：

- 针对应用场景评估
- 多个任务平均
- A/B测试

## 8. 理论理解 | Theoretical Understanding

### 8.1 为什么低维有效？

**流形假设**：

- 语言数据在低维流形上
- 高维嵌入是冗余的

**信息论视角**：

- 词的信息有限
- d=100-1000维足够编码

### 8.2 PMI矩阵分解

**Levy & Goldberg (2014)**:

**定理**：

Skip-Gram with negative sampling等价于隐式分解：

```text
v(w)ᵀ v'(c) = PMI(w, c) - log k
```

其中 k 是负样本数

**意义**：

- Word2Vec ≈ 矩阵分解
- 连接神经方法与传统方法

### 8.3 上下文嵌入的表达能力

**问题**：为什么Transformer学到的表示这么强？

**假说**：

1. **层次组合性**：底层语法 → 高层语义
2. **注意力检索**：动态聚合相关信息
3. **大规模预训练**：学习丰富语言知识

**理论空白**：完整理解仍是开放问题

## 9. 权威参考文献 | Authoritative References

### 学术论文

1. **Mikolov, T., et al. (2013)**. "Efficient estimation of word representations in vector space". *ICLR*.
2. **Mikolov, T., et al. (2013)**. "Distributed representations of words and phrases". *NeurIPS*.
3. **Pennington, J., et al. (2014)**. "GloVe: Global vectors for word representation". *EMNLP*.
4. **Peters, M. E., et al. (2018)**. "Deep contextualized word representations". *NAACL*.
5. **Levy, O., & Goldberg, Y. (2014)**. "Neural word embedding as implicit matrix factorization". *NeurIPS*.
6. **Bolukbasi, T., et al. (2016)**. "Man is to computer programmer as woman is to homemaker?". *NeurIPS*.
7. **Gao, T., et al. (2021)**. "SimCSE: Simple contrastive learning of sentence embeddings". *EMNLP*.

### 标准教材

1. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).
2. **Goldberg, Y. (2017)**. *Neural Network Methods for Natural Language Processing*. Morgan & Claypool.

## 10. 关键要点总结 | Key Takeaways

1. **分布式表示**：低维稠密向量，编码语义
2. **Word2Vec**：预测式，负采样高效
3. **GloVe**：计数式，拟合共现统计
4. **上下文嵌入**：解决一词多义，现代标准
5. **几何性质**：类比关系，向量运算有意义
6. **偏见问题**：嵌入继承训练数据偏见
7. **理论联系**：神经方法 ≈ 矩阵分解
8. **评估多样**：内在+外在，任务依赖
9. **持续演进**：静态 → 上下文 → 大模型嵌入
10. **理论未完**：为何有效仍需深入研究

---

**下一步阅读**：

- [03.2 神经语言模型](03.2_Neural_Language_Models.md)
- [03.6 上下文窗口与记忆机制](03.6_Context_Window_Memory.md)
- [04.1 语义向量空间](../04_Semantic_Models/04.1_Semantic_Vector_Spaces.md)
