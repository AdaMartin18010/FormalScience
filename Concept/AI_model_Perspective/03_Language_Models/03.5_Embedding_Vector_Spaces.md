# åµŒå…¥å‘é‡ç©ºé—´ç†è®º | Embedding Vector Spaces Theory

## ç›®å½• | Table of Contents

- [åµŒå…¥å‘é‡ç©ºé—´ç†è®º | Embedding Vector Spaces Theory](#åµŒå…¥å‘é‡ç©ºé—´ç†è®º--embedding-vector-spaces-theory)
  - [ç›®å½• | Table of Contents](#ç›®å½•--table-of-contents)
  - [æ¦‚è¿° | Overview](#æ¦‚è¿°--overview)
  - [1. ä»ç¬¦å·åˆ°å‘é‡ | From Symbols to Vectors](#1-ä»ç¬¦å·åˆ°å‘é‡--from-symbols-to-vectors)
    - [1.1 ç¬¦å·è¡¨ç¤ºçš„å±€é™](#11-ç¬¦å·è¡¨ç¤ºçš„å±€é™)
    - [1.2 åˆ†å¸ƒå¼è¡¨ç¤º](#12-åˆ†å¸ƒå¼è¡¨ç¤º)
    - [1.3 åˆ†å¸ƒå‡è®¾çš„ç†è®ºåŸºç¡€](#13-åˆ†å¸ƒå‡è®¾çš„ç†è®ºåŸºç¡€)
  - [2. Word2Vec | Word2Vec](#2-word2vec--word2vec)
    - [2.1 CBOW (Continuous Bag-of-Words)](#21-cbow-continuous-bag-of-words)
    - [2.2 Skip-Gram](#22-skip-gram)
    - [2.3 è´Ÿé‡‡æ · (Negative Sampling)](#23-è´Ÿé‡‡æ ·-negative-sampling)
    - [2.4 å­é‡‡æ · (Subsampling)](#24-å­é‡‡æ ·-subsampling)
    - [2.5 Word2Vecçš„æ€§è´¨](#25-word2vecçš„æ€§è´¨)
  - [3. GloVe | Global Vectors](#3-glove--global-vectors)
    - [3.1 åŠ¨æœº](#31-åŠ¨æœº)
    - [3.2 æ¨¡å‹](#32-æ¨¡å‹)
    - [3.3 vs Word2Vec](#33-vs-word2vec)
  - [4. ä¸Šä¸‹æ–‡åµŒå…¥ | Contextual Embeddings](#4-ä¸Šä¸‹æ–‡åµŒå…¥--contextual-embeddings)
    - [4.1 é™æ€ vs ä¸Šä¸‹æ–‡åµŒå…¥](#41-é™æ€-vs-ä¸Šä¸‹æ–‡åµŒå…¥)
    - [4.2 ELMo (Embeddings from Language Models)](#42-elmo-embeddings-from-language-models)
    - [4.3 BERTçš„åµŒå…¥](#43-bertçš„åµŒå…¥)
    - [4.4 GPTçš„åµŒå…¥](#44-gptçš„åµŒå…¥)
  - [5. åµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„ | Geometry of Embedding Spaces](#5-åµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„--geometry-of-embedding-spaces)
    - [5.1 ç›¸ä¼¼åº¦åº¦é‡](#51-ç›¸ä¼¼åº¦åº¦é‡)
    - [5.2 ç±»æ¯”å…³ç³»](#52-ç±»æ¯”å…³ç³»)
    - [5.3 å¤šä¹‰è¯çš„è¡¨ç¤º](#53-å¤šä¹‰è¯çš„è¡¨ç¤º)
    - [5.4 åè§ (Bias)](#54-åè§-bias)
  - [6. é«˜çº§æŠ€æœ¯ | Advanced Techniques](#6-é«˜çº§æŠ€æœ¯--advanced-techniques)
    - [6.1 FastText](#61-fasttext)
    - [6.2 Sentence-BERT](#62-sentence-bert)
    - [6.3 å¯¹æ¯”å­¦ä¹ ](#63-å¯¹æ¯”å­¦ä¹ )
  - [7. è¯„ä¼°åµŒå…¥è´¨é‡ | Evaluating Embeddings](#7-è¯„ä¼°åµŒå…¥è´¨é‡--evaluating-embeddings)
    - [7.1 å†…åœ¨è¯„ä¼°](#71-å†…åœ¨è¯„ä¼°)
    - [7.2 å¤–åœ¨è¯„ä¼°](#72-å¤–åœ¨è¯„ä¼°)
    - [7.3 è¯„ä¼°çš„æŒ‘æˆ˜](#73-è¯„ä¼°çš„æŒ‘æˆ˜)
  - [8. ç†è®ºç†è§£ | Theoretical Understanding](#8-ç†è®ºç†è§£--theoretical-understanding)
    - [8.1 ä¸ºä»€ä¹ˆä½ç»´æœ‰æ•ˆï¼Ÿ](#81-ä¸ºä»€ä¹ˆä½ç»´æœ‰æ•ˆ)
    - [8.2 PMIçŸ©é˜µåˆ†è§£](#82-pmiçŸ©é˜µåˆ†è§£)
    - [8.3 ä¸Šä¸‹æ–‡åµŒå…¥çš„è¡¨è¾¾èƒ½åŠ›](#83-ä¸Šä¸‹æ–‡åµŒå…¥çš„è¡¨è¾¾èƒ½åŠ›)
  - [9. æƒå¨å‚è€ƒæ–‡çŒ® | Authoritative References](#9-æƒå¨å‚è€ƒæ–‡çŒ®--authoritative-references)
    - [å­¦æœ¯è®ºæ–‡](#å­¦æœ¯è®ºæ–‡)
    - [æ ‡å‡†æ•™æ](#æ ‡å‡†æ•™æ)
  - [10. å…³é”®è¦ç‚¹æ€»ç»“ | Key Takeaways](#10-å…³é”®è¦ç‚¹æ€»ç»“--key-takeaways)

---

## æ¦‚è¿° | Overview

è¯åµŒå…¥å°†ç¦»æ•£ç¬¦å·æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´ï¼Œæ˜¯ç°ä»£NLPçš„åŸºç¡€ã€‚æœ¬æ–‡æ¡£ç³»ç»Ÿåˆ†æä»Word2Vecåˆ°ä¸Šä¸‹æ–‡åµŒå…¥çš„ç†è®ºä¸å®è·µã€‚

## 1. ä»ç¬¦å·åˆ°å‘é‡ | From Symbols to Vectors

### 1.1 ç¬¦å·è¡¨ç¤ºçš„å±€é™

**One-Hotç¼–ç **ï¼š

```text
è¯æ±‡è¡¨V = {cat, dog, apple, orange, ...}
"cat" â†’ [1, 0, 0, 0, ...]
"dog" â†’ [0, 1, 0, 0, ...]
```

**é—®é¢˜**ï¼š

- âŒ ç»´åº¦=|V|ï¼ˆé€šå¸¸10K-100Kï¼‰
- âŒ ç¨€ç–å‘é‡
- âŒ æ‰€æœ‰è¯ç­‰è·ï¼šd(cat, dog) = d(cat, apple)
- âŒ æ— è¯­ä¹‰ä¿¡æ¯

### 1.2 åˆ†å¸ƒå¼è¡¨ç¤º

**æ ¸å¿ƒæ€æƒ³**ï¼š

```text
"cat" â†’ [0.2, -0.5, 0.8, 0.1, -0.3, ...]  (d=100-1000)
"dog" â†’ [0.3, -0.4, 0.7, 0.2, -0.2, ...]  (ç›¸è¿‘ï¼)
```

**ä¼˜åŠ¿**ï¼š

- âœ… ä½ç»´ç¨ å¯†ï¼ˆd << |V|ï¼‰
- âœ… ç›¸ä¼¼è¯æœ‰ç›¸ä¼¼å‘é‡
- âœ… ç¼–ç è¯­ä¹‰ä¿¡æ¯
- âœ… å‚æ•°å…±äº«

### 1.3 åˆ†å¸ƒå‡è®¾çš„ç†è®ºåŸºç¡€

**Harris (1954)**ï¼š
> å‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„è¯æœ‰ç›¸ä¼¼å«ä¹‰

**Firth (1957)**ï¼š
> "You shall know a word by the company it keeps"

**æ•°å­¦è¡¨è¾¾**ï¼š

```text
sim(wâ‚, wâ‚‚) âˆ ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦(wâ‚, wâ‚‚)
```

## 2. Word2Vec | Word2Vec

### 2.1 CBOW (Continuous Bag-of-Words)

**Mikolov et al. (2013)**:

**ç›®æ ‡**ï¼šä»ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯

```text
ä¸Šä¸‹æ–‡ï¼š"The cat sits on the ___"
ç›®æ ‡ï¼šé¢„æµ‹ "mat"
```

**æ¨¡å‹**ï¼š

```text
è¾“å…¥ï¼šä¸Šä¸‹æ–‡è¯çš„åµŒå…¥ {wâ‚‹â‚™, ..., wâ‚‹â‚, wâ‚Šâ‚, ..., wâ‚Šâ‚™}
å¹³å‡ï¼šh = (v(wâ‚‹â‚™) + ... + v(wâ‚Šâ‚™)) / (2n)
è¾“å‡ºï¼šP(w | context) = softmax(v'(w)áµ€ h)
```

**è®­ç»ƒç›®æ ‡**ï¼š

```text
max âˆ‘ log P(wâ‚œ | wâ‚œâ‚‹â‚™, ..., wâ‚œâ‚‹â‚, wâ‚œâ‚Šâ‚, ..., wâ‚œâ‚Šâ‚™)
```

### 2.2 Skip-Gram

**ç›®æ ‡**ï¼šä»ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡

```text
ä¸­å¿ƒè¯ï¼š"cat"
ç›®æ ‡ï¼šé¢„æµ‹ä¸Šä¸‹æ–‡ "The", "sits", "on", ...
```

**æ¨¡å‹**ï¼š

```text
è¾“å…¥ï¼šä¸­å¿ƒè¯åµŒå…¥ v(wâ‚œ)
è¾“å‡ºï¼šP(wc | wâ‚œ) = softmax(v'(wc)áµ€ v(wâ‚œ))
```

**è®­ç»ƒç›®æ ‡**ï¼š

```text
max âˆ‘ âˆ‘ log P(wâ‚œâ‚Šâ±¼ | wâ‚œ)
    t  jâˆˆ[-n,n], jâ‰ 0
```

**ç›´è§‰**ï¼š

- CBOWï¼šé¢„æµ‹ä¸­å¿ƒ
- Skip-Gramï¼šé¢„æµ‹å‘¨å›´

### 2.3 è´Ÿé‡‡æ · (Negative Sampling)

**Softmaxé—®é¢˜**ï¼š

```text
P(wâ‚’ | wáµ¢) = exp(v'(wâ‚’)áµ€ v(wáµ¢)) / âˆ‘_{wâˆˆV} exp(v'(w)áµ€ v(wáµ¢))
```

åˆ†æ¯éœ€è¦éå†æ•´ä¸ªè¯æ±‡è¡¨ O(|V|)

**è´Ÿé‡‡æ ·è§£å†³**ï¼š

```text
ä¸è®¡ç®—å®Œæ•´Softmaxï¼Œè½¬ä¸ºäºŒåˆ†ç±»ï¼š
ç›®æ ‡ï¼šåŒºåˆ†çœŸå®ä¸Šä¸‹æ–‡ vs éšæœºå™ªå£°
```

**ç›®æ ‡å‡½æ•°**ï¼š

```text
log Ïƒ(v'(wâ‚’)áµ€ v(wáµ¢)) + âˆ‘_{k=1}^K ğ”¼_{wâ‚–~P_noise} [log Ïƒ(-v'(wâ‚–)áµ€ v(wáµ¢))]
```

å…¶ä¸­ï¼š

- Ïƒ(x) = 1/(1+eâ»Ë£)
- Kï¼šè´Ÿæ ·æœ¬æ•°ï¼ˆé€šå¸¸5-20ï¼‰
- P_noiseï¼šå™ªå£°åˆ†å¸ƒï¼ˆé€šå¸¸âˆè¯é¢‘^0.75ï¼‰

**æ•ˆæœ**ï¼š

- å¤æ‚åº¦ä» O(|V|) é™åˆ° O(K)
- è´¨é‡ä¸å®Œæ•´Softmaxç›¸å½“

### 2.4 å­é‡‡æ · (Subsampling)

**é—®é¢˜**ï¼šé«˜é¢‘è¯ï¼ˆ"the", "a"ï¼‰ä¿¡æ¯å°‘ä½†å å¤šæ•°

**è§£å†³**ï¼šä»¥æ¦‚ç‡ä¸¢å¼ƒé«˜é¢‘è¯

```text
P(ä¸¢å¼ƒwáµ¢) = 1 - âˆš(t / f(wáµ¢))
```

å…¶ä¸­ï¼š

- f(wáµ¢)ï¼šè¯é¢‘
- tï¼šé˜ˆå€¼ï¼ˆé€šå¸¸10â»âµï¼‰

**æ•ˆæœ**ï¼š

- åŠ é€Ÿè®­ç»ƒ
- æé«˜è´¨é‡ï¼ˆæ›´å…³æ³¨å†…å®¹è¯ï¼‰

### 2.5 Word2Vecçš„æ€§è´¨

**å‡ ä½•æ€§è´¨**ï¼š

```text
v(king) - v(man) + v(woman) â‰ˆ v(queen)
v(Paris) - v(France) + v(Germany) â‰ˆ v(Berlin)
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

**ç†è®ºè§£é‡Š (Levy & Goldberg, 2014)**ï¼š

Skip-Gram with negative samplingéšå¼åˆ†è§£çŸ©é˜µï¼š

```text
v(wáµ¢)áµ€ v'(wâ±¼) = PMI(wáµ¢, wâ±¼) - log k
```

å…¶ä¸­ PMI = ç‚¹äº’ä¿¡æ¯

**æ„ä¹‰**ï¼šWord2Vecå­¦ä¹ å…±ç°ç»Ÿè®¡çš„ä½ç§©åˆ†è§£

## 3. GloVe | Global Vectors

### 3.1 åŠ¨æœº

**Pennington et al. (2014)**:

**è§‚å¯Ÿ**ï¼š

- Word2Vecï¼šå±€éƒ¨ä¸Šä¸‹æ–‡çª—å£
- LSAï¼šå…¨å±€å…±ç°çŸ©é˜µ
- èƒ½å¦ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼Ÿ

### 3.2 æ¨¡å‹

**ç›®æ ‡**ï¼šæ‹Ÿåˆè¯å…±ç°ç»Ÿè®¡

**æŸå¤±å‡½æ•°**ï¼š

```text
J = âˆ‘ f(Xáµ¢â±¼) (wáµ¢áµ€ wÌƒâ±¼ + báµ¢ + bÌƒâ±¼ - log Xáµ¢â±¼)Â²
```

å…¶ä¸­ï¼š

- Xáµ¢â±¼ï¼šè¯iå’Œè¯jçš„å…±ç°æ¬¡æ•°
- wáµ¢, wÌƒâ±¼ï¼šè¯å‘é‡
- f(x)ï¼šæƒé‡å‡½æ•°

**æƒé‡å‡½æ•°**ï¼š

```text
f(x) = {
  (x/xâ‚˜â‚â‚“)^Î±  if x < xâ‚˜â‚â‚“
  1           otherwise
}
```

é˜²æ­¢é«˜é¢‘è¯ä¸»å¯¼æŸå¤±

### 3.3 vs Word2Vec

| ç»´åº¦ | Word2Vec | GloVe |
|------|---------|-------|
| **æ–¹æ³•** | é¢„æµ‹å¼ | è®¡æ•°å¼ |
| **ä¿¡æ¯** | å±€éƒ¨ä¸Šä¸‹æ–‡ | å…¨å±€å…±ç° |
| **ç›®æ ‡** | æœ€å¤§ä¼¼ç„¶ | æœ€å°äºŒä¹˜ |
| **è®­ç»ƒ** | åœ¨çº¿ | æ‰¹é‡ |
| **æ€§èƒ½** | ç›¸å½“ | ç›¸å½“ |

**å®è·µä¸­**ï¼š

- ä¸¤è€…æ€§èƒ½ç›¸è¿‘
- Word2Vecæ›´æµè¡Œï¼ˆæ›´æ—©ã€æ›´ç®€å•ï¼‰

## 4. ä¸Šä¸‹æ–‡åµŒå…¥ | Contextual Embeddings

### 4.1 é™æ€ vs ä¸Šä¸‹æ–‡åµŒå…¥

**é™æ€åµŒå…¥ (Word2Vec, GloVe)**ï¼š

```text
"bank" â†’ å›ºå®šå‘é‡ï¼ˆä¸ç®¡ä¸Šä¸‹æ–‡ï¼‰
"river bank" â†’ åŒä¸€å‘é‡
"investment bank" â†’ åŒä¸€å‘é‡
```

**é—®é¢˜**ï¼šä¸€è¯å¤šä¹‰

**ä¸Šä¸‹æ–‡åµŒå…¥**ï¼š

```text
"river bank" â†’ å‘é‡â‚
"investment bank" â†’ å‘é‡â‚‚ï¼ˆä¸åŒï¼ï¼‰
```

### 4.2 ELMo (Embeddings from Language Models)

**Peters et al. (2018)**:

**æ–¹æ³•**ï¼šåŒå‘LSTMè¯­è¨€æ¨¡å‹

**æ¶æ„**ï¼š

```text
å‰å‘LSTMï¼šâ†’â†’â†’
è¾“å…¥ï¼štokenåºåˆ—
è¾“å‡ºï¼šæ¯å±‚çš„éšçŠ¶æ€ hâƒ—â±â±¼

åå‘LSTMï¼šâ†â†â†
è¾“å‡ºï¼šæ¯å±‚çš„éšçŠ¶æ€ hâƒ–â±â±¼

ELMoè¡¨ç¤ºï¼š
ELMo(wâ±¼) = Î³ âˆ‘áµ¢ sáµ¢ [hâƒ—â±â±¼; hâƒ–â±â±¼]
```

å…¶ä¸­ sáµ¢ æ˜¯å¯å­¦ä¹ çš„æƒé‡

**ä½¿ç”¨æ–¹å¼**ï¼š

```text
å›ºå®šELMoåµŒå…¥ + ä»»åŠ¡ç‰¹å®šæ¨¡å‹
```

**çªç ´**ï¼š

- ä¸Šä¸‹æ–‡æ•æ„Ÿ
- å¤šä¸ªNLPä»»åŠ¡SOTA
- é¢„è®­ç»ƒèŒƒå¼çš„å¼€ç«¯

### 4.3 BERTçš„åµŒå…¥

**Transformerç¼–ç å™¨**:

**æ¯å±‚çš„è¡¨ç¤º**ï¼š

```text
Layer 0ï¼štoken + position embedding
Layer 1-12ï¼šTransformerå±‚
```

**ä¸åŒå±‚æ•æ‰ä¸åŒä¿¡æ¯**ï¼š

- **æµ…å±‚**ï¼šå¥æ³•ã€è¯æ€§
- **ä¸­å±‚**ï¼šè¯­ä¹‰ã€å…±æŒ‡
- **æ·±å±‚**ï¼šä»»åŠ¡ç‰¹å®š

**ä½¿ç”¨**ï¼š

- é€šå¸¸å–æœ€å4å±‚çš„å¹³å‡
- æˆ–å¾®è°ƒæ•´ä¸ªæ¨¡å‹

### 4.4 GPTçš„åµŒå…¥

**è‡ªå›å½’Transformer**:

**ç‰¹ç‚¹**ï¼š

- å•å‘ï¼ˆåªçœ‹å·¦ä¾§ï¼‰
- é¢„è®­ç»ƒç›®æ ‡ï¼šä¸‹ä¸€è¯é¢„æµ‹

**ä½¿ç”¨**ï¼š

- é€šå¸¸å–æœ€åä¸€å±‚
- æˆ–å¾®è°ƒ

## 5. åµŒå…¥ç©ºé—´çš„å‡ ä½•ç»“æ„ | Geometry of Embedding Spaces

### 5.1 ç›¸ä¼¼åº¦åº¦é‡

**ä½™å¼¦ç›¸ä¼¼åº¦**ï¼š

```text
sim(u, v) = (u Â· v) / (â€–uâ€– â€–vâ€–)
```

èŒƒå›´ï¼š[-1, 1]

**æ¬§æ°è·ç¦»**ï¼š

```text
d(u, v) = â€–u - vâ€–â‚‚
```

**ä¸ºä»€ä¹ˆä½™å¼¦æ›´å¸¸ç”¨ï¼Ÿ**

- è§„èŒƒåŒ–ï¼ˆåªçœ‹æ–¹å‘ï¼‰
- ä¸å‘é‡æ¨¡é•¿æ— å…³

### 5.2 ç±»æ¯”å…³ç³»

**å‘é‡è¿ç®—**ï¼š

```text
v(queen) â‰ˆ v(king) - v(man) + v(woman)
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

**çº¿æ€§å­ç©ºé—´å‡è¯´**ï¼š

- è¯­ä¹‰å…³ç³»å¯¹åº”å‘é‡ç©ºé—´çš„çº¿æ€§å­ç©ºé—´
- ä¾‹å¦‚ï¼šæ€§åˆ«å…³ç³»å½¢æˆä¸€ä¸ªæ–¹å‘

### 5.3 å¤šä¹‰è¯çš„è¡¨ç¤º

**é—®é¢˜**ï¼š"bank"æœ‰å¤šä¸ªå«ä¹‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **é™æ€åµŒå…¥**ï¼šå¹³å‡æ‰€æœ‰å«ä¹‰
   - ç®€å•ä½†æœ‰æŸ

2. **å¤šåŸå‹åµŒå…¥**ï¼šæ¯ä¸ªå«ä¹‰ä¸€ä¸ªå‘é‡
   - å¤æ‚ï¼Œéœ€è¦èšç±»

3. **ä¸Šä¸‹æ–‡åµŒå…¥**ï¼šåŠ¨æ€ç”Ÿæˆ
   - æœ€è‡ªç„¶ï¼Œç°ä»£æ ‡å‡†

### 5.4 åè§ (Bias)

**å‘ç° (Bolukbasi et al., 2016)**ï¼š

```text
v(he) - v(she) â‰ˆ v(doctor) - v(nurse)
v(man) - v(woman) â‰ˆ v(programmer) - v(homemaker)
```

**æ€§åˆ«åè§æ–¹å‘**ï¼š

```text
g = v(he) - v(she)
```

**å»åè§**ï¼š

1. **è¯†åˆ«åè§æ–¹å‘**
2. **ä¸­æ€§åŒ–**ï¼šç§»é™¤è¯¥æ–¹å‘çš„åˆ†é‡
3. **å‡ç­‰åŒ–**ï¼šå¯¹åº”è¯å¯¹ç­‰è·

**å±€é™**ï¼š

- åªèƒ½ç¼“è§£ï¼Œéš¾ä»¥å®Œå…¨æ¶ˆé™¤
- å¯èƒ½å½±å“æ€§èƒ½
- åè§æ¥è‡ªè®­ç»ƒæ•°æ®

## 6. é«˜çº§æŠ€æœ¯ | Advanced Techniques

### 6.1 FastText

**Bojanowski et al. (2017)**:

**åˆ›æ–°**ï¼šå­è¯ä¿¡æ¯

```text
"apple" â†’ {"<ap", "app", "ppl", "ple", "le>"}
v(apple) = âˆ‘ v(n-gram)
```

**ä¼˜åŠ¿**ï¼š

- å¤„ç†OOVè¯
- å­¦ä¹ è¯ç¼€
- å½¢æ€ä¸°å¯Œè¯­è¨€è¡¨ç°å¥½

### 6.2 Sentence-BERT

**Reimers & Gurevych (2019)**:

**ç›®æ ‡**ï¼šå¥å­çº§åµŒå…¥

**æ–¹æ³•**ï¼š

```text
1. BERTç¼–ç ä¸¤ä¸ªå¥å­
2. æ± åŒ–ï¼ˆmean/CLSï¼‰
3. ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒ
```

**æŸå¤±å‡½æ•°**ï¼š

```text
L = max(0, â€–u - vâºâ€–Â² - â€–u - vâ»â€–Â² + Îµ)
```

**åº”ç”¨**ï¼š

- è¯­ä¹‰æœç´¢
- èšç±»
- é‡å¤æ£€æµ‹

### 6.3 å¯¹æ¯”å­¦ä¹ 

**SimCSE (Gao et al., 2021)**:

**æ— ç›‘ç£ç‰ˆæœ¬**ï¼š

```text
åŒä¸€å¥å­é€šè¿‡Dropoutä¸¤æ¬¡ â†’ æ­£æ ·æœ¬å¯¹
ä¸åŒå¥å­ â†’ è´Ÿæ ·æœ¬å¯¹
```

**ç›‘ç£ç‰ˆæœ¬**ï¼š

```text
NLIæ•°æ®é›†ï¼š
è•´å«å¥ â†’ æ­£æ ·æœ¬
çŸ›ç›¾å¥ â†’ è´Ÿæ ·æœ¬
```

**æ•ˆæœ**ï¼š

- æ˜¾è‘—æå‡å¥å­è¡¨ç¤ºè´¨é‡
- ç®€å•ä½†æœ‰æ•ˆ

## 7. è¯„ä¼°åµŒå…¥è´¨é‡ | Evaluating Embeddings

### 7.1 å†…åœ¨è¯„ä¼°

**è¯ç›¸ä¼¼åº¦ä»»åŠ¡**ï¼š

ä¸äººç±»ç›¸ä¼¼åº¦åˆ¤æ–­çš„ç›¸å…³æ€§

æ•°æ®é›†ï¼šWordSim-353, SimLex-999

**è¯ç±»æ¯”ä»»åŠ¡**ï¼š

```text
man : woman :: king : ?
```

å‡†ç¡®ç‡ï¼šç­”æ¡ˆåœ¨top-kä¸­

### 7.2 å¤–åœ¨è¯„ä¼°

**ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**ï¼š

- æ–‡æœ¬åˆ†ç±»
- å‘½åå®ä½“è¯†åˆ«
- æƒ…æ„Ÿåˆ†æ
- é—®ç­”

**æ ‡å‡†**ï¼š

- å›ºå®šåµŒå…¥ + æµ…å±‚æ¨¡å‹
- æˆ–å¾®è°ƒæ•´ä¸ªæ¨¡å‹

### 7.3 è¯„ä¼°çš„æŒ‘æˆ˜

**é—®é¢˜**ï¼š

- å†…åœ¨vså¤–åœ¨è¯„ä¼°ç›¸å…³æ€§å¼±
- ä»»åŠ¡ä¾èµ–æ€§
- æ²¡æœ‰å•ä¸€"æœ€ä½³"åµŒå…¥

**å®è·µ**ï¼š

- é’ˆå¯¹åº”ç”¨åœºæ™¯è¯„ä¼°
- å¤šä¸ªä»»åŠ¡å¹³å‡
- A/Bæµ‹è¯•

## 8. ç†è®ºç†è§£ | Theoretical Understanding

### 8.1 ä¸ºä»€ä¹ˆä½ç»´æœ‰æ•ˆï¼Ÿ

**æµå½¢å‡è®¾**ï¼š

- è¯­è¨€æ•°æ®åœ¨ä½ç»´æµå½¢ä¸Š
- é«˜ç»´åµŒå…¥æ˜¯å†—ä½™çš„

**ä¿¡æ¯è®ºè§†è§’**ï¼š

- è¯çš„ä¿¡æ¯æœ‰é™
- d=100-1000ç»´è¶³å¤Ÿç¼–ç 

### 8.2 PMIçŸ©é˜µåˆ†è§£

**Levy & Goldberg (2014)**:

**å®šç†**ï¼š

Skip-Gram with negative samplingç­‰ä»·äºéšå¼åˆ†è§£ï¼š

```text
v(w)áµ€ v'(c) = PMI(w, c) - log k
```

å…¶ä¸­ k æ˜¯è´Ÿæ ·æœ¬æ•°

**æ„ä¹‰**ï¼š

- Word2Vec â‰ˆ çŸ©é˜µåˆ†è§£
- è¿æ¥ç¥ç»æ–¹æ³•ä¸ä¼ ç»Ÿæ–¹æ³•

### 8.3 ä¸Šä¸‹æ–‡åµŒå…¥çš„è¡¨è¾¾èƒ½åŠ›

**é—®é¢˜**ï¼šä¸ºä»€ä¹ˆTransformerå­¦åˆ°çš„è¡¨ç¤ºè¿™ä¹ˆå¼ºï¼Ÿ

**å‡è¯´**ï¼š

1. **å±‚æ¬¡ç»„åˆæ€§**ï¼šåº•å±‚è¯­æ³• â†’ é«˜å±‚è¯­ä¹‰
2. **æ³¨æ„åŠ›æ£€ç´¢**ï¼šåŠ¨æ€èšåˆç›¸å…³ä¿¡æ¯
3. **å¤§è§„æ¨¡é¢„è®­ç»ƒ**ï¼šå­¦ä¹ ä¸°å¯Œè¯­è¨€çŸ¥è¯†

**ç†è®ºç©ºç™½**ï¼šå®Œæ•´ç†è§£ä»æ˜¯å¼€æ”¾é—®é¢˜

## 9. æƒå¨å‚è€ƒæ–‡çŒ® | Authoritative References

### å­¦æœ¯è®ºæ–‡

1. **Mikolov, T., et al. (2013)**. "Efficient estimation of word representations in vector space". *ICLR*.
2. **Mikolov, T., et al. (2013)**. "Distributed representations of words and phrases". *NeurIPS*.
3. **Pennington, J., et al. (2014)**. "GloVe: Global vectors for word representation". *EMNLP*.
4. **Peters, M. E., et al. (2018)**. "Deep contextualized word representations". *NAACL*.
5. **Levy, O., & Goldberg, Y. (2014)**. "Neural word embedding as implicit matrix factorization". *NeurIPS*.
6. **Bolukbasi, T., et al. (2016)**. "Man is to computer programmer as woman is to homemaker?". *NeurIPS*.
7. **Gao, T., et al. (2021)**. "SimCSE: Simple contrastive learning of sentence embeddings". *EMNLP*.

### æ ‡å‡†æ•™æ

1. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).
2. **Goldberg, Y. (2017)**. *Neural Network Methods for Natural Language Processing*. Morgan & Claypool.

## 10. å…³é”®è¦ç‚¹æ€»ç»“ | Key Takeaways

1. **åˆ†å¸ƒå¼è¡¨ç¤º**ï¼šä½ç»´ç¨ å¯†å‘é‡ï¼Œç¼–ç è¯­ä¹‰
2. **Word2Vec**ï¼šé¢„æµ‹å¼ï¼Œè´Ÿé‡‡æ ·é«˜æ•ˆ
3. **GloVe**ï¼šè®¡æ•°å¼ï¼Œæ‹Ÿåˆå…±ç°ç»Ÿè®¡
4. **ä¸Šä¸‹æ–‡åµŒå…¥**ï¼šè§£å†³ä¸€è¯å¤šä¹‰ï¼Œç°ä»£æ ‡å‡†
5. **å‡ ä½•æ€§è´¨**ï¼šç±»æ¯”å…³ç³»ï¼Œå‘é‡è¿ç®—æœ‰æ„ä¹‰
6. **åè§é—®é¢˜**ï¼šåµŒå…¥ç»§æ‰¿è®­ç»ƒæ•°æ®åè§
7. **ç†è®ºè”ç³»**ï¼šç¥ç»æ–¹æ³• â‰ˆ çŸ©é˜µåˆ†è§£
8. **è¯„ä¼°å¤šæ ·**ï¼šå†…åœ¨+å¤–åœ¨ï¼Œä»»åŠ¡ä¾èµ–
9. **æŒç»­æ¼”è¿›**ï¼šé™æ€ â†’ ä¸Šä¸‹æ–‡ â†’ å¤§æ¨¡å‹åµŒå…¥
10. **ç†è®ºæœªå®Œ**ï¼šä¸ºä½•æœ‰æ•ˆä»éœ€æ·±å…¥ç ”ç©¶

---

**ä¸‹ä¸€æ­¥é˜…è¯»**ï¼š

- [03.2 ç¥ç»è¯­è¨€æ¨¡å‹](03.2_Neural_Language_Models.md)
- [03.6 ä¸Šä¸‹æ–‡çª—å£ä¸è®°å¿†æœºåˆ¶](03.6_Context_Window_Memory.md)
- [04.1 è¯­ä¹‰å‘é‡ç©ºé—´](../04_Semantic_Models/04.1_Semantic_Vector_Spaces.md)
