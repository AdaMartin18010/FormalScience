# ç¥ç»è¯­è¨€æ¨¡å‹ | Neural Language Models

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **æœ€åæ›´æ–°**: 2025-10-27  
> **æ–‡æ¡£è§„æ¨¡**: 675è¡Œ | ç¥ç»è¯­è¨€æ¨¡å‹ç†è®ºä¸å®è·µ  
> **é˜…è¯»å»ºè®®**: æœ¬æ–‡ç³»ç»Ÿä»‹ç»RNN/LSTMç­‰ç¥ç»è¯­è¨€æ¨¡å‹ï¼Œè¿æ¥ç»Ÿè®¡æ¨¡å‹ä¸ç°ä»£å¤§æ¨¡å‹

---

## ç›®å½• | Table of Contents

- [ç¥ç»è¯­è¨€æ¨¡å‹ | Neural Language Models](#ç¥ç»è¯­è¨€æ¨¡å‹--neural-language-models)

## æ ¸å¿ƒæ¦‚å¿µæ·±åº¦åˆ†æ

<details>
<summary><b>ğŸ§ ğŸ”„ ç‚¹å‡»å±•å¼€ï¼šç¥ç»è¯­è¨€æ¨¡å‹æ¼”è¿›å…¨æ™¯æ·±åº¦åˆ†æ</b></summary>

æœ¬èŠ‚æ·±å…¥å‰–æç¥ç»è¯­è¨€æ¨¡å‹ä»å‰é¦ˆç½‘ç»œåˆ°å¾ªç¯ç½‘ç»œå†åˆ°é¢„è®­ç»ƒæ¨¡å‹çš„å®Œæ•´æ¼”è¿›ï¼Œæ­ç¤ºä»ç¬¦å·åˆ°å‘é‡ã€ä»ç»Ÿè®¡åˆ°ç¥ç»çš„æ·±åˆ»å˜é©ã€‚

### 1ï¸âƒ£ ç¥ç»è¯­è¨€æ¨¡å‹æ¦‚å¿µå®šä¹‰å¡

**æ¦‚å¿µåç§°**: ç¥ç»è¯­è¨€æ¨¡å‹ï¼ˆNeural Language Model, NLMï¼‰

**å†…æ¶µï¼ˆæœ¬è´¨å±æ€§ï¼‰**:

**ğŸ”¹ æ ¸å¿ƒå®šä¹‰**:
ä½¿ç”¨ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„æ¦‚ç‡æ¨¡å‹ï¼Œå»ºæ¨¡è‡ªç„¶è¯­è¨€åºåˆ—çš„åˆ†å¸ƒï¼š
$$
P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^{T} P_\theta(w_t \mid w_1, \ldots, w_{t-1})
$$
å…¶ä¸­ $\theta$ æ˜¯ç¥ç»ç½‘ç»œå‚æ•°ã€‚

**ğŸ”¹ ä¸‰å¤§é©å‘½æ€§çªç ´**:

| ç»´åº¦ | ç»Ÿè®¡n-gram | ç¥ç»è¯­è¨€æ¨¡å‹ | çªç ´æ„ä¹‰ |
|------|-----------|-------------|---------|
| **è¯è¡¨ç¤º** | one-hotç¦»æ•£ | åˆ†å¸ƒå¼å‘é‡embedding | ç¼–ç è¯­ä¹‰ç›¸ä¼¼æ€§ |
| **ä¸Šä¸‹æ–‡** | å›ºå®šçª—å£n-1è¯ | ç†è®ºä¸Šæ— é™ï¼ˆRNNï¼‰ | é•¿ç¨‹ä¾èµ– |
| **æ³›åŒ–** | æœªè§n-gram = 0 | å¹³æ»‘æ³›åŒ– | ç»„åˆæ€§ |

**ğŸ”¹ åˆ†å¸ƒå‡è®¾ï¼ˆç†è®ºåŸºç¡€ï¼‰**:
- **Harris (1954)**: "Words in similar contexts have similar meanings"
- **Firth (1957)**: "You shall know a word by the company it keeps"
- **æ•°å­¦å½¢å¼åŒ–**: $\text{sim}(w_i, w_j) \propto \text{sim}(\text{context}(w_i), \text{context}(w_j))$

**å¤–å»¶ï¼ˆèŒƒå›´è¾¹ç•Œï¼‰**:

| ç»´åº¦ | åŒ…å« âœ… | ä¸åŒ…å« âŒ |
|------|---------|----------|
| **æ¨¡å‹ç±»å‹** | å‰é¦ˆã€RNNã€LSTMã€GRU | Transformerï¼ˆç‹¬ç«‹ç« èŠ‚ï¼‰ |
| **è®­ç»ƒèŒƒå¼** | é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ | ç›‘ç£å¾®è°ƒï¼ˆä¸‹æ¸¸ä»»åŠ¡ï¼‰ |
| **ç²’åº¦** | è¯çº§ã€å­—ç¬¦çº§ã€å­è¯çº§ | å¥å­çº§ã€æ–‡æ¡£çº§ï¼ˆä¸“é—¨æ¨¡å‹ï¼‰ |
| **æ–¹å‘æ€§** | å•å‘ã€åŒå‘ | å¤šæ¨¡æ€ï¼ˆéçº¯æ–‡æœ¬ï¼‰ |

**å±æ€§ç»´åº¦è¡¨**:

| ç»´åº¦ | å€¼/æè¿° | è¯´æ˜ |
|------|---------|------|
| **å¥ åŸºè®ºæ–‡** | Bengio et al. 2003 | FFNN-LMï¼Œé¦–ä¸ªç¥ç»LM |
| **çªç ´æ—¶åˆ»** | LSTM-LM (2012) | å›°æƒ‘åº¦æå‡35% |
| **é¢„è®­ç»ƒå…ƒå¹´** | 2018 (ELMo, ULMFiT) | è¿ç§»å­¦ä¹ èŒƒå¼ |
| **å‚æ•°é‡** | ç™¾ä¸‡-æ•°äº¿ï¼ˆé¢„Transformerï¼‰ | æ¯”n-gramå¤šï¼Œæ¯”LLMå°‘ |
| **è®­ç»ƒæ—¶é—´** | GPUå¤©-å‘¨ | æ¯”n-gramæ…¢ï¼Œæ¯”LLMå¿« |
| **å…¸å‹å›°æƒ‘åº¦** | PTB: 78 (LSTM) â†’ 60 (é¢„è®­ç»ƒ) | æŒç»­æ”¹è¿› |
| **é•¿ç¨‹ä¾èµ–** | âš ï¸ æœ‰é™ï¼ˆå‡ åtokensï¼‰ | LSTMæ ¸å¿ƒä¼˜åŠ¿ |
| **å¯è§£é‡Šæ€§** | âš ï¸ é»‘ç›’ | vs n-gramé€æ˜è§„åˆ™ |

---

### 2ï¸âƒ£ ç¥ç»è¯­è¨€æ¨¡å‹æ¼”è¿›å…¨æ™¯å›¾

```mermaid
graph TB
    Traditional[ç»Ÿè®¡n-gram<br/>1980-2000s]
    
    Traditional --> T1[ç¨€ç–æ€§]
    Traditional --> T2[ç»´åº¦ç¾éš¾]
    Traditional --> T3[æ— è¯­ä¹‰]
    
    T1 --> Breakthrough[é©å‘½æ€§çªç ´<br/>2003]
    T2 --> Breakthrough
    T3 --> Breakthrough
    
    Breakthrough --> Bengio[Bengio FFNN-LM<br/>2003]
    Bengio --> B1[åˆ†å¸ƒå¼è¯è¡¨ç¤º]
    Bengio --> B2[å‚æ•°å…±äº«]
    Bengio --> B3[å¹³æ»‘æ³›åŒ–]
    
    B1 --> RNN_Era[RNNæ—¶ä»£<br/>2010-2015]
    B2 --> RNN_Era
    B3 --> RNN_Era
    
    RNN_Era --> R1[æ ‡å‡†RNN<br/>æ¢¯åº¦æ¶ˆå¤±]
    RNN_Era --> R2[LSTM 2012<br/>35%æå‡]
    RNN_Era --> R3[GRU 2014<br/>æ›´ç®€å•]
    
    R2 --> Advanced[é«˜çº§æŠ€æœ¯<br/>2015-2017]
    R3 --> Advanced
    
    Advanced --> A1[Variational Dropout<br/>2016]
    Advanced --> A2[Layer Norm<br/>2016]
    Advanced --> A3[æ®‹å·®è¿æ¥<br/>ResNeté£æ ¼]
    Advanced --> A4[æ³¨æ„åŠ›æœºåˆ¶<br/>2014-2017]
    
    A1 --> PreTrain[é¢„è®­ç»ƒæ—¶ä»£<br/>2018]
    A2 --> PreTrain
    A3 --> PreTrain
    A4 --> PreTrain
    
    PreTrain --> P1[ULMFiT 2018<br/>è¿ç§»å­¦ä¹ ]
    PreTrain --> P2[ELMo 2018<br/>åŒå‘ä¸Šä¸‹æ–‡]
    
    P1 --> Transformer[Transformeræ—¶ä»£<br/>2018+]
    P2 --> Transformer
    
    Transformer --> Tr1[BERT 2018]
    Transformer --> Tr2[GPT-2/3 2019/20]
    Transformer --> Tr3[LLMæ—¶ä»£]
    
    style Traditional fill:#ffd93d,stroke:#333,stroke-width:2px
    style Breakthrough fill:#ff6b6b,stroke:#333,stroke-width:4px
    style RNN_Era fill:#6bcf7f,stroke:#333,stroke-width:3px
    style PreTrain fill:#a29bfe,stroke:#333,stroke-width:3px
```

---

### 3ï¸âƒ£ æ¨¡å‹æ¶æ„è¯¦ç»†å¯¹æ¯”çŸ©é˜µ

| æ¨¡å‹ | ä¸Šä¸‹æ–‡é•¿åº¦ | æ¢¯åº¦é—®é¢˜ | å‚æ•°é‡ | å›°æƒ‘åº¦(PTB) | è®­ç»ƒé€Ÿåº¦ | ä¸»è¦ä¼˜åŠ¿ | ä¸»è¦å±€é™ |
|------|-----------|---------|--------|------------|---------|---------|---------|
| **n-gram** | å›ºå®šn-1 | N/A | æå¤§ | ~140 | âœ…âœ…âœ… å¿« | é€æ˜ã€å¿«é€Ÿ | ç¨€ç–ã€æ— æ³›åŒ– |
| **Bengio FFNN** | å›ºå®šn-1 | âœ… æ—  | ä¸­ç­‰ | ~110 | âœ…âœ… è¾ƒå¿« | é¦–ä¸ªç¥ç»LM | å›ºå®šçª—å£ |
| **æ ‡å‡†RNN** | ç†è®ºâˆ | âŒâŒ ä¸¥é‡ | å° | ~120 | âœ… ä¸­ç­‰ | ä»»æ„é•¿åº¦ | æ¢¯åº¦æ¶ˆå¤± |
| **LSTM** | å®é™…~100 | âš ï¸ ç¼“è§£ | ä¸­ç­‰ | ~78 | âš ï¸ è¾ƒæ…¢ | é•¿ç¨‹ä¾èµ– | å¤æ‚ã€æ…¢ |
| **GRU** | å®é™…~100 | âš ï¸ ç¼“è§£ | è¾ƒå° | ~80 | âœ… ä¸­ç­‰ | ç®€å•é«˜æ•ˆ | ç•¥é€ŠLSTM |
| **ELMo** | å®é™…~200 | âš ï¸ ç¼“è§£ | å¤§ | ~60 | âŒ æ…¢ | ä¸Šä¸‹æ–‡åŒ–embedding | è®¡ç®—å¯†é›† |

**å…³é”®æ´å¯Ÿ**:
- **LSTMæ˜¯RNNæ—¶ä»£çš„å·…å³°**: 35%å›°æƒ‘åº¦æå‡ï¼Œè§£é”é•¿ç¨‹ä¾èµ–
- **å‚æ•°é‡-æ€§èƒ½trade-off**: æ›´å¤§æ¨¡å‹æ€§èƒ½æ›´å¥½ï¼Œä½†æ”¶ç›Šé€’å‡
- **é¢„è®­ç»ƒæ”¹å˜æ¸¸æˆè§„åˆ™**: 2018å¹´åèŒƒå¼è½¬ç§»

---

### 4ï¸âƒ£ RNNæ¶æ„æ·±åº¦è§£ææ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((RNNè¯­è¨€æ¨¡å‹))
    æ¶æ„ç»„ä»¶
      åµŒå…¥å±‚
        è¯â†’å‘é‡ â„áµˆ
        å…±äº«å‚æ•°
        è¯­ä¹‰ç¼–ç 
      å¾ªç¯å±‚
        éšçŠ¶æ€ hâ‚œ
        æ—¶é—´å±•å¼€
        è®°å¿†æœºåˆ¶
      è¾“å‡ºå±‚
        åˆ†ç±»å™¨
        |V|ç»´logits
        Softmaxå½’ä¸€åŒ–
    è®­ç»ƒç®—æ³•
      BPTT
        å±•å¼€Tæ­¥
        é“¾å¼æ³•åˆ™
        æ¢¯åº¦å›ä¼ 
      æ¢¯åº¦é—®é¢˜
        æ¶ˆå¤± <1
        çˆ†ç‚¸ >1
        æ¢¯åº¦è£å‰ª
      ä¼˜åŒ–å™¨
        SGD
        Adam
        å­¦ä¹ ç‡è°ƒåº¦
    LSTMæ”¹è¿›
      é—¨æœºåˆ¶
        é—å¿˜é—¨ fâ‚œ
        è¾“å…¥é—¨ iâ‚œ
        è¾“å‡ºé—¨ oâ‚œ
      CellçŠ¶æ€
        é•¿æœŸè®°å¿†
        çº¿æ€§ä¼ æ’­
        ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
      æ€§èƒ½
        PTB: 78
        35%æå‡
        æ–°æ ‡å‡†
    GRUç®€åŒ–
      ä¸¤é—¨
        æ›´æ–°é—¨ zâ‚œ
        é‡ç½®é—¨ râ‚œ
      æ›´ç®€å•
        2/3å‚æ•°
        æ›´å¿«è®­ç»ƒ
      æ€§èƒ½ç›¸å½“
        ç•¥é€ŠLSTM
        å®ç”¨æŠ˜è¡·
    é«˜çº§æŠ€æœ¯
      æ­£åˆ™åŒ–
        Variational Dropout
        åŒmaskè·¨æ—¶é—´
        é˜²è¿‡æ‹Ÿåˆ
      å½’ä¸€åŒ–
        Layer Norm
        ç¨³å®šè®­ç»ƒ
        åŠ é€Ÿæ”¶æ•›
      æ®‹å·®
        è·³è·ƒè¿æ¥
        æ·±å±‚å¯è®­ç»ƒ
      æ³¨æ„åŠ›
        é€‰æ‹©æ€§èšç„¦
        æƒé‡åˆ†é…
        Transformerå‰èº«
```

---

### 5ï¸âƒ£ ä»ç¬¦å·åˆ°å‘é‡çš„èŒƒå¼è½¬å˜

| ç»´åº¦ | ç¬¦å·èŒƒå¼ï¼ˆn-gramï¼‰ | å‘é‡èŒƒå¼ï¼ˆNeural LMï¼‰ | é©å‘½æ€§æ„ä¹‰ |
|------|-------------------|---------------------|-----------|
| **è¡¨ç¤ºç©ºé—´** | ç¦»æ•£one-hot $\{0,1\}^{\|V\|}$ | è¿ç»­dense $\mathbb{R}^d$ | ç¼–ç è¯­ä¹‰ |
| **ç›¸ä¼¼æ€§** | $\delta_{ij}$ (å®Œå…¨ç‹¬ç«‹) | $\cos(\mathbf{v}_i, \mathbf{v}_j)$ | å¹³æ»‘æ³›åŒ– |
| **ç»´åº¦** | $\|V\|$ (10K-100K) | $d$ (50-1000) | é™ç»´ |
| **ç¨€ç–æ€§** | æåº¦ç¨€ç– | ç¨ å¯† | ä¿¡æ¯å¯†åº¦ |
| **ç»„åˆæ€§** | âŒ æ—  | âœ… å‘é‡è¿ç®— | king-man+womanâ‰ˆqueen |
| **å­¦ä¹ æ–¹å¼** | ç»Ÿè®¡è®¡æ•° | åå‘ä¼ æ’­ | ç«¯åˆ°ç«¯ä¼˜åŒ– |

**åˆ†å¸ƒå‡è®¾çš„æ•°å­¦å½¢å¼åŒ–**:

$$
\begin{align}
\text{Context}(w) &= \{w' : P(w' \mid w) > \epsilon\} \\
\text{sim}_{\text{context}}(w_i, w_j) &= \frac{|\text{Context}(w_i) \cap \text{Context}(w_j)|}{|\text{Context}(w_i) \cup \text{Context}(w_j)|} \\
\text{sim}_{\text{vector}}(w_i, w_j) &= \cos(\mathbf{v}_i, \mathbf{v}_j) = \frac{\mathbf{v}_i \cdot \mathbf{v}_j}{\|\mathbf{v}_i\| \|\mathbf{v}_j\|}
\end{align}
$$

**ç›®æ ‡**: $\text{sim}_{\text{vector}} \approx \text{sim}_{\text{context}}$

---

### 6ï¸âƒ£ LSTMé—¨æœºåˆ¶è¯¦ç»†åˆ†æ

**LSTMè§£å†³æ¢¯åº¦æ¶ˆå¤±çš„æ•°å­¦åŸç†**:

**æ ‡å‡†RNNæ¢¯åº¦**:
$$
\frac{\partial h_t}{\partial h_{t-k}} = \prod_{i=1}^{k} \frac{\partial h_{t-i+1}}{\partial h_{t-i}} = \prod_{i=1}^{k} W^T \cdot \text{diag}(\sigma'(z_{t-i}))
$$

å½“ $k$ å¤§æ—¶ï¼Œè‹¥ $\|W\| < 1 \land \sigma' < 1 \Rightarrow$ æ¢¯åº¦æŒ‡æ•°çº§æ¶ˆå¤±ã€‚

**LSTM CellçŠ¶æ€æ¢¯åº¦**:
$$
\frac{\partial C_t}{\partial C_{t-k}} = \prod_{i=1}^{k} f_{t-i+1}
$$

å…³é”®ï¼š**çº¿æ€§ä¾èµ–**ï¼Œé—å¿˜é—¨$f \approx 1$æ—¶æ¢¯åº¦ä¿æŒï¼

**é—¨æœºåˆ¶å¯¹æ¯”**:

| é—¨ | å…¬å¼ | ä½œç”¨ | æ•°å­¦ç›´è§‰ |
|---|------|------|---------|
| **é—å¿˜é—¨** | $f_t = \sigma(W_f [h_{t-1}, x_t])$ | ä¿ç•™å¤šå°‘å†å² | æ§åˆ¶æ¢¯åº¦è¡°å‡é€Ÿç‡ |
| **è¾“å…¥é—¨** | $i_t = \sigma(W_i [h_{t-1}, x_t])$ | æ¥å—å¤šå°‘æ–°ä¿¡æ¯ | é€‰æ‹©æ€§æ›´æ–° |
| **è¾“å‡ºé—¨** | $o_t = \sigma(W_o [h_{t-1}, x_t])$ | è¾“å‡ºå¤šå°‘åˆ°éšè—å±‚ | æ§åˆ¶ä¿¡æ¯æµå‡º |

**Cellæ›´æ–°å…¬å¼**:
$$
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
$$

**åŠ æ³•æ“ä½œ**æ˜¯å…³é”®ï¼šæ¢¯åº¦å¯ä»¥"è·³è¿‡"å¤šä¸ªæ—¶é—´æ­¥ä¼ æ’­ï¼

---

### 7ï¸âƒ£ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ¼”è¿›è·¯å¾„

| æ¨¡å‹ | å¹´ä»½ | æ¶æ„ | è®­ç»ƒä»»åŠ¡ | å…³é”®åˆ›æ–° | å½±å“ |
|------|------|------|---------|---------|------|
| **Word2Vec** | 2013 | æµ…å±‚FFNN | Skip-gram/CBOW | é™æ€è¯å‘é‡ | âš ï¸âš ï¸âš ï¸ å¥ åŸº |
| **GloVe** | 2014 | çŸ©é˜µåˆ†è§£ | å…±ç°çŸ©é˜µ | å…¨å±€ç»Ÿè®¡ | âš ï¸âš ï¸ è¡¥å…… |
| **ULMFiT** | 2018 | LSTM | è¯­è¨€å»ºæ¨¡ | è¿ç§»å­¦ä¹ ä¸‰æ­¥ | âš ï¸âš ï¸âš ï¸âš ï¸ èŒƒå¼è½¬ç§» |
| **ELMo** | 2018 | åŒå‘LSTM | åŒå‘LM | ä¸Šä¸‹æ–‡åŒ–embedding | âš ï¸âš ï¸âš ï¸âš ï¸ çªç ´ |
| **BERT** | 2018 | Transformer | Masked LM | åŒå‘Transformer | âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸ é©å‘½ |

**ULMFiTä¸‰æ­¥è¿ç§»å­¦ä¹ **:
1. **é€šç”¨åŸŸé¢„è®­ç»ƒ**: å¤§è§„æ¨¡è¯­æ–™è¯­è¨€å»ºæ¨¡
2. **ç›®æ ‡åŸŸå¾®è°ƒ**: ç›®æ ‡é¢†åŸŸç»§ç»­è¯­è¨€å»ºæ¨¡
3. **åˆ†ç±»å™¨å¾®è°ƒ**: ä¸‹æ¸¸ä»»åŠ¡æœ‰ç›‘ç£è®­ç»ƒ

**ELMoä¸Šä¸‹æ–‡åŒ–embedding**:
$$
\text{ELMo}(w_k) = \gamma \sum_{j=0}^{L} s_j \mathbf{h}_{k,j}^{LM}
$$
å…¶ä¸­ $\mathbf{h}_{k,j}^{LM}$ æ˜¯ç¬¬$j$å±‚çš„éšè—çŠ¶æ€ï¼Œ$s_j$æ˜¯å­¦ä¹ çš„æƒé‡ã€‚

**å…³é”®æ´å¯Ÿ**: ä¸åŒå±‚ç¼–ç ä¸åŒä¿¡æ¯
- **åº•å±‚**: è¯­æ³•ï¼ˆPOS taggingå¥½ï¼‰
- **é«˜å±‚**: è¯­ä¹‰ï¼ˆWSDå¥½ï¼‰

---

### 8ï¸âƒ£ ç†è®ºèƒ½åŠ›åˆ†æ

**è¡¨è¾¾èƒ½åŠ›ï¼ˆChomskyå±‚æ¬¡ï¼‰**:

| æ¨¡å‹ç±»å‹ | ç†è®ºèƒ½åŠ› | å®é™…èƒ½åŠ› | å·®è·åŸå›  |
|---------|---------|---------|---------|
| **å‰é¦ˆFFNN** | æ­£åˆ™è¯­è¨€REG | REG | å›ºå®šçª—å£é™åˆ¶ |
| **RNNï¼ˆç†è®ºï¼‰** | å›¾çµå®Œå¤‡RE | - | æ— é™ç²¾åº¦å®æ•° |
| **RNNï¼ˆå®é™…ï¼‰** | - | REGåˆ°ç®€å•CFL | æœ‰é™ç²¾åº¦ã€æ¢¯åº¦æ¶ˆå¤± |
| **LSTMï¼ˆå®é™…ï¼‰** | - | ç®€å•CFL | ç¼“è§£ä½†æœªè§£å†³ |

**æ³›åŒ–èƒ½åŠ›**:

$$
\begin{align}
\text{æ³›åŒ–è¯¯å·®} &= \mathbb{E}_{(x,y) \sim \mathcal{D}}[\mathcal{L}(f_\theta(x), y)] \\
&\leq \text{è®­ç»ƒè¯¯å·®} + \mathcal{O}\left(\sqrt{\frac{d}{m}}\right)
\end{align}
$$

å…¶ä¸­ $d$ æ˜¯å‚æ•°é‡ï¼Œ$m$ æ˜¯è®­ç»ƒæ ·æœ¬æ•°ã€‚

**é•¿ç¨‹ä¾èµ–ç†è®ºé™åˆ¶**:

**Bengio et al. (1994)**: æ¢¯åº¦æ¶ˆå¤±æ˜¯æ ¹æœ¬é™åˆ¶
$$
\left\|\frac{\partial \mathcal{L}_t}{\partial h_{t-\tau}}\right\| \leq \eta^{\tau}
$$
å…¶ä¸­ $\eta < 1$ æ˜¯è¡°å‡å› å­ï¼Œ$\tau$ æ˜¯è·ç¦»ã€‚

**LSTMç¼“è§£ä½†æœªè§£å†³**: å®é™…æœ‰æ•ˆè·ç¦»~100 tokens

---

### 9ï¸âƒ£ æ€§èƒ½å¯¹æ¯”ä¸æœªæ¥æ–¹å‘

**Penn Treebankå›°æƒ‘åº¦å†å²**:

| å¹´ä»½ | æ¨¡å‹ | å›°æƒ‘åº¦ | æå‡ | å…³é”®æŠ€æœ¯ |
|------|------|--------|------|---------|
| ~2000 | 5-gram+KN | ~140 | baseline | ç»Ÿè®¡å¹³æ»‘ |
| 2003 | Bengio FFNN | ~110 | 21% | ç¥ç»ç½‘ç»œ |
| 2010 | RNN-LM | ~120 | -9% | å¾ªç¯ç»“æ„ |
| 2012 | LSTM-LM | ~78 | 35% | é—¨æœºåˆ¶ |
| 2016 | LSTM+Dropout | ~73 | 6% | æ­£åˆ™åŒ– |
| 2018 | ELMo | ~60 | 18% | é¢„è®­ç»ƒ |
| 2018+ | Transformer | ~20-30 | 50-67% | æ³¨æ„åŠ›+å¤§è§„æ¨¡ |

**æœªæ¥æ–¹å‘**:

```yaml
çŸ­æœŸï¼ˆ2025-2027ï¼‰:
  - é•¿ä¸Šä¸‹æ–‡RNN: æ”¹è¿›LSTMé—¨æœºåˆ¶
  - æ··åˆæ¶æ„: RNN+Transformerä¼˜åŠ¿ç»“åˆ
  - é«˜æ•ˆè®­ç»ƒ: ç¨€ç–æ¿€æ´»ã€é‡åŒ–

ä¸­æœŸï¼ˆ2027-2030ï¼‰:
  - ç¥ç»ç¬¦å·LM: ç»“åˆç¬¦å·è§„åˆ™
  - å¯è§£é‡ŠLM: ç†è§£å†…éƒ¨è¡¨ç¤º
  - æŒç»­å­¦ä¹ : åœ¨çº¿æ›´æ–°æ— ç¾éš¾æ€§é—å¿˜

é•¿æœŸï¼ˆ2030+ï¼‰:
  - ç±»äººè¯­è¨€ä¹ å¾—: å°‘æ ·æœ¬å­¦ä¹ 
  - å› æœè¯­è¨€æ¨¡å‹: è¶…è¶Šå…³è”åˆ°å› æœ
  - æ„è¯†è¯­è¨€æ¨¡å‹: è‡ªæˆ‘åæ€èƒ½åŠ›
```

---

### ğŸ”Ÿ æ ¸å¿ƒæ´å¯Ÿä¸è®¾è®¡åŸåˆ™

**äº”å¤§æ ¸å¿ƒå®šå¾‹**:

1. **åˆ†å¸ƒå‡è®¾å®šå¾‹**
   $$
   \text{è¯­ä¹‰ç›¸ä¼¼æ€§} \propto \text{ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§}
   $$
   - ç¥ç»LMçš„ç†è®ºåŸºç¡€
   - ä»å…±ç°å­¦ä¹ æ„ä¹‰

2. **å‚æ•°å…±äº«å®šå¾‹**
   $$
   \text{æ³›åŒ–èƒ½åŠ›} \propto \frac{1}{\text{ç‹¬ç«‹å‚æ•°æ•°}}
   $$
   - åµŒå…¥å±‚å…³é”®ï¼š$|V|d \ll |V|^2$
   - æ‰“ç ´ç¨€ç–æ€§è¯…å’’

3. **æ¢¯åº¦è¡°å‡å®šå¾‹**
   $$
   \left\|\frac{\partial \mathcal{L}}{\partial h_{t-\tau}}\right\| \sim e^{-\tau/\lambda}
   $$
   - RNNæ ¹æœ¬é™åˆ¶
   - LSTMç¼“è§£ä½†æœªè§£å†³

4. **é—¨æ§ä¿æŠ¤å®šå¾‹**
   $$
   \frac{\partial C_t}{\partial C_{t-k}} = \prod f_i \approx 1 \quad \text{å½“} \; f_i \approx 1
   $$
   - LSTMæ ¸å¿ƒæœºåˆ¶
   - çº¿æ€§ä¼ æ’­ä¿æŒæ¢¯åº¦

5. **é¢„è®­ç»ƒè¿ç§»å®šå¾‹**
   $$
   \text{ä¸‹æ¸¸æ€§èƒ½} = f(\text{é¢„è®­ç»ƒæ•°æ®é‡}, \text{æ¨¡å‹å®¹é‡})
   $$
   - 2018å¹´èŒƒå¼è½¬ç§»
   - æ•°æ®å’Œè§„æ¨¡æ˜¯ç‹é“

**å®è·µè®¾è®¡åŸåˆ™**:

```yaml
åŸåˆ™1_åµŒå…¥ä¼˜å…ˆ:
  è®¤çŸ¥: åˆ†å¸ƒå¼è¡¨ç¤ºæ˜¯åŸºç¡€
  è¡ŒåŠ¨: æŠ•èµ„é«˜è´¨é‡è¯åµŒå…¥
  ä¾‹å­: é¢„è®­ç»ƒWord2Vec/GloVe

åŸåˆ™2_é—¨æ§æœºåˆ¶:
  è®¤çŸ¥: é•¿ç¨‹ä¾èµ–éœ€è¦çº¿æ€§è·¯å¾„
  è¡ŒåŠ¨: ä¼˜å…ˆä½¿ç”¨LSTM/GRUè€ŒéRNN
  ä¾‹å­: é»˜è®¤LSTM foråºåˆ—å»ºæ¨¡

åŸåˆ™3_æ­£åˆ™åŒ–å¿…é¡»:
  è®¤çŸ¥: ç¥ç»LMæ˜“è¿‡æ‹Ÿåˆ
  è¡ŒåŠ¨: Dropout+LayerNormæ ‡é…
  ä¾‹å­: Variational Dropout for RNN

åŸåˆ™4_é¢„è®­ç»ƒä¸ºç‹:
  è®¤çŸ¥: æ— ç›‘ç£é¢„è®­ç»ƒæä¾›å¼ºå…ˆéªŒ
  è¡ŒåŠ¨: å°½å¯èƒ½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
  ä¾‹å­: ELMo â†’ BERT â†’ GPT

åŸåˆ™5_å›°æƒ‘åº¦éç»ˆç‚¹:
  è®¤çŸ¥: å›°æƒ‘åº¦æ˜¯æ‰‹æ®µéç›®çš„
  è¡ŒåŠ¨: å…³æ³¨ä¸‹æ¸¸ä»»åŠ¡å®é™…æ€§èƒ½
  ä¾‹å­: ä½å›°æƒ‘åº¦â‰ å¥½ç¿»è¯‘/é—®ç­”
```

**ç»ˆææ´å¯Ÿ**:

> **"ç¥ç»è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒä¸æ˜¯æ¨¡ä»¿n-gramï¼Œè€Œæ˜¯å­¦ä¹ è¯­è¨€çš„è¿ç»­å‡ ä½•ç»“æ„ã€‚ä»ç¦»æ•£ç¬¦å·åˆ°è¿ç»­å‘é‡ï¼Œä»å±€éƒ¨ç»Ÿè®¡åˆ°å…¨å±€è¯­ä¹‰ï¼Œè¿™æ˜¯æ·±åº¦å­¦ä¹ å¯¹NLPçš„æ ¹æœ¬è´¡çŒ®ã€‚LSTMæ—¶ä»£è§£å†³äº†'å¦‚ä½•è®°å¿†'ï¼ŒTransformeræ—¶ä»£è§£å†³äº†'å¦‚ä½•æ³¨æ„'ï¼Œä¸‹ä¸€ä¸ªæ—¶ä»£å°†è§£å†³'å¦‚ä½•ç†è§£'ã€‚"**

**å…ƒè®¤çŸ¥**:
- **åµŒå…¥æ˜¯æ¡¥æ¢**: è¿æ¥ç¦»æ•£è¯­è¨€å’Œè¿ç»­ä¼˜åŒ–
- **å¾ªç¯æ˜¯è®°å¿†**: RNNå¼•å…¥æ—¶é—´æ¦‚å¿µ
- **é—¨æ§æ˜¯æ™ºæ…§**: LSTMé€‰æ‹©æ€§è®°å¿†/é—å¿˜
- **é¢„è®­ç»ƒæ˜¯æ·å¾„**: ç«™åœ¨å·¨äººè‚©è†€ä¸Š
- **Transformeræ˜¯æœªæ¥**: ä½†RNNæ€æƒ³æ°¸å­˜

</details>

---

## ç›®å½• | Table of Contents

- [ç¥ç»è¯­è¨€æ¨¡å‹ | Neural Language Models](#ç¥ç»è¯­è¨€æ¨¡å‹--neural-language-models)
  - [ç›®å½• | Table of Contents](#ç›®å½•--table-of-contents)
  - [æ¦‚è¿° | Overview](#æ¦‚è¿°--overview)
  - [1. ä»ç¬¦å·åˆ°å‘é‡ | From Symbols to Vectors](#1-ä»ç¬¦å·åˆ°å‘é‡--from-symbols-to-vectors)
    - [1.1 è¯è¡¨ç¤ºçš„æ¼”è¿›](#11-è¯è¡¨ç¤ºçš„æ¼”è¿›)
    - [1.2 åˆ†å¸ƒå‡è®¾ (Distributional Hypothesis)](#12-åˆ†å¸ƒå‡è®¾-distributional-hypothesis)
    - [1.3 è¯åµŒå…¥çš„æ€§è´¨](#13-è¯åµŒå…¥çš„æ€§è´¨)
  - [2. å‰é¦ˆç¥ç»è¯­è¨€æ¨¡å‹ | Feed-Forward Neural Language Models](#2-å‰é¦ˆç¥ç»è¯­è¨€æ¨¡å‹--feed-forward-neural-language-models)
    - [2.1 Bengio æ¨¡å‹ (2003)](#21-bengio-æ¨¡å‹-2003)
    - [2.2 è®­ç»ƒç›®æ ‡](#22-è®­ç»ƒç›®æ ‡)
    - [2.3 Softmax ç“¶é¢ˆ](#23-softmax-ç“¶é¢ˆ)
    - [2.4 ä¼˜åŠ¿ä¸å±€é™](#24-ä¼˜åŠ¿ä¸å±€é™)
  - [3. å¾ªç¯ç¥ç»è¯­è¨€æ¨¡å‹ | Recurrent Neural Language Models](#3-å¾ªç¯ç¥ç»è¯­è¨€æ¨¡å‹--recurrent-neural-language-models)
    - [3.1 æ ‡å‡†RNNè¯­è¨€æ¨¡å‹](#31-æ ‡å‡†rnnè¯­è¨€æ¨¡å‹)
    - [3.2 BPTT ç®—æ³•](#32-bptt-ç®—æ³•)
    - [3.3 æ¢¯åº¦é—®é¢˜](#33-æ¢¯åº¦é—®é¢˜)
    - [3.4 LSTMè¯­è¨€æ¨¡å‹](#34-lstmè¯­è¨€æ¨¡å‹)
    - [3.5 GRUè¯­è¨€æ¨¡å‹](#35-gruè¯­è¨€æ¨¡å‹)
  - [4. é«˜çº§æŠ€æœ¯ | Advanced Techniques](#4-é«˜çº§æŠ€æœ¯--advanced-techniques)
    - [4.1 Dropout for RNN](#41-dropout-for-rnn)
    - [4.2 Layer Normalization](#42-layer-normalization)
    - [4.3 æ®‹å·®è¿æ¥](#43-æ®‹å·®è¿æ¥)
    - [4.4 æ³¨æ„åŠ›æœºåˆ¶](#44-æ³¨æ„åŠ›æœºåˆ¶)
  - [5. ç‰¹æ®Šæ¶æ„ | Special Architectures](#5-ç‰¹æ®Šæ¶æ„--special-architectures)
    - [5.1 åŒå‘è¯­è¨€æ¨¡å‹](#51-åŒå‘è¯­è¨€æ¨¡å‹)
    - [5.2 å­—ç¬¦çº§è¯­è¨€æ¨¡å‹](#52-å­—ç¬¦çº§è¯­è¨€æ¨¡å‹)
    - [5.3 å­è¯çº§è¯­è¨€æ¨¡å‹](#53-å­è¯çº§è¯­è¨€æ¨¡å‹)
  - [6. é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ | Pre-trained Language Models](#6-é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹--pre-trained-language-models)
    - [6.1 ULMFiT (2018)](#61-ulmfit-2018)
    - [6.2 ELMo (2018)](#62-elmo-2018)
    - [6.3 ä»LMåˆ°é¢„è®­ç»ƒçš„æ¼”è¿›](#63-ä»lmåˆ°é¢„è®­ç»ƒçš„æ¼”è¿›)
  - [7. æ€§èƒ½å¯¹æ¯” | Performance Comparison](#7-æ€§èƒ½å¯¹æ¯”--performance-comparison)
    - [7.1 Penn Treebank](#71-penn-treebank)
    - [7.2 One Billion Word Benchmark](#72-one-billion-word-benchmark)
    - [7.3 è§‚å¯Ÿ](#73-è§‚å¯Ÿ)
  - [8. ç†è®ºåˆ†æ | Theoretical Analysis](#8-ç†è®ºåˆ†æ--theoretical-analysis)
    - [8.1 è¡¨è¾¾èƒ½åŠ›](#81-è¡¨è¾¾èƒ½åŠ›)
    - [8.2 æ³›åŒ–èƒ½åŠ›](#82-æ³›åŒ–èƒ½åŠ›)
    - [8.3 é•¿ç¨‹ä¾èµ–çš„ç†è®ºé™åˆ¶](#83-é•¿ç¨‹ä¾èµ–çš„ç†è®ºé™åˆ¶)
  - [9. æƒå¨å‚è€ƒæ–‡çŒ® | Authoritative References](#9-æƒå¨å‚è€ƒæ–‡çŒ®--authoritative-references)
    - [Wikipedia æ¡ç›®](#wikipedia-æ¡ç›®)
    - [å­¦æœ¯è®ºæ–‡](#å­¦æœ¯è®ºæ–‡)
    - [æ ‡å‡†æ•™æ](#æ ‡å‡†æ•™æ)
  - [10. å…³é”®è¦ç‚¹æ€»ç»“ | Key Takeaways](#10-å…³é”®è¦ç‚¹æ€»ç»“--key-takeaways)

---

## æ¦‚è¿° | Overview

ç¥ç»è¯­è¨€æ¨¡å‹ä½¿ç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡è¯­è¨€åºåˆ—ï¼Œå…‹æœäº†ä¼ ç»Ÿn-gramæ¨¡å‹çš„è¯¸å¤šå±€é™ã€‚æœ¬æ–‡æ¡£æ·±å…¥åˆ†æä»å‰é¦ˆç½‘ç»œåˆ°å¾ªç¯ç½‘ç»œçš„ç¥ç»è¯­è¨€æ¨¡å‹æ¼”è¿›ã€‚

## 1. ä»ç¬¦å·åˆ°å‘é‡ | From Symbols to Vectors

### 1.1 è¯è¡¨ç¤ºçš„æ¼”è¿›

**One-Hot ç¼–ç **ï¼š

```text
"cat" â†’ [0, 0, ..., 1, ..., 0]  (ç»´åº¦ = |V|)
```

**é—®é¢˜**ï¼š

- ç»´åº¦ç¾éš¾
- æ— è¯­ä¹‰ä¿¡æ¯
- è¯ä¹‹é—´å®Œå…¨ç‹¬ç«‹

**åˆ†å¸ƒå¼è¡¨ç¤º**ï¼š

```text
"cat" â†’ [0.2, -0.5, 0.8, 0.1]  (ç»´åº¦ = d << |V|)
"dog" â†’ [0.3, -0.4, 0.7, 0.2]  (ç›¸è¿‘ï¼)
```

**ä¼˜åŠ¿**ï¼š

- ä½ç»´ç¨ å¯†
- ç¼–ç è¯­ä¹‰ç›¸ä¼¼æ€§
- å‚æ•°å…±äº«

### 1.2 åˆ†å¸ƒå‡è®¾ (Distributional Hypothesis)

**Harris (1954)**ï¼š
> "Words that occur in similar contexts tend to have similar meanings"
> å‡ºç°åœ¨ç›¸ä¼¼ä¸Šä¸‹æ–‡ä¸­çš„è¯å€¾å‘äºæœ‰ç›¸ä¼¼å«ä¹‰

**Firth (1957)**ï¼š
> "You shall know a word by the company it keeps"
> è¯çš„æ„ä¹‰ç”±å…¶ä¼´éšè¯å†³å®š

**æ„ä¹‰**ï¼š

- ä»å…±ç°ç»Ÿè®¡å­¦ä¹ è¯­ä¹‰
- ç¥ç»è¯­è¨€æ¨¡å‹çš„ç†è®ºåŸºç¡€

### 1.3 è¯åµŒå…¥çš„æ€§è´¨

**å‡ ä½•æ€§è´¨**ï¼š

```text
vec("king") - vec("man") + vec("woman") â‰ˆ vec("queen")
```

**è¯­ä¹‰èšç±»**ï¼š

- ç›¸ä¼¼è¯åœ¨ç©ºé—´ä¸­é è¿‘
- è¯­ä¹‰å…³ç³»è¡¨ç¤ºä¸ºå‘é‡è¿ç®—

**ç»´åº¦è§£é‡Š**ï¼š

- æŸäº›ç»´åº¦å¯èƒ½å¯¹åº”è¯­ä¹‰ç‰¹å¾
- ä½†é€šå¸¸æ˜¯åˆ†å¸ƒå¼çš„ï¼ˆéš¾ä»¥è§£é‡Šï¼‰

## 2. å‰é¦ˆç¥ç»è¯­è¨€æ¨¡å‹ | Feed-Forward Neural Language Models

### 2.1 Bengio æ¨¡å‹ (2003)

**æ¶æ„**ï¼š

```text
è¾“å…¥ï¼šå‰ n-1 ä¸ªè¯ (wáµ¢â‚‹â‚™â‚Šâ‚, ..., wáµ¢â‚‹â‚)
  â†“
åµŒå…¥å±‚ï¼šæ¯ä¸ªè¯ â†’ d ç»´å‘é‡
  â†“
æ‹¼æ¥ï¼š(n-1) Ã— d ç»´å‘é‡
  â†“
éšè—å±‚ï¼šh = tanh(W_hidden Â· concat + b)
  â†“
è¾“å‡ºå±‚ï¼šscores = W_output Â· h + b
  â†“
Softmaxï¼šP(wáµ¢ | å‰æ–‡) = softmax(scores)
```

**å½¢å¼åŒ–**ï¼š

```text
C(w) âˆˆ â„áµˆ          # è¯ w çš„åµŒå…¥
x = [C(wáµ¢â‚‹â‚™â‚Šâ‚); ...; C(wáµ¢â‚‹â‚)]  # æ‹¼æ¥
h = tanh(Hx + d)   # éšè—å±‚
y = Uh + b         # è¾“å‡ºåˆ†æ•°
P(wáµ¢ | å‰æ–‡) = softmax(y)
```

**å‚æ•°**ï¼š

- Cï¼š|V| Ã— dï¼ˆåµŒå…¥çŸ©é˜µï¼‰
- Hï¼šh Ã— ((n-1)d)ï¼ˆéšè—å±‚æƒé‡ï¼‰
- Uï¼š|V| Ã— hï¼ˆè¾“å‡ºæƒé‡ï¼‰

**æ€»å‚æ•°**ï¼šO(|V|d + hdÂ·n + |V|h)

### 2.2 è®­ç»ƒç›®æ ‡

**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼š

```text
max âˆ‘ log P(wáµ¢ | wáµ¢â‚‹â‚™â‚Šâ‚, ..., wáµ¢â‚‹â‚)
```

**äº¤å‰ç†µæŸå¤±**ï¼š

```text
L = -âˆ‘ log P(wáµ¢ | wáµ¢â‚‹â‚™â‚Šâ‚, ..., wáµ¢â‚‹â‚)
```

**éšæœºæ¢¯åº¦ä¸‹é™**ï¼š

- å°æ‰¹é‡
- åå‘ä¼ æ’­
- å­¦ä¹ ç‡è°ƒåº¦

### 2.3 Softmax ç“¶é¢ˆ

**è®¡ç®—å¤æ‚åº¦**ï¼š

```text
Softmax(scores) = exp(scoreáµ¢) / âˆ‘â±¼ exp(scoreâ±¼)
```

åˆ†æ¯æ±‚å’Œï¼šO(|V|)ï¼Œæ¯ä¸ªè¯æ¯æ¬¡é¢„æµ‹éƒ½éœ€è¦

**é—®é¢˜**ï¼š|V| é€šå¸¸å¾ˆå¤§ï¼ˆ10K - 1Mï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **åˆ†å±‚Softmax (Hierarchical Softmax)**
2. **è´Ÿé‡‡æ · (Negative Sampling)**
3. **é‡è¦æ€§é‡‡æ · (Importance Sampling)**
4. **è‡ªé€‚åº”Softmax (Adaptive Softmax)**

### 2.4 ä¼˜åŠ¿ä¸å±€é™

**ä¼˜åŠ¿**ï¼š

- âœ… è‡ªåŠ¨å­¦ä¹ è¯è¡¨ç¤º
- âœ… æ³›åŒ–åˆ°ç›¸ä¼¼è¯
- âœ… å‚æ•°å…±äº«
- âœ… æ€§èƒ½ä¼˜äºn-gram

**å±€é™**ï¼š

- âŒ ä¸Šä¸‹æ–‡çª—å£ä»å›ºå®šï¼ˆnï¼‰
- âŒ æ— æ³•æ•æ‰é•¿ç¨‹ä¾èµ–
- âŒ è¯è¢‹è¯­ä¹‰ï¼ˆä½ç½®ä¸æ•æ„Ÿï¼‰

## 3. å¾ªç¯ç¥ç»è¯­è¨€æ¨¡å‹ | Recurrent Neural Language Models

### 3.1 æ ‡å‡†RNNè¯­è¨€æ¨¡å‹

**Mikolov et al. (2010)**:

**æ¶æ„**ï¼š

```text
è¾“å…¥ï¼šwâ‚œ
  â†“
åµŒå…¥ï¼šeâ‚œ = E[wâ‚œ]
  â†“
å¾ªç¯ï¼šhâ‚œ = Ïƒ(W_hh hâ‚œâ‚‹â‚ + W_xe eâ‚œ)
  â†“
è¾“å‡ºï¼šyâ‚œ = W_yh hâ‚œ
  â†“
Softmaxï¼šP(wâ‚œâ‚Šâ‚) = softmax(yâ‚œ)
```

**å…³é”®ç‰¹æ€§**ï¼š

- éšçŠ¶æ€ hâ‚œ ç´¯ç§¯å†å²ä¿¡æ¯
- ç†è®ºä¸Šæ— é™ä¸Šä¸‹æ–‡
- å‚æ•°ä¸ä¾èµ–äºåºåˆ—é•¿åº¦

**è®­ç»ƒ**ï¼š

```text
æŸå¤±ï¼šL = -âˆ‘â‚œ log P(wâ‚œ | wâ‚, ..., wâ‚œâ‚‹â‚)
ç®—æ³•ï¼šBPTT (Backpropagation Through Time)
```

### 3.2 BPTT ç®—æ³•

**å±•å¼€RNN**ï¼š

```text
wâ‚ â†’ hâ‚ â†’ yâ‚
wâ‚‚ â†’ hâ‚‚ â†’ yâ‚‚
...
wâ‚œ â†’ hâ‚œ â†’ yâ‚œ
```

**æ¢¯åº¦è®¡ç®—**ï¼š

```text
âˆ‚L/âˆ‚W = âˆ‘â‚œ âˆ‚Lâ‚œ/âˆ‚W
```

éœ€è¦ä» t åå‘ä¼ æ’­åˆ° 1

**æˆªæ–­BPTT (Truncated BPTT)**ï¼š

- åªå›ä¼  k æ­¥
- å‡å°‘è®¡ç®—æˆæœ¬
- ç‰ºç‰²ä¸€äº›é•¿ç¨‹ä¿¡æ¯

### 3.3 æ¢¯åº¦é—®é¢˜

**æ¢¯åº¦æ¶ˆå¤±**ï¼š

```text
âˆ‚hâ‚œ/âˆ‚hâ‚ = âˆáµ¢â‚Œâ‚‚áµ— âˆ‚háµ¢/âˆ‚háµ¢â‚‹â‚
```

è‹¥ ||âˆ‚háµ¢/âˆ‚háµ¢â‚‹â‚|| < 1ï¼Œåˆ™ä¹˜ç§¯æŒ‡æ•°è¡°å‡

**æ¢¯åº¦çˆ†ç‚¸**ï¼š

è‹¥ ||âˆ‚háµ¢/âˆ‚háµ¢â‚‹â‚|| > 1ï¼Œåˆ™ä¹˜ç§¯æŒ‡æ•°å¢é•¿

**è§£å†³æ–¹æ³•**ï¼š

1. **æ¢¯åº¦è£å‰ª (Gradient Clipping)**ï¼š

   ```text
   if ||g|| > threshold:
       g = threshold Â· g / ||g||
   ```

2. **æ›´å¥½çš„æ¿€æ´»å‡½æ•°**ï¼šReLU

3. **æ›´å¥½çš„åˆå§‹åŒ–**ï¼šXavier, He

4. **æ¶æ„æ”¹è¿›**ï¼šLSTM, GRU

### 3.4 LSTMè¯­è¨€æ¨¡å‹

**Sundermeyer et al. (2012)**:

**LSTMå•å…ƒ**ï¼š

```text
fâ‚œ = Ïƒ(W_f Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_f)  # é—å¿˜é—¨
iâ‚œ = Ïƒ(W_i Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_i)  # è¾“å…¥é—¨
CÌƒâ‚œ = tanh(W_C Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_C)  # å€™é€‰cell
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ  # æ›´æ–°cell
oâ‚œ = Ïƒ(W_o Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_o)  # è¾“å‡ºé—¨
hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)  # éšçŠ¶æ€
```

**ä¼˜åŠ¿**ï¼š

- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- å­¦ä¹ é•¿ç¨‹ä¾èµ–
- æ€§èƒ½æ˜¾è‘—æå‡

**å›°æƒ‘åº¦æ”¹è¿›**ï¼š

```text
Penn Treebank:
RNN-LM: ~120
LSTM-LM: ~78  (35%æå‡ï¼)
```

### 3.5 GRUè¯­è¨€æ¨¡å‹

**Cho et al. (2014)**:

**GRUå•å…ƒ**ï¼ˆæ›´ç®€å•ï¼‰ï¼š

```text
zâ‚œ = Ïƒ(W_z Â· [hâ‚œâ‚‹â‚, xâ‚œ])  # æ›´æ–°é—¨
râ‚œ = Ïƒ(W_r Â· [hâ‚œâ‚‹â‚, xâ‚œ])  # é‡ç½®é—¨
hÌƒâ‚œ = tanh(W Â· [râ‚œ âŠ™ hâ‚œâ‚‹â‚, xâ‚œ])  # å€™é€‰éšçŠ¶æ€
hâ‚œ = (1 - zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ  # æ›´æ–°éšçŠ¶æ€
```

**vs LSTM**ï¼š

- å‚æ•°æ›´å°‘ï¼ˆ2/3çš„é—¨ï¼‰
- è®¡ç®—æ›´å¿«
- æ€§èƒ½ç›¸å½“

## 4. é«˜çº§æŠ€æœ¯ | Advanced Techniques

### 4.1 Dropout for RNN

**æ ‡å‡†Dropouté—®é¢˜**ï¼š

- åœ¨æ—¶é—´æ­¥ä¹‹é—´dropoutï¼šç ´åå¾ªç¯ç»“æ„
- ä¸åœ¨æ—¶é—´æ­¥ä¹‹é—´dropoutï¼šè¿‡æ‹Ÿåˆ

**Variational Dropout (Gal & Ghahramani, 2016)**ï¼š

```text
åŒä¸€ä¸ªdropout maskåœ¨æ‰€æœ‰æ—¶é—´æ­¥é‡å¤ä½¿ç”¨
```

**æ•ˆæœ**ï¼š

- é˜²æ­¢è¿‡æ‹Ÿåˆ
- å›°æƒ‘åº¦è¿›ä¸€æ­¥é™ä½

### 4.2 Layer Normalization

**Batch Normalization é—®é¢˜**ï¼š

- RNNåºåˆ—é•¿åº¦ä¸ä¸€
- å°æ‰¹é‡ç»Ÿè®¡ä¸ç¨³å®š

**Layer Normalization (Ba et al., 2016)**ï¼š

```text
å¯¹æ¯ä¸ªæ ·æœ¬çš„éšè—å•å…ƒå½’ä¸€åŒ–ï¼ˆè€Œéæ‰¹æ¬¡ï¼‰
```

**æ•ˆæœ**ï¼š

- è®­ç»ƒç¨³å®š
- æ”¶æ•›æ›´å¿«

### 4.3 æ®‹å·®è¿æ¥

**æ·±å±‚RNNé—®é¢˜**ï¼š

- å¤šå±‚å åŠ æ—¶æ¢¯åº¦æ¶ˆå¤±
- è®­ç»ƒå›°éš¾

**æ®‹å·®è¿æ¥**ï¼š

```text
hâ‚œ^(l) = hâ‚œ^(l-1) + RNN_layer(hâ‚œ^(l-1))
```

**å…è®¸è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ**ï¼ˆå¦‚8-16å±‚ï¼‰

### 4.4 æ³¨æ„åŠ›æœºåˆ¶

**Bahdanau et al. (2014)**:

**åŠ¨æœº**ï¼š

- RNNæŠŠæ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šç»´åº¦éšçŠ¶æ€
- é•¿åºåˆ—ä¿¡æ¯æŸå¤±

**æ³¨æ„åŠ›**ï¼š

```text
câ‚œ = âˆ‘áµ¢ Î±â‚œáµ¢ háµ¢  # åŠ æƒå’Œæ‰€æœ‰ç¼–ç å™¨éšçŠ¶æ€
Î±â‚œáµ¢ = softmax(score(hâ‚œ, háµ¢))  # æ³¨æ„åŠ›æƒé‡
```

**åœ¨è¯­è¨€æ¨¡å‹ä¸­**ï¼š

- ç”Ÿæˆæ—¶å…³æ³¨å‰æ–‡çš„ä¸åŒä½ç½®
- æ€§èƒ½æå‡
- ä¸ºTransformeré“ºè·¯

## 5. ç‰¹æ®Šæ¶æ„ | Special Architectures

### 5.1 åŒå‘è¯­è¨€æ¨¡å‹

**ELMo (Peters et al., 2018)**:

**åŠ¨æœº**ï¼š

- å‰å‘LMï¼šåªçœ‹å·¦ä¾§
- åå‘LMï¼šåªçœ‹å³ä¾§
- ç»“åˆï¼šå®Œæ•´ä¸Šä¸‹æ–‡

**æ¶æ„**ï¼š

```text
å‰å‘LSTMï¼šP(wâ‚œ | wâ‚, ..., wâ‚œâ‚‹â‚)
åå‘LSTMï¼šP(wâ‚œ | wâ‚œâ‚Šâ‚, ..., wâ‚™)
ç»“åˆï¼šç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç¤º
```

**æ³¨æ„**ï¼š

- ä¸æ˜¯çœŸæ­£çš„è¯­è¨€æ¨¡å‹ï¼ˆæ— æ³•ç”Ÿæˆï¼‰
- ç”¨äºè¡¨ç¤ºå­¦ä¹ 

### 5.2 å­—ç¬¦çº§è¯­è¨€æ¨¡å‹

**Sutskever et al. (2011)**:

**åŠ¨æœº**ï¼š

- è¯çº§ï¼šOOVé—®é¢˜
- å­—ç¬¦çº§ï¼šæ— OOV

**å®ç°**ï¼š

```text
è¾“å…¥ï¼šå­—ç¬¦åºåˆ—
hâ‚ â†’ hâ‚‚ â†’ ... â†’ hâ‚™
è¾“å‡ºï¼šä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡
```

**ä¼˜åŠ¿**ï¼š

- å¼€æ”¾è¯æ±‡
- å­¦ä¹ å½¢æ€å­¦

**æŒ‘æˆ˜**ï¼š

- åºåˆ—æ›´é•¿
- è®­ç»ƒæ›´æ…¢
- é•¿ç¨‹ä¾èµ–æ›´éš¾

### 5.3 å­è¯çº§è¯­è¨€æ¨¡å‹

**BPE, WordPiece, SentencePiece**:

**åŠ¨æœº**ï¼š

- è¯çº§ï¼šOOVï¼Œå¤§è¯æ±‡è¡¨
- å­—ç¬¦çº§ï¼šå¤ªé•¿
- å­è¯ï¼šå¹³è¡¡

**ä¾‹å­**ï¼š

```text
"unbelievable" â†’ ["un", "believ", "able"]
```

**ä¼˜åŠ¿**ï¼š

- å›ºå®šå¤§å°è¯æ±‡è¡¨ï¼ˆå¦‚32Kï¼‰
- æ— OOV
- å­¦ä¹ è¯ç¼€

**ç°ä»£æ ‡å‡†**ï¼š

- GPTç³»åˆ—ï¼šBPE
- BERTï¼šWordPiece
- T5ï¼šSentencePiece

## 6. é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ | Pre-trained Language Models

### 6.1 ULMFiT (2018)

**Howard & Ruder (2018)**:

**ä¸‰é˜¶æ®µ**ï¼š

1. **é¢„è®­ç»ƒLM**ï¼šå¤§è§„æ¨¡é€šç”¨æ–‡æœ¬
2. **å¾®è°ƒLM**ï¼šç›®æ ‡åŸŸæ–‡æœ¬
3. **å¾®è°ƒåˆ†ç±»å™¨**ï¼šæ ‡æ³¨æ•°æ®

**æŠ€æœ¯**ï¼š

- åˆ¤åˆ«å¼å¾®è°ƒ
- å€¾æ–œä¸‰è§’å­¦ä¹ ç‡
- é€å±‚è§£å†»

### 6.2 ELMo (2018)

**åŒå‘LSTMè¯­è¨€æ¨¡å‹**:

**åˆ›æ–°**ï¼š

- æ·±å±‚åŒå‘
- ä¸Šä¸‹æ–‡ç›¸å…³è¡¨ç¤º
- å„å±‚è¡¨ç¤ºçš„åŠ æƒç»„åˆ

**ä½¿ç”¨**ï¼š

```text
å›ºå®šELMoåµŒå…¥ + ä¸‹æ¸¸æ¨¡å‹
```

**å½±å“**ï¼š

- å¤šä¸ªNLPä»»åŠ¡SOTA
- è¯æ˜é¢„è®­ç»ƒçš„å¨åŠ›

### 6.3 ä»LMåˆ°é¢„è®­ç»ƒçš„æ¼”è¿›

**è¶‹åŠ¿**ï¼š

```text
n-gram (1980s-2000s)
  â†“
ç¥ç»LM (2000s-2010s)
  â†“
RNN/LSTM-LM (2010-2017)
  â†“
é¢„è®­ç»ƒLM (2018-)
  â†“
Transformer-LM (2018-)
  â†“
å¤§è¯­è¨€æ¨¡å‹ (2019-)
```

## 7. æ€§èƒ½å¯¹æ¯” | Performance Comparison

### 7.1 Penn Treebank

| æ¨¡å‹ | å‚æ•°é‡ | æµ‹è¯•å›°æƒ‘åº¦ |
|------|--------|-----------|
| **Kneser-Ney 5-gram** | - | 141 |
| **å‰é¦ˆç¥ç»LM** | ~10M | 137 |
| **RNN-LM** | ~5M | 123 |
| **LSTM-LM** | ~10M | 78 |
| **LSTM + Dropout** | ~24M | 66 |
| **AWD-LSTM** | ~24M | 57 |
| **Transformer** | ~30M | 56 |

### 7.2 One Billion Word Benchmark

å¤§è§„æ¨¡æ•°æ®é›†ï¼š

| æ¨¡å‹ | æµ‹è¯•å›°æƒ‘åº¦ |
|------|-----------|
| **LSTM-LM** | 43.7 |
| **Big LSTM-LM** | 30.0 |
| **Transformer-LM** | 23.7 |

### 7.3 è§‚å¯Ÿ

**è¶‹åŠ¿**ï¼š

1. ç¥ç»æ¨¡å‹è¿œè¶…n-gram
2. LSTMæ˜¾è‘—ä¼˜äºç®€å•RNN
3. æ·±åº¦+æ­£åˆ™åŒ–æŒç»­æ”¹è¿›
4. Transformeræˆä¸ºæ–°æ ‡å‡†
5. è§„æ¨¡è¶Šå¤§æ€§èƒ½è¶Šå¥½

## 8. ç†è®ºåˆ†æ | Theoretical Analysis

### 8.1 è¡¨è¾¾èƒ½åŠ›

**å®šç† (Siegelmann & Sontag, 1995)**ï¼š
> å®æ•°æƒé‡RNNæ˜¯å›¾çµå®Œå¤‡çš„

**æ„ä¹‰**ï¼š

- RNNç†è®ºä¸Šå¯ä»¥è¯†åˆ«ä»»ä½•å¯è®¡ç®—åºåˆ—
- ä½†å®è·µä¸­å—é™äºç²¾åº¦ã€é•¿åº¦ã€ä¼˜åŒ–

### 8.2 æ³›åŒ–èƒ½åŠ›

**ç»éªŒè§„å¾‹**ï¼š

- è¿‡å‚æ•°åŒ–æœ‰ç›Š
- å¤§æ¨¡å‹ + å¤§æ•°æ® â†’ æ›´å¥½æ³›åŒ–
- ä¸ä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ ç†è®ºç›¸æ‚–

**ç°ä»£ç†è®ºå°è¯•**ï¼š

- éšå¼æ­£åˆ™åŒ–
- æŸå¤±æ™¯è§‚å¹³å¦æ€§
- Neural Tangent Kernel

### 8.3 é•¿ç¨‹ä¾èµ–çš„ç†è®ºé™åˆ¶

**Bengio et al. (1994)**ï¼š

- ç®€å•RNNéš¾ä»¥å­¦ä¹ é•¿ç¨‹ä¾èµ–
- æ¢¯åº¦æ¶ˆå¤±æ˜¯æ ¹æœ¬åŸå› 

**LSTMçš„æ”¹è¿›**ï¼š

- é—¨æœºåˆ¶æä¾›æ¢¯åº¦é«˜é€Ÿå…¬è·¯
- ä½†ä»æœ‰é™åˆ¶ï¼ˆå®è·µä¸­~1000æ­¥ï¼‰

## 9. æƒå¨å‚è€ƒæ–‡çŒ® | Authoritative References

### Wikipedia æ¡ç›®

1. [Recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network)
2. [Long short-term memory](https://en.wikipedia.org/wiki/Long_short-term_memory)
3. [Word embedding](https://en.wikipedia.org/wiki/Word_embedding)
4. [Backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time)

### å­¦æœ¯è®ºæ–‡

1. **Bengio, Y., et al. (2003)**. "A neural probabilistic language model". *JMLR*.
2. **Mikolov, T., et al. (2010)**. "Recurrent neural network based language model". *Interspeech*.
3. **Hochreiter, S., & Schmidhuber, J. (1997)**. "Long short-term memory". *Neural Computation*.
4. **Cho, K., et al. (2014)**. "Learning phrase representations using RNN encoder-decoder". *EMNLP*.
5. **Gal, Y., & Ghahramani, Z. (2016)**. "A theoretically grounded application of dropout in RNNs". *NeurIPS*.
6. **Peters, M. E., et al. (2018)**. "Deep contextualized word representations". *NAACL*.
7. **Howard, J., & Ruder, S. (2018)**. "Universal language model fine-tuning for text classification". *ACL*.

### æ ‡å‡†æ•™æ

1. **Goodfellow, I., Bengio, Y., & Courville, A. (2016)**. *Deep Learning*. MIT Press.
   - ç¬¬10ç« ï¼šåºåˆ—å»ºæ¨¡
2. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).
   - ç¬¬7-9ç« ï¼šç¥ç»ç½‘ç»œä¸è¯­è¨€æ¨¡å‹
3. **Goldberg, Y. (2017)**. *Neural Network Methods for Natural Language Processing*. Morgan & Claypool.

## 10. å…³é”®è¦ç‚¹æ€»ç»“ | Key Takeaways

1. **åˆ†å¸ƒå¼è¡¨ç¤ºçš„å¨åŠ›**ï¼šè¯åµŒå…¥è‡ªåŠ¨ç¼–ç è¯­ä¹‰ï¼Œå®ç°æ³›åŒ–
2. **RNNçš„çªç ´**ï¼šç†è®ºæ— é™ä¸Šä¸‹æ–‡ï¼Œç»Ÿä¸€å¤„ç†ä»»æ„é•¿åº¦åºåˆ—
3. **LSTMçš„å…³é”®**ï¼šé—¨æœºåˆ¶è§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œå­¦ä¹ é•¿ç¨‹ä¾èµ–
4. **è®­ç»ƒæŠ€å·§**ï¼šDropoutã€Layer Normã€æ®‹å·®è¿æ¥éƒ½å¾ˆé‡è¦
5. **é¢„è®­ç»ƒèŒƒå¼**ï¼šå¤§è§„æ¨¡é¢„è®­ç»ƒ+å¾®è°ƒæˆä¸ºæ ‡å‡†æµç¨‹
6. **è§„æ¨¡å®šå¾‹**ï¼šæ›´å¤§æ¨¡å‹+æ›´å¤šæ•°æ®â†’æ›´å¥½æ€§èƒ½
7. **Transformerçš„å´›èµ·**ï¼šæœ€ç»ˆå–ä»£RNNæˆä¸ºæ–°æ ‡å‡†
8. **ç†è®ºvså®è·µå·®è·**ï¼šç†è®ºå›¾çµå®Œå¤‡ï¼Œå®è·µå—å„ç§é™åˆ¶

---

**ä¸‹ä¸€æ­¥é˜…è¯»**ï¼š

- [03.1 ç»Ÿè®¡è¯­è¨€æ¨¡å‹](03.1_Statistical_Language_Models.md)
- [03.3 Transformerå¤§è¯­è¨€æ¨¡å‹ç†è®º](03.3_Transformer_LLM_Theory.md)
- [02.2 RNNä¸Transformeræ¶æ„](../02_Neural_Network_Theory/02.2_RNN_Transformer_Architecture.md)

---

## å¯¼èˆª | Navigation

**ä¸Šä¸€ç¯‡**: [â† 03.1 ç»Ÿè®¡è¯­è¨€æ¨¡å‹](./03.1_Statistical_Language_Models.md)  
**ä¸‹ä¸€ç¯‡**: [03.3 Transformer LLMç†è®º â†’](./03.3_Transformer_LLM_Theory.md)  
**è¿”å›ç›®å½•**: [â†‘ AIæ¨¡å‹è§†è§’æ€»è§ˆ](../README.md)

---

## ç›¸å…³ä¸»é¢˜ | Related Topics

### æœ¬ç« èŠ‚
- [03.1 ç»Ÿè®¡è¯­è¨€æ¨¡å‹](./03.1_Statistical_Language_Models.md)
- [03.3 Transformer LLMç†è®º](./03.3_Transformer_LLM_Theory.md)
- [03.4 Tokenç”Ÿæˆæœºåˆ¶](./03.4_Token_Generation_Mechanisms.md)
- [03.5 åµŒå…¥å‘é‡ç©ºé—´](./03.5_Embedding_Vector_Spaces.md)
- [03.6 ä¸Šä¸‹æ–‡çª—å£ä¸è®°å¿†](./03.6_Context_Window_Memory.md)

### ç›¸å…³ç« èŠ‚
- [02.2 RNNä¸Transformeræ¶æ„](../02_Neural_Network_Theory/02.2_RNN_Transformer_Architecture.md)
- [04.2 è¿ç»­è¡¨ç¤ºç†è®º](../04_Semantic_Models/04.2_Continuous_Representation_Theory.md)

### è·¨è§†è§’é“¾æ¥
- [FormalLanguage_Perspective](../../FormalLanguage_Perspective/README.md)
- [Information_Theory_Perspective](../../Information_Theory_Perspective/README.md)