# 神经语言模型 | Neural Language Models

## 概述 | Overview

神经语言模型使用神经网络建模语言序列，克服了传统n-gram模型的诸多局限。本文档深入分析从前馈网络到循环网络的神经语言模型演进。

## 1. 从符号到向量 | From Symbols to Vectors

### 1.1 词表示的演进

**One-Hot 编码**：

```text
"cat" → [0, 0, ..., 1, ..., 0]  (维度 = |V|)
```

**问题**：

- 维度灾难
- 无语义信息
- 词之间完全独立

**分布式表示**：

```text
"cat" → [0.2, -0.5, 0.8, 0.1]  (维度 = d << |V|)
"dog" → [0.3, -0.4, 0.7, 0.2]  (相近！)
```

**优势**：

- 低维稠密
- 编码语义相似性
- 参数共享

### 1.2 分布假设 (Distributional Hypothesis)

**Harris (1954)**：
> "Words that occur in similar contexts tend to have similar meanings"
> 出现在相似上下文中的词倾向于有相似含义

**Firth (1957)**：
> "You shall know a word by the company it keeps"
> 词的意义由其伴随词决定

**意义**：

- 从共现统计学习语义
- 神经语言模型的理论基础

### 1.3 词嵌入的性质

**几何性质**：

```text
vec("king") - vec("man") + vec("woman") ≈ vec("queen")
```

**语义聚类**：

- 相似词在空间中靠近
- 语义关系表示为向量运算

**维度解释**：

- 某些维度可能对应语义特征
- 但通常是分布式的（难以解释）

## 2. 前馈神经语言模型 | Feed-Forward Neural Language Models

### 2.1 Bengio 模型 (2003)

**架构**：

```text
输入：前 n-1 个词 (wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
  ↓
嵌入层：每个词 → d 维向量
  ↓
拼接：(n-1) × d 维向量
  ↓
隐藏层：h = tanh(W_hidden · concat + b)
  ↓
输出层：scores = W_output · h + b
  ↓
Softmax：P(wᵢ | 前文) = softmax(scores)
```

**形式化**：

```text
C(w) ∈ ℝᵈ          # 词 w 的嵌入
x = [C(wᵢ₋ₙ₊₁); ...; C(wᵢ₋₁)]  # 拼接
h = tanh(Hx + d)   # 隐藏层
y = Uh + b         # 输出分数
P(wᵢ | 前文) = softmax(y)
```

**参数**：

- C：|V| × d（嵌入矩阵）
- H：h × ((n-1)d)（隐藏层权重）
- U：|V| × h（输出权重）

**总参数**：O(|V|d + hd·n + |V|h)

### 2.2 训练目标

**最大似然估计**：

```text
max ∑ log P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
```

**交叉熵损失**：

```text
L = -∑ log P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
```

**随机梯度下降**：

- 小批量
- 反向传播
- 学习率调度

### 2.3 Softmax 瓶颈

**计算复杂度**：

```text
Softmax(scores) = exp(scoreᵢ) / ∑ⱼ exp(scoreⱼ)
```

分母求和：O(|V|)，每个词每次预测都需要

**问题**：|V| 通常很大（10K - 1M）

**解决方案**：

1. **分层Softmax (Hierarchical Softmax)**
2. **负采样 (Negative Sampling)**
3. **重要性采样 (Importance Sampling)**
4. **自适应Softmax (Adaptive Softmax)**

### 2.4 优势与局限

**优势**：

- ✅ 自动学习词表示
- ✅ 泛化到相似词
- ✅ 参数共享
- ✅ 性能优于n-gram

**局限**：

- ❌ 上下文窗口仍固定（n）
- ❌ 无法捕捉长程依赖
- ❌ 词袋语义（位置不敏感）

## 3. 循环神经语言模型 | Recurrent Neural Language Models

### 3.1 标准RNN语言模型

**Mikolov et al. (2010)**:

**架构**：

```text
输入：wₜ
  ↓
嵌入：eₜ = E[wₜ]
  ↓
循环：hₜ = σ(W_hh hₜ₋₁ + W_xe eₜ)
  ↓
输出：yₜ = W_yh hₜ
  ↓
Softmax：P(wₜ₊₁) = softmax(yₜ)
```

**关键特性**：

- 隐状态 hₜ 累积历史信息
- 理论上无限上下文
- 参数不依赖于序列长度

**训练**：

```text
损失：L = -∑ₜ log P(wₜ | w₁, ..., wₜ₋₁)
算法：BPTT (Backpropagation Through Time)
```

### 3.2 BPTT 算法

**展开RNN**：

```text
w₁ → h₁ → y₁
w₂ → h₂ → y₂
...
wₜ → hₜ → yₜ
```

**梯度计算**：

```text
∂L/∂W = ∑ₜ ∂Lₜ/∂W
```

需要从 t 反向传播到 1

**截断BPTT (Truncated BPTT)**：

- 只回传 k 步
- 减少计算成本
- 牺牲一些长程信息

### 3.3 梯度问题

**梯度消失**：

```text
∂hₜ/∂h₁ = ∏ᵢ₌₂ᵗ ∂hᵢ/∂hᵢ₋₁
```

若 ||∂hᵢ/∂hᵢ₋₁|| < 1，则乘积指数衰减

**梯度爆炸**：

若 ||∂hᵢ/∂hᵢ₋₁|| > 1，则乘积指数增长

**解决方法**：

1. **梯度裁剪 (Gradient Clipping)**：

   ```text
   if ||g|| > threshold:
       g = threshold · g / ||g||
   ```

2. **更好的激活函数**：ReLU

3. **更好的初始化**：Xavier, He

4. **架构改进**：LSTM, GRU

### 3.4 LSTM语言模型

**Sundermeyer et al. (2012)**:

**LSTM单元**：

```text
fₜ = σ(W_f · [hₜ₋₁, xₜ] + b_f)  # 遗忘门
iₜ = σ(W_i · [hₜ₋₁, xₜ] + b_i)  # 输入门
C̃ₜ = tanh(W_C · [hₜ₋₁, xₜ] + b_C)  # 候选cell
Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ  # 更新cell
oₜ = σ(W_o · [hₜ₋₁, xₜ] + b_o)  # 输出门
hₜ = oₜ ⊙ tanh(Cₜ)  # 隐状态
```

**优势**：

- 缓解梯度消失
- 学习长程依赖
- 性能显著提升

**困惑度改进**：

```text
Penn Treebank:
RNN-LM: ~120
LSTM-LM: ~78  (35%提升！)
```

### 3.5 GRU语言模型

**Cho et al. (2014)**:

**GRU单元**（更简单）：

```text
zₜ = σ(W_z · [hₜ₋₁, xₜ])  # 更新门
rₜ = σ(W_r · [hₜ₋₁, xₜ])  # 重置门
h̃ₜ = tanh(W · [rₜ ⊙ hₜ₋₁, xₜ])  # 候选隐状态
hₜ = (1 - zₜ) ⊙ hₜ₋₁ + zₜ ⊙ h̃ₜ  # 更新隐状态
```

**vs LSTM**：

- 参数更少（2/3的门）
- 计算更快
- 性能相当

## 4. 高级技术 | Advanced Techniques

### 4.1 Dropout for RNN

**标准Dropout问题**：

- 在时间步之间dropout：破坏循环结构
- 不在时间步之间dropout：过拟合

**Variational Dropout (Gal & Ghahramani, 2016)**：

```text
同一个dropout mask在所有时间步重复使用
```

**效果**：

- 防止过拟合
- 困惑度进一步降低

### 4.2 Layer Normalization

**Batch Normalization 问题**：

- RNN序列长度不一
- 小批量统计不稳定

**Layer Normalization (Ba et al., 2016)**：

```text
对每个样本的隐藏单元归一化（而非批次）
```

**效果**：

- 训练稳定
- 收敛更快

### 4.3 残差连接

**深层RNN问题**：

- 多层叠加时梯度消失
- 训练困难

**残差连接**：

```text
hₜ^(l) = hₜ^(l-1) + RNN_layer(hₜ^(l-1))
```

**允许训练更深的网络**（如8-16层）

### 4.4 注意力机制

**Bahdanau et al. (2014)**:

**动机**：

- RNN把所有信息压缩到固定维度隐状态
- 长序列信息损失

**注意力**：

```text
cₜ = ∑ᵢ αₜᵢ hᵢ  # 加权和所有编码器隐状态
αₜᵢ = softmax(score(hₜ, hᵢ))  # 注意力权重
```

**在语言模型中**：

- 生成时关注前文的不同位置
- 性能提升
- 为Transformer铺路

## 5. 特殊架构 | Special Architectures

### 5.1 双向语言模型

**ELMo (Peters et al., 2018)**:

**动机**：

- 前向LM：只看左侧
- 后向LM：只看右侧
- 结合：完整上下文

**架构**：

```text
前向LSTM：P(wₜ | w₁, ..., wₜ₋₁)
后向LSTM：P(wₜ | wₜ₊₁, ..., wₙ)
结合：用于下游任务的表示
```

**注意**：

- 不是真正的语言模型（无法生成）
- 用于表示学习

### 5.2 字符级语言模型

**Sutskever et al. (2011)**:

**动机**：

- 词级：OOV问题
- 字符级：无OOV

**实现**：

```text
输入：字符序列
h₁ → h₂ → ... → hₙ
输出：下一个字符的概率
```

**优势**：

- 开放词汇
- 学习形态学

**挑战**：

- 序列更长
- 训练更慢
- 长程依赖更难

### 5.3 子词级语言模型

**BPE, WordPiece, SentencePiece**:

**动机**：

- 词级：OOV，大词汇表
- 字符级：太长
- 子词：平衡

**例子**：

```text
"unbelievable" → ["un", "believ", "able"]
```

**优势**：

- 固定大小词汇表（如32K）
- 无OOV
- 学习词缀

**现代标准**：

- GPT系列：BPE
- BERT：WordPiece
- T5：SentencePiece

## 6. 预训练语言模型 | Pre-trained Language Models

### 6.1 ULMFiT (2018)

**Howard & Ruder (2018)**:

**三阶段**：

1. **预训练LM**：大规模通用文本
2. **微调LM**：目标域文本
3. **微调分类器**：标注数据

**技术**：

- 判别式微调
- 倾斜三角学习率
- 逐层解冻

### 6.2 ELMo (2018)

**双向LSTM语言模型**:

**创新**：

- 深层双向
- 上下文相关表示
- 各层表示的加权组合

**使用**：

```text
固定ELMo嵌入 + 下游模型
```

**影响**：

- 多个NLP任务SOTA
- 证明预训练的威力

### 6.3 从LM到预训练的演进

**趋势**：

```text
n-gram (1980s-2000s)
  ↓
神经LM (2000s-2010s)
  ↓
RNN/LSTM-LM (2010-2017)
  ↓
预训练LM (2018-)
  ↓
Transformer-LM (2018-)
  ↓
大语言模型 (2019-)
```

## 7. 性能对比 | Performance Comparison

### 7.1 Penn Treebank

| 模型 | 参数量 | 测试困惑度 |
|------|--------|-----------|
| **Kneser-Ney 5-gram** | - | 141 |
| **前馈神经LM** | ~10M | 137 |
| **RNN-LM** | ~5M | 123 |
| **LSTM-LM** | ~10M | 78 |
| **LSTM + Dropout** | ~24M | 66 |
| **AWD-LSTM** | ~24M | 57 |
| **Transformer** | ~30M | 56 |

### 7.2 One Billion Word Benchmark

大规模数据集：

| 模型 | 测试困惑度 |
|------|-----------|
| **LSTM-LM** | 43.7 |
| **Big LSTM-LM** | 30.0 |
| **Transformer-LM** | 23.7 |

### 7.3 观察

**趋势**：

1. 神经模型远超n-gram
2. LSTM显著优于简单RNN
3. 深度+正则化持续改进
4. Transformer成为新标准
5. 规模越大性能越好

## 8. 理论分析 | Theoretical Analysis

### 8.1 表达能力

**定理 (Siegelmann & Sontag, 1995)**：
> 实数权重RNN是图灵完备的

**意义**：

- RNN理论上可以识别任何可计算序列
- 但实践中受限于精度、长度、优化

### 8.2 泛化能力

**经验规律**：

- 过参数化有益
- 大模型 + 大数据 → 更好泛化
- 与传统统计学习理论相悖

**现代理论尝试**：

- 隐式正则化
- 损失景观平坦性
- Neural Tangent Kernel

### 8.3 长程依赖的理论限制

**Bengio et al. (1994)**：

- 简单RNN难以学习长程依赖
- 梯度消失是根本原因

**LSTM的改进**：

- 门机制提供梯度高速公路
- 但仍有限制（实践中~1000步）

## 9. 权威参考文献 | Authoritative References

### Wikipedia 条目

1. [Recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network)
2. [Long short-term memory](https://en.wikipedia.org/wiki/Long_short-term_memory)
3. [Word embedding](https://en.wikipedia.org/wiki/Word_embedding)
4. [Backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time)

### 学术论文

1. **Bengio, Y., et al. (2003)**. "A neural probabilistic language model". *JMLR*.
2. **Mikolov, T., et al. (2010)**. "Recurrent neural network based language model". *Interspeech*.
3. **Hochreiter, S., & Schmidhuber, J. (1997)**. "Long short-term memory". *Neural Computation*.
4. **Cho, K., et al. (2014)**. "Learning phrase representations using RNN encoder-decoder". *EMNLP*.
5. **Gal, Y., & Ghahramani, Z. (2016)**. "A theoretically grounded application of dropout in RNNs". *NeurIPS*.
6. **Peters, M. E., et al. (2018)**. "Deep contextualized word representations". *NAACL*.
7. **Howard, J., & Ruder, S. (2018)**. "Universal language model fine-tuning for text classification". *ACL*.

### 标准教材

1. **Goodfellow, I., Bengio, Y., & Courville, A. (2016)**. *Deep Learning*. MIT Press.
   - 第10章：序列建模
2. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).
   - 第7-9章：神经网络与语言模型
3. **Goldberg, Y. (2017)**. *Neural Network Methods for Natural Language Processing*. Morgan & Claypool.

## 10. 关键要点总结 | Key Takeaways

1. **分布式表示的威力**：词嵌入自动编码语义，实现泛化
2. **RNN的突破**：理论无限上下文，统一处理任意长度序列
3. **LSTM的关键**：门机制解决梯度消失，学习长程依赖
4. **训练技巧**：Dropout、Layer Norm、残差连接都很重要
5. **预训练范式**：大规模预训练+微调成为标准流程
6. **规模定律**：更大模型+更多数据→更好性能
7. **Transformer的崛起**：最终取代RNN成为新标准
8. **理论vs实践差距**：理论图灵完备，实践受各种限制

---

**下一步阅读**：

- [03.1 统计语言模型](03.1_Statistical_Language_Models.md)
- [03.3 Transformer大语言模型理论](03.3_Transformer_LLM_Theory.md)
- [02.2 RNN与Transformer架构](../02_Neural_Network_Theory/02.2_RNN_Transformer_Architecture.md)
