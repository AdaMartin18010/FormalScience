# 统计语言模型 | Statistical Language Models

> **文档版本**: v1.0.0
> **最后更新**: 2025-10-27
> **文档规模**: 629行 | 统计语言模型基础理论
> **阅读建议**: 本文介绍n-gram等经典统计语言模型，是理解现代神经语言模型的历史基础

---

## 📊 核心概念深度分析

<details>
<summary><b>📊 点击展开：统计语言模型多维分析框架</b></summary>
   本节提供统计语言模型的全景式深度分析，包括概念定义、n-gram模型演进、局限性分析、信息论视角和历史遗产。

    ### 1️⃣ 统计语言模型概念定义卡

    **概念名称**: 统计语言模型（Statistical Language Model, SLM）

    **内涵（本质属性）**:

    **🔹 核心思想**:

    - **概率建模**: 将语言建模为词序列的概率分布 $P(w_1, w_2, \ldots, w_n)$
    - **马尔可夫假设**: 利用局部上下文近似长程依赖
    - **频率主义**: 基于语料统计频率估计概率

    **🔹 形式化定义**:
    $$
    P(w_1, w_2, \ldots, w_n) = \prod_{i=1}^{n} P(w_i \mid w_1, \ldots, w_{i-1})
    $$

    **n-gram近似**:
    $$
    P(w_i \mid w_1, \ldots, w_{i-1}) \approx P(w_i \mid w_{i-n+1}, \ldots, w_{i-1})
    $$

    **评估指标（困惑度）**:
    $$
    \text{PPL} = P(w_1, \ldots, w_n)^{-1/n} = \exp\left(-\frac{1}{n}\sum_{i=1}^{n} \log P(w_i \mid \text{history})\right)
    $$

    **外延（范围边界）**:

    | 维度 | 包含 ✅ | 不包含 ❌ |
    |------|---------|----------|
    | **模型类型** | n-gram、回退、插值 | 神经网络、Transformer |
    | **表示方式** | 离散符号、稀疏矩阵 | 分布式表示、词嵌入 |
    | **上下文长度** | 固定窗口（n-1词） | 无限上下文、全局注意力 |
    | **泛化能力** | 词级别匹配 | 语义相似性泛化 |

    **属性维度表**:

    | 维度 | 值/描述 | 说明 |
    |------|---------|------|
    | **提出时间** | 1950s（Shannon）→1990s（平滑技术） | 信息论驱动 |
    | **理论基础** | 概率论、信息论、马尔可夫链 | Shannon熵 1.0-1.5 bits/字符 |
    | **典型n值** | n=2(bigram)、n=3(trigram)、n=5 | n↑性能↑但参数爆炸 |
    | **参数数量** | $\|V\|^n$ | V=100K,n=3→$10^{15}$ |
    | **训练方式** | 最大似然估计（计数） | $O(N)$线性时间 |
    | **主要挑战** | 数据稀疏、上下文受限 | Zipf定律导致长尾 |
    | **巅峰期** | 1990-2010年代 | 被神经LM取代 |
    | **历史地位** | NLP基础、大语言模型前身 | 核心概念传承至今 |

    ---

    ### 2️⃣ n-gram模型演进全景图

    ```mermaid
    graph TB
        Origin[1948: Shannon信息论]

        Origin --> Unigram[Unigram n=1]
        Origin --> Bigram[Bigram n=2]
        Origin --> Trigram[Trigram n=3]
        Origin --> HigherN[5-gram及以上]

        Unigram --> U_Def["P(w_i) = count(w_i)/N"]
        Unigram --> U_Prob[独立假设：词间无关]
        Unigram --> U_Use[词频统计、基线模型]

        Bigram --> B_Def["P(w_i|w_{i-1})"]
        Bigram --> B_Prob[一阶马尔可夫]
        Bigram --> B_Use[拼写纠错、语音识别]

        Trigram --> T_Def["P(w_i|w_{i-2},w_{i-1})"]
        Trigram --> T_Prob[二阶马尔可夫]
        Trigram --> T_Use[机器翻译、文本生成]

        HigherN --> H_Def["P(w_i|w_{i-n+1},...,w_{i-1})"]
        HigherN --> H_Prob[高阶依赖]
        HigherN --> H_Problem[参数爆炸 |V|^n]

        Unigram --> Problem[核心挑战]
        Bigram --> Problem
        Trigram --> Problem
        HigherN --> Problem

        Problem --> Sparse[数据稀疏性]
        Problem --> Context[上下文受限]
        Problem --> Param[参数爆炸]

        Sparse --> Smoothing[平滑技术]
        Smoothing --> Laplace[Laplace加法平滑]
        Smoothing --> GoodTuring[Good-Turing]
        Smoothing --> KneserNey[Kneser-Ney ✅最佳]

        Context --> Backoff[回退Backoff]
        Context --> Interpolation[插值Interpolation]

        Param --> Neural[1990s→神经语言模型]
        Neural --> Bengio[2003: 前馈神经LM]
        Neural --> RNN[2010: RNN-LM]
        Neural --> LSTM[2014: LSTM-LM]
        Neural --> Transformer[2017: Transformer]

        style Origin fill:#ff6b6b,stroke:#333,stroke-width:4px
        style KneserNey fill:#ffd93d,stroke:#333,stroke-width:3px
        style Transformer fill:#6bcf7f,stroke:#333,stroke-width:3px
    ```

    ---

    ### 3️⃣ n-gram模型多维对比矩阵

    | 模型 | 上下文长度 | 参数数量 | 训练复杂度 | 推理速度 | 困惑度(PTB) | 优势 | 劣势 |
    |------|-----------|---------|-----------|---------|------------|------|------|
    | **Unigram** | 0 | $\|V\|$ | $O(N)$ | $O(1)$ | ~962 | 极快、极简 | 无上下文、性能差 |
    | **Bigram** | 1词 | $\|V\|^2$ | $O(N)$ | $O(1)$ | ~200 | 快速、可解释 | 上下文太短 |
    | **Trigram** | 2词 | $\|V\|^3$ | $O(N)$ | $O(1)$ | ~150 | 平衡性能/复杂度 | 长程依赖差 |
    | **5-gram+KN** | 4词 | $\|V\|^5$(稀疏) | $O(N)$ | $O(1)$ | ~141 | 经典最佳n-gram | 仍无法捕捉长依赖 |
    | **前馈神经LM** | 固定n | $\|V\|d + O(d^2)$ | $O(Nd)$ | $O(d^2)$ | ~137 | 分布式表示 | 上下文固定 |
    | **RNN-LM** | 理论∞ | $O(d^2 + \|V\|d)$ | $O(Nd^2)$ | $O(d^2)$ | ~123 | 可变长上下文 | 梯度消失 |
    | **LSTM-LM** | 实际~200 | $O(d^2 + \|V\|d)$ | $O(Nd^2)$ | $O(d^2)$ | ~78 | 长程依赖 | 顺序瓶颈 |
    | **Transformer-LM** | 512-2048 | $O(L^2d + \|V\|d)$ | $O(NL^2d)$ | $O(L^2d)$ | ~56 | 并行、注意力 | 计算密集 |

    **符号说明**: $N$=语料大小, $\|V\|$=词汇表大小, $d$=隐藏层维度, $L$=序列长度

    ---

    ### 4️⃣ 平滑技术演进思维导图

    ```mermaid
    mindmap
      root((平滑技术<br/>Smoothing))
        问题根源
          数据稀疏 Zipf定律
            80%词出现<10次
            大量n-gram计数为0
          零概率问题
            P(未见过的n-gram) = 0
            整句概率=0
        加法平滑
          Laplace +1平滑
            公式 (c+1)/(N+|V|)
            过度平滑
          Add-k平滑
            k<1改进
        Good-Turing平滑
          核心思想
            基于频率的频率
            r→r* = (r+1)N_{r+1}/N_r
          适用场景
            中频词效果好
          局限
            需要大量数据
        Kneser-Ney平滑
          绝对折扣
            c→max(c-δ,0)
          continuation概率
            P_cont(w) ∝ |{v:count(v,w)>0}|
          Modified KN
            经典n-gram最佳
            SRILM工具包实现
          递归插值
            高阶→低阶平滑传递
        回退vs插值
          Backoff回退
            高阶不存在→回退低阶
            Katz回退
          Interpolation插值
            线性组合各阶
            λ_i权重优化
          对比
            回退节省计算
            插值性能更好
    ```

    ---

    ### 5️⃣ n-gram局限性分析矩阵

    | 局限性类型 | 具体表现 | 数学原因 | 实际影响 | 平滑技术能否解决 | 神经LM解决方案 |
    |----------|---------|---------|---------|---------------|--------------|
    | **上下文窗口受限** | 无法捕捉长程依赖 | 马尔可夫假设 $k$ 阶 | 无法理解复杂句法 | ❌ 无法 | RNN/LSTM/Transformer |
    | **泛化能力差** | "cat"和"dog"无法共享 | 离散符号表示 | 相似句子无法泛化 | ❌ 无法 | 词嵌入、分布式表示 |
    | **参数爆炸** | $\|V\|^n$ 指数增长 | 组合爆炸 | $n>5$ 不可行 | ✅ 部分（稀疏存储） | 参数共享 $O(\|V\|d)$ |
    | **数据稀疏** | 长尾n-gram未出现 | Zipf定律 | 大量零概率 | ✅ 主要目标 | 连续空间泛化 |
    | **OOV问题** | 未登录词无法处理 | 闭合词汇表 | 新词、专有名词 | ❌ 无法 | 子词/字符级模型 |
    | **语义盲** | 无法理解"银行"歧义 | 频率统计 | 错误消歧 | ❌ 无法 | 上下文嵌入（BERT） |
    | **计算瓶颈** | 5-gram需数百GB | 存储所有n-gram | 内存/磁盘限制 | ✅ 剪枝 | 压缩表示 |

    ---

    ### 6️⃣ 信息论视角深度分析

    ```mermaid
    graph TB
        Shannon[Shannon 1951: 语言熵]

        Shannon --> Entropy[熵 H(L)]
        Entropy --> E_Def["H(L) = lim -1/n Σ P(w₁...wₙ)logP(w₁...wₙ)"]
        Entropy --> E_Value[英语: 1.0-1.5 bits/字符<br/>约10 bits/词]

        Shannon --> CrossEntropy[交叉熵 H(P,Q)]
        CrossEntropy --> CE_Def["H(P,Q) = -Σ P(x)logQ(x)"]
        CrossEntropy --> CE_Property["H(P,Q) ≥ H(P) 等号⇔P=Q"]

        CrossEntropy --> Model[模型评估]
        Model --> Goal[目标: H(真实,模型)→H(真实)]
        Model --> PPL[困惑度=exp(H)]

        PPL --> PPL_Def["PPL = P(w₁...wₙ)^(-1/n)"]
        PPL --> PPL_Mean[平均分支因子]
        PPL --> PPL_Lower[越低越好]

        Entropy --> Conditional[条件熵]
        Conditional --> C_Def["H(Wₜ|W₁...Wₜ₋₁)"]
        Conditional --> LM_Goal[语言模型: 最小化条件熵]

        Shannon --> Applications[应用]
        Applications --> Compression[数据压缩]
        Applications --> Coding[信源编码]
        Applications --> Prediction[预测理论]

        style Shannon fill:#ff6b6b,stroke:#333,stroke-width:4px
        style PPL fill:#ffd93d,stroke:#333,stroke-width:3px
        style LM_Goal fill:#6bcf7f,stroke:#333,stroke-width:2px
    ```

    **熵与困惑度关系**:
    $$
    \text{PPL} = 2^{H(P,Q)} \quad \text{（以2为底时）}
    $$

    **困惑度物理意义**: 模型在每个位置平均"惊讶"于多少个候选词

    ---

    ### 7️⃣ 统计LM→神经LM演进路径

    ```mermaid
    graph LR
        SLM[统计语言模型<br/>1950s-2000s]

        SLM --> Problem1[问题1: 离散表示]
        SLM --> Problem2[问题2: 稀疏性]
        SLM --> Problem3[问题3: 固定上下文]

        Problem1 --> Sol1[解决: 分布式表示]
        Sol1 --> Word2Vec[Word2Vec 2013]
        Sol1 --> Embedding[词嵌入层]

        Problem2 --> Sol2[解决: 参数共享]
        Sol2 --> FFNN[前馈神经LM 2003]
        Sol2 --> Shared[相似词共享参数]

        Problem3 --> Sol3[解决: 动态上下文]
        Sol3 --> RNN[RNN-LM 2010]
        Sol3 --> LSTM[LSTM-LM 2014]
        Sol3 --> Attention[注意力机制 2015]
        Sol3 --> Transformer[Transformer 2017]

        Word2Vec --> Modern[现代LLM]
        FFNN --> Modern
        RNN --> Modern
        LSTM --> Modern
        Transformer --> Modern

        Modern --> GPT[GPT系列 2018-]
        Modern --> BERT[BERT系列 2018-]
        Modern --> T5[T5/GLM等]

        SLM -.核心概念传承.-> Modern

        Legacy1[困惑度评估]
        Legacy2[条件概率建模]
        Legacy3[平滑→正则化]
        Legacy4[回退→集成学习]

        SLM --> Legacy1
        SLM --> Legacy2
        SLM --> Legacy3
        SLM --> Legacy4

        Legacy1 -.仍在使用.-> Modern
        Legacy2 -.仍在使用.-> Modern
        Legacy3 -.演变形式.-> Modern
        Legacy4 -.演变形式.-> Modern

        style SLM fill:#ff6b6b,stroke:#333,stroke-width:4px
        style Modern fill:#6bcf7f,stroke:#333,stroke-width:3px
        style Transformer fill:#ffd93d,stroke:#333,stroke-width:2px
    ```

    ---

    ### 8️⃣ Penn Treebank困惑度历史对比

    | 模型类型 | 具体模型 | 测试PPL | 年份 | 参数量 | 训练时间 | 关键创新 |
    |---------|---------|---------|------|--------|---------|---------|
    | **经典n-gram** | 5-gram + Modified KN | ~141 | 1995 | ~$10^9$(稀疏) | 分钟级 | 最佳平滑技术 |
    | **神经LM开创** | 前馈神经LM (Bengio) | ~137 | 2003 | ~10M | 小时级 | 分布式表示 |
    | **循环架构** | RNN-LM | ~123 | 2010 | ~10M | 小时级 | 动态上下文 |
    | **长短期记忆** | LSTM-LM (单层) | ~115 | 2012 | ~10M | 小时级 | 门控机制 |
    | **深度LSTM** | LSTM-LM (多层) | ~78 | 2014 | ~50M | 天级 | 深度+Dropout |
    | **注意力** | Transformer-LM | ~56 | 2017 | ~100M | 天级 | 自注意力 |
    | **大规模预训练** | GPT-2 (fine-tuned) | ~35 | 2019 | 1.5B | 周级 | 规模+数据 |
    | **当代SOTA** | GPT-3 (zero-shot) | ~20 | 2020 | 175B | 月级 | 少样本学习 |

    **趋势总结**:

    - 困惑度: 141 → 20 (降低86%)
    - 参数量: $10^9$ → $10^{11}$ (增长100倍)
    - 时间跨度: 25年 (1995-2020)

    ---

    ### 9️⃣ 统计LM遗产与现代应用

    | 遗产类别 | 经典形式 | 现代演变 | 仍在使用的场景 | 原因 |
    |---------|---------|---------|--------------|------|
    | **评估指标** | 困惑度PPL | 仍是标准指标 | 所有语言模型 | 理论清晰、可解释 |
    | **概率建模** | $P(w\|h)$ | 注意力权重、softmax | Transformer解码 | 概率框架不变 |
    | **平滑思想** | Kneser-Ney | Dropout、正则化 | 训练正则化 | 防止过拟合 |
    | **回退/插值** | n-gram混合 | 集成学习、蒸馏 | 模型融合 | 多样性增益 |
    | **上下文依赖** | 马尔可夫链 | 注意力机制 | Transformer | 依赖建模核心 |
    | **快速推理** | O(1)查表 | 量化、剪枝 | 移动端部署 | 资源受限 |
    | **低资源场景** | n-gram | 仍直接使用 | <1MB数据 | 样本高效 |
    | **基线模型** | 5-gram | 评估改进 | 研究对比 | 可复现、稳定 |

    ---

    ### 🔟 核心洞察与设计原则

    **三大基本定律**:

    1. **Shannon熵下界定律**
      $$
      H(P, Q) \geq H(P) \quad \text{等号成立} \Leftrightarrow P = Q
      $$
      - 任何模型的交叉熵≥语言真实熵
      - 模型优化目标：逼近真实分布

    2. **Zipf定律与长尾分布**
      $$
      f(r) \propto \frac{1}{r^{\alpha}} \quad \text{其中 } \alpha \approx 1
      $$
      - 少数高频词占大部分出现
      - 大量低频词导致稀疏性

    3. **参数-性能权衡定律**
      $$
      \text{PPL} \propto \frac{1}{\log(\text{Params})} \quad \text{（经验）}
      $$
      - n-gram: $\|V\|^n$ 指数增长
      - 神经LM: $O(\|V\|d + d^2)$ 线性

    **实践设计原则**:

    ```yaml
      原则1_平滑必不可少:
        描述: n-gram必须平滑，否则零概率泛滥
        推荐: Modified Kneser-Ney（经典最佳）

      原则2_上下文权衡:
        描述: n↑性能↑但参数↑
        实践: n=3或5，再大不划算

      原则3_插值优于回退:
        描述: 插值利用所有阶信息
        代价: 计算略慢，但性能更好

      原则4_困惑度解释:
        描述: PPL是模型质量金标准
        注意: 不同语料不可比

      原则5_低资源首选:
        描述: <1MB数据，n-gram可能优于神经LM
        原因: 样本效率高，不易过拟合
    ```

    **统计LM适用checklist**:

    - [ ] 数据量 < 1MB？ → 考虑n-gram
    - [ ] 需要实时推理(<1ms)？ → n-gram更快
    - [ ] 内存极度受限(<10MB)？ → n-gram更小
    - [ ] 需要完全可解释？ → n-gram透明
    - [ ] 只需基线对比？ → n-gram稳定
    - [ ] 追求SOTA性能？ → 神经/Transformer

    **历史地位总结**:

    > "统计语言模型是NLP的拼音阶段——虽已被深度学习取代，但其核心思想（概率建模、困惑度、条件依赖）仍是现代LLM的理论基石。"

</details>

---

## 📋 目录

- [统计语言模型 | Statistical Language Models](#统计语言模型--statistical-language-models)
  - [📊 核心概念深度分析](#-核心概念深度分析)
  - [📋 目录](#-目录)
  - [概述 | Overview](#概述--overview)
  - [1. 语言模型的定义 | Definition of Language Models](#1-语言模型的定义--definition-of-language-models)
    - [1.1 形式化定义](#11-形式化定义)
    - [1.2 语言模型的任务](#12-语言模型的任务)
    - [1.3 评估指标](#13-评估指标)
  - [2. n-gram 模型 | n-gram Models](#2-n-gram-模型--n-gram-models)
    - [2.1 马尔可夫假设](#21-马尔可夫假设)
    - [2.2 最大似然估计](#22-最大似然估计)
    - [2.3 稀疏性问题](#23-稀疏性问题)
    - [2.4 平滑技术](#24-平滑技术)
    - [2.5 回退与插值](#25-回退与插值)
  - [3. n-gram 的局限性 | Limitations of n-grams](#3-n-gram-的局限性--limitations-of-n-grams)
    - [3.1 上下文窗口受限](#31-上下文窗口受限)
    - [3.2 泛化能力差](#32-泛化能力差)
    - [3.3 参数爆炸](#33-参数爆炸)
    - [3.4 数据稀疏](#34-数据稀疏)
  - [4. 神经语言模型的动机 | Motivation for Neural Language Models](#4-神经语言模型的动机--motivation-for-neural-language-models)
    - [4.1 分布式表示](#41-分布式表示)
    - [4.2 前馈神经语言模型](#42-前馈神经语言模型)
    - [4.3 循环神经语言模型](#43-循环神经语言模型)
    - [4.4 LSTM 语言模型](#44-lstm-语言模型)
  - [5. 语言建模的信息论视角 | Information-Theoretic View](#5-语言建模的信息论视角--information-theoretic-view)
    - [5.1 熵与语言](#51-熵与语言)
    - [5.2 交叉熵与模型质量](#52-交叉熵与模型质量)
    - [5.3 条件熵](#53-条件熵)
  - [6. 经典应用 | Classical Applications](#6-经典应用--classical-applications)
    - [6.1 语音识别](#61-语音识别)
    - [6.2 机器翻译](#62-机器翻译)
    - [6.3 拼写纠错](#63-拼写纠错)
    - [6.4 文本生成](#64-文本生成)
  - [7. 性能对比 | Performance Comparison](#7-性能对比--performance-comparison)
    - [7.1 Penn Treebank 困惑度](#71-penn-treebank-困惑度)
    - [7.2 趋势](#72-趋势)
  - [8. 统计语言模型的遗产 | Legacy of Statistical LMs](#8-统计语言模型的遗产--legacy-of-statistical-lms)
    - [8.1 仍在使用的场景](#81-仍在使用的场景)
    - [8.2 核心概念的传承](#82-核心概念的传承)
  - [9. 权威参考文献 | Authoritative References](#9-权威参考文献--authoritative-references)
    - [Wikipedia 条目](#wikipedia-条目)
    - [学术论文](#学术论文)
    - [标准教材](#标准教材)
  - [10. 关键要点总结 | Key Takeaways](#10-关键要点总结--key-takeaways)
  - [导航 | Navigation](#导航--navigation)
  - [相关主题 | Related Topics](#相关主题--related-topics)
    - [本章节](#本章节)
    - [相关章节](#相关章节)
    - [跨视角链接](#跨视角链接)

---

## 概述 | Overview

统计语言模型是自然语言处理的基础，通过概率分布建模语言序列。本文档系统阐述从n-gram到神经语言模型的演进，为理解现代大语言模型奠定基础。

## 1. 语言模型的定义 | Definition of Language Models

### 1.1 形式化定义

**语言模型**：定义在词序列上的概率分布

```text
P(w₁, w₂, ..., wₙ)
```

其中 wᵢ ∈ V（词汇表）

**链式法则分解**：

```text
P(w₁, w₂, ..., wₙ) = ∏ᵢ₌₁ⁿ P(wᵢ | w₁, ..., wᵢ₋₁)
```

**目标**：估计条件概率 P(wᵢ | w₁, ..., wᵢ₋₁)

### 1.2 语言模型的任务

**1. 生成**：

```text
采样 w ~ P(w | 前文)
```

**2. 评估**：

```text
给句子打分：P(句子)
```

**3. 预测**：

```text
argmax_w P(w | 前文)
```

**4. 完形填空**：

```text
P(w_missing | 上下文)
```

### 1.3 评估指标

**困惑度 (Perplexity, PPL)**：

```text
PPL = exp(-1/N ∑ᵢ log P(wᵢ | w₁, ..., wᵢ₋₁))
     = P(w₁, ..., wₙ)^(-1/N)
```

**意义**：

- 平均分支因子
- 模型对测试数据的"惊讶"程度
- 越低越好

**交叉熵**：

```text
H = -1/N ∑ᵢ log P(wᵢ | w₁, ..., wᵢ₋₁)
```

**关系**：PPL = exp(H)

## 2. n-gram 模型 | n-gram Models

### 2.1 马尔可夫假设

**问题**：完整历史 w₁, ..., wᵢ₋₁ 太长

**n-gram 假设**：

```text
P(wᵢ | w₁, ..., wᵢ₋₁) ≈ P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
```

只依赖于前 n-1 个词

**常见模型**：

1. **Unigram (n=1)**：

   ```text
   P(wᵢ) = count(wᵢ) / N
   ```

2. **Bigram (n=2)**：

   ```text
   P(wᵢ | wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)
   ```

3. **Trigram (n=3)**：

   ```text
   P(wᵢ | wᵢ₋₂, wᵢ₋₁) = count(wᵢ₋₂, wᵢ₋₁, wᵢ) / count(wᵢ₋₂, wᵢ₋₁)
   ```

### 2.2 最大似然估计

**训练**：计数

```text
P_MLE(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁) =
    count(wᵢ₋ₙ₊₁, ..., wᵢ) / count(wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
```

**例子**（Bigram）：

训练数据：

```text
"I am Sam. Sam I am. I do not like green eggs and ham."
```

学到的概率：

```text
P(Sam | I) = 2/3
P(am | I) = 1/3
P(I | Sam) = 1/2
...
```

### 2.3 稀疏性问题

**挑战**：许多 n-gram 从未出现

**问题**：

```text
P(wᵢ | context) = 0  如果 (context, wᵢ) 未出现
```

**后果**：

- 无法处理新组合
- 整句概率为 0

### 2.4 平滑技术

**1. 加法平滑 (Laplace Smoothing)**：

```text
P(wᵢ | wᵢ₋₁) = (count(wᵢ₋₁, wᵢ) + 1) / (count(wᵢ₋₁) + |V|)
```

**2. Good-Turing 平滑**：

基于频率的频率重新分配概率质量

**3. Kneser-Ney 平滑**：

```text
P_KN(wᵢ | wᵢ₋₁) = max(count(wᵢ₋₁, wᵢ) - δ, 0) / count(wᵢ₋₁)
                  + λ(wᵢ₋₁) P_continuation(wᵢ)
```

其中 P_continuation 基于 wᵢ 出现在多少种上下文中

**最佳实践**：Modified Kneser-Ney 是经典 n-gram 的最佳平滑方法

### 2.5 回退与插值

**回退 (Backoff)**：

```text
P(wᵢ | wᵢ₋₂, wᵢ₋₁) = {
  P_trigram(wᵢ | wᵢ₋₂, wᵢ₋₁)  如果 trigram 出现过
  α · P_bigram(wᵢ | wᵢ₋₁)      否则回退到 bigram
}
```

**插值 (Interpolation)**：

```text
P(wᵢ | wᵢ₋₂, wᵢ₋₁) =
    λ₃ P_trigram(wᵢ | wᵢ₋₂, wᵢ₋₁) +
    λ₂ P_bigram(wᵢ | wᵢ₋₁) +
    λ₁ P_unigram(wᵢ)
```

其中 λ₁ + λ₂ + λ₃ = 1

**优势**：结合不同阶的信息

## 3. n-gram 的局限性 | Limitations of n-grams

### 3.1 上下文窗口受限

**问题**：

```text
"The computer which I had just put into the machine room on the fifth floor crashed."
```

- Trigram 只看 "fifth floor"
- 无法捕捉 "computer" 和 "crashed" 的长程依赖

### 3.2 泛化能力差

**相似句子无法泛化**：

```text
"The cat sat on the mat"  → 高概率
"The dog sat on the mat"  → 低概率（如果未见过）
```

**原因**：把每个词当做独立符号，不理解语义相似性

### 3.3 参数爆炸

**参数数量**：

```text
n-gram 模型参数：|V|^n
```

**例子**：

- V = 100,000
- Trigram：10^15 个参数
- 不可行

### 3.4 数据稀疏

**Zipf 定律**：

```text
词频 ∝ 1 / rank
```

**结果**：

- 大部分词很少出现
- 大部分 n-gram 从未出现
- 平滑只是部分解决

## 4. 神经语言模型的动机 | Motivation for Neural Language Models

### 4.1 分布式表示

**核心思想**：

> 词嵌入到连续向量空间，相似词有相似表示

**Word2Vec (Mikolov et al., 2013)**：

```text
"cat" → [0.2, -0.5, 0.8, ...]
"dog" → [0.3, -0.4, 0.7, ...]  （接近！）
```

**优势**：

- 自动泛化到相似词
- 参数共享

### 4.2 前馈神经语言模型

**Bengio et al. (2003)**：

```text
P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁) = softmax(Wₕ h + b)

其中：
h = tanh(Wₑ [e(wᵢ₋ₙ₊₁); ...; e(wᵢ₋₁)] + bₕ)
e(w) 是词 w 的嵌入
```

**架构**：

```text
输入词 → 嵌入层 → 隐藏层 → Softmax → 概率分布
```

**参数数量**：

```text
嵌入：|V| × d
隐藏：(n-1)d × h + h × |V|
```

远小于 |V|^n

### 4.3 循环神经语言模型

**RNN-LM (Mikolov et al., 2010)**：

```text
hₜ = σ(Wₕ hₜ₋₁ + Wₓ e(wₜ) + bₕ)
P(wₜ₊₁ | w₁, ..., wₜ) = softmax(Wₒ hₜ + bₒ)
```

**优势**：

- 理论上无限上下文
- 参数不依赖于历史长度

**挑战**：

- 梯度消失/爆炸
- 长程依赖困难
- 训练慢（顺序）

### 4.4 LSTM 语言模型

**解决长程依赖**：

```text
使用 LSTM 单元代替简单 RNN
```

**成就**：

- 2015年前的 state-of-the-art
- 困惑度大幅下降

## 5. 语言建模的信息论视角 | Information-Theoretic View

### 5.1 熵与语言

**语言的熵**：

```text
H(L) = lim_{n→∞} -1/n ∑_{w₁,...,wₙ} P(w₁, ..., wₙ) log P(w₁, ..., wₙ)
```

**Shannon (1951) 估计**：

- 英语熵约 1.0 - 1.5 bits/字符
- 或约 10 bits/词

### 5.2 交叉熵与模型质量

**交叉熵**：

```text
H(P, Q) = -∑ P(x) log Q(x)
```

其中：

- P：真实分布
- Q：模型分布

**关系**：

```text
H(P, Q) ≥ H(P)
```

等号成立当且仅当 P = Q

**模型评估**：

```text
H(测试数据, 模型) 越接近 H(语言)，模型越好
```

### 5.3 条件熵

**条件熵**：

```text
H(Wₜ | W₁, ..., Wₜ₋₁) = -∑ P(w₁, ..., wₜ) log P(wₜ | w₁, ..., wₜ₋₁)
```

**语言模型目标**：最小化条件熵

## 6. 经典应用 | Classical Applications

### 6.1 语音识别

**声学模型 + 语言模型**：

```text
argmax_W P(W | A) = argmax_W P(A | W) P(W)
```

其中：

- A：声学特征
- W：词序列
- P(W)：语言模型

### 6.2 机器翻译

**噪声信道模型**：

```text
argmax_T P(T | S) = argmax_T P(S | T) P(T)
```

其中：

- S：源语言
- T：目标语言
- P(T)：目标语言模型

### 6.3 拼写纠错

**候选排序**：

```text
"teh" → {"the", "tea", "ten"}
选择 argmax P(候选 | 上下文)
```

### 6.4 文本生成

**采样**：

```text
w₁ ~ P(w)
w₂ ~ P(w | w₁)
w₃ ~ P(w | w₁, w₂)
...
```

**解码策略**：

- 贪心：argmax P(w | 前文)
- Beam search：保持 top-k
- 采样：随机采样

## 7. 性能对比 | Performance Comparison

### 7.1 Penn Treebank 困惑度

**数据集**：Penn Treebank，词汇量约1万

| 模型 | 测试困惑度 | 年份 |
|------|-----------|------|
| **5-gram + KN平滑** | ~141 | 经典 |
| **前馈神经LM** | ~137 | 2003 |
| **RNN-LM** | ~123 | 2010 |
| **LSTM-LM** | ~78 | 2014 |
| **Transformer-LM** | ~56 | 2017 |
| **GPT-2** | ~35 | 2019 |

### 7.2 趋势

**观察**：

1. 神经模型显著优于 n-gram
2. LSTM 显著优于简单 RNN
3. Transformer 成为新标准
4. 规模（数据+模型）持续提升性能

## 8. 统计语言模型的遗产 | Legacy of Statistical LMs

### 8.1 仍在使用的场景

**1. 低资源场景**：

- n-gram 快速、简单
- 小数据集上可能更好

**2. 实时系统**：

- 推理速度快
- 内存占用小

**3. 基线模型**：

- 评估神经模型改进

### 8.2 核心概念的传承

**1. 困惑度**：

- 仍是主要评估指标

**2. 平滑思想**：

- 演变为正则化、dropout

**3. 回退/插值**：

- 演变为集成、多任务学习

**4. 上下文依赖**：

- 演变为注意力机制

## 9. 权威参考文献 | Authoritative References

### Wikipedia 条目

1. [Language model](https://en.wikipedia.org/wiki/Language_model)
2. [N-gram](https://en.wikipedia.org/wiki/N-gram)
3. [Perplexity](https://en.wikipedia.org/wiki/Perplexity)
4. [Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)
5. [Word embedding](https://en.wikipedia.org/wiki/Word_embedding)

### 学术论文

1. **Shannon, C. E. (1951)**. "Prediction and entropy of printed English". _Bell System Technical Journal_.
   - 语言熵的开创性工作

2. **Bengio, Y., et al. (2003)**. "A neural probabilistic language model". _JMLR_.
   - 神经语言模型的奠基之作

3. **Mikolov, T., et al. (2010)**. "Recurrent neural network based language model". _Interspeech_.
   - RNN 语言模型

4. **Mikolov, T., et al. (2013)**. "Efficient estimation of word representations in vector space". _ICLR_.
   - Word2Vec

5. **Sundermeyer, M., et al. (2012)**. "LSTM neural networks for language modeling". _Interspeech_.
   - LSTM 语言模型

### 标准教材

1. **Jurafsky, D., & Martin, J. H. (2023)**. _Speech and Language Processing_ (3rd ed.).
   - 第3章：n-gram 语言模型
   - 第7章：神经网络与神经语言模型

2. **Manning, C. D., & Schütze, H. (1999)**. _Foundations of Statistical Natural Language Processing_. MIT Press.
   - 经典统计 NLP 教材

3. **Goldberg, Y. (2017)**. _Neural Network Methods for Natural Language Processing_. Morgan & Claypool.
   - 第10章：语言建模

## 10. 关键要点总结 | Key Takeaways

1. **语言模型的本质**：
   - 词序列的概率分布
   - 预测下一个词

2. **n-gram 模型**：
   - 马尔可夫假设
   - 简单、快速、可解释
   - 但上下文受限、泛化差

3. **平滑技术**：
   - 处理稀疏性
   - Kneser-Ney 最佳

4. **神经语言模型**：
   - 分布式表示
   - 自动泛化
   - 更长上下文

5. **困惑度**：
   - 标准评估指标
   - 越低越好

6. **演进趋势**：
   - n-gram → 神经 → Transformer
   - 性能持续提升

7. **理论基础**：
   - 信息论
   - 熵、交叉熵
   - 最小化不确定性

8. **历史意义**：
   - 为现代大语言模型奠定基础
   - 核心概念仍然重要

---

**下一步阅读**：

- [03.2 神经语言模型](03.2_Neural_Language_Models.md)
- [03.3 Transformer大语言模型理论](03.3_Transformer_LLM_Theory.md)
- [03.4 Token生成机制](03.4_Token_Generation_Mechanisms.md)

---

## 导航 | Navigation

**上一篇**: [← 02.5 通用逼近定理](../02_Neural_Network_Theory/02.5_Universal_Approximation_Theorem.md)
**下一篇**: [03.2 神经语言模型 →](./03.2_Neural_Language_Models.md)
**返回目录**: [↑ AI模型视角总览](../README.md)

---

## 相关主题 | Related Topics

### 本章节

- [03.2 神经语言模型](./03.2_Neural_Language_Models.md)
- [03.3 Transformer LLM理论](./03.3_Transformer_LLM_Theory.md)
- [03.4 Token生成机制](./03.4_Token_Generation_Mechanisms.md)
- [03.5 嵌入向量空间](./03.5_Embedding_Vector_Spaces.md)
- [03.6 上下文窗口与记忆](./03.6_Context_Window_Memory.md)

### 相关章节

- [02.1 神经网络基础](../02_Neural_Network_Theory/02.1_Neural_Network_Foundations.md)

### 跨视角链接

- [FormalLanguage_Perspective: 形式语言](../../FormalLanguage_Perspective/README.md)
- [Information_Theory_Perspective: 信息论](../../Information_Theory_Perspective/README.md)
