# 统计语言模型 | Statistical Language Models

> **文档版本**: v1.0.0  
> **最后更新**: 2025-10-27  
> **文档规模**: 629行 | 统计语言模型基础理论  
> **阅读建议**: 本文介绍n-gram等经典统计语言模型，是理解现代神经语言模型的历史基础

---

## 目录 | Table of Contents

- [统计语言模型 | Statistical Language Models](#统计语言模型--statistical-language-models)
  - [目录 | Table of Contents](#目录--table-of-contents)
  - [概述 | Overview](#概述--overview)
  - [1. 语言模型的定义 | Definition of Language Models](#1-语言模型的定义--definition-of-language-models)
    - [1.1 形式化定义](#11-形式化定义)
    - [1.2 语言模型的任务](#12-语言模型的任务)
    - [1.3 评估指标](#13-评估指标)
  - [2. n-gram 模型 | n-gram Models](#2-n-gram-模型--n-gram-models)
    - [2.1 马尔可夫假设](#21-马尔可夫假设)
    - [2.2 最大似然估计](#22-最大似然估计)
    - [2.3 稀疏性问题](#23-稀疏性问题)
    - [2.4 平滑技术](#24-平滑技术)
    - [2.5 回退与插值](#25-回退与插值)
  - [3. n-gram 的局限性 | Limitations of n-grams](#3-n-gram-的局限性--limitations-of-n-grams)
    - [3.1 上下文窗口受限](#31-上下文窗口受限)
    - [3.2 泛化能力差](#32-泛化能力差)
    - [3.3 参数爆炸](#33-参数爆炸)
    - [3.4 数据稀疏](#34-数据稀疏)
  - [4. 神经语言模型的动机 | Motivation for Neural Language Models](#4-神经语言模型的动机--motivation-for-neural-language-models)
    - [4.1 分布式表示](#41-分布式表示)
    - [4.2 前馈神经语言模型](#42-前馈神经语言模型)
    - [4.3 循环神经语言模型](#43-循环神经语言模型)
    - [4.4 LSTM 语言模型](#44-lstm-语言模型)
  - [5. 语言建模的信息论视角 | Information-Theoretic View](#5-语言建模的信息论视角--information-theoretic-view)
    - [5.1 熵与语言](#51-熵与语言)
    - [5.2 交叉熵与模型质量](#52-交叉熵与模型质量)
    - [5.3 条件熵](#53-条件熵)
  - [6. 经典应用 | Classical Applications](#6-经典应用--classical-applications)
    - [6.1 语音识别](#61-语音识别)
    - [6.2 机器翻译](#62-机器翻译)
    - [6.3 拼写纠错](#63-拼写纠错)
    - [6.4 文本生成](#64-文本生成)
  - [7. 性能对比 | Performance Comparison](#7-性能对比--performance-comparison)
    - [7.1 Penn Treebank 困惑度](#71-penn-treebank-困惑度)
    - [7.2 趋势](#72-趋势)
  - [8. 统计语言模型的遗产 | Legacy of Statistical LMs](#8-统计语言模型的遗产--legacy-of-statistical-lms)
    - [8.1 仍在使用的场景](#81-仍在使用的场景)
    - [8.2 核心概念的传承](#82-核心概念的传承)
  - [9. 权威参考文献 | Authoritative References](#9-权威参考文献--authoritative-references)
    - [Wikipedia 条目](#wikipedia-条目)
    - [学术论文](#学术论文)
    - [标准教材](#标准教材)
  - [10. 关键要点总结 | Key Takeaways](#10-关键要点总结--key-takeaways)

---

## 概述 | Overview

统计语言模型是自然语言处理的基础，通过概率分布建模语言序列。本文档系统阐述从n-gram到神经语言模型的演进，为理解现代大语言模型奠定基础。

## 1. 语言模型的定义 | Definition of Language Models

### 1.1 形式化定义

**语言模型**：定义在词序列上的概率分布

```text
P(w₁, w₂, ..., wₙ)
```

其中 wᵢ ∈ V（词汇表）

**链式法则分解**：

```text
P(w₁, w₂, ..., wₙ) = ∏ᵢ₌₁ⁿ P(wᵢ | w₁, ..., wᵢ₋₁)
```

**目标**：估计条件概率 P(wᵢ | w₁, ..., wᵢ₋₁)

### 1.2 语言模型的任务

**1. 生成**：

```text
采样 w ~ P(w | 前文)
```

**2. 评估**：

```text
给句子打分：P(句子)
```

**3. 预测**：

```text
argmax_w P(w | 前文)
```

**4. 完形填空**：

```text
P(w_missing | 上下文)
```

### 1.3 评估指标

**困惑度 (Perplexity, PPL)**：

```text
PPL = exp(-1/N ∑ᵢ log P(wᵢ | w₁, ..., wᵢ₋₁))
     = P(w₁, ..., wₙ)^(-1/N)
```

**意义**：

- 平均分支因子
- 模型对测试数据的"惊讶"程度
- 越低越好

**交叉熵**：

```text
H = -1/N ∑ᵢ log P(wᵢ | w₁, ..., wᵢ₋₁)
```

**关系**：PPL = exp(H)

## 2. n-gram 模型 | n-gram Models

### 2.1 马尔可夫假设

**问题**：完整历史 w₁, ..., wᵢ₋₁ 太长

**n-gram 假设**：

```text
P(wᵢ | w₁, ..., wᵢ₋₁) ≈ P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
```

只依赖于前 n-1 个词

**常见模型**：

1. **Unigram (n=1)**：

   ```text
   P(wᵢ) = count(wᵢ) / N
   ```

2. **Bigram (n=2)**：

   ```text
   P(wᵢ | wᵢ₋₁) = count(wᵢ₋₁, wᵢ) / count(wᵢ₋₁)
   ```

3. **Trigram (n=3)**：

   ```text
   P(wᵢ | wᵢ₋₂, wᵢ₋₁) = count(wᵢ₋₂, wᵢ₋₁, wᵢ) / count(wᵢ₋₂, wᵢ₋₁)
   ```

### 2.2 最大似然估计

**训练**：计数

```text
P_MLE(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁) = 
    count(wᵢ₋ₙ₊₁, ..., wᵢ) / count(wᵢ₋ₙ₊₁, ..., wᵢ₋₁)
```

**例子**（Bigram）：

训练数据：

```text
"I am Sam. Sam I am. I do not like green eggs and ham."
```

学到的概率：

```text
P(Sam | I) = 2/3
P(am | I) = 1/3
P(I | Sam) = 1/2
...
```

### 2.3 稀疏性问题

**挑战**：许多 n-gram 从未出现

**问题**：

```text
P(wᵢ | context) = 0  如果 (context, wᵢ) 未出现
```

**后果**：

- 无法处理新组合
- 整句概率为 0

### 2.4 平滑技术

**1. 加法平滑 (Laplace Smoothing)**：

```text
P(wᵢ | wᵢ₋₁) = (count(wᵢ₋₁, wᵢ) + 1) / (count(wᵢ₋₁) + |V|)
```

**2. Good-Turing 平滑**：

基于频率的频率重新分配概率质量

**3. Kneser-Ney 平滑**：

```text
P_KN(wᵢ | wᵢ₋₁) = max(count(wᵢ₋₁, wᵢ) - δ, 0) / count(wᵢ₋₁) 
                  + λ(wᵢ₋₁) P_continuation(wᵢ)
```

其中 P_continuation 基于 wᵢ 出现在多少种上下文中

**最佳实践**：Modified Kneser-Ney 是经典 n-gram 的最佳平滑方法

### 2.5 回退与插值

**回退 (Backoff)**：

```text
P(wᵢ | wᵢ₋₂, wᵢ₋₁) = {
  P_trigram(wᵢ | wᵢ₋₂, wᵢ₋₁)  如果 trigram 出现过
  α · P_bigram(wᵢ | wᵢ₋₁)      否则回退到 bigram
}
```

**插值 (Interpolation)**：

```text
P(wᵢ | wᵢ₋₂, wᵢ₋₁) = 
    λ₃ P_trigram(wᵢ | wᵢ₋₂, wᵢ₋₁) +
    λ₂ P_bigram(wᵢ | wᵢ₋₁) +
    λ₁ P_unigram(wᵢ)
```

其中 λ₁ + λ₂ + λ₃ = 1

**优势**：结合不同阶的信息

## 3. n-gram 的局限性 | Limitations of n-grams

### 3.1 上下文窗口受限

**问题**：

```text
"The computer which I had just put into the machine room on the fifth floor crashed."
```

- Trigram 只看 "fifth floor"
- 无法捕捉 "computer" 和 "crashed" 的长程依赖

### 3.2 泛化能力差

**相似句子无法泛化**：

```text
"The cat sat on the mat"  → 高概率
"The dog sat on the mat"  → 低概率（如果未见过）
```

**原因**：把每个词当做独立符号，不理解语义相似性

### 3.3 参数爆炸

**参数数量**：

```text
n-gram 模型参数：|V|^n
```

**例子**：

- V = 100,000
- Trigram：10^15 个参数
- 不可行

### 3.4 数据稀疏

**Zipf 定律**：

```text
词频 ∝ 1 / rank
```

**结果**：

- 大部分词很少出现
- 大部分 n-gram 从未出现
- 平滑只是部分解决

## 4. 神经语言模型的动机 | Motivation for Neural Language Models

### 4.1 分布式表示

**核心思想**：

> 词嵌入到连续向量空间，相似词有相似表示

**Word2Vec (Mikolov et al., 2013)**：

```text
"cat" → [0.2, -0.5, 0.8, ...]
"dog" → [0.3, -0.4, 0.7, ...]  （接近！）
```

**优势**：

- 自动泛化到相似词
- 参数共享

### 4.2 前馈神经语言模型

**Bengio et al. (2003)**：

```text
P(wᵢ | wᵢ₋ₙ₊₁, ..., wᵢ₋₁) = softmax(Wₕ h + b)

其中：
h = tanh(Wₑ [e(wᵢ₋ₙ₊₁); ...; e(wᵢ₋₁)] + bₕ)
e(w) 是词 w 的嵌入
```

**架构**：

```text
输入词 → 嵌入层 → 隐藏层 → Softmax → 概率分布
```

**参数数量**：

```text
嵌入：|V| × d
隐藏：(n-1)d × h + h × |V|
```

远小于 |V|^n

### 4.3 循环神经语言模型

**RNN-LM (Mikolov et al., 2010)**：

```text
hₜ = σ(Wₕ hₜ₋₁ + Wₓ e(wₜ) + bₕ)
P(wₜ₊₁ | w₁, ..., wₜ) = softmax(Wₒ hₜ + bₒ)
```

**优势**：

- 理论上无限上下文
- 参数不依赖于历史长度

**挑战**：

- 梯度消失/爆炸
- 长程依赖困难
- 训练慢（顺序）

### 4.4 LSTM 语言模型

**解决长程依赖**：

```text
使用 LSTM 单元代替简单 RNN
```

**成就**：

- 2015年前的 state-of-the-art
- 困惑度大幅下降

## 5. 语言建模的信息论视角 | Information-Theoretic View

### 5.1 熵与语言

**语言的熵**：

```text
H(L) = lim_{n→∞} -1/n ∑_{w₁,...,wₙ} P(w₁, ..., wₙ) log P(w₁, ..., wₙ)
```

**Shannon (1951) 估计**：

- 英语熵约 1.0 - 1.5 bits/字符
- 或约 10 bits/词

### 5.2 交叉熵与模型质量

**交叉熵**：

```text
H(P, Q) = -∑ P(x) log Q(x)
```

其中：

- P：真实分布
- Q：模型分布

**关系**：

```text
H(P, Q) ≥ H(P)
```

等号成立当且仅当 P = Q

**模型评估**：

```text
H(测试数据, 模型) 越接近 H(语言)，模型越好
```

### 5.3 条件熵

**条件熵**：

```text
H(Wₜ | W₁, ..., Wₜ₋₁) = -∑ P(w₁, ..., wₜ) log P(wₜ | w₁, ..., wₜ₋₁)
```

**语言模型目标**：最小化条件熵

## 6. 经典应用 | Classical Applications

### 6.1 语音识别

**声学模型 + 语言模型**：

```text
argmax_W P(W | A) = argmax_W P(A | W) P(W)
```

其中：

- A：声学特征
- W：词序列
- P(W)：语言模型

### 6.2 机器翻译

**噪声信道模型**：

```text
argmax_T P(T | S) = argmax_T P(S | T) P(T)
```

其中：

- S：源语言
- T：目标语言
- P(T)：目标语言模型

### 6.3 拼写纠错

**候选排序**：

```text
"teh" → {"the", "tea", "ten"}
选择 argmax P(候选 | 上下文)
```

### 6.4 文本生成

**采样**：

```text
w₁ ~ P(w)
w₂ ~ P(w | w₁)
w₃ ~ P(w | w₁, w₂)
...
```

**解码策略**：

- 贪心：argmax P(w | 前文)
- Beam search：保持 top-k
- 采样：随机采样

## 7. 性能对比 | Performance Comparison

### 7.1 Penn Treebank 困惑度

**数据集**：Penn Treebank，词汇量约1万

| 模型 | 测试困惑度 | 年份 |
|------|-----------|------|
| **5-gram + KN平滑** | ~141 | 经典 |
| **前馈神经LM** | ~137 | 2003 |
| **RNN-LM** | ~123 | 2010 |
| **LSTM-LM** | ~78 | 2014 |
| **Transformer-LM** | ~56 | 2017 |
| **GPT-2** | ~35 | 2019 |

### 7.2 趋势

**观察**：

1. 神经模型显著优于 n-gram
2. LSTM 显著优于简单 RNN
3. Transformer 成为新标准
4. 规模（数据+模型）持续提升性能

## 8. 统计语言模型的遗产 | Legacy of Statistical LMs

### 8.1 仍在使用的场景

**1. 低资源场景**：

- n-gram 快速、简单
- 小数据集上可能更好

**2. 实时系统**：

- 推理速度快
- 内存占用小

**3. 基线模型**：

- 评估神经模型改进

### 8.2 核心概念的传承

**1. 困惑度**：

- 仍是主要评估指标

**2. 平滑思想**：

- 演变为正则化、dropout

**3. 回退/插值**：

- 演变为集成、多任务学习

**4. 上下文依赖**：

- 演变为注意力机制

## 9. 权威参考文献 | Authoritative References

### Wikipedia 条目

1. [Language model](https://en.wikipedia.org/wiki/Language_model)
2. [N-gram](https://en.wikipedia.org/wiki/N-gram)
3. [Perplexity](https://en.wikipedia.org/wiki/Perplexity)
4. [Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)
5. [Word embedding](https://en.wikipedia.org/wiki/Word_embedding)

### 学术论文

1. **Shannon, C. E. (1951)**. "Prediction and entropy of printed English". *Bell System Technical Journal*.
   - 语言熵的开创性工作

2. **Bengio, Y., et al. (2003)**. "A neural probabilistic language model". *JMLR*.
   - 神经语言模型的奠基之作

3. **Mikolov, T., et al. (2010)**. "Recurrent neural network based language model". *Interspeech*.
   - RNN 语言模型

4. **Mikolov, T., et al. (2013)**. "Efficient estimation of word representations in vector space". *ICLR*.
   - Word2Vec

5. **Sundermeyer, M., et al. (2012)**. "LSTM neural networks for language modeling". *Interspeech*.
   - LSTM 语言模型

### 标准教材

1. **Jurafsky, D., & Martin, J. H. (2023)**. *Speech and Language Processing* (3rd ed.).
   - 第3章：n-gram 语言模型
   - 第7章：神经网络与神经语言模型

2. **Manning, C. D., & Schütze, H. (1999)**. *Foundations of Statistical Natural Language Processing*. MIT Press.
   - 经典统计 NLP 教材

3. **Goldberg, Y. (2017)**. *Neural Network Methods for Natural Language Processing*. Morgan & Claypool.
   - 第10章：语言建模

## 10. 关键要点总结 | Key Takeaways

1. **语言模型的本质**：
   - 词序列的概率分布
   - 预测下一个词

2. **n-gram 模型**：
   - 马尔可夫假设
   - 简单、快速、可解释
   - 但上下文受限、泛化差

3. **平滑技术**：
   - 处理稀疏性
   - Kneser-Ney 最佳

4. **神经语言模型**：
   - 分布式表示
   - 自动泛化
   - 更长上下文

5. **困惑度**：
   - 标准评估指标
   - 越低越好

6. **演进趋势**：
   - n-gram → 神经 → Transformer
   - 性能持续提升

7. **理论基础**：
   - 信息论
   - 熵、交叉熵
   - 最小化不确定性

8. **历史意义**：
   - 为现代大语言模型奠定基础
   - 核心概念仍然重要

---

**下一步阅读**：

- [03.2 神经语言模型](03.2_Neural_Language_Models.md)
- [03.3 Transformer大语言模型理论](03.3_Transformer_LLM_Theory.md)
- [03.4 Token生成机制](03.4_Token_Generation_Mechanisms.md)
