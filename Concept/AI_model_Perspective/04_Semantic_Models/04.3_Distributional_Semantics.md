# 分布式语义学（Distributional Semantics）

## 目录 | Table of Contents

- [分布式语义学（Distributional Semantics）](#分布式语义学distributional-semantics)
- [目录](#目录)
- [引言](#引言)
  - [核心思想](#核心思想)
  - [与传统语义学的对比](#与传统语义学的对比)
- [分布假设：理论基石](#分布假设理论基石)
  - [1. Firth的原初表述（1957）](#1-firth的原初表述1957)
  - [2. Harris的分布假设（1954）](#2-harris的分布假设1954)
  - [3. Wittgenstein的使用论（1953）](#3-wittgenstein的使用论1953)
  - [4. 现代形式化](#4-现代形式化)
- [分布式语义的历史发展](#分布式语义的历史发展)
  - [1. 早期：向量空间模型（1970s-1990s）](#1-早期向量空间模型1970s-1990s)
    - [Salton的向量空间模型（1975）](#salton的向量空间模型1975)
    - [TF-IDF（Term Frequency-Inverse Document Frequency）](#tf-idfterm-frequency-inverse-document-frequency)
  - [2. 中期：潜在语义分析（1990s）](#2-中期潜在语义分析1990s)
    - [LSA（Latent Semantic Analysis, 1990）](#lsalatent-semantic-analysis-1990)
  - [3. 现代：神经词嵌入（2010s）](#3-现代神经词嵌入2010s)
    - [Word2Vec（2013）](#word2vec2013)
    - [GloVe（2014）](#glove2014)
    - [上下文化表示（2018+）](#上下文化表示2018)
- [分布式语义的数学形式化](#分布式语义的数学形式化)
  - [1. 上下文定义](#1-上下文定义)
    - [固定窗口上下文](#固定窗口上下文)
    - [依存句法上下文](#依存句法上下文)
  - [2. 共现矩阵（Co-occurrence Matrix）](#2-共现矩阵co-occurrence-matrix)
    - [原始计数的问题](#原始计数的问题)
  - [3. 加权方案](#3-加权方案)
    - [点互信息（Pointwise Mutual Information, PMI）](#点互信息pointwise-mutual-information-pmi)
    - [正点互信息（Positive PMI, PPMI）](#正点互信息positive-pmi-ppmi)
  - [4. 降维方法](#4-降维方法)
    - [奇异值分解（SVD）](#奇异值分解svd)
    - [非负矩阵分解（NMF）](#非负矩阵分解nmf)
- [从共现统计到语义表示](#从共现统计到语义表示)
  - [1. Word2Vec的隐含矩阵分解](#1-word2vec的隐含矩阵分解)
  - [2. GloVe的显式矩阵分解](#2-glove的显式矩阵分解)
  - [3. 统一视角](#3-统一视角)
- [分布式语义的心理学基础](#分布式语义的心理学基础)
  - [1. 联结主义与神经网络](#1-联结主义与神经网络)
  - [2. 原型理论（Prototype Theory）](#2-原型理论prototype-theory)
  - [3. 概念空间（Conceptual Spaces）](#3-概念空间conceptual-spaces)
- [分布式语义 vs 形式语义](#分布式语义-vs-形式语义)
  - [对比](#对比)
  - [互补性](#互补性)
- [分布式语义的局限性](#分布式语义的局限性)
  - [1. 反事实问题（Grounding Problem）](#1-反事实问题grounding-problem)
  - [2. 组合性问题（Compositionality Problem）](#2-组合性问题compositionality-problem)
  - [3. 逻辑推理问题（Logical Reasoning Problem）](#3-逻辑推理问题logical-reasoning-problem)
  - [4. 偏见放大问题（Bias Amplification）](#4-偏见放大问题bias-amplification)
- [总结](#总结)
  - [核心要点](#核心要点)
  - [哲学反思](#哲学反思)
  - [未来方向](#未来方向)
- [参考文献](#参考文献)
  - [基础理论](#基础理论)
  - [历史文献](#历史文献)
  - [经典方法](#经典方法)
  - [现代方法](#现代方法)
  - [理论分析](#理论分析)
  - [心理学基础](#心理学基础)
  - [哲学与批评](#哲学与批评)

---

## 目录

- [引言](#引言)
- [分布假设：理论基石](#分布假设理论基石)
- [分布式语义的历史发展](#分布式语义的历史发展)
- [分布式语义的数学形式化](#分布式语义的数学形式化)
- [从共现统计到语义表示](#从共现统计到语义表示)
- [分布式语义的心理学基础](#分布式语义的心理学基础)
- [分布式语义 vs 形式语义](#分布式语义-vs-形式语义)
- [分布式语义的局限性](#分布式语义的局限性)
- [总结](#总结)
- [参考文献](#参考文献)

---

## 引言

**分布式语义学**（Distributional Semantics）是现代自然语言处理和AI的理论基础，它基于一个简单而深刻的思想：

> **"You shall know a word by the company it keeps."**
>
> **"词的意义由其所处的语境决定。"**
>
> — J. R. Firth (1957)

### 核心思想

**分布假设**（Distributional Hypothesis）认为：

> **在相似上下文中出现的词具有相似的意义。**

形式化：

```text
Context(w₁) ≈ Context(w₂)  ⇒  Meaning(w₁) ≈ Meaning(w₂)
```

### 与传统语义学的对比

| 维度 | 形式语义学 | 分布式语义学 | 参考文献 |
|------|-----------|-------------|----------|
| **意义来源** | 逻辑公式、真值条件 | 语言使用的统计模式 | [Wittgenstein, 1953](https://en.wikipedia.org/wiki/Philosophical_Investigations) |
| **表示方式** | 符号、谓词逻辑 | 向量、矩阵 | [Turney & Pantel, 2010](https://www.jair.org/index.php/jair/article/view/10640) |
| **学习方式** | 人工定义 | 从数据中自动学习 | [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) |
| **哲学基础** | 指称论（Reference Theory） | 使用论（Use Theory） | [Wikipedia: Meaning (philosophy of language)](https://en.wikipedia.org/wiki/Meaning_(philosophy_of_language)) |

**参考文献**：

- [Wikipedia: Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)
- [Turney & Pantel, 2010](https://www.jair.org/index.php/jair/article/view/10640) - From Frequency to Meaning: Vector Space Models of Semantics

---

## 分布假设：理论基石

### 1. Firth的原初表述（1957）

**J. R. Firth** 在1957年提出：

> **"You shall know a word by the company it keeps."**

**含义**：

- 词的意义不是内在的、固定的
- 词的意义来自于它的**分布**（在语料库中的使用模式）

**例子**：

```text
"cat" 常出现在以下上下文中：
  - "I have a ___."
  - "The ___ is sleeping."
  - "Feed the ___."
  - "___ and dog"

"dog" 也常出现在类似上下文中
⇒ "cat" 和 "dog" 意义相近
```

**参考文献**：

- [Firth, 1957](https://en.wikipedia.org/wiki/Distributional_semantics) - A Synopsis of Linguistic Theory 1930-1955

### 2. Harris的分布假设（1954）

**Zellig Harris** 更早提出了类似思想：

> **"Difference in meaning correlates with difference in distribution."**
>
> **"意义的差异对应于分布的差异。"**

**数学直觉**：

```text
Meaning : Words → SemanticSpace
Distribution : Words → ContextSpace

分布假设：Meaning ∝ Distribution
```

**参考文献**：

- [Harris, 1954](https://www.jstor.org/stable/411805) - Distributional Structure

### 3. Wittgenstein的使用论（1953）

**Ludwig Wittgenstein** 在《哲学研究》中提出：

> **"The meaning of a word is its use in the language."**
>
> **"词的意义就是它在语言中的使用。"**

**哲学基础**：

- 拒绝**指称论**（词的意义=它所指的对象）
- 提出**使用论**（词的意义=它的使用方式）

**与分布假设的联系**：

```text
使用（Use） → 分布（Distribution） → 向量（Vector）
```

**参考文献**：

- [Wikipedia: Philosophical Investigations](https://en.wikipedia.org/wiki/Philosophical_Investigations)
- [Wikipedia: Use Theory](https://en.wikipedia.org/wiki/Use_theory)

### 4. 现代形式化

**定义（分布假设）**：

设：

- Σ：词汇表
- Context(w)：词 w 的上下文分布

则分布假设断言：

```text
∀w₁, w₂ ∈ Σ : d_context(Context(w₁), Context(w₂)) ≈ d_semantic(Meaning(w₁), Meaning(w₂))
```

其中：

- d_context：上下文分布的距离度量
- d_semantic：语义距离度量

**参考文献**：

- [Lenci, 2018](https://www.annualreviews.org/doi/10.1146/annurev-linguistics-030514-125254) - Distributional Models of Word Meaning

---

## 分布式语义的历史发展

### 1. 早期：向量空间模型（1970s-1990s）

#### Salton的向量空间模型（1975）

**应用于信息检索**：

- 文档表示为词的向量
- 查询-文档匹配用余弦相似度

```text
doc = [tf₁, tf₂, ..., tf|V|]  （词频向量）
```

**参考文献**：

- [Wikipedia: Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)
- [Salton et al., 1975](https://dl.acm.org/doi/10.1145/361219.361220) - A Vector Space Model for Automatic Indexing

#### TF-IDF（Term Frequency-Inverse Document Frequency）

**公式**：

```text
TF-IDF(w, d) = TF(w, d) × IDF(w)

TF(w, d) = count(w in d) / |d|
IDF(w) = log(N / DF(w))
```

其中：

- N：文档总数
- DF(w)：包含词 w 的文档数

**直觉**：

- ✅ 在某文档中频繁出现的词重要（TF）
- ✅ 在所有文档中都出现的词不重要（IDF）

**参考文献**：

- [Wikipedia: TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

### 2. 中期：潜在语义分析（1990s）

#### LSA（Latent Semantic Analysis, 1990）

**核心思想**：

用**奇异值分解（SVD）**降维，发现潜在语义结构。

**方法**：

1. 构建词-文档矩阵 X ∈ ℝ|V|×D
2. SVD分解：X ≈ U Σ Vᵀ
3. 保留前 k 个奇异值：X_k = U_k Σ_k V_k^T
4. 词向量：U_k 的行向量

**优势**：

- ✅ 发现同义词（通过低秩近似）
- ✅ 降低维度（从 |V| 降到 k）

**局限**：

- ❌ 计算成本高（SVD是 O(mn²)）
- ❌ 难以处理新词（需要重新分解）

**参考文献**：

- [Wikipedia: Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
- [Deerwester et al., 1990](https://www.jstor.org/stable/41407138) - Indexing by Latent Semantic Analysis

### 3. 现代：神经词嵌入（2010s）

#### Word2Vec（2013）

**革命性突破**：

- 用神经网络学习词向量
- 捕捉语义类比关系

```text
vec(king) - vec(man) + vec(woman) ≈ vec(queen)
```

**两种架构**：

1. **CBOW**（Continuous Bag-of-Words）：

    ```text
    P(wₜ | wₜ₋ₙ, ..., wₜ₋₁, wₜ₊₁, ..., wₜ₊ₙ)
    ```

    从上下文预测中心词

2. **Skip-Gram**：

    ```text
    P(wₜ₋ₙ, ..., wₜ₋₁, wₜ₊₁, ..., wₜ₊ₙ | wₜ)
    ```

从中心词预测上下文

**参考文献**：

- [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Efficient Estimation of Word Representations in Vector Space
- [Wikipedia: Word2vec](https://en.wikipedia.org/wiki/Word2vec)

#### GloVe（2014）

**核心思想**：

结合**全局统计信息**（共现矩阵）和**局部预测**（Word2Vec）。

**目标函数**：

```text
J = ∑ᵢⱼ f(Xᵢⱼ) (𝒖ᵢᵀ 𝒗ⱼ + bᵢ + cⱼ - log Xᵢⱼ)²
```

**参考文献**：

- [Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf) - GloVe: Global Vectors for Word Representation

#### 上下文化表示（2018+）

**ELMo, BERT, GPT**：

- 每个词的向量**依赖于上下文**
- 解决了一词多义问题

```text
vec("bank", "river bank") ≠ vec("bank", "bank account")
```

**参考文献**：

- [Peters et al., 2018](https://arxiv.org/abs/1802.05365) - Deep Contextualized Word Representations
- [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) - BERT

---

## 分布式语义的数学形式化

### 1. 上下文定义

#### 固定窗口上下文

**定义**：

词 w 在位置 t 的**上下文**是其前后 n 个词：

```text
Context(wₜ) = {wₜ₋ₙ, ..., wₜ₋₁, wₜ₊₁, ..., wₜ₊ₙ}
```

**例子**（n=2）：

```text
句子："The cat sat on the mat"
Context(sat) = {cat, the, on, the}
```

#### 依存句法上下文

**定义**：

词 w 的上下文是与它有**句法依存关系**的词：

```text
Context(w) = {(r, w') | (w, r, w') ∈ Dependencies}
```

**例子**：

```text
"The cat sat on the mat"
Context(sat) = {(nsubj, cat), (prep, on)}
```

**优势**：

- ✅ 捕捉长距离依赖
- ✅ 更精确的语法信息

**参考文献**：

- [Padó & Lapata, 2007](https://aclanthology.org/J07-4004/) - Dependency-Based Construction of Semantic Space Models

### 2. 共现矩阵（Co-occurrence Matrix）

**定义**：

**共现矩阵** X ∈ ℝ|V|×|C| 记录词与上下文的共现次数：

```text
Xᵢⱼ = count(word i appears with context j)
```

**两种类型**：

1. **词-文档矩阵**：C = 文档集
2. **词-词矩阵**：C = 词汇表（窗口内共现）

#### 原始计数的问题

**问题**：

- ❌ 高频词主导（如"the", "is"）
- ❌ 稀疏性（大部分元素是0）
- ❌ 维度灾难（|V| × |V| 很大）

**解决方案**：加权和降维。

### 3. 加权方案

#### 点互信息（Pointwise Mutual Information, PMI）

**定义**：

```text
PMI(w, c) = log P(w, c) / (P(w) P(c))
```

**含义**：

- PMI > 0：w 和 c 正相关（共现比随机期望多）
- PMI = 0：w 和 c 独立
- PMI < 0：w 和 c 负相关

**估计**：

```text
P(w, c) ≈ count(w, c) / N
P(w) ≈ count(w) / N
P(c) ≈ count(c) / N
```

因此：

```text
PMI(w, c) = log count(w, c) × N / (count(w) × count(c))
```

#### 正点互信息（Positive PMI, PPMI）

**问题**：

- PMI对低频共现不可靠（可能是大负数）

**解决方案**：

```text
PPMI(w, c) = max(0, PMI(w, c))
```

**参考文献**：

- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)
- [Church & Hanks, 1990](https://aclanthology.org/J90-1003/) - Word Association Norms, Mutual Information, and Lexicography

### 4. 降维方法

#### 奇异值分解（SVD）

**目标**：

将高维稀疏矩阵 X 降维到 k 维稠密向量。

**方法**：

```text
X ≈ U_k Σ_k V_k^T
```

词向量：U_k 的行向量（或 U_k Σ_k 的行向量）。

#### 非负矩阵分解（NMF）

**约束**：

```text
X ≈ WH  其中 W, H ≥ 0
```

**优势**：

- ✅ 可解释性更强（非负约束）

**参考文献**：

- [Wikipedia: Non-negative Matrix Factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)

---

## 从共现统计到语义表示

### 1. Word2Vec的隐含矩阵分解

**惊人发现**（Levy & Goldberg, 2014）：

> **Word2Vec的Skip-Gram模型实际上是在隐式地分解一个PMI矩阵！**

**形式化**：

Skip-Gram的目标是最大化：

```text
∑ᵢⱼ log P(cⱼ | wᵢ)
```

经过推导，这等价于：

```text
𝒖ᵢᵀ 𝒗ⱼ ≈ PMI(wᵢ, cⱼ) - log k
```

其中 k 是负采样数。

**意义**：

- ✅ 统一了基于计数和基于预测的方法
- ✅ 揭示了神经网络方法的理论基础

**参考文献**：

- [Levy & Goldberg, 2014](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html) - Neural Word Embedding as Implicit Matrix Factorization

### 2. GloVe的显式矩阵分解

**GloVe**直接优化：

```text
J = ∑ᵢⱼ f(Xᵢⱼ) (𝒖ᵢᵀ 𝒗ⱼ + bᵢ + cⱼ - log Xᵢⱼ)²
```

**含义**：

- 词向量的内积应该接近共现次数的对数
- 使用权重函数 f(x) 削弱高频词的影响

**权重函数**：

```text
f(x) = (x / x_max)^α  if x < x_max
     = 1              otherwise
```

典型值：α=0.75, x_max=100

**参考文献**：

- [Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf) - GloVe: Global Vectors for Word Representation

### 3. 统一视角

**结论**：

> **无论是基于计数的方法（LSA, PMI），还是基于预测的方法（Word2Vec, GloVe），本质上都是在从共现统计中提取语义信息。**

**统一框架**：

```text
共现统计 → 矩阵/目标函数 → 向量表示
  ↓             ↓                ↓
 数据        中间表示           语义
```

**参考文献**：

- [Baroni et al., 2014](https://aclanthology.org/P14-1023/) - Don't Count, Predict! A Systematic Comparison

---

## 分布式语义的心理学基础

### 1. 联结主义与神经网络

**Connectionism**（连接主义）：

- 认知由简单单元的大规模并行连接产生
- 对应于**神经网络**的计算模型

**与分布式语义的联系**：

```text
神经元 ↔ 向量维度
连接权重 ↔ 词向量
激活模式 ↔ 语义表示
```

**参考文献**：

- [Wikipedia: Connectionism](https://en.wikipedia.org/wiki/Connectionism)
- [Rumelhart & McClelland, 1986](https://mitpress.mit.edu/9780262680530/parallel-distributed-processing/) - Parallel Distributed Processing

### 2. 原型理论（Prototype Theory）

**Eleanor Rosch** 的原型理论：

> **概念不是由必要充分条件定义，而是由典型实例（原型）及其相似度定义。**

**例子**：

- "鸟"的原型：知更鸟、麻雀
- 企鹅是"鸟"，但不是典型的鸟

**与分布式语义的联系**：

```text
原型 ↔ 向量空间中的聚类中心
相似度 ↔ 向量距离
```

**参考文献**：

- [Wikipedia: Prototype Theory](https://en.wikipedia.org/wiki/Prototype_theory)
- [Rosch, 1973](https://www.sciencedirect.com/science/article/pii/S0022537173800051) - Natural Categories

### 3. 概念空间（Conceptual Spaces）

**Peter Gärdenfors** 的概念空间理论：

> **概念可以表示为几何空间中的区域。**

**维度**：

- 颜色空间：色调、饱和度、亮度
- 味道空间：甜、酸、苦、咸、鲜

**与分布式语义的联系**：

- 概念空间 ≈ 语义向量空间
- 凸区域 ≈ 语义聚类

**参考文献**：

- [Wikipedia: Conceptual Spaces](https://en.wikipedia.org/wiki/Conceptual_space)
- [Gärdenfors, 2000](https://mitpress.mit.edu/9780262571999/conceptual-spaces/) - Conceptual Spaces: The Geometry of Thought

---

## 分布式语义 vs 形式语义

### 对比

| 维度 | 形式语义学（Formal Semantics） | 分布式语义学（Distributional Semantics） |
|------|-------------------------------|----------------------------------------|
| **基础** | 模型论、逻辑学 | 统计学、向量空间 |
| **意义** | 真值条件、指称 | 使用模式、分布 |
| **表示** | 逻辑公式 | 向量 |
| **组合性** | λ-演算、函数应用 | 向量运算（加、张量积） |
| **推理** | 演绎推理（Modus Ponens） | 相似度匹配 |
| **真假** | 二值（真/假） | 连续度量 |
| **优势** | 精确、可解释 | 鲁棒、可学习 |
| **劣势** | 脆弱、知识获取瓶颈 | 近似、不可解释 |

**参考文献**：

- [Wikipedia: Formal Semantics](https://en.wikipedia.org/wiki/Formal_semantics_(linguistics))
- [Boleda & Herbelot, 2016](https://www.aclweb.org/anthology/J16-3001/) - Formal Distributional Semantics: Introduction to the Special Issue

### 互补性

**现代趋势**：结合两者优势。

**例子**：

1. **神经-符号系统**（Neurosymbolic AI）：

    ```text
    符号推理 + 神经表示
    ```

2. **概率编程**：

    ```text
    逻辑程序 + 概率分布
    ```

3. **知识图谱嵌入**：

    ```text
    三元组 (head, relation, tail) → 向量表示
    ```

**参考文献**：

- [Wikipedia: Neuro-Symbolic AI](https://en.wikipedia.org/wiki/Neuro-symbolic_AI)

---

## 分布式语义的局限性

### 1. 反事实问题（Grounding Problem）

**问题**：

> **分布式语义只从语言中学习，缺乏对真实世界的"接地"（Grounding）。**

**例子**：

```text
vec(unicorn) 可以通过语言学习
但"独角兽"在真实世界中不存在
```

**Searle的中文房间论证**：

- 仅从符号操作（或分布统计）无法获得真正的"理解"

**参考文献**：

- [Wikipedia: Symbol Grounding Problem](https://en.wikipedia.org/wiki/Symbol_grounding_problem)
- [Searle, 1980](https://en.wikipedia.org/wiki/Chinese_room) - Minds, Brains, and Programs

### 2. 组合性问题（Compositionality Problem）

**问题**：

简单的向量运算（如加法）**不足以表达复杂的语义组合**。

**例子**：

```text
vec("not happy") ≠ -vec("happy")  （否定）
vec("very happy") ≠ 2 × vec("happy")  （程度）
```

**解决尝试**：

- 张量积
- 递归神经网络
- Transformer

**参考文献**：

- [Coecke et al., 2010](https://arxiv.org/abs/1003.4394) - Mathematical Foundations for a Compositional Distributional Model of Meaning

### 3. 逻辑推理问题（Logical Reasoning Problem）

**问题**：

分布式语义不支持**严格的逻辑推理**。

**例子**：

```text
前提1：所有人都会死
前提2：苏格拉底是人
结论：苏格拉底会死  （演绎推理）
```

**分布式语义的失败**：

```text
cos(vec("Socrates"), vec("mortal")) ≈ 0.6  （只是相似度，不是必然）
```

**参考文献**：

- [Marcus & Davis, 2019](https://arxiv.org/abs/1906.05833) - Rebooting AI: Building Artificial Intelligence We Can Trust

### 4. 偏见放大问题（Bias Amplification）

**问题**：

分布式语义会**编码并放大**训练数据中的社会偏见。

**例子**：

```text
vec(programmer) - vec(man) + vec(woman) ≈ vec(homemaker)
```

**原因**：

语料库反映了社会的刻板印象和不平等。

**缓解方法**：

- 去偏置算法
- 对抗训练
- 数据平衡

**参考文献**：

- [Bolukbasi et al., 2016](https://arxiv.org/abs/1607.06520) - Man is to Computer Programmer as Woman is to Homemaker?

---

## 总结

### 核心要点

1. **理论基石**：分布假设（词的意义由其上下文决定）
2. **哲学基础**：使用论（Wittgenstein）、分布结构（Harris）
3. **历史发展**：向量空间模型 → LSA → Word2Vec → 上下文化表示
4. **数学形式化**：共现矩阵、PMI、矩阵分解
5. **统一视角**：计数方法和预测方法本质相同
6. **心理学基础**：连接主义、原型理论、概念空间
7. **与形式语义的对比**：互补而非对立
8. **局限性**：接地问题、组合性、逻辑推理、偏见

### 哲学反思

> **分布式语义学揭示了一个深刻的洞察：意义不是内在的、固定的，而是关系的、涌现的。词的意义不在于它"是什么"，而在于它"如何被使用"。**

### 未来方向

1. **多模态接地**：结合视觉、听觉等感知信息
2. **组合语义**：更好的语义组合机制（如张量网络）
3. **神经-符号融合**：结合分布式和形式语义的优势
4. **去偏置**：构建更公平的语义表示

---

## 参考文献

### 基础理论

1. [Wikipedia: Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)
2. [Turney & Pantel, 2010](https://www.jair.org/index.php/jair/article/view/10640) - From Frequency to Meaning: Vector Space Models of Semantics
3. [Lenci, 2018](https://www.annualreviews.org/doi/10.1146/annurev-linguistics-030514-125254) - Distributional Models of Word Meaning

### 历史文献

1. [Firth, 1957](https://en.wikipedia.org/wiki/Distributional_semantics) - A Synopsis of Linguistic Theory 1930-1955
2. [Harris, 1954](https://www.jstor.org/stable/411805) - Distributional Structure
3. [Wittgenstein, 1953](https://en.wikipedia.org/wiki/Philosophical_Investigations) - Philosophical Investigations

### 经典方法

1. [Wikipedia: Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)
2. [Wikipedia: Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
3. [Deerwester et al., 1990](https://www.jstor.org/stable/41407138) - Indexing by Latent Semantic Analysis

### 现代方法

1. [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Efficient Estimation of Word Representations in Vector Space
2. [Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf) - GloVe: Global Vectors for Word Representation
3. [Peters et al., 2018](https://arxiv.org/abs/1802.05365) - Deep Contextualized Word Representations
4. [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) - BERT

### 理论分析

1. [Levy & Goldberg, 2014](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html) - Neural Word Embedding as Implicit Matrix Factorization
2. [Baroni et al., 2014](https://aclanthology.org/P14-1023/) - Don't Count, Predict!

### 心理学基础

1. [Wikipedia: Connectionism](https://en.wikipedia.org/wiki/Connectionism)
2. [Wikipedia: Prototype Theory](https://en.wikipedia.org/wiki/Prototype_theory)
3. [Gärdenfors, 2000](https://mitpress.mit.edu/9780262571999/conceptual-spaces/) - Conceptual Spaces

### 哲学与批评

1. [Wikipedia: Chinese Room](https://en.wikipedia.org/wiki/Chinese_room)
2. [Marcus & Davis, 2019](https://arxiv.org/abs/1906.05833) - Rebooting AI
3. [Bolukbasi et al., 2016](https://arxiv.org/abs/1607.06520) - Man is to Computer Programmer as Woman is to Homemaker?

---

*本文档系统阐述了分布式语义学的理论基础、历史发展和哲学意涵，为理解现代AI的语义表示提供了完整的理论框架。*
