# 语义相似度度量（Semantic Similarity Metrics）

## 目录 | Table of Contents

- [语义相似度度量（Semantic Similarity Metrics）](#语义相似度度量semantic-similarity-metrics)
- [目录](#目录)
- [引言](#引言)
  - [核心问题](#核心问题)
  - [应用场景](#应用场景)
- [相似度的数学基础](#相似度的数学基础)
  - [1. 相似度与距离](#1-相似度与距离)
    - [相似度函数（Similarity Function）](#相似度函数similarity-function)
    - [距离函数（Distance Function）](#距离函数distance-function)
    - [相似度与距离的转换](#相似度与距离的转换)
  - [2. 内积空间中的相似度](#2-内积空间中的相似度)
- [基于向量的相似度度量](#基于向量的相似度度量)
  - [1. 余弦相似度（Cosine Similarity）](#1-余弦相似度cosine-similarity)
  - [2. 欧几里得距离（Euclidean Distance）](#2-欧几里得距离euclidean-distance)
  - [3. 曼哈顿距离（Manhattan Distance）](#3-曼哈顿距离manhattan-distance)
  - [4. Jaccard相似度](#4-jaccard相似度)
  - [5. Pearson相关系数](#5-pearson相关系数)
- [基于概率的相似度度量](#基于概率的相似度度量)
  - [1. KL散度（Kullback-Leibler Divergence）](#1-kl散度kullback-leibler-divergence)
  - [2. JS散度（Jensen-Shannon Divergence）](#2-js散度jensen-shannon-divergence)
  - [3. Wasserstein距离（Earth Mover's Distance）](#3-wasserstein距离earth-movers-distance)
- [基于路径的相似度度量](#基于路径的相似度度量)
  - [1. WordNet简介](#1-wordnet简介)
  - [2. 路径长度（Path Length）](#2-路径长度path-length)
  - [3. Leacock-Chodorow相似度](#3-leacock-chodorow相似度)
  - [4. Wu-Palmer相似度](#4-wu-palmer相似度)
  - [5. Resnik相似度](#5-resnik相似度)
- [深度学习时代的相似度度量](#深度学习时代的相似度度量)
  - [1. 学习的相似度函数](#1-学习的相似度函数)
  - [2. 孪生网络（Siamese Networks）](#2-孪生网络siamese-networks)
  - [3. 三元组损失（Triplet Loss）](#3-三元组损失triplet-loss)
  - [4. 对比学习（Contrastive Learning）](#4-对比学习contrastive-learning)
- [句子和文档级别的相似度](#句子和文档级别的相似度)
  - [1. 平均词向量（Averaged Word Embeddings）](#1-平均词向量averaged-word-embeddings)
  - [2. Sentence-BERT](#2-sentence-bert)
  - [3. BERTScore](#3-bertscore)
  - [4. 文档相似度：TF-IDF + 余弦](#4-文档相似度tf-idf-余弦)
- [跨模态相似度度量](#跨模态相似度度量)
  - [1. 图像-文本相似度](#1-图像-文本相似度)
  - [2. 多模态相似度](#2-多模态相似度)
- [相似度度量的评估](#相似度度量的评估)
  - [1. 内在评估（Intrinsic Evaluation）](#1-内在评估intrinsic-evaluation)
    - [人类判断相关性](#人类判断相关性)
    - [类比任务](#类比任务)
  - [2. 外在评估（Extrinsic Evaluation）](#2-外在评估extrinsic-evaluation)
- [总结](#总结)
  - [核心要点](#核心要点)
  - [选择指南](#选择指南)
  - [未来方向](#未来方向)
- [参考文献](#参考文献)
  - [综述](#综述)
  - [数学基础](#数学基础)
  - [基于路径](#基于路径)
  - [深度学习](#深度学习)
  - [句子级别](#句子级别)
  - [跨模态](#跨模态)
  - [评估](#评估)

---

## 引言

**语义相似度**（Semantic Similarity）是AI和自然语言处理的核心概念，它量化了两个语义对象（词、句子、文档）的"接近程度"。

### 核心问题

1. **本体论问题**：什么是"相似"？
2. **度量论问题**：如何量化相似度？
3. **认识论问题**：相似度是客观的还是主观的？
4. **计算问题**：如何高效计算相似度？

### 应用场景

| 应用 | 任务 | 相似度用途 |
|------|------|-----------|
| **信息检索** | 搜索引擎 | 查询-文档相似度 |
| **推荐系统** | 商品推荐 | 用户-商品相似度 |
| **问答系统** | 答案匹配 | 问题-答案相似度 |
| **文本分类** | 主题识别 | 文档-类别相似度 |
| **机器翻译** | 翻译质量评估 | 译文-参考相似度 |
| **对话系统** | 意图识别 | 用户输入-意图相似度 |

**参考文献**：

- [Wikipedia: Semantic Similarity](https://en.wikipedia.org/wiki/Semantic_similarity)
- [Gomaa & Fahmy, 2013](https://arxiv.org/abs/1310.8059) - A Survey of Text Similarity Approaches

---

## 相似度的数学基础

### 1. 相似度与距离

#### 相似度函数（Similarity Function）

**定义**：

**相似度函数** sim : X × X → ℝ 满足：

1. **非负性**：sim(x, y) ≥ 0
2. **对称性**：sim(x, y) = sim(y, x)
3. **自相似最大**：sim(x, x) ≥ sim(x, y) ∀y

**归一化相似度**：

```text
sim : X × X → [0, 1]
```

其中 1 表示完全相同，0 表示完全不同。

#### 距离函数（Distance Function）

**定义**：

**距离函数**（度量）d : X × X → ℝ₊ 满足：

1. **非负性**：d(x, y) ≥ 0，且 d(x, y) = 0 ⟺ x = y
2. **对称性**：d(x, y) = d(y, x)
3. **三角不等式**：d(x, z) ≤ d(x, y) + d(y, z)

#### 相似度与距离的转换

**常见转换**：

1. **线性转换**：

    ```text
    sim(x, y) = 1 - d(x, y) / d_max
    ```

2. **指数转换**：

    ```text
    sim(x, y) = exp(-d(x, y))
    ```

3. **Gauss核**：

    ```text
    sim(x, y) = exp(-d(x, y)² / (2σ²))
    ```

**参考文献**：

- [Wikipedia: Metric Space](https://en.wikipedia.org/wiki/Metric_space)
- [Wikipedia: Similarity Measure](https://en.wikipedia.org/wiki/Similarity_measure)

### 2. 内积空间中的相似度

**希尔伯特空间**：

在内积空间中，内积 ⟨·,·⟩ 自然地定义了相似度：

```text
sim(𝒙, 𝒚) = ⟨𝒙, 𝒚⟩
```

**Cauchy-Schwarz不等式**：

```text
|⟨𝒙, 𝒚⟩| ≤ ‖𝒙‖ ‖𝒚‖
```

**推论**：

```text
-1 ≤ ⟨𝒙, 𝒚⟩ / (‖𝒙‖ ‖𝒚‖) ≤ 1
```

这就是**余弦相似度**。

**参考文献**：

- [Wikipedia: Inner Product Space](https://en.wikipedia.org/wiki/Inner_product_space)
- [Wikipedia: Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)

---

## 基于向量的相似度度量

### 1. 余弦相似度（Cosine Similarity）

**定义**：

```text
cos(𝒖, 𝒗) = ⟨𝒖, 𝒗⟩ / (‖𝒖‖ ‖𝒗‖) = ∑ᵢ uᵢvᵢ / (√∑ᵢ uᵢ² √∑ᵢ vᵢ²)
```

**几何意义**：

两个向量夹角的余弦值。

**值域**：[-1, 1]

- cos(𝒖, 𝒗) = 1：完全相同方向（最相似）
- cos(𝒖, 𝒗) = 0：正交（无关）
- cos(𝒖, 𝒗) = -1：完全相反方向（最不相似）

**优势**：

- ✅ 不受向量长度影响（只看方向）
- ✅ 适合高维稀疏向量（如文本）
- ✅ **AI中最常用的相似度度量**

**例子**：

```text
𝒖 = [1, 2, 3]
𝒗 = [2, 4, 6]
cos(𝒖, 𝒗) = (1×2 + 2×4 + 3×6) / (√14 × √56) = 28 / 28 = 1
```

𝒗 = 2𝒖，所以完全相同方向。

**参考文献**：

- [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)

### 2. 欧几里得距离（Euclidean Distance）

**定义**：

```text
d_E(𝒖, 𝒗) = ‖𝒖 - 𝒗‖₂ = √∑ᵢ (uᵢ - vᵢ)²
```

**几何意义**：

两点间的直线距离。

**转换为相似度**：

```text
sim(𝒖, 𝒗) = 1 / (1 + d_E(𝒖, 𝒗))
```

或

```text
sim(𝒖, 𝒗) = exp(-d_E(𝒖, 𝒗)²)
```

**特点**：

- ✅ 直观
- ❌ 受向量长度影响
- ❌ 高维空间中区分度下降（维度灾难）

**参考文献**：

- [Wikipedia: Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)

### 3. 曼哈顿距离（Manhattan Distance）

**定义**：

```text
d_M(𝒖, 𝒗) = ‖𝒖 - 𝒗‖₁ = ∑ᵢ |uᵢ - vᵢ|
```

**几何意义**：

沿坐标轴的距离之和（如城市街区）。

**特点**：

- ✅ 计算简单
- ✅ 对异常值更鲁棒（相比欧几里得距离）

**参考文献**：

- [Wikipedia: Taxicab Geometry](https://en.wikipedia.org/wiki/Taxicab_geometry)

### 4. Jaccard相似度

**定义**（对于集合）：

```text
J(A, B) = |A ∩ B| / |A ∪ B|
```

**推广到向量**（二值向量）：

```text
J(𝒖, 𝒗) = ∑ᵢ min(uᵢ, vᵢ) / ∑ᵢ max(uᵢ, vᵢ)
```

**应用**：

- 文档去重
- 协同过滤

**参考文献**：

- [Wikipedia: Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)

### 5. Pearson相关系数

**定义**：

```text
r(𝒖, 𝒗) = cov(𝒖, 𝒗) / (σ_𝒖 σ_𝒗)
```

其中：

```text
cov(𝒖, 𝒗) = E[(𝒖 - μ_𝒖)(𝒗 - μ_𝒗)]
```

**值域**：[-1, 1]

**特点**：

- ✅ 捕捉线性相关性
- ✅ 不受平移和缩放影响

**与余弦相似度的关系**：

对中心化向量，Pearson相关系数等于余弦相似度。

**参考文献**：

- [Wikipedia: Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)

---

## 基于概率的相似度度量

### 1. KL散度（Kullback-Leibler Divergence）

**定义**：

```text
D_KL(P ‖ Q) = ∑ᵢ P(i) log(P(i) / Q(i))
```

或（连续情况）：

```text
D_KL(P ‖ Q) = ∫ p(x) log(p(x) / q(x)) dx
```

**性质**：

- ✅ 非负：D_KL(P ‖ Q) ≥ 0
- ✅ D_KL(P ‖ Q) = 0 ⟺ P = Q
- ❌ 不对称：D_KL(P ‖ Q) ≠ D_KL(Q ‖ P)
- ❌ 不满足三角不等式

**含义**：

P相对于Q的"额外信息量"。

**应用**：

- 语言模型评估（困惑度）
- 变分推断

**参考文献**：

- [Wikipedia: Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)

### 2. JS散度（Jensen-Shannon Divergence）

**定义**：

```text
D_JS(P, Q) = (1/2) D_KL(P ‖ M) + (1/2) D_KL(Q ‖ M)
```

其中 M = (P + Q) / 2。

**性质**：

- ✅ 对称：D_JS(P, Q) = D_JS(Q, P)
- ✅ 有界：0 ≤ D_JS(P, Q) ≤ log 2
- ✅ 满足度量条件的平方根：√D_JS(P, Q) 是度量

**转换为相似度**：

```text
sim(P, Q) = 1 - √D_JS(P, Q) / √log 2
```

**参考文献**：

- [Wikipedia: Jensen-Shannon Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)

### 3. Wasserstein距离（Earth Mover's Distance）

**定义**：

```text
W_p(P, Q) = (inf_{γ ∈ Π(P,Q)} ∫∫ d(x,y)^p dγ(x,y))^{1/p}
```

**直觉**：

把分布P"搬运"到分布Q所需的最小"工作量"。

**优势**：

- ✅ 对分布的支撑集不要求重叠
- ✅ 提供了平滑的距离度量

**应用**：

- 生成模型（WGAN）
- 文档相似度

**参考文献**：

- [Wikipedia: Wasserstein Metric](https://en.wikipedia.org/wiki/Wasserstein_metric)
- [Arjovsky et al., 2017](https://arxiv.org/abs/1701.07875) - Wasserstein GAN

---

## 基于路径的相似度度量

这些方法基于**知识图谱**或**语义网络**（如WordNet）中的路径长度。

### 1. WordNet简介

**WordNet**：

- 英语词汇的语义网络
- 组织为**同义词集合**（Synsets）
- 通过**语义关系**（is-a, part-of等）连接

**参考文献**：

- [Wikipedia: WordNet](https://en.wikipedia.org/wiki/WordNet)
- [Miller, 1995](https://dl.acm.org/doi/10.1145/219717.219748) - WordNet: A Lexical Database for English

### 2. 路径长度（Path Length）

**定义**：

```text
sim_path(c₁, c₂) = 1 / (1 + len(shortest_path(c₁, c₂)))
```

**例子**：

```text
WordNet层次：
  entity
    ├── living_thing
    │     ├── animal
    │     │     ├── mammal
    │     │     │     ├── cat
    │     │     │     └── dog
    │     └── plant

path(cat, dog) = 2  （经过mammal）
sim_path(cat, dog) = 1 / (1 + 2) = 0.33
```

**参考文献**：

- [Rada et al., 1989](https://ieeexplore.ieee.org/document/21465) - Development and Application of a Metric on Semantic Nets

### 3. Leacock-Chodorow相似度

**定义**：

```text
sim_LC(c₁, c₂) = -log(len(shortest_path(c₁, c₂)) / (2 × depth(taxonomy)))
```

**归一化**：

考虑分类层次的深度。

**参考文献**：

- [Leacock & Chodorow, 1998](https://aclanthology.org/J98-1006/) - Combining Local Context and WordNet Similarity

### 4. Wu-Palmer相似度

**定义**：

```text
sim_WP(c₁, c₂) = 2 × depth(LCS(c₁, c₂)) / (depth(c₁) + depth(c₂))
```

其中 LCS = Least Common Subsumer（最低公共祖先）。

**例子**：

```text
depth(cat) = 5
depth(dog) = 5
depth(LCS(cat, dog)) = depth(mammal) = 4

sim_WP(cat, dog) = 2 × 4 / (5 + 5) = 0.8
```

**参考文献**：

- [Wu & Palmer, 1994](https://aclanthology.org/P94-1019/) - Verbs Semantics and Lexical Selection

### 5. Resnik相似度

**基于信息内容**（Information Content）：

**定义**：

```text
IC(c) = -log P(c)
```

其中 P(c) 是概念 c 在语料库中出现的概率。

**Resnik相似度**：

```text
sim_Res(c₁, c₂) = IC(LCS(c₁, c₂))
```

**直觉**：

两个概念的共同祖先越具体（概率越小），它们越相似。

**参考文献**：

- [Resnik, 1995](https://arxiv.org/abs/cmp-lg/9511007) - Using Information Content to Evaluate Semantic Similarity

---

## 深度学习时代的相似度度量

### 1. 学习的相似度函数

**核心思想**：

用神经网络**学习**相似度函数，而非手工定义。

**一般形式**：

```text
sim_θ(x, y) = f_θ(Enc(x), Enc(y))
```

其中：

- Enc：编码器（如BERT）
- f_θ：相似度计算（如MLP）
- θ：可学习参数

### 2. 孪生网络（Siamese Networks）

**架构**：

```text
x₁ → Enc → 𝒗₁ ↘
                 cos(𝒗₁, 𝒗₂) → sim
x₂ → Enc → 𝒗₂ ↗
```

**共享编码器**：两个输入使用同一个编码器。

**训练目标**（对比损失）：

```text
L = ∑ [ y × d(𝒗₁, 𝒗₂)² + (1-y) × max(0, m - d(𝒗₁, 𝒗₂))² ]
```

其中：

- y = 1：相似对
- y = 0：不相似对
- m：边界

**参考文献**：

- [Wikipedia: Siamese Neural Network](https://en.wikipedia.org/wiki/Siamese_neural_network)
- [Bromley et al., 1993](https://proceedings.neurips.cc/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html) - Signature Verification using a Siamese Time Delay Neural Network

### 3. 三元组损失（Triplet Loss）

**训练数据**：

```text
(anchor, positive, negative)
```

- anchor：锚点
- positive：与anchor相似
- negative：与anchor不相似

**目标**：

```text
d(anchor, positive) < d(anchor, negative)
```

**损失函数**：

```text
L = ∑ max(0, d(a, p) - d(a, n) + margin)
```

**参考文献**：

- [Schroff et al., 2015](https://arxiv.org/abs/1503.03832) - FaceNet: A Unified Embedding for Face Recognition

### 4. 对比学习（Contrastive Learning）

**核心思想**：

拉近**正样本对**，推开**负样本对**。

**InfoNCE损失**：

```text
L = -log(exp(sim(x, x⁺) / τ) / ∑_{x⁻} exp(sim(x, x⁻) / τ))
```

其中：

- x⁺：正样本
- x⁻：负样本
- τ：温度参数

**应用**：

- SimCLR（视觉）
- SimCSE（文本）

**参考文献**：

- [Chen et al., 2020](https://arxiv.org/abs/2002.05709) - A Simple Framework for Contrastive Learning of Visual Representations
- [Gao et al., 2021](https://arxiv.org/abs/2104.08821) - SimCSE: Simple Contrastive Learning of Sentence Embeddings

---

## 句子和文档级别的相似度

### 1. 平均词向量（Averaged Word Embeddings）

**最简单方法**：

```text
vec(sentence) = (1/n) ∑ᵢ vec(wᵢ)
```

**改进：加权平均**（如TF-IDF权重）

```text
vec(sentence) = ∑ᵢ weight(wᵢ) × vec(wᵢ)
```

**优势**：

- ✅ 简单、快速

**劣势**：

- ❌ 丢失词序信息
- ❌ 忽略句法结构

### 2. Sentence-BERT

**核心思想**：

用孪生BERT网络学习句子嵌入。

**架构**：

```text
sentence → BERT → [CLS] token → 𝒗
```

**训练**：

使用自然语言推理（NLI）数据集：

```text
(premise, hypothesis, label)
label ∈ {entailment, contradiction, neutral}
```

**优势**：

- ✅ 高质量句子嵌入
- ✅ 快速推理（预计算嵌入后只需计算余弦相似度）

**参考文献**：

- [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) - Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

### 3. BERTScore

**核心思想**：

用BERT的上下文嵌入计算token级别的相似度，然后聚合。

**步骤**：

1. 对两个句子分别用BERT编码
2. 计算每对token的余弦相似度
3. 用贪心匹配或最优匹配聚合

**公式**：

```text
Precision = (1/|𝒙|) ∑ᵢ max_j cos(𝒙ᵢ, 𝒚ⱼ)
Recall = (1/|𝒚|) ∑ⱼ max_i cos(𝒙ᵢ, 𝒚ⱼ)
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**优势**：

- ✅ 考虑上下文
- ✅ 与人类判断高度相关

**参考文献**：

- [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) - BERTScore: Evaluating Text Generation with BERT

### 4. 文档相似度：TF-IDF + 余弦

**经典方法**：

1. 将文档表示为TF-IDF向量
2. 计算余弦相似度

```text
𝒅₁, 𝒅₂ ∈ ℝ|V|
sim(𝒅₁, 𝒅₂) = cos(𝒅₁, 𝒅₂)
```

**优势**：

- ✅ 简单、高效
- ✅ 可扩展到大规模文档集

**参考文献**：

- [Wikipedia: TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

---

## 跨模态相似度度量

### 1. 图像-文本相似度

**目标**：

度量图像和文本描述的相似度。

**方法：CLIP（Contrastive Language-Image Pre-training）**-

**架构**：

```text
Image → Image Encoder → 𝒗_img ↘
                                 cos(𝒗_img, 𝒗_txt)
Text → Text Encoder → 𝒗_txt ↗
```

**训练**：

对比学习，匹配的图像-文本对相似度高。

**应用**：

- 图像检索
- 图像生成（如DALL-E）

**参考文献**：

- [Radford et al., 2021](https://arxiv.org/abs/2103.00020) - Learning Transferable Visual Models From Natural Language Supervision

### 2. 多模态相似度

**一般框架**：

将不同模态映射到**统一的语义空间**：

```text
Text → Enc_text → 𝒛 ∈ ℝᵈ
Image → Enc_img → 𝒛 ∈ ℝᵈ
Audio → Enc_audio → 𝒛 ∈ ℝᵈ

sim(x₁, x₂) = cos(𝒛₁, 𝒛₂)
```

**参考文献**：

- [Wikipedia: Multimodal Learning](https://en.wikipedia.org/wiki/Multimodal_learning)

---

## 相似度度量的评估

### 1. 内在评估（Intrinsic Evaluation）

#### 人类判断相关性

**数据集**：

- **SimLex-999**：999对词，带人类相似度评分
- **WordSim-353**：353对词，带人类相似度评分

**评估指标**：

```text
Spearman相关系数 = 模型相似度排序 与 人类相似度排序的相关性
```

**参考文献**：

- [Hill et al., 2015](https://arxiv.org/abs/1408.3456) - SimLex-999: Evaluating Semantic Models

#### 类比任务

**例子**：

```text
king - man + woman ≈ ?
```

**评估**：

模型能否正确回答"queen"。

**参考文献**：

- [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Linguistic Regularities in Continuous Space

### 2. 外在评估（Extrinsic Evaluation）

**下游任务表现**：

- 文本分类
- 信息检索
- 问答系统
- 机器翻译

**原则**：

> **好的相似度度量应该提升下游任务的性能。**

---

## 总结

### 核心要点

1. **数学基础**：相似度 vs 距离、内积空间
2. **基于向量**：余弦相似度（最常用）、欧几里得距离、Jaccard
3. **基于概率**：KL散度、JS散度、Wasserstein距离
4. **基于路径**：WordNet、路径长度、信息内容
5. **深度学习**：孪生网络、三元组损失、对比学习
6. **句子级别**：平均词向量、Sentence-BERT、BERTScore
7. **跨模态**：CLIP、多模态统一空间
8. **评估**：人类判断相关性、类比任务、下游任务

### 选择指南

| 场景 | 推荐度量 | 原因 |
|------|---------|------|
| **词级别** | 余弦相似度（Word2Vec/GloVe） | 标准方法 |
| **句子级别** | Sentence-BERT | 高质量、快速 |
| **语义相似度（上下文敏感）** | BERT嵌入 + 余弦 | 考虑上下文 |
| **文档级别** | TF-IDF + 余弦 | 简单高效 |
| **概念相似度** | WordNet路径方法 | 利用知识图谱 |
| **生成文本评估** | BERTScore | 与人类判断相关 |
| **跨模态** | CLIP | 图像-文本对齐 |

### 未来方向

1. **可解释相似度**：理解为什么两个对象相似
2. **多粒度相似度**：词、短语、句子、文档的统一框架
3. **个性化相似度**：根据用户偏好调整相似度度量
4. **对抗鲁棒性**：相似度度量对对抗攻击的鲁棒性

---

## 参考文献

### 综述

1. [Wikipedia: Semantic Similarity](https://en.wikipedia.org/wiki/Semantic_similarity)
2. [Gomaa & Fahmy, 2013](https://arxiv.org/abs/1310.8059) - A Survey of Text Similarity Approaches

### 数学基础

1. [Wikipedia: Metric Space](https://en.wikipedia.org/wiki/Metric_space)
2. [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
3. [Wikipedia: Inner Product Space](https://en.wikipedia.org/wiki/Inner_product_space)

### 基于路径

1. [Wikipedia: WordNet](https://en.wikipedia.org/wiki/WordNet)
2. [Miller, 1995](https://dl.acm.org/doi/10.1145/219717.219748) - WordNet: A Lexical Database for English
3. [Resnik, 1995](https://arxiv.org/abs/cmp-lg/9511007) - Using Information Content to Evaluate Semantic Similarity

### 深度学习

1. [Schroff et al., 2015](https://arxiv.org/abs/1503.03832) - FaceNet: A Unified Embedding
2. [Chen et al., 2020](https://arxiv.org/abs/2002.05709) - SimCLR
3. [Gao et al., 2021](https://arxiv.org/abs/2104.08821) - SimCSE

### 句子级别

1. [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) - Sentence-BERT
2. [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) - BERTScore

### 跨模态

1. [Radford et al., 2021](https://arxiv.org/abs/2103.00020) - CLIP

### 评估

1. [Hill et al., 2015](https://arxiv.org/abs/1408.3456) - SimLex-999

---

*本文档全面梳理了语义相似度度量的理论基础、具体方法和评估标准，为理解和选择合适的相似度度量提供了系统指南。*
