# 语义相似度度量（Semantic Similarity Metrics）

> **文档版本**: v1.0.0  
> **最后更新**: 2025-10-27  
> **文档规模**: 929行 | 多维度语义相似度计算方法  
> **阅读建议**: 本文全面介绍各类语义相似度度量方法，是实践NLP任务的重要参考

---

## 核心概念深度分析

<details>
<summary><b>📏🔍 点击展开：语义相似度度量全景深度解析</b></summary>

本节深入剖析余弦相似度vs欧氏距离、KL vs JS vs Wasserstein、孪生网络与对比学习。

### 1️⃣ 语义相似度度量概念定义卡

**概念名称**: 语义相似度度量（Semantic Similarity Metrics）

**内涵（本质属性）**:

**🔹 核心定义**:
语义相似度度量是量化两个语义对象（词、句、文档）之间"接近程度"的数学函数，建立从语义空间到实数的映射，用于衡量意义的相似性。

$$
\text{Similarity}: \mathcal{S} \times \mathcal{S} \to \mathbb{R} \quad \text{where } \mathcal{S} = \text{语义空间}
$$

**🔹 相似度vs距离核心对比**:

| 维度 | 相似度（Similarity） | 距离（Distance） | 转换关系 |
|------|-------------------|----------------|---------|
| **值域** | [0, 1]或[-1, 1] | [0, ∞) | 互逆关系 |
| **解释** | 越大越相似 | 越小越相似 | $d = 1 - s$ |
| **性质** | 对称性 | 度量空间公理 | 可互换 |
| **示例** | 余弦相似度、Jaccard | 欧氏距离、曼哈顿距离 | 多种转换 |

**外延（范围边界）**:

| 维度 | 语义相似度包含 ✅ | 不包含 ❌ |
|------|--------------|----------|
| **方法** | 余弦、KL散度、WordNet路径 | 字符编辑距离、哈希碰撞 |
| **层次** | 词级、句级、文档级 | 字符级、音素级 |
| **模态** | 文本、图像、音频、多模态 | 纯结构数据（无语义） |

**属性维度表**:

| 维度 | 值/描述 | 说明 |
|------|---------|------|
| **经典方法** | 余弦、欧氏、KL散度 | 数学基础 |
| **神经方法** | 孪生网络、对比学习 | 端到端学习 |
| **应用** | 检索、推荐、分类、翻译 | 核心工具 |
| **演化** | 人工设计→学习获得 | 深度学习趋势 |

---

### 2️⃣ 语义相似度度量全景图谱

```mermaid
graph TB
    SSM[语义相似度度量<br/>Semantic Similarity Metrics]
    
    SSM --> CoreQ[核心问题:<br/>如何量化相似?]
    
    CoreQ --> Philosophy[哲学问题]
    Philosophy --> P1[本体论:<br/>什么是相似?]
    Philosophy --> P2[度量论:<br/>如何量化?]
    Philosophy --> P3[认识论:<br/>客观vs主观?]
    
    Categories[四大类别]
    
    Categories --> Vector[基于向量<br/>Vector-Based]
    Categories --> Probability[基于概率<br/>Probability-Based]
    Categories --> Path[基于路径<br/>Path-Based]
    Categories --> Neural[基于神经网络<br/>Neural-Based]
    
    Vector --> V1[余弦相似度<br/>Cosine]
    Vector --> V2[欧氏距离<br/>Euclidean]
    Vector --> V3[曼哈顿距离<br/>Manhattan]
    Vector --> V4[Jaccard]
    
    V1 --> CosineProp[不受magnitude影响<br/>only direction]
    V2 --> EuclidProp[受magnitude影响<br/>absolute distance]
    
    Probability --> Prob1[KL散度<br/>非对称]
    Probability --> Prob2[JS散度<br/>对称]
    Probability --> Prob3[Wasserstein<br/>最优传输]
    
    Prob1 --> KLProp[D_KL&#40;P||Q&#41; ≠ D_KL&#40;Q||P&#41;]
    Prob2 --> JSProp[JS = 0.5KL&#40;P||M&#41; + 0.5KL&#40;Q||M&#41;]
    Prob3 --> WassProp[考虑几何距离]
    
    Path --> WordNet[WordNet<br/>语义网络]
    Path --> Path1[路径长度<br/>Path Length]
    Path --> Path2[Wu-Palmer<br/>最近共同祖先]
    Path --> Path3[Resnik<br/>信息内容]
    
    Neural --> N1[孪生网络<br/>Siamese]
    Neural --> N2[三元组损失<br/>Triplet Loss]
    Neural --> N3[对比学习<br/>Contrastive]
    
    N1 --> SiameseProp[共享权重<br/>学习度量]
    N2 --> TripletProp[anchor, positive, negative]
    N3 --> ContrastProp[InfoNCE loss]
    
    Applications[应用层次]
    
    Applications --> WordLevel[词级:<br/>Word2Vec相似]
    Applications --> SentLevel[句级:<br/>Sentence-BERT]
    Applications --> DocLevel[文档级:<br/>TF-IDF+Cosine]
    Applications --> CrossModal[跨模态:<br/>CLIP]
    
    Evaluation[评估方法]
    
    Evaluation --> Intrinsic[内在评估:<br/>人类判断相关性]
    Evaluation --> Extrinsic[外在评估:<br/>下游任务性能]
    
    style SSM fill:#9b59b6,stroke:#333,stroke-width:4px
    style Categories fill:#3498db,stroke:#333,stroke-width:4px
    style Neural fill:#2ecc71,stroke:#333,stroke-width:4px
    style Applications fill:#e67e22,stroke:#333,stroke-width:4px
```

---

### 3️⃣ 余弦vs欧氏距离深度对比

| 维度 | 余弦相似度 | 欧氏距离 | 关键差异 |
|------|-----------|---------|---------|
| **公式** | $\cos(\theta) = \frac{x \cdot y}{\|x\|\|y\|}$ | $d = \sqrt{\sum(x_i - y_i)^2}$ | 角度vs绝对距离 |
| **值域** | [-1, 1] | [0, ∞) | 有界vs无界 |
| **magnitude敏感** | ❌ 否（仅方向） | ✅ 是（绝对差） | **核心区别** |
| **归一化需求** | 内建归一化 | 建议归一化 | 余弦自动处理 |
| **高维性能** | ✅✅ 稳定 | ⚠️ 维度灾难 | 余弦优势 |
| **稀疏向量** | ✅ 高效 | 中 | 点积快 |
| **适用场景** | 文本相似度（tf-idf）| 聚类（k-means） | 不同偏好 |
| **计算复杂度** | O(d) | O(d) | 相同 |

**数学详解**:

$$
\begin{align}
\text{余弦相似度} &: \\
\cos(\theta) &= \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum x_i y_i}{\sqrt{\sum x_i^2} \cdot \sqrt{\sum y_i^2}} \\
\text{范围} &: [-1, 1], \quad 1=\text{完全相同方向}, 0=\text{正交}, -1=\text{相反} \\
\\
\text{欧氏距离} &: \\
d(x, y) &= ||x - y||_2 = \sqrt{\sum_{i=1}^d (x_i - y_i)^2} \\
\text{范围} &: [0, \infty), \quad 0=\text{完全相同}, \text{越大越不同} \\
\\
\text{转换关系} &: \\
\text{归一化后} &: ||x|| = ||y|| = 1 \Rightarrow d^2(x,y) = 2(1 - \cos(\theta))
\end{align}
$$

**深度分析**:

```yaml
余弦相似度（Cosine Similarity）:
  核心思想:
    - 测量向量夹角
    - 忽略大小（magnitude），只看方向
  
  为什么有效?
    - 文本向量:长文档vs短文档
    - TF-IDF值:长度不同但主题相同
    - 余弦捕捉主题相似性
  
  示例:
    文档A: [1, 0, 1]（短文档，出现"cat", "dog"）
    文档B: [10, 0, 10]（长文档，出现"cat", "dog"各10次）
    
    余弦: 1.0（完全相似，主题相同）
    欧氏: 12.7（差异大，因为长度不同）
    → 余弦更合理
  
  数学性质:
    - 内建归一化
    - 对缩放不变: cos(cx, cy) = cos(x, y)
    - 高维稳定（维度灾难缓解）

欧氏距离（Euclidean Distance）:
  核心思想:
    - 测量直线距离
    - 同时考虑方向和大小
  
  为什么有用?
    - 聚类任务（k-means）
    - 最近邻搜索
    - 物理空间类比
  
  问题:
    - magnitude敏感
      - [1,0,1] vs [10,0,10]: d=12.7
      - 主题相同but距离大
    
    - 维度灾难
      - 高维空间距离集中
      - 所有点等距
      - 相似度失效
  
  解决:
    - 归一化: x' = x/||x||
    - 归一化后: d²=2(1-cos θ)
    - 等价于余弦（归一化后）

曼哈顿距离（Manhattan Distance）:
  d(x,y) = Σ|x_i - y_i|
  
  特点:
    - L1范数
    - 格网距离（城市街区）
    - 对异常值鲁棒
  
  应用:
    - 高维空间（vs L2）
    - 稀疏数据
    - Lasso正则化

选择建议:
  文本相似度:
    - 首选: 余弦（不受长度影响）
    - 备选: Jaccard（集合相似）
  
  图像相似度:
    - 原始像素: 欧氏（或L1）
    - 特征向量: 余弦
  
  聚类:
    - K-means: 欧氏（默认）
    - 文本聚类: 余弦
  
  推荐系统:
    - 用户-物品: 余弦
    - 协同过滤: Pearson相关
```

---

### 4️⃣ KL vs JS vs Wasserstein深度对比

**三大散度公式**:

$$
\begin{align}
\text{KL散度} &: \\
D_{KL}(P || Q) &= \sum_i P(i) \log \frac{P(i)}{Q(i)} = \mathbb{E}_P[\log \frac{P}{Q}] \\
\\
\text{JS散度} &: \\
D_{JS}(P, Q) &= \frac{1}{2}D_{KL}(P || M) + \frac{1}{2}D_{KL}(Q || M), \quad M = \frac{P + Q}{2} \\
\\
\text{Wasserstein距离} &: \\
W_p(P, Q) &= \inf_{\gamma \in \Gamma(P,Q)} \mathbb{E}_{(x,y) \sim \gamma}[||x - y||_p^p]^{1/p}
\end{align}
$$

| 维度 | KL散度 | JS散度 | Wasserstein距离 | 关键差异 |
|------|--------|--------|----------------|---------|
| **对称性** | ❌ 非对称 | ✅ 对称 | ✅ 对称 | KL特殊 |
| **值域** | [0, ∞) | [0, log2] | [0, ∞) | JS有界 |
| **退化** | $Q(i)=0, P(i)>0 \Rightarrow \infty$ | 鲁棒 | 鲁棒 | KL不稳定 |
| **几何意义** | 信息论（相对熵） | 信息论（对称化） | 最优传输 | 不同视角 |
| **可微性** | ⚠️ 不连续 | ⚠️ 不连续 | ✅✅ 光滑 | Wasserstein优势 |
| **计算** | 简单 | 简单 | 复杂（需优化） | 效率差异 |
| **应用** | VAE、信息论 | GAN变体 | **Wasserstein GAN** | 不同领域 |

**深度分析**:

```yaml
KL散度（Kullback-Leibler Divergence）:
  定义: D_KL(P||Q) = Σ P(i) log [P(i)/Q(i)]
  
  含义:
    - 用Q近似P的信息损失
    - 从P视角看Q的"距离"
    - 相对熵
  
  性质:
    - 非对称: D_KL(P||Q) ≠ D_KL(Q||P)
    - 非负: D_KL ≥ 0
    - D_KL=0 ⟺ P=Q
  
  问题:
    - Q(i)=0 但 P(i)>0 → ∞
    - 不满足三角不等式（非度量）
    - 不连续（P和Q不重叠时）
  
  应用:
    - VAE: KL(q(z|x)||p(z))
    - EM算法: 最大化似然
    - 信息论: 编码效率

JS散度（Jensen-Shannon Divergence）:
  定义: JS = 0.5·KL(P||M) + 0.5·KL(Q||M)
       M = (P+Q)/2
  
  改进:
    - 对称化KL
    - 有界: [0, log2]
    - 无无穷大问题
  
  性质:
    - 对称: JS(P,Q) = JS(Q,P)
    - 非负
    - 平方根是度量
  
  应用:
    - GAN变体（JS GAN）
    - 分布比较
    - 聚类

Wasserstein距离（Earth Mover's Distance）:
  定义: 从P"搬运"到Q的最小代价
  
  直观:
    - P是一堆土的分布
    - Q是一堆坑的分布
    - 最小搬运成本
  
  数学:
    W_p(P,Q) = inf_{γ} E_{(x,y)~γ}[||x-y||^p]^{1/p}
    γ: P和Q的联合分布（耦合）
  
  革命性优势（vs KL/JS）:
    1. 不重叠时仍有意义
       - P在x=0, Q在x=1
       - KL/JS: 突变、不连续
       - Wasserstein: W=1 光滑
    
    2. 考虑几何距离
       - KL/JS: 概率差异
       - Wasserstein: 几何结构
       → 更符合人类直觉
    
    3. 可微性
       - KL/JS: 不重叠时梯度消失
       - Wasserstein: 处处可微
       → GAN训练稳定
  
  Wasserstein GAN（2017）:
    - 用Wasserstein替代JS
    - 解决模式崩溃
    - 训练稳定性提升
    - 梯度不消失
  
  计算挑战:
    - 精确计算: NP-hard
    - Sinkhorn算法: 近似
    - 对偶形式: Kantorovich-Rubinstein
    - 实践: Lipschitz约束

三者比较示例:
  P: [1, 0, 0, 0]
  Q: [0, 0, 0, 1]
  （四个位置，P全在第1位，Q全在第4位）
  
  KL(P||Q): ∞ （Q在P支撑上为0）
  JS(P, Q): log2（最大）
  W_1(P, Q): 3 （搬运3个单位距离）
  
  → Wasserstein最合理

当前共识（2024）:
  - NLP: 余弦相似度主导（词嵌入）
  - GAN: Wasserstein距离主导（训练稳定）
  - VAE: KL散度主导（理论简洁）
  - 分布比较: JS或Wasserstein
```

---

### 🔟 核心洞察与终极评估

**五大核心定律**:

1. **余弦-方向定律**
   $$
   \cos(\theta) = \frac{x \cdot y}{||x|| \cdot ||y||} \quad \text{（忽略magnitude）}
   $$
   - 文本相似度首选

2. **归一化等价定律**
   $$
   ||x|| = ||y|| = 1 \Rightarrow d^2_{\text{Euclid}} = 2(1 - \cos(\theta))
   $$
   - 归一化后余弦≈欧氏

3. **KL非对称定律**
   $$
   D_{KL}(P||Q) \neq D_{KL}(Q||P)
   $$
   - 不是真正的"距离"

4. **Wasserstein光滑定律**
   $$
   \text{不重叠时} \quad W_1(P,Q) = \text{几何距离} \quad \text{（可微）}
   $$
   - GAN训练稳定性关键

5. **神经度量学习定律**
   $$
   \text{Triplet Loss}: ||f(a) - f(p)||^2 + \alpha < ||f(a) - f(n)||^2
   $$
   - 端到端学习最优度量

**终极洞察**:

> **"语义相似度度量是量化意义接近程度的核心工具。四大类别：①基于向量（余弦、欧氏、曼哈顿）：简单高效，余弦因不受magnitude影响成为文本首选②基于概率（KL、JS、Wasserstein）：KL非对称且退化问题，JS对称化but不连续，Wasserstein考虑几何距离且光滑，是GAN训练稳定性关键③基于路径（WordNet）：利用语义网络层次结构，人工构建but覆盖有限④基于神经网络（孪生、三元组、对比学习）：端到端学习最优度量，是当前主流。核心权衡：余弦vs欧氏（方向vs绝对距离）、KL vs Wasserstein（简单vs稳定）、人工设计vs学习获得（可解释vs性能）。数学基础：度量空间公理（非负、对称、三角不等式），内积空间（余弦），信息论（KL/JS），最优传输（Wasserstein）。应用层次：词级（Word2Vec余弦）、句级（Sentence-BERT）、文档级（TF-IDF+余弦）、跨模态（CLIP）。演化趋势：从人工设计（余弦、KL）到神经学习（孪生网络、对比学习），从单模态到多模态。Wasserstein革命（2017）：解决GAN模式崩溃和训练不稳定，通过考虑几何距离实现梯度光滑。当前主导：①文本：余弦相似度②GAN：Wasserstein距离③端到端：对比学习（InfoNCE）。选择指南：任务决定度量——检索用余弦、GAN用Wasserstein、聚类用欧氏（归一化后）。语义相似度度量是连接符号世界与数值计算的桥梁。"**

**元认知**:
- **核心分类**: 向量、概率、路径、神经
- **文本首选**: 余弦相似度（方向不受magnitude）
- **GAN关键**: Wasserstein距离（光滑可微）
- **当前趋势**: 神经学习度量（孪生、对比）
- **应用层次**: 词→句→文档→跨模态
- **选择原则**: 任务决定度量
- **未来方向**: 多模态、端到端学习

</details>

---

## 📋 目录

- [核心概念深度分析](#核心概念深度分析)
- [目录 | Table of Contents](#目录-table-of-contents)
- [引言](#引言)
  - [核心问题](#核心问题)
  - [应用场景](#应用场景)
- [相似度的数学基础](#相似度的数学基础)
  - [1. 相似度与距离](#1-相似度与距离)
    - [相似度函数（Similarity Function）](#相似度函数similarity-function)
    - [距离函数（Distance Function）](#距离函数distance-function)
    - [相似度与距离的转换](#相似度与距离的转换)
  - [2. 内积空间中的相似度](#2-内积空间中的相似度)
- [基于向量的相似度度量](#基于向量的相似度度量)
  - [1. 余弦相似度（Cosine Similarity）](#1-余弦相似度cosine-similarity)
  - [2. 欧几里得距离（Euclidean Distance）](#2-欧几里得距离euclidean-distance)
  - [3. 曼哈顿距离（Manhattan Distance）](#3-曼哈顿距离manhattan-distance)
  - [4. Jaccard相似度](#4-jaccard相似度)
  - [5. Pearson相关系数](#5-pearson相关系数)
- [基于概率的相似度度量](#基于概率的相似度度量)
  - [1. KL散度（Kullback-Leibler Divergence）](#1-kl散度kullback-leibler-divergence)
  - [2. JS散度（Jensen-Shannon Divergence）](#2-js散度jensen-shannon-divergence)
  - [3. Wasserstein距离（Earth Mover's Distance）](#3-wasserstein距离earth-movers-distance)
- [基于路径的相似度度量](#基于路径的相似度度量)
  - [1. WordNet简介](#1-wordnet简介)
  - [2. 路径长度（Path Length）](#2-路径长度path-length)
  - [3. Leacock-Chodorow相似度](#3-leacock-chodorow相似度)
  - [4. Wu-Palmer相似度](#4-wu-palmer相似度)
  - [5. Resnik相似度](#5-resnik相似度)
- [深度学习时代的相似度度量](#深度学习时代的相似度度量)
  - [1. 学习的相似度函数](#1-学习的相似度函数)
  - [2. 孪生网络（Siamese Networks）](#2-孪生网络siamese-networks)
  - [3. 三元组损失（Triplet Loss）](#3-三元组损失triplet-loss)
  - [4. 对比学习（Contrastive Learning）](#4-对比学习contrastive-learning)
- [句子和文档级别的相似度](#句子和文档级别的相似度)
  - [1. 平均词向量（Averaged Word Embeddings）](#1-平均词向量averaged-word-embeddings)
  - [2. Sentence-BERT](#2-sentence-bert)
  - [3. BERTScore](#3-bertscore)
  - [4. 文档相似度：TF-IDF + 余弦](#4-文档相似度tf-idf-余弦)
- [跨模态相似度度量](#跨模态相似度度量)
  - [1. 图像-文本相似度](#1-图像-文本相似度)
  - [2. 多模态相似度](#2-多模态相似度)
- [相似度度量的评估](#相似度度量的评估)
  - [1. 内在评估（Intrinsic Evaluation）](#1-内在评估intrinsic-evaluation)
    - [人类判断相关性](#人类判断相关性)
    - [类比任务](#类比任务)
  - [2. 外在评估（Extrinsic Evaluation）](#2-外在评估extrinsic-evaluation)
- [总结](#总结)
  - [核心要点](#核心要点)
  - [选择指南](#选择指南)
  - [未来方向](#未来方向)
- [参考文献](#参考文献)
  - [综述](#综述)
  - [数学基础](#数学基础)
  - [基于路径](#基于路径)
  - [深度学习](#深度学习)
  - [句子级别](#句子级别)
  - [跨模态](#跨模态)
  - [评估](#评估)
- [导航 | Navigation](#导航-navigation)
- [相关主题 | Related Topics](#相关主题-related-topics)
  - [本章节](#本章节)
  - [相关章节](#相关章节)
  - [跨视角链接](#跨视角链接)

---

## 引言

**语义相似度**（Semantic Similarity）是AI和自然语言处理的核心概念，它量化了两个语义对象（词、句子、文档）的"接近程度"。

### 核心问题

1. **本体论问题**：什么是"相似"？
2. **度量论问题**：如何量化相似度？
3. **认识论问题**：相似度是客观的还是主观的？
4. **计算问题**：如何高效计算相似度？

### 应用场景

| 应用 | 任务 | 相似度用途 |
|------|------|-----------|
| **信息检索** | 搜索引擎 | 查询-文档相似度 |
| **推荐系统** | 商品推荐 | 用户-商品相似度 |
| **问答系统** | 答案匹配 | 问题-答案相似度 |
| **文本分类** | 主题识别 | 文档-类别相似度 |
| **机器翻译** | 翻译质量评估 | 译文-参考相似度 |
| **对话系统** | 意图识别 | 用户输入-意图相似度 |

**参考文献**：

- [Wikipedia: Semantic Similarity](https://en.wikipedia.org/wiki/Semantic_similarity)
- [Gomaa & Fahmy, 2013](https://arxiv.org/abs/1310.8059) - A Survey of Text Similarity Approaches

---

## 相似度的数学基础

### 1. 相似度与距离

#### 相似度函数（Similarity Function）

**定义**：

**相似度函数** sim : X × X → ℝ 满足：

1. **非负性**：sim(x, y) ≥ 0
2. **对称性**：sim(x, y) = sim(y, x)
3. **自相似最大**：sim(x, x) ≥ sim(x, y) ∀y

**归一化相似度**：

```text
sim : X × X → [0, 1]
```

其中 1 表示完全相同，0 表示完全不同。

#### 距离函数（Distance Function）

**定义**：

**距离函数**（度量）d : X × X → ℝ₊ 满足：

1. **非负性**：d(x, y) ≥ 0，且 d(x, y) = 0 ⟺ x = y
2. **对称性**：d(x, y) = d(y, x)
3. **三角不等式**：d(x, z) ≤ d(x, y) + d(y, z)

#### 相似度与距离的转换

**常见转换**：

1. **线性转换**：

    ```text
    sim(x, y) = 1 - d(x, y) / d_max
    ```

2. **指数转换**：

    ```text
    sim(x, y) = exp(-d(x, y))
    ```

3. **Gauss核**：

    ```text
    sim(x, y) = exp(-d(x, y)² / (2σ²))
    ```

**参考文献**：

- [Wikipedia: Metric Space](https://en.wikipedia.org/wiki/Metric_space)
- [Wikipedia: Similarity Measure](https://en.wikipedia.org/wiki/Similarity_measure)

### 2. 内积空间中的相似度

**希尔伯特空间**：

在内积空间中，内积 ⟨·,·⟩ 自然地定义了相似度：

```text
sim(𝒙, 𝒚) = ⟨𝒙, 𝒚⟩
```

**Cauchy-Schwarz不等式**：

```text
|⟨𝒙, 𝒚⟩| ≤ ‖𝒙‖ ‖𝒚‖
```

**推论**：

```text
-1 ≤ ⟨𝒙, 𝒚⟩ / (‖𝒙‖ ‖𝒚‖) ≤ 1
```

这就是**余弦相似度**。

**参考文献**：

- [Wikipedia: Inner Product Space](https://en.wikipedia.org/wiki/Inner_product_space)
- [Wikipedia: Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)

---

## 基于向量的相似度度量

### 1. 余弦相似度（Cosine Similarity）

**定义**：

```text
cos(𝒖, 𝒗) = ⟨𝒖, 𝒗⟩ / (‖𝒖‖ ‖𝒗‖) = ∑ᵢ uᵢvᵢ / (√∑ᵢ uᵢ² √∑ᵢ vᵢ²)
```

**几何意义**：

两个向量夹角的余弦值。

**值域**：[-1, 1]

- cos(𝒖, 𝒗) = 1：完全相同方向（最相似）
- cos(𝒖, 𝒗) = 0：正交（无关）
- cos(𝒖, 𝒗) = -1：完全相反方向（最不相似）

**优势**：

- ✅ 不受向量长度影响（只看方向）
- ✅ 适合高维稀疏向量（如文本）
- ✅ **AI中最常用的相似度度量**

**例子**：

```text
𝒖 = [1, 2, 3]
𝒗 = [2, 4, 6]
cos(𝒖, 𝒗) = (1×2 + 2×4 + 3×6) / (√14 × √56) = 28 / 28 = 1
```

𝒗 = 2𝒖，所以完全相同方向。

**参考文献**：

- [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)

### 2. 欧几里得距离（Euclidean Distance）

**定义**：

```text
d_E(𝒖, 𝒗) = ‖𝒖 - 𝒗‖₂ = √∑ᵢ (uᵢ - vᵢ)²
```

**几何意义**：

两点间的直线距离。

**转换为相似度**：

```text
sim(𝒖, 𝒗) = 1 / (1 + d_E(𝒖, 𝒗))
```

或

```text
sim(𝒖, 𝒗) = exp(-d_E(𝒖, 𝒗)²)
```

**特点**：

- ✅ 直观
- ❌ 受向量长度影响
- ❌ 高维空间中区分度下降（维度灾难）

**参考文献**：

- [Wikipedia: Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)

### 3. 曼哈顿距离（Manhattan Distance）

**定义**：

```text
d_M(𝒖, 𝒗) = ‖𝒖 - 𝒗‖₁ = ∑ᵢ |uᵢ - vᵢ|
```

**几何意义**：

沿坐标轴的距离之和（如城市街区）。

**特点**：

- ✅ 计算简单
- ✅ 对异常值更鲁棒（相比欧几里得距离）

**参考文献**：

- [Wikipedia: Taxicab Geometry](https://en.wikipedia.org/wiki/Taxicab_geometry)

### 4. Jaccard相似度

**定义**（对于集合）：

```text
J(A, B) = |A ∩ B| / |A ∪ B|
```

**推广到向量**（二值向量）：

```text
J(𝒖, 𝒗) = ∑ᵢ min(uᵢ, vᵢ) / ∑ᵢ max(uᵢ, vᵢ)
```

**应用**：

- 文档去重
- 协同过滤

**参考文献**：

- [Wikipedia: Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)

### 5. Pearson相关系数

**定义**：

```text
r(𝒖, 𝒗) = cov(𝒖, 𝒗) / (σ_𝒖 σ_𝒗)
```

其中：

```text
cov(𝒖, 𝒗) = E[(𝒖 - μ_𝒖)(𝒗 - μ_𝒗)]
```

**值域**：[-1, 1]

**特点**：

- ✅ 捕捉线性相关性
- ✅ 不受平移和缩放影响

**与余弦相似度的关系**：

对中心化向量，Pearson相关系数等于余弦相似度。

**参考文献**：

- [Wikipedia: Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)

---

## 基于概率的相似度度量

### 1. KL散度（Kullback-Leibler Divergence）

**定义**：

```text
D_KL(P ‖ Q) = ∑ᵢ P(i) log(P(i) / Q(i))
```

或（连续情况）：

```text
D_KL(P ‖ Q) = ∫ p(x) log(p(x) / q(x)) dx
```

**性质**：

- ✅ 非负：D_KL(P ‖ Q) ≥ 0
- ✅ D_KL(P ‖ Q) = 0 ⟺ P = Q
- ❌ 不对称：D_KL(P ‖ Q) ≠ D_KL(Q ‖ P)
- ❌ 不满足三角不等式

**含义**：

P相对于Q的"额外信息量"。

**应用**：

- 语言模型评估（困惑度）
- 变分推断

**参考文献**：

- [Wikipedia: Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)

### 2. JS散度（Jensen-Shannon Divergence）

**定义**：

```text
D_JS(P, Q) = (1/2) D_KL(P ‖ M) + (1/2) D_KL(Q ‖ M)
```

其中 M = (P + Q) / 2。

**性质**：

- ✅ 对称：D_JS(P, Q) = D_JS(Q, P)
- ✅ 有界：0 ≤ D_JS(P, Q) ≤ log 2
- ✅ 满足度量条件的平方根：√D_JS(P, Q) 是度量

**转换为相似度**：

```text
sim(P, Q) = 1 - √D_JS(P, Q) / √log 2
```

**参考文献**：

- [Wikipedia: Jensen-Shannon Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)

### 3. Wasserstein距离（Earth Mover's Distance）

**定义**：

```text
W_p(P, Q) = (inf_{γ ∈ Π(P,Q)} ∫∫ d(x,y)^p dγ(x,y))^{1/p}
```

**直觉**：

把分布P"搬运"到分布Q所需的最小"工作量"。

**优势**：

- ✅ 对分布的支撑集不要求重叠
- ✅ 提供了平滑的距离度量

**应用**：

- 生成模型（WGAN）
- 文档相似度

**参考文献**：

- [Wikipedia: Wasserstein Metric](https://en.wikipedia.org/wiki/Wasserstein_metric)
- [Arjovsky et al., 2017](https://arxiv.org/abs/1701.07875) - Wasserstein GAN

---

## 基于路径的相似度度量

这些方法基于**知识图谱**或**语义网络**（如WordNet）中的路径长度。

### 1. WordNet简介

**WordNet**：

- 英语词汇的语义网络
- 组织为**同义词集合**（Synsets）
- 通过**语义关系**（is-a, part-of等）连接

**参考文献**：

- [Wikipedia: WordNet](https://en.wikipedia.org/wiki/WordNet)
- [Miller, 1995](https://dl.acm.org/doi/10.1145/219717.219748) - WordNet: A Lexical Database for English

### 2. 路径长度（Path Length）

**定义**：

```text
sim_path(c₁, c₂) = 1 / (1 + len(shortest_path(c₁, c₂)))
```

**例子**：

```text
WordNet层次：
  entity
    ├── living_thing
    │     ├── animal
    │     │     ├── mammal
    │     │     │     ├── cat
    │     │     │     └── dog
    │     └── plant

path(cat, dog) = 2  （经过mammal）
sim_path(cat, dog) = 1 / (1 + 2) = 0.33
```

**参考文献**：

- [Rada et al., 1989](https://ieeexplore.ieee.org/document/21465) - Development and Application of a Metric on Semantic Nets

### 3. Leacock-Chodorow相似度

**定义**：

```text
sim_LC(c₁, c₂) = -log(len(shortest_path(c₁, c₂)) / (2 × depth(taxonomy)))
```

**归一化**：

考虑分类层次的深度。

**参考文献**：

- [Leacock & Chodorow, 1998](https://aclanthology.org/J98-1006/) - Combining Local Context and WordNet Similarity

### 4. Wu-Palmer相似度

**定义**：

```text
sim_WP(c₁, c₂) = 2 × depth(LCS(c₁, c₂)) / (depth(c₁) + depth(c₂))
```

其中 LCS = Least Common Subsumer（最低公共祖先）。

**例子**：

```text
depth(cat) = 5
depth(dog) = 5
depth(LCS(cat, dog)) = depth(mammal) = 4

sim_WP(cat, dog) = 2 × 4 / (5 + 5) = 0.8
```

**参考文献**：

- [Wu & Palmer, 1994](https://aclanthology.org/P94-1019/) - Verbs Semantics and Lexical Selection

### 5. Resnik相似度

**基于信息内容**（Information Content）：

**定义**：

```text
IC(c) = -log P(c)
```

其中 P(c) 是概念 c 在语料库中出现的概率。

**Resnik相似度**：

```text
sim_Res(c₁, c₂) = IC(LCS(c₁, c₂))
```

**直觉**：

两个概念的共同祖先越具体（概率越小），它们越相似。

**参考文献**：

- [Resnik, 1995](https://arxiv.org/abs/cmp-lg/9511007) - Using Information Content to Evaluate Semantic Similarity

---

## 深度学习时代的相似度度量

### 1. 学习的相似度函数

**核心思想**：

用神经网络**学习**相似度函数，而非手工定义。

**一般形式**：

```text
sim_θ(x, y) = f_θ(Enc(x), Enc(y))
```

其中：

- Enc：编码器（如BERT）
- f_θ：相似度计算（如MLP）
- θ：可学习参数

### 2. 孪生网络（Siamese Networks）

**架构**：

```text
x₁ → Enc → 𝒗₁ ↘
                 cos(𝒗₁, 𝒗₂) → sim
x₂ → Enc → 𝒗₂ ↗
```

**共享编码器**：两个输入使用同一个编码器。

**训练目标**（对比损失）：

```text
L = ∑ [ y × d(𝒗₁, 𝒗₂)² + (1-y) × max(0, m - d(𝒗₁, 𝒗₂))² ]
```

其中：

- y = 1：相似对
- y = 0：不相似对
- m：边界

**参考文献**：

- [Wikipedia: Siamese Neural Network](https://en.wikipedia.org/wiki/Siamese_neural_network)
- [Bromley et al., 1993](https://proceedings.neurips.cc/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html) - Signature Verification using a Siamese Time Delay Neural Network

### 3. 三元组损失（Triplet Loss）

**训练数据**：

```text
(anchor, positive, negative)
```

- anchor：锚点
- positive：与anchor相似
- negative：与anchor不相似

**目标**：

```text
d(anchor, positive) < d(anchor, negative)
```

**损失函数**：

```text
L = ∑ max(0, d(a, p) - d(a, n) + margin)
```

**参考文献**：

- [Schroff et al., 2015](https://arxiv.org/abs/1503.03832) - FaceNet: A Unified Embedding for Face Recognition

### 4. 对比学习（Contrastive Learning）

**核心思想**：

拉近**正样本对**，推开**负样本对**。

**InfoNCE损失**：

```text
L = -log(exp(sim(x, x⁺) / τ) / ∑_{x⁻} exp(sim(x, x⁻) / τ))
```

其中：

- x⁺：正样本
- x⁻：负样本
- τ：温度参数

**应用**：

- SimCLR（视觉）
- SimCSE（文本）

**参考文献**：

- [Chen et al., 2020](https://arxiv.org/abs/2002.05709) - A Simple Framework for Contrastive Learning of Visual Representations
- [Gao et al., 2021](https://arxiv.org/abs/2104.08821) - SimCSE: Simple Contrastive Learning of Sentence Embeddings

---

## 句子和文档级别的相似度

### 1. 平均词向量（Averaged Word Embeddings）

**最简单方法**：

```text
vec(sentence) = (1/n) ∑ᵢ vec(wᵢ)
```

**改进：加权平均**（如TF-IDF权重）

```text
vec(sentence) = ∑ᵢ weight(wᵢ) × vec(wᵢ)
```

**优势**：

- ✅ 简单、快速

**劣势**：

- ❌ 丢失词序信息
- ❌ 忽略句法结构

### 2. Sentence-BERT

**核心思想**：

用孪生BERT网络学习句子嵌入。

**架构**：

```text
sentence → BERT → [CLS] token → 𝒗
```

**训练**：

使用自然语言推理（NLI）数据集：

```text
(premise, hypothesis, label)
label ∈ {entailment, contradiction, neutral}
```

**优势**：

- ✅ 高质量句子嵌入
- ✅ 快速推理（预计算嵌入后只需计算余弦相似度）

**参考文献**：

- [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) - Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

### 3. BERTScore

**核心思想**：

用BERT的上下文嵌入计算token级别的相似度，然后聚合。

**步骤**：

1. 对两个句子分别用BERT编码
2. 计算每对token的余弦相似度
3. 用贪心匹配或最优匹配聚合

**公式**：

```text
Precision = (1/|𝒙|) ∑ᵢ max_j cos(𝒙ᵢ, 𝒚ⱼ)
Recall = (1/|𝒚|) ∑ⱼ max_i cos(𝒙ᵢ, 𝒚ⱼ)
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**优势**：

- ✅ 考虑上下文
- ✅ 与人类判断高度相关

**参考文献**：

- [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) - BERTScore: Evaluating Text Generation with BERT

### 4. 文档相似度：TF-IDF + 余弦

**经典方法**：

1. 将文档表示为TF-IDF向量
2. 计算余弦相似度

```text
𝒅₁, 𝒅₂ ∈ ℝ|V|
sim(𝒅₁, 𝒅₂) = cos(𝒅₁, 𝒅₂)
```

**优势**：

- ✅ 简单、高效
- ✅ 可扩展到大规模文档集

**参考文献**：

- [Wikipedia: TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

---

## 跨模态相似度度量

### 1. 图像-文本相似度

**目标**：

度量图像和文本描述的相似度。

**方法：CLIP（Contrastive Language-Image Pre-training）**-

**架构**：

```text
Image → Image Encoder → 𝒗_img ↘
                                 cos(𝒗_img, 𝒗_txt)
Text → Text Encoder → 𝒗_txt ↗
```

**训练**：

对比学习，匹配的图像-文本对相似度高。

**应用**：

- 图像检索
- 图像生成（如DALL-E）

**参考文献**：

- [Radford et al., 2021](https://arxiv.org/abs/2103.00020) - Learning Transferable Visual Models From Natural Language Supervision

### 2. 多模态相似度

**一般框架**：

将不同模态映射到**统一的语义空间**：

```text
Text → Enc_text → 𝒛 ∈ ℝᵈ
Image → Enc_img → 𝒛 ∈ ℝᵈ
Audio → Enc_audio → 𝒛 ∈ ℝᵈ

sim(x₁, x₂) = cos(𝒛₁, 𝒛₂)
```

**参考文献**：

- [Wikipedia: Multimodal Learning](https://en.wikipedia.org/wiki/Multimodal_learning)

---

## 相似度度量的评估

### 1. 内在评估（Intrinsic Evaluation）

#### 人类判断相关性

**数据集**：

- **SimLex-999**：999对词，带人类相似度评分
- **WordSim-353**：353对词，带人类相似度评分

**评估指标**：

```text
Spearman相关系数 = 模型相似度排序 与 人类相似度排序的相关性
```

**参考文献**：

- [Hill et al., 2015](https://arxiv.org/abs/1408.3456) - SimLex-999: Evaluating Semantic Models

#### 类比任务

**例子**：

```text
king - man + woman ≈ ?
```

**评估**：

模型能否正确回答"queen"。

**参考文献**：

- [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Linguistic Regularities in Continuous Space

### 2. 外在评估（Extrinsic Evaluation）

**下游任务表现**：

- 文本分类
- 信息检索
- 问答系统
- 机器翻译

**原则**：

> **好的相似度度量应该提升下游任务的性能。**

---

## 总结

### 核心要点

1. **数学基础**：相似度 vs 距离、内积空间
2. **基于向量**：余弦相似度（最常用）、欧几里得距离、Jaccard
3. **基于概率**：KL散度、JS散度、Wasserstein距离
4. **基于路径**：WordNet、路径长度、信息内容
5. **深度学习**：孪生网络、三元组损失、对比学习
6. **句子级别**：平均词向量、Sentence-BERT、BERTScore
7. **跨模态**：CLIP、多模态统一空间
8. **评估**：人类判断相关性、类比任务、下游任务

### 选择指南

| 场景 | 推荐度量 | 原因 |
|------|---------|------|
| **词级别** | 余弦相似度（Word2Vec/GloVe） | 标准方法 |
| **句子级别** | Sentence-BERT | 高质量、快速 |
| **语义相似度（上下文敏感）** | BERT嵌入 + 余弦 | 考虑上下文 |
| **文档级别** | TF-IDF + 余弦 | 简单高效 |
| **概念相似度** | WordNet路径方法 | 利用知识图谱 |
| **生成文本评估** | BERTScore | 与人类判断相关 |
| **跨模态** | CLIP | 图像-文本对齐 |

### 未来方向

1. **可解释相似度**：理解为什么两个对象相似
2. **多粒度相似度**：词、短语、句子、文档的统一框架
3. **个性化相似度**：根据用户偏好调整相似度度量
4. **对抗鲁棒性**：相似度度量对对抗攻击的鲁棒性

---

## 参考文献

### 综述

1. [Wikipedia: Semantic Similarity](https://en.wikipedia.org/wiki/Semantic_similarity)
2. [Gomaa & Fahmy, 2013](https://arxiv.org/abs/1310.8059) - A Survey of Text Similarity Approaches

### 数学基础

1. [Wikipedia: Metric Space](https://en.wikipedia.org/wiki/Metric_space)
2. [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
3. [Wikipedia: Inner Product Space](https://en.wikipedia.org/wiki/Inner_product_space)

### 基于路径

1. [Wikipedia: WordNet](https://en.wikipedia.org/wiki/WordNet)
2. [Miller, 1995](https://dl.acm.org/doi/10.1145/219717.219748) - WordNet: A Lexical Database for English
3. [Resnik, 1995](https://arxiv.org/abs/cmp-lg/9511007) - Using Information Content to Evaluate Semantic Similarity

### 深度学习

1. [Schroff et al., 2015](https://arxiv.org/abs/1503.03832) - FaceNet: A Unified Embedding
2. [Chen et al., 2020](https://arxiv.org/abs/2002.05709) - SimCLR
3. [Gao et al., 2021](https://arxiv.org/abs/2104.08821) - SimCSE

### 句子级别

1. [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) - Sentence-BERT
2. [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) - BERTScore

### 跨模态

1. [Radford et al., 2021](https://arxiv.org/abs/2103.00020) - CLIP

### 评估

1. [Hill et al., 2015](https://arxiv.org/abs/1408.3456) - SimLex-999

---

*本文档全面梳理了语义相似度度量的理论基础、具体方法和评估标准，为理解和选择合适的相似度度量提供了系统指南。*

---

## 导航 | Navigation

**上一篇**: [← 04.3 分布式语义](./04.3_Distributional_Semantics.md)  
**下一篇**: [04.5 多模态语义整合 →](./04.5_Multimodal_Semantic_Integration.md)  
**返回目录**: [↑ AI模型视角总览](../README.md)

---

## 相关主题 | Related Topics

### 本章节
- [04.1 语义向量空间](./04.1_Semantic_Vector_Spaces.md)
- [04.2 连续表示理论](./04.2_Continuous_Representation_Theory.md)
- [04.3 分布式语义](./04.3_Distributional_Semantics.md)
- [04.5 多模态语义整合](./04.5_Multimodal_Semantic_Integration.md)
- [04.6 黄氏语义模型分析](./04.6_Huang_Semantic_Model_Analysis.md)

### 相关章节
- [03.5 嵌入向量空间](../03_Language_Models/03.5_Embedding_Vector_Spaces.md)

### 跨视角链接
- [Information_Theory_Perspective: 距离度量](../../Information_Theory_Perspective/README.md)