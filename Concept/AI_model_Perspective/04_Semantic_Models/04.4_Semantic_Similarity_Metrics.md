# 4.4 Semantic Similarity Metrics

> **子主题编号**: 04.4
> **主题**: AI模型视角

> **子主题编号**: 04.4
> **主题**: AI模型视角
> **最后更新**: 2025-10-27
> **文档规模**: 929行 | 多维度语义相似度计算方法
> **阅读建议**: 本文全面介绍各类语义相似度度量方法，是实践NLP任务的重要参考

---

## 📋 目录

- [语义相似度度量（Semantic Similarity Metrics）](#语义相似度度量semantic-similarity-metrics)
  - [📋 目录](#-目录)
  - [1 核心概念深度分析](#1-核心概念深度分析)
    - [1.1 语义相似度度量概念定义卡](#11-语义相似度度量概念定义卡)
    - [1.2 语义相似度度量全景图谱](#12-语义相似度度量全景图谱)
    - [1.3 余弦vs欧氏距离深度对比](#13-余弦vs欧氏距离深度对比)
    - [1.4 KL vs JS vs Wasserstein深度对比](#14-kl-vs-js-vs-wasserstein深度对比)
    - [1.5 核心洞察与终极评估](#15-核心洞察与终极评估)
  - [3 基于路径的相似度度量](#3-基于路径的相似度度量)
    - [3.1 WordNet简介](#31-wordnet简介)
    - [3.2 路径长度Path Length](#32-路径长度path-length)
    - [3.3 Leacock-Chodorow相似度](#33-leacock-chodorow相似度)
    - [3.4 Wu-Palmer相似度](#34-wu-palmer相似度)
    - [3.5 Resnik相似度](#35-resnik相似度)
  - [4 深度学习时代的相似度度量](#4-深度学习时代的相似度度量)
    - [4.1 学习的相似度函数](#41-学习的相似度函数)
    - [4.2 孪生网络Siamese Networks](#42-孪生网络siamese-networks)
    - [4.3 三元组损失Triplet Loss](#43-三元组损失triplet-loss)
    - [4.4 对比学习Contrastive Learning](#44-对比学习contrastive-learning)
  - [5 句子和文档级别的相似度](#5-句子和文档级别的相似度)
    - [5.1 平均词向量Averaged Word Embeddings](#51-平均词向量averaged-word-embeddings)
    - [5.2 Sentence-BERT](#52-sentence-bert)
    - [5.3 BERTScore](#53-bertscore)
    - [5.4 文档相似度：TF-IDF 余弦](#54-文档相似度tf-idf-余弦)
  - [6 跨模态相似度度量](#6-跨模态相似度度量)
    - [6.1 图像-文本相似度](#61-图像-文本相似度)
    - [6.2 多模态相似度](#62-多模态相似度)
  - [7 相似度度量的评估](#7-相似度度量的评估)
    - [7.1 内在评估Intrinsic Evaluation](#71-内在评估intrinsic-evaluation)
      - [1.1.1 人类判断相关性](#111-人类判断相关性)
      - [1.1.2 类比任务](#112-类比任务)
    - [7.2 外在评估Extrinsic Evaluation](#72-外在评估extrinsic-evaluation)
  - [8 总结](#8-总结)
    - [8.1 核心要点](#81-核心要点)
    - [8.2 选择指南](#82-选择指南)
    - [8.3 未来方向](#83-未来方向)
  - [参考文献](#参考文献)
    - [8.4 综述](#84-综述)
    - [8.5 数学基础](#85-数学基础)
    - [8.6 基于路径](#86-基于路径)
    - [8.7 深度学习](#87-深度学习)
    - [8.8 句子级别](#88-句子级别)
    - [8.9 跨模态](#89-跨模态)
    - [8.10 评估](#810-评估)
  - [导航 | Navigation](#导航--navigation)
  - [相关主题 | Related Topics](#相关主题--related-topics)
    - [8.11 本章节](#811-本章节)
    - [8.12 相关章节](#812-相关章节)
    - [8.13 跨视角链接](#813-跨视角链接)

## 1 核心概念深度分析

<details>
<summary><b>📏🔍 点击展开：语义相似度度量全景深度解析</b></summary>

本节深入剖析余弦相似度vs欧氏距离、KL vs JS vs Wasserstein、孪生网络与对比学习。

### 1.1 语义相似度度量概念定义卡

**概念名称**: 语义相似度度量（Semantic Similarity Metrics）

**内涵（本质属性）**:

**🔹 核心定义**:
语义相似度度量是量化两个语义对象（词、句、文档）之间"接近程度"的数学函数，建立从语义空间到实数的映射，用于衡量意义的相似性。

$$
\text{Similarity}: \mathcal{S} \times \mathcal{S} \to \mathbb{R} \quad \text{where } \mathcal{S} = \text{语义空间}
$$

**🔹 相似度vs距离核心对比**:

| 维度 | 相似度（Similarity） | 距离（Distance） | 转换关系 |
|------|-------------------|----------------|---------|
| **值域** | [0, 1]或[-1, 1] | [0, ∞) | 互逆关系 |
| **解释** | 越大越相似 | 越小越相似 | $d = 1 - s$ |
| **性质** | 对称性 | 度量空间公理 | 可互换 |
| **示例** | 余弦相似度、Jaccard | 欧氏距离、曼哈顿距离 | 多种转换 |

**外延（范围边界）**:

| 维度 | 语义相似度包含 ✅ | 不包含 ❌ |
|------|--------------|----------|
| **方法** | 余弦、KL散度、WordNet路径 | 字符编辑距离、哈希碰撞 |
| **层次** | 词级、句级、文档级 | 字符级、音素级 |
| **模态** | 文本、图像、音频、多模态 | 纯结构数据（无语义） |

**属性维度表**:

| 维度 | 值/描述 | 说明 |
|------|---------|------|
| **经典方法** | 余弦、欧氏、KL散度 | 数学基础 |
| **神经方法** | 孪生网络、对比学习 | 端到端学习 |
| **应用** | 检索、推荐、分类、翻译 | 核心工具 |
| **演化** | 人工设计→学习获得 | 深度学习趋势 |

---

### 1.2 语义相似度度量全景图谱

```mermaid
graph TB
    SSM[语义相似度度量<br/>Semantic Similarity Metrics]

    SSM --> CoreQ[核心问题:<br/>如何量化相似?]

    CoreQ --> Philosophy[哲学问题]
    Philosophy --> P1[本体论:<br/>什么是相似?]
    Philosophy --> P2[度量论:<br/>如何量化?]
    Philosophy --> P3[认识论:<br/>客观vs主观?]

    Categories[四大类别]

    Categories --> Vector[基于向量<br/>Vector-Based]
    Categories --> Probability[基于概率<br/>Probability-Based]
    Categories --> Path[基于路径<br/>Path-Based]
    Categories --> Neural[基于神经网络<br/>Neural-Based]

    Vector --> V1[余弦相似度<br/>Cosine]
    Vector --> V2[欧氏距离<br/>Euclidean]
    Vector --> V3[曼哈顿距离<br/>Manhattan]
    Vector --> V4[Jaccard]

    V1 --> CosineProp[不受magnitude影响<br/>only direction]
    V2 --> EuclidProp[受magnitude影响<br/>absolute distance]

    Probability --> Prob1[KL散度<br/>非对称]
    Probability --> Prob2[JS散度<br/>对称]
    Probability --> Prob3[Wasserstein<br/>最优传输]

    Prob1 --> KLProp[D_KL&#40;P||Q&#41; ≠ D_KL&#40;Q||P&#41;]
    Prob2 --> JSProp[JS = 0.5KL&#40;P||M&#41; + 0.5KL&#40;Q||M&#41;]
    Prob3 --> WassProp[考虑几何距离]

    Path --> WordNet[WordNet<br/>语义网络]
    Path --> Path1[路径长度<br/>Path Length]
    Path --> Path2[Wu-Palmer<br/>最近共同祖先]
    Path --> Path3[Resnik<br/>信息内容]

    Neural --> N1[孪生网络<br/>Siamese]
    Neural --> N2[三元组损失<br/>Triplet Loss]
    Neural --> N3[对比学习<br/>Contrastive]

    N1 --> SiameseProp[共享权重<br/>学习度量]
    N2 --> TripletProp[anchor, positive, negative]
    N3 --> ContrastProp[InfoNCE loss]

    Applications[应用层次]

    Applications --> WordLevel[词级:<br/>Word2Vec相似]
    Applications --> SentLevel[句级:<br/>Sentence-BERT]
    Applications --> DocLevel[文档级:<br/>TF-IDF+Cosine]
    Applications --> CrossModal[跨模态:<br/>CLIP]

    Evaluation[评估方法]

    Evaluation --> Intrinsic[内在评估:<br/>人类判断相关性]
    Evaluation --> Extrinsic[外在评估:<br/>下游任务性能]

    style SSM fill:#9b59b6,stroke:#333,stroke-width:4px
    style Categories fill:#3498db,stroke:#333,stroke-width:4px
    style Neural fill:#2ecc71,stroke:#333,stroke-width:4px
    style Applications fill:#e67e22,stroke:#333,stroke-width:4px
```

---

### 1.3 余弦vs欧氏距离深度对比

| 维度 | 余弦相似度 | 欧氏距离 | 关键差异 |
|------|-----------|---------|---------|
| **公式** | $\cos(\theta) = \frac{x \cdot y}{\|x\|\|y\|}$ | $d = \sqrt{\sum(x_i - y_i)^2}$ | 角度vs绝对距离 |
| **值域** | [-1, 1] | [0, ∞) | 有界vs无界 |
| **magnitude敏感** | ❌ 否（仅方向） | ✅ 是（绝对差） | **核心区别** |
| **归一化需求** | 内建归一化 | 建议归一化 | 余弦自动处理 |
| **高维性能** | ✅✅ 稳定 | ⚠️ 维度灾难 | 余弦优势 |
| **稀疏向量** | ✅ 高效 | 中 | 点积快 |
| **适用场景** | 文本相似度（tf-idf）| 聚类（k-means） | 不同偏好 |
| **计算复杂度** | O(d) | O(d) | 相同 |

**数学详解**:

$$
\begin{align}
\text{余弦相似度} &: \\
\cos(\theta) &= \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum x_i y_i}{\sqrt{\sum x_i^2} \cdot \sqrt{\sum y_i^2}} \\
\text{范围} &: [-1, 1], \quad 1=\text{完全相同方向}, 0=\text{正交}, -1=\text{相反} \\
\\
\text{欧氏距离} &: \\
d(x, y) &= ||x - y||_2 = \sqrt{\sum_{i=1}^d (x_i - y_i)^2} \\
\text{范围} &: [0, \infty), \quad 0=\text{完全相同}, \text{越大越不同} \\
\\
\text{转换关系} &: \\
\text{归一化后} &: ||x|| = ||y|| = 1 \Rightarrow d^2(x,y) = 2(1 - \cos(\theta))
\end{align}
$$

**深度分析**:

```yaml
余弦相似度（Cosine Similarity）:
  核心思想:
    - 测量向量夹角
    - 忽略大小（magnitude），只看方向

  为什么有效?
    - 文本向量:长文档vs短文档
    - TF-IDF值:长度不同但主题相同
    - 余弦捕捉主题相似性

  示例:
    文档A: [1, 0, 1]（短文档，出现"cat", "dog"）
    文档B: [10, 0, 10]（长文档，出现"cat", "dog"各10次）

    余弦: 1.0（完全相似，主题相同）
    欧氏: 12.7（差异大，因为长度不同）
    → 余弦更合理

  数学性质:
    - 内建归一化
    - 对缩放不变: cos(cx, cy) = cos(x, y)
    - 高维稳定（维度灾难缓解）

欧氏距离（Euclidean Distance）:
  核心思想:
    - 测量直线距离
    - 同时考虑方向和大小

  为什么有用?
    - 聚类任务（k-means）
    - 最近邻搜索
    - 物理空间类比

  问题:
    - magnitude敏感
      - [1,0,1] vs [10,0,10]: d=12.7
      - 主题相同but距离大

    - 维度灾难
      - 高维空间距离集中
      - 所有点等距
      - 相似度失效

  解决:
    - 归一化: x' = x/||x||
    - 归一化后: d²=2(1-cos θ)
    - 等价于余弦（归一化后）

曼哈顿距离（Manhattan Distance）:
  d(x,y) = Σ|x_i - y_i|

  特点:
    - L1范数
    - 格网距离（城市街区）
    - 对异常值鲁棒

  应用:
    - 高维空间（vs L2）
    - 稀疏数据
    - Lasso正则化

选择建议:
  文本相似度:
    - 首选: 余弦（不受长度影响）
    - 备选: Jaccard（集合相似）

  图像相似度:
    - 原始像素: 欧氏（或L1）
    - 特征向量: 余弦

  聚类:
    - K-means: 欧氏（默认）
    - 文本聚类: 余弦

  推荐系统:
    - 用户-物品: 余弦
    - 协同过滤: Pearson相关
```

---

### 1.4 KL vs JS vs Wasserstein深度对比

**三大散度公式**:

$$
\begin{align}
\text{KL散度} &: \\
D_{KL}(P || Q) &= \sum_i P(i) \log \frac{P(i)}{Q(i)} = \mathbb{E}_P[\log \frac{P}{Q}] \\
\\
\text{JS散度} &: \\
D_{JS}(P, Q) &= \frac{1}{2}D_{KL}(P || M) + \frac{1}{2}D_{KL}(Q || M), \quad M = \frac{P + Q}{2} \\
\\
\text{Wasserstein距离} &: \\
W_p(P, Q) &= \inf_{\gamma \in \Gamma(P,Q)} \mathbb{E}_{(x,y) \sim \gamma}[||x - y||_p^p]^{1/p}
\end{align}
$$

| 维度 | KL散度 | JS散度 | Wasserstein距离 | 关键差异 |
|------|--------|--------|----------------|---------|
| **对称性** | ❌ 非对称 | ✅ 对称 | ✅ 对称 | KL特殊 |
| **值域** | [0, ∞) | [0, log2] | [0, ∞) | JS有界 |
| **退化** | $Q(i)=0, P(i)>0 \Rightarrow \infty$ | 鲁棒 | 鲁棒 | KL不稳定 |
| **几何意义** | 信息论（相对熵） | 信息论（对称化） | 最优传输 | 不同视角 |
| **可微性** | ⚠️ 不连续 | ⚠️ 不连续 | ✅✅ 光滑 | Wasserstein优势 |
| **计算** | 简单 | 简单 | 复杂（需优化） | 效率差异 |
| **应用** | VAE、信息论 | GAN变体 | **Wasserstein GAN** | 不同领域 |

**深度分析**:

```yaml
KL散度（Kullback-Leibler Divergence）:
  定义: D_KL(P||Q) = Σ P(i) log [P(i)/Q(i)]

  含义:
    - 用Q近似P的信息损失
    - 从P视角看Q的"距离"
    - 相对熵

  性质:
    - 非对称: D_KL(P||Q) ≠ D_KL(Q||P)
    - 非负: D_KL ≥ 0
    - D_KL=0 ⟺ P=Q

  问题:
    - Q(i)=0 但 P(i)>0 → ∞
    - 不满足三角不等式（非度量）
    - 不连续（P和Q不重叠时）

  应用:
    - VAE: KL(q(z|x)||p(z))
    - EM算法: 最大化似然
    - 信息论: 编码效率

JS散度（Jensen-Shannon Divergence）:
  定义: JS = 0.5·KL(P||M) + 0.5·KL(Q||M)
       M = (P+Q)/2

  改进:
    - 对称化KL
    - 有界: [0, log2]
    - 无无穷大问题

  性质:
    - 对称: JS(P,Q) = JS(Q,P)
    - 非负
    - 平方根是度量

  应用:
    - GAN变体（JS GAN）
    - 分布比较
    - 聚类

Wasserstein距离（Earth Mover's Distance）:
  定义: 从P"搬运"到Q的最小代价

  直观:
    - P是一堆土的分布
    - Q是一堆坑的分布
    - 最小搬运成本

  数学:
    W_p(P,Q) = inf_{γ} E_{(x,y)~γ}[||x-y||^p]^{1/p}
    γ: P和Q的联合分布（耦合）

  革命性优势（vs KL/JS）:
    1. 不重叠时仍有意义
       - P在x=0, Q在x=1
       - KL/JS: 突变、不连续
       - Wasserstein: W=1 光滑

    2. 考虑几何距离
       - KL/JS: 概率差异
       - Wasserstein: 几何结构
       → 更符合人类直觉

    3. 可微性
       - KL/JS: 不重叠时梯度消失
       - Wasserstein: 处处可微
       → GAN训练稳定

  Wasserstein GAN（2017）:
    - 用Wasserstein替代JS
    - 解决模式崩溃
    - 训练稳定性提升
    - 梯度不消失

  计算挑战:
    - 精确计算: NP-hard
    - Sinkhorn算法: 近似
    - 对偶形式: Kantorovich-Rubinstein
    - 实践: Lipschitz约束

三者比较示例:
  P: [1, 0, 0, 0]
  Q: [0, 0, 0, 1]
  （四个位置，P全在第1位，Q全在第4位）

  KL(P||Q): ∞ （Q在P支撑上为0）
  JS(P, Q): log2（最大）
  W_1(P, Q): 3 （搬运3个单位距离）

  → Wasserstein最合理

当前共识（2024）:
  - NLP: 余弦相似度主导（词嵌入）
  - GAN: Wasserstein距离主导（训练稳定）
  - VAE: KL散度主导（理论简洁）
  - 分布比较: JS或Wasserstein
```

---

### 1.5 核心洞察与终极评估

**五大核心定律**:

1. **余弦-方向定律**
   $$
   \cos(\theta) = \frac{x \cdot y}{||x|| \cdot ||y||} \quad \text{（忽略magnitude）}
   $$
   - 文本相似度首选

2. **归一化等价定律**
   $$
   ||x|| = ||y|| = 1 \Rightarrow d^2_{\text{Euclid}} = 2(1 - \cos(\theta))
   $$
   - 归一化后余弦≈欧氏

3. **KL非对称定律**
   $$
   D_{KL}(P||Q) \neq D_{KL}(Q||P)
   $$
   - 不是真正的"距离"

4. **Wasserstein光滑定律**
   $$
   \text{不重叠时} \quad W_1(P,Q) = \text{几何距离} \quad \text{（可微）}
   $$
   - GAN训练稳定性关键

5. **神经度量学习定律**
   $$
   \text{Triplet Loss}: ||f(a) - f(p)||^2 + \alpha < ||f(a) - f(n)||^2
   $$
   - 端到端学习最优度量

**终极洞察**:

> **"语义相似度度量是量化意义接近程度的核心工具。四大类别：①基于向量（余弦、欧氏、曼哈顿）：简单高效，余弦因不受magnitude影响成为文本首选②基于概率（KL、JS、Wasserstein）：KL非对称且退化问题，JS对称化but不连续，Wasserstein考虑几何距离且光滑，是GAN训练稳定性关键③基于路径（WordNet）：利用语义网络层次结构，人工构建but覆盖有限④基于神经网络（孪生、三元组、对比学习）：端到端学习最优度量，是当前主流。核心权衡：余弦vs欧氏（方向vs绝对距离）、KL vs Wasserstein（简单vs稳定）、人工设计vs学习获得（可解释vs性能）。数学基础：度量空间公理（非负、对称、三角不等式），内积空间（余弦），信息论（KL/JS），最优传输（Wasserstein）。应用层次：词级（Word2Vec余弦）、句级（Sentence-BERT）、文档级（TF-IDF+余弦）、跨模态（CLIP）。演化趋势：从人工设计（余弦、KL）到神经学习（孪生网络、对比学习），从单模态到多模态。Wasserstein革命（2017）：解决GAN模式崩溃和训练不稳定，通过考虑几何距离实现梯度光滑。当前主导：①文本：余弦相似度②GAN：Wasserstein距离③端到端：对比学习（InfoNCE）。选择指南：任务决定度量——检索用余弦、GAN用Wasserstein、聚类用欧氏（归一化后）。语义相似度度量是连接符号世界与数值计算的桥梁。"**

**元认知**:

- **核心分类**: 向量、概率、路径、神经
- **文本首选**: 余弦相似度（方向不受magnitude）
- **GAN关键**: Wasserstein距离（光滑可微）
- **当前趋势**: 神经学习度量（孪生、对比）
- **应用层次**: 词→句→文档→跨模态
- **选择原则**: 任务决定度量
- **未来方向**: 多模态、端到端学习

</details>

---

## 3 基于路径的相似度度量

这些方法基于**知识图谱**或**语义网络**（如WordNet）中的路径长度。

### 3.1 WordNet简介

**WordNet**：

- 英语词汇的语义网络
- 组织为**同义词集合**（Synsets）
- 通过**语义关系**（is-a, part-of等）连接

**参考文献**：

- [Wikipedia: WordNet](https://en.wikipedia.org/wiki/WordNet)
- [Miller, 1995](https://dl.acm.org/doi/10.1145/219717.219748) - WordNet: A Lexical Database for English

### 3.2 路径长度Path Length

**定义**：

```text
sim_path(c₁, c₂) = 1 / (1 + len(shortest_path(c₁, c₂)))
```

**例子**：

```text
WordNet层次：
  entity
    ├── living_thing
    │     ├── animal
    │     │     ├── mammal
    │     │     │     ├── cat
    │     │     │     └── dog
    │     └── plant

path(cat, dog) = 2  （经过mammal）
sim_path(cat, dog) = 1 / (1 + 2) = 0.33
```

**参考文献**：

- [Rada et al., 1989](https://ieeexplore.ieee.org/document/21465) - Development and Application of a Metric on Semantic Nets

### 3.3 Leacock-Chodorow相似度

**定义**：

```text
sim_LC(c₁, c₂) = -log(len(shortest_path(c₁, c₂)) / (2 × depth(taxonomy)))
```

**归一化**：

考虑分类层次的深度。

**参考文献**：

- [Leacock & Chodorow, 1998](https://aclanthology.org/J98-1006/) - Combining Local Context and WordNet Similarity

### 3.4 Wu-Palmer相似度

**定义**：

```text
sim_WP(c₁, c₂) = 2 × depth(LCS(c₁, c₂)) / (depth(c₁) + depth(c₂))
```

其中 LCS = Least Common Subsumer（最低公共祖先）。

**例子**：

```text
depth(cat) = 5
depth(dog) = 5
depth(LCS(cat, dog)) = depth(mammal) = 4

sim_WP(cat, dog) = 2 × 4 / (5 + 5) = 0.8
```

**参考文献**：

- [Wu & Palmer, 1994](https://aclanthology.org/P94-1019/) - Verbs Semantics and Lexical Selection

### 3.5 Resnik相似度

**基于信息内容**（Information Content）：

**定义**：

```text
IC(c) = -log P(c)
```

其中 P(c) 是概念 c 在语料库中出现的概率。

**Resnik相似度**：

```text
sim_Res(c₁, c₂) = IC(LCS(c₁, c₂))
```

**直觉**：

两个概念的共同祖先越具体（概率越小），它们越相似。

**参考文献**：

- [Resnik, 1995](https://arxiv.org/abs/cmp-lg/9511007) - Using Information Content to Evaluate Semantic Similarity

---

## 4 深度学习时代的相似度度量

### 4.1 学习的相似度函数

**核心思想**：

用神经网络**学习**相似度函数，而非手工定义。

**一般形式**：

```text
sim_θ(x, y) = f_θ(Enc(x), Enc(y))
```

其中：

- Enc：编码器（如BERT）
- f_θ：相似度计算（如MLP）
- θ：可学习参数

### 4.2 孪生网络Siamese Networks

**架构**：

```text
x₁ → Enc → 𝒗₁ ↘
                 cos(𝒗₁, 𝒗₂) → sim
x₂ → Enc → 𝒗₂ ↗
```

**共享编码器**：两个输入使用同一个编码器。

**训练目标**（对比损失）：

```text
L = ∑ [ y × d(𝒗₁, 𝒗₂)² + (1-y) × max(0, m - d(𝒗₁, 𝒗₂))² ]
```

其中：

- y = 1：相似对
- y = 0：不相似对
- m：边界

**参考文献**：

- [Wikipedia: Siamese Neural Network](https://en.wikipedia.org/wiki/Siamese_neural_network)
- [Bromley et al., 1993](https://proceedings.neurips.cc/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html) - Signature Verification using a Siamese Time Delay Neural Network

### 4.3 三元组损失Triplet Loss

**训练数据**：

```text
(anchor, positive, negative)
```

- anchor：锚点
- positive：与anchor相似
- negative：与anchor不相似

**目标**：

```text
d(anchor, positive) < d(anchor, negative)
```

**损失函数**：

```text
L = ∑ max(0, d(a, p) - d(a, n) + margin)
```

**参考文献**：

- [Schroff et al., 2015](https://arxiv.org/abs/1503.03832) - FaceNet: A Unified Embedding for Face Recognition

### 4.4 对比学习Contrastive Learning

**核心思想**：

拉近**正样本对**，推开**负样本对**。

**InfoNCE损失**：

```text
L = -log(exp(sim(x, x⁺) / τ) / ∑_{x⁻} exp(sim(x, x⁻) / τ))
```

其中：

- x⁺：正样本
- x⁻：负样本
- τ：温度参数

**应用**：

- SimCLR（视觉）
- SimCSE（文本）

**参考文献**：

- [Chen et al., 2020](https://arxiv.org/abs/2002.05709) - A Simple Framework for Contrastive Learning of Visual Representations
- [Gao et al., 2021](https://arxiv.org/abs/2104.08821) - SimCSE: Simple Contrastive Learning of Sentence Embeddings

---

## 5 句子和文档级别的相似度

### 5.1 平均词向量Averaged Word Embeddings

**最简单方法**：

```text
vec(sentence) = (1/n) ∑ᵢ vec(wᵢ)
```

**改进：加权平均**（如TF-IDF权重）

```text
vec(sentence) = ∑ᵢ weight(wᵢ) × vec(wᵢ)
```

**优势**：

- ✅ 简单、快速

**劣势**：

- ❌ 丢失词序信息
- ❌ 忽略句法结构

### 5.2 Sentence-BERT

**核心思想**：

用孪生BERT网络学习句子嵌入。

**架构**：

```text
sentence → BERT → [CLS] token → 𝒗
```

**训练**：

使用自然语言推理（NLI）数据集：

```text
(premise, hypothesis, label)
label ∈ {entailment, contradiction, neutral}
```

**优势**：

- ✅ 高质量句子嵌入
- ✅ 快速推理（预计算嵌入后只需计算余弦相似度）

**参考文献**：

- [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) - Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

### 5.3 BERTScore

**核心思想**：

用BERT的上下文嵌入计算token级别的相似度，然后聚合。

**步骤**：

1. 对两个句子分别用BERT编码
2. 计算每对token的余弦相似度
3. 用贪心匹配或最优匹配聚合

**公式**：

```text
Precision = (1/|𝒙|) ∑ᵢ max_j cos(𝒙ᵢ, 𝒚ⱼ)
Recall = (1/|𝒚|) ∑ⱼ max_i cos(𝒙ᵢ, 𝒚ⱼ)
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**优势**：

- ✅ 考虑上下文
- ✅ 与人类判断高度相关

**参考文献**：

- [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) - BERTScore: Evaluating Text Generation with BERT

### 5.4 文档相似度：TF-IDF 余弦

**经典方法**：

1. 将文档表示为TF-IDF向量
2. 计算余弦相似度

```text
𝒅₁, 𝒅₂ ∈ ℝ|V|
sim(𝒅₁, 𝒅₂) = cos(𝒅₁, 𝒅₂)
```

**优势**：

- ✅ 简单、高效
- ✅ 可扩展到大规模文档集

**参考文献**：

- [Wikipedia: TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

---

## 6 跨模态相似度度量

### 6.1 图像-文本相似度

**目标**：

度量图像和文本描述的相似度。

**方法：CLIP（Contrastive Language-Image Pre-training）**-

**架构**：

```text
Image → Image Encoder → 𝒗_img ↘
                                 cos(𝒗_img, 𝒗_txt)
Text → Text Encoder → 𝒗_txt ↗
```

**训练**：

对比学习，匹配的图像-文本对相似度高。

**应用**：

- 图像检索
- 图像生成（如DALL-E）

**参考文献**：

- [Radford et al., 2021](https://arxiv.org/abs/2103.00020) - Learning Transferable Visual Models From Natural Language Supervision

### 6.2 多模态相似度

**一般框架**：

将不同模态映射到**统一的语义空间**：

```text
Text → Enc_text → 𝒛 ∈ ℝᵈ
Image → Enc_img → 𝒛 ∈ ℝᵈ
Audio → Enc_audio → 𝒛 ∈ ℝᵈ

sim(x₁, x₂) = cos(𝒛₁, 𝒛₂)
```

**参考文献**：

- [Wikipedia: Multimodal Learning](https://en.wikipedia.org/wiki/Multimodal_learning)

---

## 7 相似度度量的评估

### 7.1 内在评估Intrinsic Evaluation

#### 1.1.1 人类判断相关性

**数据集**：

- **SimLex-999**：999对词，带人类相似度评分
- **WordSim-353**：353对词，带人类相似度评分

**评估指标**：

```text
Spearman相关系数 = 模型相似度排序 与 人类相似度排序的相关性
```

**参考文献**：

- [Hill et al., 2015](https://arxiv.org/abs/1408.3456) - SimLex-999: Evaluating Semantic Models

#### 1.1.2 类比任务

**例子**：

```text
king - man + woman ≈ ?
```

**评估**：

模型能否正确回答"queen"。

**参考文献**：

- [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Linguistic Regularities in Continuous Space

### 7.2 外在评估Extrinsic Evaluation

**下游任务表现**：

- 文本分类
- 信息检索
- 问答系统
- 机器翻译

**原则**：

> **好的相似度度量应该提升下游任务的性能。**

---

## 8 总结

### 8.1 核心要点

1. **数学基础**：相似度 vs 距离、内积空间
2. **基于向量**：余弦相似度（最常用）、欧几里得距离、Jaccard
3. **基于概率**：KL散度、JS散度、Wasserstein距离
4. **基于路径**：WordNet、路径长度、信息内容
5. **深度学习**：孪生网络、三元组损失、对比学习
6. **句子级别**：平均词向量、Sentence-BERT、BERTScore
7. **跨模态**：CLIP、多模态统一空间
8. **评估**：人类判断相关性、类比任务、下游任务

### 8.2 选择指南

| 场景 | 推荐度量 | 原因 |
|------|---------|------|
| **词级别** | 余弦相似度（Word2Vec/GloVe） | 标准方法 |
| **句子级别** | Sentence-BERT | 高质量、快速 |
| **语义相似度（上下文敏感）** | BERT嵌入 + 余弦 | 考虑上下文 |
| **文档级别** | TF-IDF + 余弦 | 简单高效 |
| **概念相似度** | WordNet路径方法 | 利用知识图谱 |
| **生成文本评估** | BERTScore | 与人类判断相关 |
| **跨模态** | CLIP | 图像-文本对齐 |

### 8.3 未来方向

1. **可解释相似度**：理解为什么两个对象相似
2. **多粒度相似度**：词、短语、句子、文档的统一框架
3. **个性化相似度**：根据用户偏好调整相似度度量
4. **对抗鲁棒性**：相似度度量对对抗攻击的鲁棒性

---

## 参考文献

### 8.4 综述

1. [Wikipedia: Semantic Similarity](https://en.wikipedia.org/wiki/Semantic_similarity)
2. [Gomaa & Fahmy, 2013](https://arxiv.org/abs/1310.8059) - A Survey of Text Similarity Approaches

### 8.5 数学基础

1. [Wikipedia: Metric Space](https://en.wikipedia.org/wiki/Metric_space)
2. [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
3. [Wikipedia: Inner Product Space](https://en.wikipedia.org/wiki/Inner_product_space)

### 8.6 基于路径

1. [Wikipedia: WordNet](https://en.wikipedia.org/wiki/WordNet)
2. [Miller, 1995](https://dl.acm.org/doi/10.1145/219717.219748) - WordNet: A Lexical Database for English
3. [Resnik, 1995](https://arxiv.org/abs/cmp-lg/9511007) - Using Information Content to Evaluate Semantic Similarity

### 8.7 深度学习

1. [Schroff et al., 2015](https://arxiv.org/abs/1503.03832) - FaceNet: A Unified Embedding
2. [Chen et al., 2020](https://arxiv.org/abs/2002.05709) - SimCLR
3. [Gao et al., 2021](https://arxiv.org/abs/2104.08821) - SimCSE

### 8.8 句子级别

1. [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) - Sentence-BERT
2. [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) - BERTScore

### 8.9 跨模态

1. [Radford et al., 2021](https://arxiv.org/abs/2103.00020) - CLIP

### 8.10 评估

1. [Hill et al., 2015](https://arxiv.org/abs/1408.3456) - SimLex-999

---

_本文档全面梳理了语义相似度度量的理论基础、具体方法和评估标准，为理解和选择合适的相似度度量提供了系统指南。_

---

## 导航 | Navigation

**上一篇**: [← 04.3 分布式语义](./04.3_Distributional_Semantics.md)
**下一篇**: [04.5 多模态语义整合 →](./04.5_Multimodal_Semantic_Integration.md)
**返回目录**: [↑ AI模型视角总览](../README.md)

---

## 相关主题 | Related Topics

### 8.11 本章节

- [04.1 语义向量空间](./04.1_Semantic_Vector_Spaces.md)
- [04.2 连续表示理论](./04.2_Continuous_Representation_Theory.md)
- [04.3 分布式语义](./04.3_Distributional_Semantics.md)
- [04.5 多模态语义整合](./04.5_Multimodal_Semantic_Integration.md)
- [04.6 黄氏语义模型分析](./04.6_Huang_Semantic_Model_Analysis.md)

### 8.12 相关章节

- [03.5 嵌入向量空间](../03_Language_Models/03.5_Embedding_Vector_Spaces.md)

### 8.13 跨视角链接

- [Information_Theory_Perspective: 距离度量](../../Information_Theory_Perspective/README.md)
- [概念交叉索引（七视角版）](../../CONCEPT_CROSS_INDEX.md) - 查看相关概念的七视角分析：
  - [互信息](../../CONCEPT_CROSS_INDEX.md#111-互信息-mutual-information-七视角) - 语义相似度的信息论度量
  - [熵](../../CONCEPT_CROSS_INDEX.md#71-熵-entropy-七视角) - 相似度分布的不确定性
  - [DIKWP模型](../../CONCEPT_CROSS_INDEX.md#61-dikwp模型-七视角) - 语义相似度的层次性
