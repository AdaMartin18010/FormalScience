# 语义向量空间（Semantic Vector Spaces）

## 目录 | Table of Contents

- [语义向量空间（Semantic Vector Spaces）](#语义向量空间semantic-vector-spaces)
- [目录](#目录)
- [引言](#引言)
  - [核心思想](#核心思想)
  - [关键问题](#关键问题)
- [从符号到向量：表示的范式转换](#从符号到向量表示的范式转换)
  - [传统符号表示](#传统符号表示)
  - [One-Hot表示](#one-hot表示)
  - [分布式表示（Distributed Representation）](#分布式表示distributed-representation)
  - [范式转换的本质](#范式转换的本质)
- [向量空间模型的数学基础](#向量空间模型的数学基础)
  - [向量空间的定义](#向量空间的定义)
  - [内积与范数](#内积与范数)
  - [距离与相似度度量](#距离与相似度度量)
    - [1. 欧几里得距离](#1-欧几里得距离)
    - [2. 余弦相似度](#2-余弦相似度)
    - [3. 余弦距离](#3-余弦距离)
- [语义向量空间的几何结构](#语义向量空间的几何结构)
  - [语义向量空间的定义](#语义向量空间的定义)
  - [几何性质](#几何性质)
    - [1. 聚类性（Clustering）](#1-聚类性clustering)
    - [2. 线性性（Linearity）](#2-线性性linearity)
    - [3. 可分性（Separability）](#3-可分性separability)
  - [拓扑结构](#拓扑结构)
    - [1. 流形结构（Manifold Structure）](#1-流形结构manifold-structure)
    - [2. 曲率（Curvature）](#2-曲率curvature)
- [语义向量空间的构建方法](#语义向量空间的构建方法)
  - [1. 基于计数的方法（Count-based）](#1-基于计数的方法count-based)
    - [术语-文档矩阵（Term-Document Matrix）](#术语-文档矩阵term-document-matrix)
    - [词-上下文矩阵（Word-Context Matrix）](#词-上下文矩阵word-context-matrix)
  - [2. 基于预测的方法（Prediction-based）](#2-基于预测的方法prediction-based)
    - [Word2Vec](#word2vec)
    - [GloVe（Global Vectors）](#gloveglobal-vectors)
  - [3. 基于神经网络的方法（Neural-based）](#3-基于神经网络的方法neural-based)
    - [上下文化表示（Contextualized Representations）](#上下文化表示contextualized-representations)
- [语义向量空间的性质](#语义向量空间的性质)
  - [1. 分布假设（Distributional Hypothesis）](#1-分布假设distributional-hypothesis)
  - [2. 组合性（Compositionality）](#2-组合性compositionality)
  - [3. 可学习性（Learnability）](#3-可学习性learnability)
- [语义向量空间的维度问题](#语义向量空间的维度问题)
  - [维度的选择](#维度的选择)
  - [维度的影响](#维度的影响)
  - [内在维度（Intrinsic Dimensionality）](#内在维度intrinsic-dimensionality)
- [语义向量空间的局限性](#语义向量空间的局限性)
  - [1. 静态性（Static Embeddings）](#1-静态性static-embeddings)
  - [2. 偏见与公平性（Bias and Fairness）](#2-偏见与公平性bias-and-fairness)
  - [3. 可解释性（Interpretability）](#3-可解释性interpretability)
  - [4. 计算成本（Computational Cost）](#4-计算成本computational-cost)
- [总结](#总结)
  - [核心要点](#核心要点)
  - [语义向量空间的意义](#语义向量空间的意义)
  - [未解问题](#未解问题)
- [参考文献](#参考文献)
  - [基础理论](#基础理论)
  - [词向量](#词向量)
  - [上下文化表示](#上下文化表示)
  - [几何与拓扑](#几何与拓扑)
  - [偏见与公平性](#偏见与公平性)
  - [教材](#教材)

---

## 引言

**语义向量空间**（Semantic Vector Space）是现代AI，特别是大语言模型的核心表示范式。它将离散的符号（词、句子、图像等）映射到连续的高维向量空间中，使得**语义上相似的对象在几何上相近**。

### 核心思想

> **将"意义"（Meaning）编码为"位置"（Position）：语义不再是符号的属性，而是向量空间中的几何关系。**

### 关键问题

1. **表示论问题**：什么是"意义"？如何用向量表示意义？
2. **几何问题**：向量空间的几何结构如何反映语义结构？
3. **学习问题**：如何从数据中学习到有意义的向量表示？
4. **哲学问题**：向量表示真的"理解"了语义，还是只是统计关联？

**参考文献**：

- [Wikipedia: Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)
- [Wikipedia: Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)

---

## 从符号到向量：表示的范式转换

### 传统符号表示

**符号主义AI**（Symbolic AI）将意义表示为**离散符号及其关系**：

```text
cat ⊑ animal  （猫是动物的子类）
∀x (cat(x) → animal(x))  （所有猫都是动物）
```

**特点**：

- ✅ **精确**：逻辑关系明确
- ✅ **可解释**：推理过程透明
- ❌ **脆弱**：难以处理模糊性和噪声
- ❌ **知识获取瓶颈**：需要手工编写规则

**参考文献**：

- [Wikipedia: Symbolic AI](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)
- [Russell & Norvig, 2020](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach) - Artificial Intelligence: A Modern Approach

### One-Hot表示

**最简单的向量表示**：每个词对应一个维度，词汇表大小为 |V|

```text
cat  = [1, 0, 0, 0, ..., 0]  ∈ ℝ|V|
dog  = [0, 1, 0, 0, ..., 0]  ∈ ℝ|V|
animal = [0, 0, 1, 0, ..., 0]  ∈ ℝ|V|
```

**问题**：

- ❌ **维度灾难**：|V| 通常是 10⁴ ~ 10⁵
- ❌ **稀疏性**：每个向量只有一个非零元素
- ❌ **无语义**：任意两个词的余弦相似度都是 0

```text
cos(cat, dog) = cos(cat, animal) = 0
```

即：cat与dog的"距离"等于cat与animal的"距离"，这显然不合理。

**参考文献**：

- [Wikipedia: One-Hot Encoding](https://en.wikipedia.org/wiki/One-hot)

### 分布式表示（Distributed Representation）

**核心思想**：将每个词表示为**低维稠密向量**（通常 d = 100 ~ 4096）

```text
cat    = [0.2, -0.5, 0.8, ..., 0.3]  ∈ ℝᵈ
dog    = [0.3, -0.4, 0.7, ..., 0.2]  ∈ ℝᵈ
animal = [0.25, -0.45, 0.75, ..., 0.25]  ∈ ℝᵈ
```

**关键性质**：

```text
cos(cat, dog) ≈ 0.85  （高）
cos(cat, animal) ≈ 0.78  （中）
cos(cat, apple) ≈ 0.12  （低）
```

**优势**：

- ✅ **低维**：d ≪ |V|
- ✅ **稠密**：所有维度都有意义
- ✅ **语义**：相似词在向量空间中相近

**参考文献**：

- [Hinton et al., 1986](https://en.wikipedia.org/wiki/Connectionism) - Learning Distributed Representations of Concepts
- [Wikipedia: Distributed Representation](https://en.wikipedia.org/wiki/Distributed_representation)

### 范式转换的本质

| 维度 | 符号表示 | 向量表示 | 参考文献 |
|------|---------|----------|----------|
| **表示方式** | 离散符号 | 连续向量 | [Bengio et al., 2013](https://arxiv.org/abs/1206.5533) |
| **语义编码** | 逻辑关系 | 几何关系 | [Turney & Pantel, 2010](https://www.jair.org/index.php/jair/article/view/10640) |
| **相似度** | 人工定义 | 自动学习 | [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) |
| **泛化能力** | 弱 | 强 | [Goodfellow et al., 2016](https://www.deeplearningbook.org/) |
| **可解释性** | 强 | 弱 | [Lipton, 2018](https://arxiv.org/abs/1606.03490) |

---

## 向量空间模型的数学基础

### 向量空间的定义

**定义（向量空间）**：

一个**向量空间** V 是一个集合，配有两个运算：

1. **向量加法**：𝒖 + 𝒗 ∈ V
2. **标量乘法**：α 𝒖 ∈ V （α ∈ ℝ）

满足以下**八条公理**：

1. 加法交换律：𝒖 + 𝒗 = 𝒗 + 𝒖
2. 加法结合律：(𝒖 + 𝒗) + 𝒘 = 𝒖 + (𝒗 + 𝒘)
3. 加法零元：∃ 𝟎, 𝒖 + 𝟎 = 𝒖
4. 加法逆元：∃ -𝒖, 𝒖 + (-𝒖) = 𝟎
5. 乘法单位元：1 · 𝒖 = 𝒖
6. 乘法结合律：α(β𝒖) = (αβ)𝒖
7. 分配律1：α(𝒖 + 𝒗) = α𝒖 + α𝒗
8. 分配律2：(α + β)𝒖 = α𝒖 + β𝒖

**AI中的向量空间**：通常是 ℝᵈ（d维欧几里得空间）

**参考文献**：

- [Wikipedia: Vector Space](https://en.wikipedia.org/wiki/Vector_space)
- [Strang, 2016](https://en.wikipedia.org/wiki/Introduction_to_Linear_Algebra) - Introduction to Linear Algebra

### 内积与范数

**定义（内积）**：

一个**内积**是函数 ⟨·,·⟩ : V × V → ℝ，满足：

1. **对称性**：⟨𝒖, 𝒗⟩ = ⟨𝒗, 𝒖⟩
2. **线性**：⟨α𝒖 + β𝒗, 𝒘⟩ = α⟨𝒖, 𝒘⟩ + β⟨𝒗, 𝒘⟩
3. **正定性**：⟨𝒖, 𝒖⟩ ≥ 0，且 ⟨𝒖, 𝒖⟩ = 0 ⟺ 𝒖 = 𝟎

**欧几里得内积**：

```text
⟨𝒖, 𝒗⟩ = 𝒖 · 𝒗 = ∑ᵢ uᵢvᵢ
```

**定义（范数）**：

由内积导出的**范数**（长度）：

```text
‖𝒖‖ = √⟨𝒖, 𝒖⟩
```

对于欧几里得空间：

```text
‖𝒖‖₂ = √(∑ᵢ uᵢ²)  （L₂范数）
```

**参考文献**：

- [Wikipedia: Inner Product Space](https://en.wikipedia.org/wiki/Inner_product_space)
- [Wikipedia: Norm (mathematics)](https://en.wikipedia.org/wiki/Norm_(mathematics))

### 距离与相似度度量

#### 1. 欧几里得距离

```text
d(𝒖, 𝒗) = ‖𝒖 - 𝒗‖₂ = √(∑ᵢ (uᵢ - vᵢ)²)
```

**特点**：

- ✅ 直观的"几何距离"
- ❌ 受向量长度影响

#### 2. 余弦相似度

```text
cos(𝒖, 𝒗) = ⟨𝒖, 𝒗⟩ / (‖𝒖‖ ‖𝒗‖)
```

**值域**：[-1, 1]

- cos(𝒖, 𝒗) = 1：完全相同方向
- cos(𝒖, 𝒗) = 0：正交（无关）
- cos(𝒖, 𝒗) = -1：完全相反方向

**特点**：

- ✅ 不受向量长度影响（只看方向）
- ✅ 符合人类对"相似度"的直觉
- ✅ **AI中最常用的相似度度量**

#### 3. 余弦距离

```text
d_cos(𝒖, 𝒗) = 1 - cos(𝒖, 𝒗)
```

**值域**：[0, 2]

**参考文献**：

- [Wikipedia: Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
- [Wikipedia: Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance)

---

## 语义向量空间的几何结构

### 语义向量空间的定义

**定义（语义向量空间）**：

一个**语义向量空间** 𝕍 是一个三元组 (ℝᵈ, Enc, Sem)，其中：

- **ℝᵈ**：d维欧几里得空间（几何空间）
- **Enc : Σ → ℝᵈ**：编码器，将符号映射到向量
- **Sem**：语义关系集合，定义了符号间的语义关系

**关键要求**：

> **几何关系 ≈ 语义关系**

形式化：对于语义关系 R(a, b)，应有：

```text
R(a, b) ⇒ d(Enc(a), Enc(b)) 较小
¬R(a, b) ⇒ d(Enc(a), Enc(b)) 较大
```

**参考文献**：

- [Turney & Pantel, 2010](https://www.jair.org/index.php/jair/article/view/10640) - From Frequency to Meaning

### 几何性质

#### 1. 聚类性（Clustering）

**语义相似的词聚集在一起**：

```text
{cat, dog, rabbit, ...} ← 动物聚类
{red, blue, green, ...} ← 颜色聚类
{run, walk, jump, ...}  ← 动作聚类
```

**数学表述**：

设 C 是一个语义类别，则：

```text
∀a, b ∈ C : E[d(Enc(a), Enc(b))] < E[d(Enc(a), Enc(x))]
                                   x ∉ C
```

#### 2. 线性性（Linearity）

**语义关系可以用向量运算表示**：

经典例子（Word2Vec）：

```text
king - man + woman ≈ queen
```

更一般地：

```text
vec(Paris) - vec(France) ≈ vec(Berlin) - vec(Germany)
```

**数学表述**：

对于关系 R : A → B，存在向量 𝒓 ∈ ℝᵈ，使得：

```text
R(a, b) ⇒ Enc(b) ≈ Enc(a) + 𝒓
```

#### 3. 可分性（Separability）

**不同语义类别线性可分**：

对于二元分类任务（如情感分析：正面/负面），存在超平面 𝒘 ∈ ℝᵈ，使得：

```text
⟨𝒘, Enc(x)⟩ > 0  ⇔  x ∈ Positive
⟨𝒘, Enc(x)⟩ < 0  ⇔  x ∈ Negative
```

**参考文献**：

- [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Linguistic Regularities in Continuous Space
- [Wikipedia: Linear Separability](https://en.wikipedia.org/wiki/Linear_separability)

### 拓扑结构

语义向量空间不仅是度量空间，还具有**拓扑结构**：

#### 1. 流形结构（Manifold Structure）

**假设**：高维语义向量空间实际上是**低维流形嵌入到高维空间**。

```text
真实语义空间 M ⊂ ℝᵈ  （M 是低维流形）
内在维度 ≪ 嵌入维度 d
```

**直觉**：虽然嵌入在 d=768 维空间，但实际"自由度"可能只有 10~100 维。

**参考文献**：

- [Wikipedia: Manifold](https://en.wikipedia.org/wiki/Manifold)
- [Fefferman et al., 2016](https://www.pnas.org/doi/full/10.1073/pnas.1408993113) - Testing the Manifold Hypothesis

#### 2. 曲率（Curvature）

最近研究表明，语义空间可能具有**负曲率**（双曲几何）：

**双曲几何的优势**：

- ✅ 更适合表示**层次结构**（如 WordNet）
- ✅ 在相同维度下有**指数级更大的容量**

**参考文献**：

- [Nickel & Kiela, 2017](https://arxiv.org/abs/1705.08039) - Poincaré Embeddings for Learning Hierarchical Representations
- [Wikipedia: Hyperbolic Geometry](https://en.wikipedia.org/wiki/Hyperbolic_geometry)

---

## 语义向量空间的构建方法

### 1. 基于计数的方法（Count-based）

#### 术语-文档矩阵（Term-Document Matrix）

**定义**：

```text
X ∈ ℝ|V|×|D|
Xᵢⱼ = 词 wᵢ 在文档 dⱼ 中出现的次数
```

**降维方法**：

- **奇异值分解（SVD）**：X ≈ U Σ Vᵀ
- **潜在语义分析（LSA）**：取 U 的前 k 列作为词向量

**参考文献**：

- [Wikipedia: Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
- [Deerwester et al., 1990](https://en.wikipedia.org/wiki/Latent_semantic_analysis) - Indexing by Latent Semantic Analysis

#### 词-上下文矩阵（Word-Context Matrix）

**定义**：

```text
X ∈ ℝ|V|×|V|
Xᵢⱼ = 词 wᵢ 与词 wⱼ 在窗口内共现的次数
```

**变体**：

- **原始计数**：Xᵢⱼ = count(wᵢ, wⱼ)
- **点互信息（PMI）**：

```text
PMI(wᵢ, wⱼ) = log P(wᵢ, wⱼ) / (P(wᵢ) P(wⱼ))
```

- **正点互信息（PPMI）**：

```text
PPMI(wᵢ, wⱼ) = max(0, PMI(wᵢ, wⱼ))
```

**参考文献**：

- [Wikipedia: Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information)

### 2. 基于预测的方法（Prediction-based）

#### Word2Vec

**核心思想**：通过**预测上下文**或**从上下文预测目标词**来学习向量。

**两种架构**：

1. **CBOW**（Continuous Bag-of-Words）：

    ```text
    P(wₜ | wₜ₋ₙ, ..., wₜ₋₁, wₜ₊₁, ..., wₜ₊ₙ)
    ```

    从上下文预测中心词

2. **Skip-Gram**：

    ```text
    P(wₜ₋ₙ, ..., wₜ₋₁, wₜ₊₁, ..., wₜ₊ₙ | wₜ)
    ```

从中心词预测上下文

**参考文献**：

- [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Efficient Estimation of Word Representations in Vector Space
- [Wikipedia: Word2vec](https://en.wikipedia.org/wiki/Word2vec)

#### GloVe（Global Vectors）

**核心思想**：结合全局统计信息（共现矩阵）和局部预测（Word2Vec）。

**目标函数**：

```text
J = ∑ᵢⱼ f(Xᵢⱼ) (𝒖ᵢᵀ 𝒗ⱼ + bᵢ + cⱼ - log Xᵢⱼ)²
```

其中：

- Xᵢⱼ：词 wᵢ 和 wⱼ 的共现次数
- f(x)：权重函数，削弱高频词的影响

**参考文献**：

- [Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf) - GloVe: Global Vectors for Word Representation
- [Wikipedia: GloVe](https://en.wikipedia.org/wiki/GloVe)

### 3. 基于神经网络的方法（Neural-based）

#### 上下文化表示（Contextualized Representations）

**传统方法的局限**：每个词只有**一个固定向量**，无法处理**一词多义**。

**解决方案**：**上下文相关的向量表示**

```text
vec("bank") 在 "river bank" 中 ≠ vec("bank") 在 "bank account" 中
```

**代表模型**：

1. **ELMo**（Embeddings from Language Models）：

    ```text
    𝒉ᵢ = BiLSTM(w₁, ..., wₙ)[i]
    ```

2. **BERT**（Bidirectional Encoder Representations from Transformers）：

    ```text
    𝒉ᵢ = Transformer(w₁, ..., wₙ)[i]
    ```

3. **GPT**（Generative Pre-trained Transformer）：

    ```text
    𝒉ᵢ = TransformerDecoder(w₁, ..., wᵢ)[i]
    ```

**参考文献**：

- [Peters et al., 2018](https://arxiv.org/abs/1802.05365) - Deep Contextualized Word Representations
- [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) - BERT
- [Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) - Improving Language Understanding

---

## 语义向量空间的性质

### 1. 分布假设（Distributional Hypothesis）

**核心原理**：

> **"A word is characterized by the company it keeps."**
>
> **"词的意义由其上下文决定。"**
>
> — J. R. Firth (1957)

**形式化**：

```text
Sem(w₁) ≈ Sem(w₂)  ⟺  Context(w₁) ≈ Context(w₂)
```

**例子**：

```text
"cat" 和 "dog" 经常出现在相似上下文中：
  - "I have a ___ as a pet."
  - "The ___ is sleeping."
  - "Feed the ___."

因此，vec(cat) ≈ vec(dog)
```

**参考文献**：

- [Firth, 1957](https://en.wikipedia.org/wiki/Distributional_semantics) - A Synopsis of Linguistic Theory
- [Harris, 1954](https://www.jstor.org/stable/411805) - Distributional Structure

### 2. 组合性（Compositionality）

**问题**：如何从词向量得到句子向量？

**简单方法**：

1. **平均**：

    ```text
    vec(sentence) = (1/n) ∑ᵢ vec(wᵢ)
    ```

2. **加权平均**（如 TF-IDF 权重）

**高级方法**：

1. **RNN/LSTM**：

    ```text
    𝒉ₜ = LSTM(𝒉ₜ₋₁, vec(wₜ))
    vec(sentence) = 𝒉ₙ
    ```

2. **Transformer**：

    ```text
    𝒉₁, ..., 𝒉ₙ = Transformer(vec(w₁), ..., vec(wₙ))
    vec(sentence) = 𝒉₁  （或平均）
    ```

**参考文献**：

- [Wikipedia: Principle of Compositionality](https://en.wikipedia.org/wiki/Principle_of_compositionality)

### 3. 可学习性（Learnability）

**关键问题**：从有限数据中学习到的向量能否泛化？

**理论保证**：

- **Johnson-Lindenstrauss引理**：高维向量可以近似保距地投影到低维空间
- **随机投影**：随机初始化的向量经过训练可以学习到有意义的表示

**经验发现**：

- ✅ 大规模语料（如10亿词）可以学习到高质量的词向量
- ✅ 预训练模型可以迁移到下游任务

**参考文献**：

- [Wikipedia: Johnson-Lindenstrauss Lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)

---

## 语义向量空间的维度问题

### 维度的选择

**经验法则**：

| 应用场景 | 典型维度 | 参考模型 |
|---------|---------|----------|
| 词向量（Word2Vec, GloVe） | 50 ~ 300 | [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) |
| 句子向量（Sentence-BERT） | 384 ~ 768 | [Reimers & Gurevych, 2019](https://arxiv.org/abs/1908.10084) |
| BERT-base | 768 | [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) |
| BERT-large | 1024 | [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) |
| GPT-3 | 12288 | [Brown et al., 2020](https://arxiv.org/abs/2005.14165) |

### 维度的影响

**维度太低**：

- ❌ 表达能力不足，难以区分细微语义差异
- ❌ 类比关系（如 king - man + woman ≈ queen）精度下降

**维度太高**：

- ❌ 过拟合风险
- ❌ 计算和存储成本增加
- ❌ **维度灾难**（Curse of Dimensionality）

### 内在维度（Intrinsic Dimensionality）

**观察**：虽然嵌入维度 d 很高（如768），但**实际有效维度可能远小于 d**。

**内在维度估计方法**：

1. **PCA分析**：看前 k 个主成分解释了多少方差
2. **局部维度估计**：估计流形的局部维度

**经验发现**：

- BERT的768维向量的内在维度约为 100~200
- 说明存在大量冗余

**参考文献**：

- [Wikipedia: Intrinsic Dimension](https://en.wikipedia.org/wiki/Intrinsic_dimension)

---

## 语义向量空间的局限性

### 1. 静态性（Static Embeddings）

**问题**：传统Word2Vec/GloVe为每个词分配**固定向量**，无法处理：

- **一词多义**（Polysemy）：

```text
"bank" 在不同上下文中有不同含义
```

- **上下文依赖**：

```text
"good" 在 "good food" 和 "good enough" 中含义不同
```

**解决方案**：上下文化表示（ELMo, BERT, GPT）

### 2. 偏见与公平性（Bias and Fairness）

**问题**：向量空间会**继承训练数据中的社会偏见**。

**例子**：

```text
vec(programmer) - vec(man) + vec(woman) ≈ vec(homemaker)
```

**原因**：训练语料中的性别刻板印象被编码到向量中。

**缓解方法**：

1. **去偏置算法**：调整向量使其在性别方向上正交
2. **对抗训练**：让模型无法从向量中预测敏感属性
3. **数据平衡**：使用更平衡的训练数据

**参考文献**：

- [Bolukbasi et al., 2016](https://arxiv.org/abs/1607.06520) - Man is to Computer Programmer as Woman is to Homemaker?
- [Wikipedia: Algorithmic Bias](https://en.wikipedia.org/wiki/Algorithmic_bias)

### 3. 可解释性（Interpretability）

**问题**：向量的**各个维度没有明确语义**。

**例子**：

```text
vec(cat)[42] = 0.73  ← 这个0.73代表什么？
```

**尝试**：

- **可解释维度**：某些维度似乎对应特定语义（如性别、时态）
- **探测任务**（Probing Tasks）：训练分类器看向量是否编码了特定信息

**参考文献**：

- [Belinkov & Glass, 2019](https://arxiv.org/abs/1812.08951) - Analysis Methods in Neural Language Processing

### 4. 计算成本（Computational Cost）

**问题**：高维向量的运算成本高昂。

| 操作 | 复杂度 | 场景 |
|------|--------|------|
| 向量点积 | O(d) | 相似度计算 |
| 矩阵乘法 | O(nd²) | Transformer |
| 最近邻搜索 | O(Nd) | 向量检索 |

**解决方案**：

1. **量化**（Quantization）：减少浮点精度（FP32 → FP16 → INT8）
2. **稀疏化**（Sparsification）：大部分维度置零
3. **近似最近邻**（ANN）：用索引结构加速检索（如 FAISS）

**参考文献**：

- [Wikipedia: Nearest Neighbor Search](https://en.wikipedia.org/wiki/Nearest_neighbor_search)
- [Johnson et al., 2019](https://arxiv.org/abs/1702.08734) - Billion-Scale Similarity Search with GPUs

---

## 总结

### 核心要点

1. **范式转换**：从符号到向量，从离散到连续
2. **分布假设**：词的意义由其上下文决定
3. **几何结构**：语义关系 ⇔ 几何关系
4. **学习方法**：计数法、预测法、神经网络法
5. **上下文化**：从静态向量到动态向量
6. **局限性**：偏见、不可解释、计算成本

### 语义向量空间的意义

> **语义向量空间是AI从符号主义到连接主义转变的核心载体。它将"意义"从离散的逻辑命题转化为连续的几何对象，使得机器学习方法能够直接处理语义。**

### 未解问题

1. **理论**：为什么向量表示能够捕捉语义？
2. **哲学**：向量相似度真的代表"理解"吗？
3. **技术**：如何构建更好的语义空间（如双曲空间、量子空间）？
4. **伦理**：如何消除向量中的偏见？

---

## 参考文献

### 基础理论

1. [Wikipedia: Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model)
2. [Wikipedia: Distributional Semantics](https://en.wikipedia.org/wiki/Distributional_semantics)
3. [Wikipedia: Vector Space](https://en.wikipedia.org/wiki/Vector_space)
4. [Turney & Pantel, 2010](https://www.jair.org/index.php/jair/article/view/10640) - From Frequency to Meaning: Vector Space Models of Semantics

### 词向量

1. [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) - Efficient Estimation of Word Representations in Vector Space
2. [Pennington et al., 2014](https://nlp.stanford.edu/pubs/glove.pdf) - GloVe: Global Vectors for Word Representation
3. [Wikipedia: Word2vec](https://en.wikipedia.org/wiki/Word2vec)
4. [Wikipedia: GloVe](https://en.wikipedia.org/wiki/GloVe)

### 上下文化表示

1. [Peters et al., 2018](https://arxiv.org/abs/1802.05365) - Deep Contextualized Word Representations (ELMo)
2. [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) - BERT: Pre-training of Deep Bidirectional Transformers
3. [Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) - Improving Language Understanding by Generative Pre-Training

### 几何与拓扑

1. [Nickel & Kiela, 2017](https://arxiv.org/abs/1705.08039) - Poincaré Embeddings for Learning Hierarchical Representations
2. [Wikipedia: Manifold](https://en.wikipedia.org/wiki/Manifold)
3. [Wikipedia: Hyperbolic Geometry](https://en.wikipedia.org/wiki/Hyperbolic_geometry)

### 偏见与公平性

1. [Bolukbasi et al., 2016](https://arxiv.org/abs/1607.06520) - Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings
2. [Wikipedia: Algorithmic Bias](https://en.wikipedia.org/wiki/Algorithmic_bias)

### 教材

1. [Goodfellow et al., 2016](https://www.deeplearningbook.org/) - Deep Learning
2. [Jurafsky & Martin, 2023](https://web.stanford.edu/~jurafsky/slp3/) - Speech and Language Processing (3rd ed.)

---

*本文档系统阐述了语义向量空间的理论基础、构建方法和性质，为理解现代AI的表示学习提供了坚实的数学和概念基础。*
