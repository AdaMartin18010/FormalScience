# æ ·æœ¬å¤æ‚åº¦ï¼ˆSample Complexityï¼‰

## ç›®å½• | Table of Contents

- [æ ·æœ¬å¤æ‚åº¦ï¼ˆSample Complexityï¼‰](#æ ·æœ¬å¤æ‚åº¦sample-complexity)
- [ç›®å½•](#ç›®å½•)
- [å¼•è¨€](#å¼•è¨€)
  - [ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ](#ä¸ºä»€ä¹ˆé‡è¦)
  - [å…³é”®å‚æ•°](#å…³é”®å‚æ•°)
- [æ ·æœ¬å¤æ‚åº¦çš„å½¢å¼åŒ–](#æ ·æœ¬å¤æ‚åº¦çš„å½¢å¼åŒ–)
  - [1. åŸºæœ¬å®šä¹‰](#1-åŸºæœ¬å®šä¹‰)
  - [2. æ¸è¿‘è¡¨ç¤º](#2-æ¸è¿‘è¡¨ç¤º)
- [æœ‰é™å‡è®¾ç©ºé—´çš„æ ·æœ¬å¤æ‚åº¦](#æœ‰é™å‡è®¾ç©ºé—´çš„æ ·æœ¬å¤æ‚åº¦)
  - [1. åŸºæœ¬ç•Œ](#1-åŸºæœ¬ç•Œ)
  - [2. ç´§ç•Œ](#2-ç´§ç•Œ)
  - [3. ä¾‹å­ï¼šå¸ƒå°”åˆå–å¼](#3-ä¾‹å­å¸ƒå°”åˆå–å¼)
- [VCç»´ä¸æ ·æœ¬å¤æ‚åº¦](#vcç»´ä¸æ ·æœ¬å¤æ‚åº¦)
  - [1. åŸºæœ¬PACå®šç†ï¼ˆæ ·æœ¬å¤æ‚åº¦ç‰ˆæœ¬ï¼‰](#1-åŸºæœ¬pacå®šç†æ ·æœ¬å¤æ‚åº¦ç‰ˆæœ¬)
  - [2. Sauer-Shelahå¼•ç†](#2-sauer-shelahå¼•ç†)
  - [3. ç²¾ç¡®å¸¸æ•°](#3-ç²¾ç¡®å¸¸æ•°)
- [Rademacherå¤æ‚åº¦](#rademacherå¤æ‚åº¦)
  - [1. å®šä¹‰](#1-å®šä¹‰)
  - [2. æ³›åŒ–ç•Œ](#2-æ³›åŒ–ç•Œ)
  - [3. ä¸VCç»´çš„å…³ç³»](#3-ä¸vcç»´çš„å…³ç³»)
- [ä¸‹ç•Œç†è®º](#ä¸‹ç•Œç†è®º)
  - [1. ä¿¡æ¯è®ºä¸‹ç•Œ](#1-ä¿¡æ¯è®ºä¸‹ç•Œ)
  - [2. VCç»´ä¸‹ç•Œ](#2-vcç»´ä¸‹ç•Œ)
  - [3. è®¡ç®—ä¸‹ç•Œ](#3-è®¡ç®—ä¸‹ç•Œ)
- [ç¥ç»ç½‘ç»œçš„æ ·æœ¬å¤æ‚åº¦](#ç¥ç»ç½‘ç»œçš„æ ·æœ¬å¤æ‚åº¦)
  - [1. åŸºäºVCç»´çš„ç•Œ](#1-åŸºäºvcç»´çš„ç•Œ)
  - [2. Norm-basedå¤æ‚åº¦](#2-norm-basedå¤æ‚åº¦)
  - [3. å‹ç¼©ç•Œï¼ˆCompression Boundsï¼‰](#3-å‹ç¼©ç•Œcompression-bounds)
  - [4. è¿‡å‚æ•°åŒ–ç†è®º](#4-è¿‡å‚æ•°åŒ–ç†è®º)
- [å®è·µä¸­çš„æ ·æœ¬å¤æ‚åº¦](#å®è·µä¸­çš„æ ·æœ¬å¤æ‚åº¦)
  - [1. å›¾åƒåˆ†ç±»](#1-å›¾åƒåˆ†ç±»)
  - [2. è¯­è¨€æ¨¡å‹](#2-è¯­è¨€æ¨¡å‹)
  - [3. Few-Shotå­¦ä¹ ](#3-few-shotå­¦ä¹ )
  - [4. æ•°æ®å¢å¼ºçš„å½±å“](#4-æ•°æ®å¢å¼ºçš„å½±å“)
- [æ€»ç»“](#æ€»ç»“)
  - [æ ¸å¿ƒè¦ç‚¹](#æ ¸å¿ƒè¦ç‚¹)
  - [ä¾èµ–å…³ç³»æ€»ç»“](#ä¾èµ–å…³ç³»æ€»ç»“)
  - [å¯¹AIçš„å¯ç¤º](#å¯¹aiçš„å¯ç¤º)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)
  - [åŸºç¡€ç†è®º](#åŸºç¡€ç†è®º)
  - [VCç»´ä¸ç”Ÿé•¿å‡½æ•°](#vcç»´ä¸ç”Ÿé•¿å‡½æ•°)
  - [Rademacherå¤æ‚åº¦1](#rademacherå¤æ‚åº¦1)
  - [ç¥ç»ç½‘ç»œ](#ç¥ç»ç½‘ç»œ)
  - [å®è·µåº”ç”¨](#å®è·µåº”ç”¨)

---

## å¼•è¨€

**æ ·æœ¬å¤æ‚åº¦**ï¼ˆSample Complexityï¼‰æ˜¯å­¦ä¹ ç†è®ºçš„æ ¸å¿ƒæ¦‚å¿µï¼Œå®ƒå›ç­”ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼š

> **ä¸ºäº†å­¦ä¹ ä¸€ä¸ªæ¦‚å¿µåˆ°æŒ‡å®šç²¾åº¦ï¼Œéœ€è¦å¤šå°‘è®­ç»ƒæ ·æœ¬ï¼Ÿ**

### ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

1. **ç†è®ºæ„ä¹‰**ï¼šåˆ»ç”»å­¦ä¹ ä»»åŠ¡çš„éš¾åº¦
2. **å®è·µæŒ‡å¯¼**ï¼šé¢„æµ‹éœ€è¦å¤šå°‘æ•°æ®
3. **ç®—æ³•è®¾è®¡**ï¼šæŒ‡å¯¼é‡‡æ ·ç­–ç•¥
4. **èµ„æºè§„åˆ’**ï¼šæ•°æ®æ ‡æ³¨æˆæœ¬ä¼°è®¡

### å…³é”®å‚æ•°

æ ·æœ¬å¤æ‚åº¦é€šå¸¸ä¾èµ–äºï¼š

- **Îµ**ï¼šç›®æ ‡ç²¾åº¦ï¼ˆè¯¯å·®å®¹å¿åº¦ï¼‰
- **Î´**ï¼šå¤±è´¥æ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰
- **d**ï¼šå‡è®¾ç©ºé—´çš„å¤æ‚åº¦ï¼ˆå¦‚VCç»´ï¼‰
- **n**ï¼šå®ä¾‹ç»´åº¦

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Sample Complexity](https://en.wikipedia.org/wiki/Sample_complexity)
- [Shalev-Shwartz & Ben-David, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Understanding Machine Learning

---

## æ ·æœ¬å¤æ‚åº¦çš„å½¢å¼åŒ–

### 1. åŸºæœ¬å®šä¹‰

**å®šä¹‰ï¼ˆæ ·æœ¬å¤æ‚åº¦ï¼‰**ï¼š

è®¾ ğ’ æ˜¯æ¦‚å¿µç±»ï¼Œğ’œ æ˜¯å­¦ä¹ ç®—æ³•ã€‚**æ ·æœ¬å¤æ‚åº¦** m_ğ’(Îµ, Î´) æ˜¯æœ€å°çš„ mï¼Œä½¿å¾—ï¼š

å¯¹äº**ä»»æ„**ï¼š

- åˆ†å¸ƒ ğ’Ÿ
- ç›®æ ‡æ¦‚å¿µ c âˆˆ ğ’

ä» ğ’Ÿ ä¸­é‡‡æ · m ä¸ªæ ·æœ¬åï¼Œç®—æ³• ğ’œ ä»¥æ¦‚ç‡è‡³å°‘ 1-Î´ è¾“å‡ºå‡è®¾ hï¼Œæ»¡è¶³ï¼š

```text
error_ğ’Ÿ(h) â‰¤ Îµ
```

**å½¢å¼åŒ–**ï¼š

```text
m_ğ’(Îµ, Î´) = min{m : âˆ€ğ’Ÿ, c, Pr[error_ğ’Ÿ(ğ’œ(S)) â‰¤ Îµ] â‰¥ 1-Î´}
```

å…¶ä¸­ S æ˜¯ä» (ğ’Ÿ, c) é‡‡æ ·çš„ m ä¸ªæ ·æœ¬ã€‚

### 2. æ¸è¿‘è¡¨ç¤º

**Big-Oè®°å·**ï¼š

æ ·æœ¬å¤æ‚åº¦é€šå¸¸å†™ä¸ºï¼š

```text
m = O(f(1/Îµ, 1/Î´, d, n))
```

**å¸¸è§å½¢å¼**ï¼š

1. **çº¿æ€§äº 1/Îµ**ï¼š

    ```text
    m = O((1/Îµ) log(1/Î´))
    ```

2. **å¹³æ–¹äº 1/Îµ**ï¼š

    ```text
    m = O((1/ÎµÂ²) log(1/Î´))
    ```

3. **ä¾èµ–äºVCç»´ d**ï¼š

    ```text
    m = O((d/Îµ) log(1/Îµ) + (1/Îµ) log(1/Î´))
    ```

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Vapnik, 1998](https://link.springer.com/book/10.1007/978-1-4757-3264-1) - Statistical Learning Theory

---

## æœ‰é™å‡è®¾ç©ºé—´çš„æ ·æœ¬å¤æ‚åº¦

### 1. åŸºæœ¬ç•Œ

**å®šç†ï¼ˆæœ‰é™å‡è®¾ç©ºé—´ï¼‰**ï¼š

è®¾ |ğ’| = k < âˆï¼Œåˆ™åœ¨å¯å®ç°æƒ…å†µä¸‹ï¼š

```text
m â‰¥ (1/Îµ) (ln k + ln(1/Î´))
```

**è¯æ˜æ€è·¯**ï¼š

1. **åå‡è®¾**ï¼šå®šä¹‰ h ä¸ºåå‡è®¾ï¼Œå¦‚æœ error_ğ’Ÿ(h) > Îµ
2. **åå‡è®¾å¹¸å­˜æ¦‚ç‡**ï¼šå•ä¸ªåå‡è®¾åœ¨ m ä¸ªæ ·æœ¬ä¸Šéƒ½"çŒœå¯¹"çš„æ¦‚ç‡ï¼š

    ```text
    Pr[error_S(h) = 0 | error_ğ’Ÿ(h) > Îµ] â‰¤ (1 - Îµ)^m
    ```

3. **å¹¶ç•Œ**ï¼ˆUnion Boundï¼‰ï¼šè‡³å°‘ä¸€ä¸ªåå‡è®¾å¹¸å­˜çš„æ¦‚ç‡ï¼š

    ```text
    Pr[å­˜åœ¨åå‡è®¾ h : error_S(h) = 0] â‰¤ k(1 - Îµ)^m
    ```

4. **è¦æ±‚**ï¼šè¿™ä¸ªæ¦‚ç‡ â‰¤ Î´

    ```text
    k(1 - Îµ)^m â‰¤ Î´
    â‡’ (1 - Îµ)^m â‰¤ Î´/k
    â‡’ m ln(1 - Îµ) â‰¤ ln(Î´/k)
    â‡’ m â‰¥ (ln k + ln(1/Î´)) / (- ln(1 - Îµ))
    â‡’ m â‰¥ (ln k + ln(1/Î´)) / Îµ  ï¼ˆå› ä¸º -ln(1-Îµ) â‰ˆ Îµï¼‰
    ```

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Shalev-Shwartz & Ben-David, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Theorem 4.1

### 2. ç´§ç•Œ

**æ›´ç²¾ç¡®çš„ç•Œ**ï¼š

```text
m = O((1/Îµ) log(|ğ’|/Î´))
```

ä¸”è¿™æ˜¯**ç´§çš„**ï¼ˆtightï¼‰ï¼Œå³å­˜åœ¨ä¸‹ç•ŒåŒ¹é…è¿™ä¸ªä¸Šç•Œã€‚

### 3. ä¾‹å­ï¼šå¸ƒå°”åˆå–å¼

**æ¦‚å¿µç±»**ï¼š

n ä¸ªå¸ƒå°”å˜é‡çš„åˆå–å¼ï¼š

```text
c = xâ‚ âˆ§ Â¬xâ‚‚ âˆ§ xâ‚ƒ âˆ§ ...
```

**å‡è®¾ç©ºé—´å¤§å°**ï¼š

```text
|ğ’| = 3^n  ï¼ˆæ¯ä¸ªå˜é‡å¯ä»¥æ˜¯ xáµ¢ã€Â¬xáµ¢ã€æˆ–ä¸å‡ºç°ï¼‰
```

**æ ·æœ¬å¤æ‚åº¦**ï¼š

```text
m = O((1/Îµ) (n log 3 + log(1/Î´)))
  = O((n/Îµ) log(1/Î´))
```

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Kearns & Vazirani, 1994](https://mitpress.mit.edu/9780262111935/an-introduction-to-computational-learning-theory/) - Example 1.2

---

## VCç»´ä¸æ ·æœ¬å¤æ‚åº¦

### 1. åŸºæœ¬PACå®šç†ï¼ˆæ ·æœ¬å¤æ‚åº¦ç‰ˆæœ¬ï¼‰

**å®šç†**ï¼š

è®¾ VC-dim(ğ’) = d < âˆã€‚

**å¯å®ç°æƒ…å†µ**ï¼š

```text
m = O((d/Îµ) log(1/Îµ) + (1/Îµ) log(1/Î´))
```

**ä¸å¯çŸ¥æƒ…å†µ**ï¼ˆAgnosticï¼‰ï¼š

```text
m = O((d/ÎµÂ²) log(1/Îµ) + (1/ÎµÂ²) log(1/Î´))
```

**å…³é”®è§‚å¯Ÿ**ï¼š

- æ ·æœ¬å¤æ‚åº¦ä¸ VCç»´ d **çº¿æ€§ç›¸å…³**
- ä¸ç²¾åº¦ Îµ **åæ¯”**ï¼ˆå¯å®ç°ï¼‰æˆ–**å¹³æ–¹åæ¯”**ï¼ˆä¸å¯çŸ¥ï¼‰
- ä¸ç½®ä¿¡åº¦å‚æ•° log(1/Î´) **çº¿æ€§ç›¸å…³**

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Vapnik, 1998](https://link.springer.com/book/10.1007/978-1-4757-3264-1) - Theorem 4.1

### 2. Sauer-Shelahå¼•ç†

**ç”Ÿé•¿å‡½æ•°**ï¼ˆGrowth Functionï¼‰ï¼š

```text
Î _ğ’(m) = max_{S:|S|=m} |{(h(xâ‚),...,h(xâ‚˜)) : h âˆˆ ğ’}|
```

å³ï¼šğ’ åœ¨å¤§å°ä¸º m çš„ç‚¹é›†ä¸Šæœ€å¤šèƒ½å®ç°å¤šå°‘ç§ä¸åŒçš„æ ‡è®°ã€‚

**Sauer-Shelahå¼•ç†**ï¼š

å¦‚æœ VC-dim(ğ’) = dï¼Œåˆ™ï¼š

```text
Î _ğ’(m) â‰¤ âˆ‘_{i=0}^d C(m,i)
```

ä¸”å½“ m > d æ—¶ï¼š

```text
Î _ğ’(m) â‰¤ (em/d)^d
```

**æ„ä¹‰**ï¼š

è™½ç„¶ |ğ’| å¯èƒ½æ— ç©·å¤§ï¼Œä½† Î _ğ’(m) åªæ˜¯ m çš„å¤šé¡¹å¼ï¼Œè¿™ä½¿å¾—æ ·æœ¬å¤æ‚åº¦ç•Œæˆä¸ºå¯èƒ½ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Sauer-Shelah Lemma](https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma)
- [Sauer, 1972](https://link.springer.com/article/10.1007/BF02189207) - On the Density of Families of Sets

### 3. ç²¾ç¡®å¸¸æ•°

**æ›´ç²¾ç¡®çš„ç•Œ**ï¼ˆAnthony & Bartlett, 1999ï¼‰ï¼š

åœ¨å¯å®ç°æƒ…å†µä¸‹ï¼Œå­˜åœ¨é€šç”¨å¸¸æ•° câ‚, câ‚‚ï¼Œä½¿å¾—ï¼š

```text
câ‚ (d/Îµ) log(1/Îµ) â‰¤ m_ğ’(Îµ, Î´) â‰¤ câ‚‚ (d/Îµ) log(1/Îµ) + (1/Îµ) log(1/Î´)
```

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Anthony & Bartlett, 1999](https://www.cambridge.org/core/books/neural-network-learning/BAAA804827E35EB5BA7B3AF35CD5B2E1) - Neural Network Learning: Theoretical Foundations

---

## Rademacherå¤æ‚åº¦

### 1. å®šä¹‰

**ç»éªŒRademacherå¤æ‚åº¦**ï¼š

è®¾ S = {xâ‚, ..., xâ‚˜} æ˜¯æ ·æœ¬é›†ï¼ŒÏƒ = (Ïƒâ‚, ..., Ïƒâ‚˜) æ˜¯Rademacheréšæœºå˜é‡ï¼ˆæ¯ä¸ª Ïƒáµ¢ âˆˆ {-1,+1} ç­‰æ¦‚ç‡ï¼‰ã€‚

```text
RÌ‚_S(ğ’) = E_Ïƒ[ sup_{hâˆˆğ’} (1/m) âˆ‘áµ¢ Ïƒáµ¢ h(xáµ¢) ]
```

**Rademacherå¤æ‚åº¦**ï¼š

```text
R_m(ğ’) = E_S[ RÌ‚_S(ğ’) ]
```

**ç›´è§‰**ï¼š

åº¦é‡å‡è®¾ç±» ğ’ èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ‹Ÿåˆ**éšæœºå™ªå£°**ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Rademacher Complexity](https://en.wikipedia.org/wiki/Rademacher_complexity)
- [Bartlett & Mendelson, 2002](https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf) - Rademacher and Gaussian Complexities

### 2. æ³›åŒ–ç•Œ

**å®šç†ï¼ˆRademacheræ³›åŒ–ç•Œï¼‰**ï¼š

ä»¥æ¦‚ç‡è‡³å°‘ 1-Î´ï¼Œå¯¹æ‰€æœ‰ h âˆˆ ğ’ï¼š

```text
error_ğ’Ÿ(h) â‰¤ error_S(h) + 2R_m(ğ’) + O(âˆš(log(1/Î´) / m))
```

**æ ·æœ¬å¤æ‚åº¦æ¨è®º**ï¼š

è¦ä½¿æ³›åŒ–è¯¯å·® â‰¤ Îµï¼Œéœ€è¦ï¼š

```text
m = O(R_m(ğ’)Â² / ÎµÂ²)
```

### 3. ä¸VCç»´çš„å…³ç³»

**å®šç†**ï¼š

å¦‚æœ VC-dim(ğ’) = dï¼Œåˆ™ï¼š

```text
R_m(ğ’) = O(âˆš(d / m))
```

**æ¨è®º**ï¼š

```text
m = O(d / ÎµÂ²)
```

è¿™ä¸VCç»´çš„æ ·æœ¬å¤æ‚åº¦ç•Œä¸€è‡´ï¼ˆä¸å¯çŸ¥æƒ…å†µï¼‰ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Shalev-Shwartz & Ben-David, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Chapter 26

---

## ä¸‹ç•Œç†è®º

### 1. ä¿¡æ¯è®ºä¸‹ç•Œ

**å®šç†ï¼ˆFanoä¸ç­‰å¼ï¼‰**ï¼š

è®¾ ğ’ = {câ‚, ..., c_k}ï¼Œåˆ™ï¼š

```text
m â‰¥ Î©(log k / Îµ)
```

**è¯æ˜æ€è·¯**ï¼š

åˆ©ç”¨ä¿¡æ¯è®ºä¸­çš„Fanoä¸ç­‰å¼ï¼Œæ ·æœ¬æ•°å¿…é¡»è¶³ä»¥ä» k ä¸ªæ¦‚å¿µä¸­åŒºåˆ†ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Fano's Inequality](https://en.wikipedia.org/wiki/Fano%27s_inequality)

### 2. VCç»´ä¸‹ç•Œ

**å®šç†**ï¼š

å¦‚æœ VC-dim(ğ’) = dï¼Œåˆ™å­˜åœ¨åˆ†å¸ƒ ğ’Ÿ å’Œç›®æ ‡æ¦‚å¿µ cï¼Œä½¿å¾—ï¼š

```text
m â‰¥ Î©(d / Îµ)
```

**æ„ä¹‰**ï¼š

ä¸Šç•Œ O((d/Îµ) log(1/Îµ)) æ˜¯**å‡ ä¹ç´§çš„**ï¼Œåªå·® log(1/Îµ) å› å­ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Anthony & Bartlett, 1999](https://www.cambridge.org/core/books/neural-network-learning/BAAA804827E35EB5BA7B3AF35CD5B2E1) - Chapter 7

### 3. è®¡ç®—ä¸‹ç•Œ

**å®šç†ï¼ˆå¯†ç å­¦å‡è®¾ä¸‹ï¼‰**ï¼š

æŸäº›æ¦‚å¿µç±»ï¼ˆå¦‚ç”µè·¯ç±»ï¼‰è™½ç„¶PACå¯å­¦ä¹ ï¼Œä½†éœ€è¦**è¶…å¤šé¡¹å¼æ ·æœ¬**ï¼Œé™¤éæŸäº›å¯†ç å­¦å‡è®¾è¢«æ‰“ç ´ã€‚

**æ„ä¹‰**ï¼š

æ ·æœ¬å¤æ‚åº¦ä¸è®¡ç®—å¤æ‚åº¦å¯èƒ½æœ‰tradeoffã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Kearns & Valiant, 1994](https://dl.acm.org/doi/10.1145/174644.174647) - Cryptographic Limitations on Learning

---

## ç¥ç»ç½‘ç»œçš„æ ·æœ¬å¤æ‚åº¦

### 1. åŸºäºVCç»´çš„ç•Œ

**å®šç†**ï¼š

è®¾ç¥ç»ç½‘ç»œæœ‰ W ä¸ªæƒé‡ï¼ŒVC-dim = O(W log W)ï¼Œåˆ™ï¼š

```text
m = O((W log W) / ÎµÂ²)
```

**é—®é¢˜**ï¼š

è¿™ä¸ªç•Œå¯¹æ·±åº¦ç½‘ç»œè¿‡äºå®½æ¾ï¼Œæ— æ³•è§£é‡Šå®é™…æ³›åŒ–æ€§èƒ½ã€‚

### 2. Norm-basedå¤æ‚åº¦

**å®šç†ï¼ˆBartlett, 1998ï¼‰**ï¼š

å¯¹äºç¥ç»ç½‘ç»œï¼Œæ³›åŒ–è¯¯å·®ä¾èµ–äºæƒé‡çš„**èŒƒæ•°**è€Œéå‚æ•°æ•°é‡ã€‚

è®¾ B æ˜¯æƒé‡çŸ©é˜µçš„è°±èŒƒæ•°ä¹‹ç§¯ï¼Œåˆ™æ ·æœ¬å¤æ‚åº¦ä¸ºï¼š

```text
m = O(BÂ² / ÎµÂ²)
```

**æ„ä¹‰**ï¼š

- å°èŒƒæ•°æƒé‡ â†’ å¥½æ³›åŒ–ï¼ˆå³ä½¿å‚æ•°å¤šï¼‰
- è§£é‡Šäº†ä¸ºä»€ä¹ˆå¤§ç½‘ç»œå¯ä»¥æ³›åŒ–

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Bartlett, 1998](https://ieeexplore.ieee.org/document/661502) - The Sample Complexity of Pattern Classification with Neural Networks

### 3. å‹ç¼©ç•Œï¼ˆCompression Boundsï¼‰

**æ€æƒ³**ï¼š

å¦‚æœå¯ä»¥å°†æ¨¡å‹"å‹ç¼©"åˆ° k ä½ï¼Œåˆ™æ ·æœ¬å¤æ‚åº¦ä¸º O(k/ÎµÂ²)ã€‚

**åº”ç”¨äºç¥ç»ç½‘ç»œ**ï¼š

- é‡åŒ–æƒé‡
- å‰ªæ
- çŸ¥è¯†è’¸é¦

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Arora et al., 2018](https://arxiv.org/abs/1802.05296) - Stronger Generalization Bounds for Deep Nets via a Compression Approach

### 4. è¿‡å‚æ•°åŒ–ç†è®º

**è§‚å¯Ÿ**ï¼š

ç°ä»£æ·±åº¦ç½‘ç»œé€šå¸¸ W â‰« mï¼ˆå‚æ•°æ•° â‰« æ ·æœ¬æ•°ï¼‰ã€‚

**ä¼ ç»Ÿç†è®ºé¢„æµ‹**ï¼š

åº”è¯¥è¿‡æ‹Ÿåˆã€‚

**å®é™…**ï¼š

åè€Œæ³›åŒ–æ›´å¥½ï¼ˆåŒä¸‹é™ç°è±¡ï¼‰ã€‚

**ç°ä»£ç†è®º**ï¼š

- **éšå¼æ­£åˆ™åŒ–**ï¼šSGDåå‘ç®€å•è§£
- **æ’å€¼å­¦ä¹ **ï¼šè¿‡å‚æ•°åŒ–ç½‘ç»œå¯ä»¥å®Œç¾æ‹Ÿåˆæ•°æ®ä¸”æ³›åŒ–
- **ç¥ç»åˆ‡çº¿æ ¸**ï¼ˆNTKï¼‰ç†è®º

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Belkin et al., 2019](https://www.pnas.org/doi/10.1073/pnas.1903070116) - Reconciling Modern Machine Learning
- [Jacot et al., 2018](https://arxiv.org/abs/1806.07572) - Neural Tangent Kernel

---

## å®è·µä¸­çš„æ ·æœ¬å¤æ‚åº¦

### 1. å›¾åƒåˆ†ç±»

**ImageNet**ï¼š

- 1000ç±»
- æ¯ç±»çº¦1000å¼ å›¾åƒ
- æ€»è®¡ï¼š~100ä¸‡å¼ å›¾åƒ

**æ¨¡å‹**ï¼š

- ResNet-50ï¼š~2500ä¸‡å‚æ•°
- æ ·æœ¬æ•°/å‚æ•°æ•° â‰ˆ 0.04

**è§‚å¯Ÿ**ï¼š

å‚æ•°æ•°è¿œè¶…æ ·æœ¬æ•°ï¼Œä½†ä»æ³›åŒ–è‰¯å¥½ã€‚

**åŸå› **ï¼š

- æ•°æ®å¢å¼ºï¼ˆå¢åŠ æœ‰æ•ˆæ ·æœ¬æ•°ï¼‰
- é¢„è®­ç»ƒï¼ˆè¿ç§»å­¦ä¹ ï¼‰
- å½’çº³åç½®ï¼ˆå·ç§¯ã€å±€éƒ¨æ€§ï¼‰

**å‚è€ƒæ–‡çŒ®**ï¼š

- [He et al., 2016](https://arxiv.org/abs/1512.03385) - Deep Residual Learning for Image Recognition

### 2. è¯­è¨€æ¨¡å‹

**GPT-3**ï¼š

- å‚æ•°ï¼š175B
- è®­ç»ƒæ•°æ®ï¼š~300B tokens

**æ ·æœ¬æ•°/å‚æ•°æ•°**ï¼š

```text
300B / 175B â‰ˆ 1.7
```

**é—®é¢˜**ï¼š

æŒ‰ä¼ ç»Ÿç†è®ºï¼Œè¿™è¿œè¿œä¸å¤Ÿã€‚

**è§£é‡Š**ï¼š

- Tokenä¸æ˜¯ç‹¬ç«‹çš„ï¼ˆåºåˆ—ç»“æ„ï¼‰
- è¯­è¨€æœ‰å¼ºç»Ÿè®¡è§„å¾‹ï¼ˆå¯é¢„æµ‹æ€§ï¼‰
- æ¨¡å‹å­¦ä¹ çš„æ˜¯**åˆ†å¸ƒ**ï¼Œä¸æ˜¯**è®°å¿†**æ¯ä¸ªtoken

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Brown et al., 2020](https://arxiv.org/abs/2005.14165) - Language Models are Few-Shot Learners

### 3. Few-Shotå­¦ä¹ 

**åœºæ™¯**ï¼š

æ¯ç±»åªæœ‰å°‘é‡æ ·æœ¬ï¼ˆå¦‚5ä¸ªï¼‰ã€‚

**æ–¹æ³•**ï¼š

- **å…ƒå­¦ä¹ **ï¼ˆMeta-Learningï¼‰ï¼šå­¦ä¹ å¦‚ä½•å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
- **è¿ç§»å­¦ä¹ **ï¼šåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†

**æ ·æœ¬å¤æ‚åº¦**ï¼š

```text
m = å¾ˆå°ï¼ˆå¦‚5-10ä¸ª/ç±»ï¼‰
```

ä½†éœ€è¦å¤§é‡**å…ƒæ•°æ®**ï¼ˆå…¶ä»–ä»»åŠ¡çš„æ•°æ®ï¼‰ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Few-Shot Learning](https://en.wikipedia.org/wiki/Few-shot_learning)
- [Finn et al., 2017](https://arxiv.org/abs/1703.03400) - Model-Agnostic Meta-Learning

### 4. æ•°æ®å¢å¼ºçš„å½±å“

**æŠ€æœ¯**ï¼š

- å›¾åƒï¼šæ—‹è½¬ã€ç¿»è½¬ã€è£å‰ªã€é¢œè‰²å˜æ¢
- æ–‡æœ¬ï¼šåŒä¹‰è¯æ›¿æ¢ã€å›è¯‘ï¼ˆBack-Translationï¼‰

**æ•ˆæœ**ï¼š

æœ‰æ•ˆå¢åŠ æ ·æœ¬æ•°ï¼Œé™ä½å®é™…æ ·æœ¬å¤æ‚åº¦ã€‚

**ç†è®º**ï¼š

æ•°æ®å¢å¼ºå¼•å…¥**å…ˆéªŒçŸ¥è¯†**ï¼ˆå¦‚å›¾åƒçš„æ—‹è½¬ä¸å˜æ€§ï¼‰ï¼Œç¼©å°å‡è®¾ç©ºé—´ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Shorten & Khoshgoftaar, 2019](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0) - A Survey on Image Data Augmentation

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **åŸºæœ¬ç•Œ**ï¼š
   - æœ‰é™å‡è®¾ï¼šm = O((1/Îµ) log|ğ’|)
   - VCç»´ï¼šm = O((d/Îµ) log(1/Îµ))ï¼ˆå¯å®ç°ï¼‰æˆ– O(d/ÎµÂ²)ï¼ˆä¸å¯çŸ¥ï¼‰

2. **Rademacherå¤æ‚åº¦**ï¼šåº¦é‡æ‹Ÿåˆéšæœºå™ªå£°çš„èƒ½åŠ›ï¼Œæä¾›æ•°æ®ä¾èµ–çš„ç•Œ

3. **ä¸‹ç•Œ**ï¼šä¿¡æ¯è®ºå’ŒVCç»´ä¸‹ç•Œè¡¨æ˜ä¸Šç•Œå‡ ä¹ç´§

4. **ç¥ç»ç½‘ç»œ**ï¼š
   - åŸºäºå‚æ•°æ•°çš„ç•Œè¿‡äºå®½æ¾
   - åŸºäºèŒƒæ•°çš„ç•Œæ›´ç´§
   - è¿‡å‚æ•°åŒ–æ‚–è®ºæœªå®Œå…¨è§£å†³

5. **å®è·µ**ï¼š
   - æ•°æ®å¢å¼º
   - è¿ç§»å­¦ä¹ 
   - å½’çº³åç½®
   - å…ƒå­¦ä¹ 

### ä¾èµ–å…³ç³»æ€»ç»“

| å‚æ•° | å¯å®ç°æƒ…å†µ | ä¸å¯çŸ¥æƒ…å†µ |
|------|-----------|-----------|
| **ç²¾åº¦ Îµ** | m âˆ 1/Îµ | m âˆ 1/ÎµÂ² |
| **ç½®ä¿¡åº¦ Î´** | m âˆ log(1/Î´) | m âˆ log(1/Î´) |
| **VCç»´ d** | m âˆ d | m âˆ d |
| **å‡è®¾æ•° \|ğ’\|** | m âˆ log\|ğ’\| | m âˆ log\|ğ’\| |

### å¯¹AIçš„å¯ç¤º

1. **ç†è®ºä¸å®è·µçš„é¸¿æ²Ÿ**ï¼š
   - ç†è®ºç•Œé€šå¸¸è¿‡äºä¿å®ˆ
   - å®é™…ç³»ç»Ÿåˆ©ç”¨äº†æ•°æ®ç»“æ„ã€å½’çº³åç½®ç­‰

2. **æ•°æ®æ˜¯å…³é”®èµ„æº**ï¼š
   - æ ·æœ¬å¤æ‚åº¦æŒ‡å¯¼æ•°æ®æ”¶é›†
   - ä½†æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦

3. **å½’çº³åç½®çš„ä»·å€¼**ï¼š
   - å¥½çš„åç½®å¯ä»¥æ˜¾è‘—é™ä½æ ·æœ¬å¤æ‚åº¦
   - æ¶æ„è®¾è®¡=æ³¨å…¥å…ˆéªŒçŸ¥è¯†

4. **è¿ç§»å­¦ä¹ çš„å¿…ç„¶æ€§**ï¼š
   - ä»å¤´è®­ç»ƒéœ€è¦å·¨é‡æ•°æ®
   - è¿ç§»å­¦ä¹ å¤ç”¨çŸ¥è¯†ï¼Œé™ä½æ ·æœ¬éœ€æ±‚

---

## å‚è€ƒæ–‡çŒ®

### åŸºç¡€ç†è®º

1. [Wikipedia: Sample Complexity](https://en.wikipedia.org/wiki/Sample_complexity)
2. [Shalev-Shwartz & Ben-David, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Understanding Machine Learning
3. [Vapnik, 1998](https://link.springer.com/book/10.1007/978-1-4757-3264-1) - Statistical Learning Theory

### VCç»´ä¸ç”Ÿé•¿å‡½æ•°

1. [Wikipedia: Sauer-Shelah Lemma](https://en.wikipedia.org/wiki/Sauer%E2%80%93Shelah_lemma)
2. [Sauer, 1972](https://link.springer.com/article/10.1007/BF02189207) - On the Density of Families of Sets
3. [Anthony & Bartlett, 1999](https://www.cambridge.org/core/books/neural-network-learning/BAAA804827E35EB5BA7B3AF35CD5B2E1) - Neural Network Learning

### Rademacherå¤æ‚åº¦1

1. [Wikipedia: Rademacher Complexity](https://en.wikipedia.org/wiki/Rademacher_complexity)
2. [Bartlett & Mendelson, 2002](https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf) - Rademacher and Gaussian Complexities

### ç¥ç»ç½‘ç»œ

1. [Bartlett, 1998](https://ieeexplore.ieee.org/document/661502) - The Sample Complexity of Pattern Classification with Neural Networks
2. [Arora et al., 2018](https://arxiv.org/abs/1802.05296) - Stronger Generalization Bounds for Deep Nets
3. [Belkin et al., 2019](https://www.pnas.org/doi/10.1073/pnas.1903070116) - Reconciling Modern Machine Learning
4. [Jacot et al., 2018](https://arxiv.org/abs/1806.07572) - Neural Tangent Kernel

### å®è·µåº”ç”¨

1. [He et al., 2016](https://arxiv.org/abs/1512.03385) - ResNet
2. [Brown et al., 2020](https://arxiv.org/abs/2005.14165) - GPT-3
3. [Finn et al., 2017](https://arxiv.org/abs/1703.03400) - Model-Agnostic Meta-Learning

---

*æœ¬æ–‡æ¡£ç³»ç»Ÿé˜è¿°äº†æ ·æœ¬å¤æ‚åº¦ç†è®ºçš„æ ¸å¿ƒæ¦‚å¿µã€ä¸»è¦å®šç†å’Œå®è·µåº”ç”¨ï¼Œä¸ºç†è§£å­¦ä¹ ä»»åŠ¡çš„æ•°æ®éœ€æ±‚æä¾›äº†ç†è®ºåŸºç¡€ã€‚*
