# æ³›åŒ–ç†è®ºï¼ˆGeneralization Theoryï¼‰

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **æœ€åæ›´æ–°**: 2025-10-27  
> **æ–‡æ¡£è§„æ¨¡**: 1043è¡Œ | æ³›åŒ–è¯¯å·®åˆ†æä¸æ·±åº¦å­¦ä¹ æ³›åŒ–ä¹‹è°œ  
> **é˜…è¯»å»ºè®®**: æœ¬æ–‡æ˜¯ç†è§£æ·±åº¦å­¦ä¹ æ³›åŒ–èƒ½åŠ›çš„ç†è®ºåŸºç¡€ï¼Œå†…å®¹å¹¿æ³›æ·±å…¥ï¼Œå»ºè®®åˆ†é˜¶æ®µç ”è¯»

---

## ç›®å½• | Table of Contents

- [æ³›åŒ–ç†è®ºï¼ˆGeneralization Theoryï¼‰](#æ³›åŒ–ç†è®ºgeneralization-theory)
- [ç›®å½•](#ç›®å½•)
- [å¼•è¨€](#å¼•è¨€)
  - [æ ¸å¿ƒé—®é¢˜](#æ ¸å¿ƒé—®é¢˜)
  - [æ³›åŒ–å¤±è´¥çš„è¡¨ç°](#æ³›åŒ–å¤±è´¥çš„è¡¨ç°)
- [æ³›åŒ–çš„å½¢å¼åŒ–](#æ³›åŒ–çš„å½¢å¼åŒ–)
  - [1. åŸºæœ¬å®šä¹‰](#1-åŸºæœ¬å®šä¹‰)
    - [æ³›åŒ–è¯¯å·®ï¼ˆGeneralization Errorï¼‰](#æ³›åŒ–è¯¯å·®generalization-error)
    - [ç»éªŒè¯¯å·®ï¼ˆEmpirical Errorï¼‰](#ç»éªŒè¯¯å·®empirical-error)
    - [æ³›åŒ–å·®è·ï¼ˆGeneralization Gapï¼‰](#æ³›åŒ–å·®è·generalization-gap)
  - [2. ä¸€è‡´æ”¶æ•›ï¼ˆUniform Convergenceï¼‰](#2-ä¸€è‡´æ”¶æ•›uniform-convergence)
  - [3. æ³›åŒ–ç•Œï¼ˆGeneralization Boundsï¼‰](#3-æ³›åŒ–ç•Œgeneralization-bounds)
- [ç»å…¸æ³›åŒ–ç•Œ](#ç»å…¸æ³›åŒ–ç•Œ)
  - [1. Hoeffdingç•Œ](#1-hoeffdingç•Œ)
  - [2. VCç»´æ³›åŒ–ç•Œ](#2-vcç»´æ³›åŒ–ç•Œ)
  - [3. Rademacherå¤æ‚åº¦ç•Œ](#3-rademacherå¤æ‚åº¦ç•Œ)
  - [4. PAC-Bayesç•Œ](#4-pac-bayesç•Œ)
- [åå·®-æ–¹å·®åˆ†è§£](#åå·®-æ–¹å·®åˆ†è§£)
  - [1. åŸºæœ¬åˆ†è§£](#1-åŸºæœ¬åˆ†è§£)
  - [2. åå·®-æ–¹å·®æƒè¡¡ï¼ˆTradeoffï¼‰](#2-åå·®-æ–¹å·®æƒè¡¡tradeoff)
  - [3. æ·±åº¦å­¦ä¹ ä¸­çš„"ç ´è£‚"](#3-æ·±åº¦å­¦ä¹ ä¸­çš„ç ´è£‚)
- [æ­£åˆ™åŒ–ç†è®º](#æ­£åˆ™åŒ–ç†è®º)
  - [1. æ­£åˆ™åŒ–çš„å®šä¹‰](#1-æ­£åˆ™åŒ–çš„å®šä¹‰)
  - [2. å¸¸è§æ­£åˆ™åŒ–æ–¹æ³•](#2-å¸¸è§æ­£åˆ™åŒ–æ–¹æ³•)
    - [L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰](#l2æ­£åˆ™åŒ–ridge)
    - [L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰](#l1æ­£åˆ™åŒ–lasso)
    - [Elastic Net](#elastic-net)
  - [3. æ­£åˆ™åŒ–çš„æ³›åŒ–æ•ˆæœ](#3-æ­£åˆ™åŒ–çš„æ³›åŒ–æ•ˆæœ)
  - [4. æ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–](#4-æ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–)
    - [Dropout](#dropout)
    - [Batch Normalization](#batch-normalization)
    - [Early Stopping](#early-stopping)
    - [Data Augmentation](#data-augmentation)
- [æ·±åº¦å­¦ä¹ çš„æ³›åŒ–ä¹‹è°œ](#æ·±åº¦å­¦ä¹ çš„æ³›åŒ–ä¹‹è°œ)
  - [1. ä¼ ç»Ÿç†è®ºçš„å›°æƒ‘](#1-ä¼ ç»Ÿç†è®ºçš„å›°æƒ‘)
  - [2. Zhangç­‰äººçš„å®éªŒ](#2-zhangç­‰äººçš„å®éªŒ)
  - [3. è§£é‡Šå°è¯•](#3-è§£é‡Šå°è¯•)
    - [1. éšå¼æ­£åˆ™åŒ–](#1-éšå¼æ­£åˆ™åŒ–)
    - [2. è¿‡å‚æ•°åŒ–çš„ä¼˜åŠ¿](#2-è¿‡å‚æ•°åŒ–çš„ä¼˜åŠ¿)
    - [3. å½’çº³åç½®](#3-å½’çº³åç½®)
    - [4. æ•°æ®ç»“æ„](#4-æ•°æ®ç»“æ„)
- [éšå¼æ­£åˆ™åŒ–](#éšå¼æ­£åˆ™åŒ–)
  - [1. SGDçš„éšå¼åç½®](#1-sgdçš„éšå¼åç½®)
  - [2. å¹³å¦æœ€å°å€¼ï¼ˆFlat Minimaï¼‰](#2-å¹³å¦æœ€å°å€¼flat-minima)
  - [3. æ‰¹é‡å¤§å°çš„å½±å“](#3-æ‰¹é‡å¤§å°çš„å½±å“)
  - [4. å­¦ä¹ ç‡çš„å½±å“](#4-å­¦ä¹ ç‡çš„å½±å“)
- [æ³›åŒ–çš„æ–°è§†è§’](#æ³›åŒ–çš„æ–°è§†è§’)
  - [1. å‹ç¼©ç•Œï¼ˆCompression Boundsï¼‰](#1-å‹ç¼©ç•Œcompression-bounds)
  - [2. ä¿¡æ¯è®ºè§†è§’](#2-ä¿¡æ¯è®ºè§†è§’)
  - [3. æµå½¢å‡è®¾](#3-æµå½¢å‡è®¾)
  - [4. åŒä¸‹é™ä¸æ’å€¼](#4-åŒä¸‹é™ä¸æ’å€¼)
- [å®è·µä¸­çš„æ³›åŒ–ç­–ç•¥](#å®è·µä¸­çš„æ³›åŒ–ç­–ç•¥)
  - [1. æ•°æ®å±‚é¢](#1-æ•°æ®å±‚é¢)
    - [1.1 å¢åŠ æ•°æ®é‡](#11-å¢åŠ æ•°æ®é‡)
    - [1.2 æ•°æ®æ¸…æ´—](#12-æ•°æ®æ¸…æ´—)
    - [1.3 è¿ç§»å­¦ä¹ ](#13-è¿ç§»å­¦ä¹ )
  - [2. æ¨¡å‹å±‚é¢](#2-æ¨¡å‹å±‚é¢)
    - [2.1 é€‰æ‹©åˆé€‚å¤æ‚åº¦](#21-é€‰æ‹©åˆé€‚å¤æ‚åº¦)
    - [2.2 æ­£åˆ™åŒ–](#22-æ­£åˆ™åŒ–)
    - [2.3 é›†æˆæ–¹æ³•](#23-é›†æˆæ–¹æ³•)
  - [3. è®­ç»ƒå±‚é¢](#3-è®­ç»ƒå±‚é¢)
    - [3.1 äº¤å‰éªŒè¯](#31-äº¤å‰éªŒè¯)
    - [3.2 æ—©åœ](#32-æ—©åœ)
    - [3.3 å­¦ä¹ ç‡è°ƒåº¦](#33-å­¦ä¹ ç‡è°ƒåº¦)
  - [4. è¯„ä¼°å±‚é¢](#4-è¯„ä¼°å±‚é¢)
    - [4.1 ç‹¬ç«‹æµ‹è¯•é›†](#41-ç‹¬ç«‹æµ‹è¯•é›†)
    - [4.2 æ³›åŒ–å·®è·ç›‘æ§](#42-æ³›åŒ–å·®è·ç›‘æ§)
- [æ€»ç»“](#æ€»ç»“)
  - [æ ¸å¿ƒè¦ç‚¹](#æ ¸å¿ƒè¦ç‚¹)
  - [ç†è®º vs å®è·µ](#ç†è®º-vs-å®è·µ)
  - [æœªè§£ä¹‹è°œ](#æœªè§£ä¹‹è°œ)
  - [å“²å­¦åæ€](#å“²å­¦åæ€)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)
  - [åŸºç¡€ç†è®º](#åŸºç¡€ç†è®º)
  - [æ³›åŒ–ç•Œ](#æ³›åŒ–ç•Œ)
  - [åå·®-æ–¹å·®](#åå·®-æ–¹å·®)
  - [æ·±åº¦å­¦ä¹ æ³›åŒ–](#æ·±åº¦å­¦ä¹ æ³›åŒ–)
  - [éšå¼æ­£åˆ™åŒ–1](#éšå¼æ­£åˆ™åŒ–1)
  - [ç¥ç»åˆ‡çº¿æ ¸](#ç¥ç»åˆ‡çº¿æ ¸)
  - [æ­£åˆ™åŒ–](#æ­£åˆ™åŒ–)
  - [ä¿¡æ¯è®º](#ä¿¡æ¯è®º)

---

## å¼•è¨€

**æ³›åŒ–**ï¼ˆGeneralizationï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒç›®æ ‡ï¼š

> **æ¨¡å‹ä¸ä»…è¦åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¥½ï¼Œæ›´è¦åœ¨æœªè§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°å¥½ã€‚**

### æ ¸å¿ƒé—®é¢˜

1. **ä¸ºä»€ä¹ˆæ¨¡å‹èƒ½å¤Ÿæ³›åŒ–ï¼Ÿ**
2. **ä»€ä¹ˆå› ç´ å½±å“æ³›åŒ–èƒ½åŠ›ï¼Ÿ**
3. **å¦‚ä½•æé«˜æ³›åŒ–æ€§èƒ½ï¼Ÿ**
4. **å¦‚ä½•é¢„æµ‹æ³›åŒ–è¯¯å·®ï¼Ÿ**

### æ³›åŒ–å¤±è´¥çš„è¡¨ç°

**è¿‡æ‹Ÿåˆ**ï¼ˆOverfittingï¼‰ï¼š

```text
è®­ç»ƒè¯¯å·®å¾ˆå°ï¼Œæµ‹è¯•è¯¯å·®å¾ˆå¤§
```

**ä¾‹å­**ï¼š

```text
è®­ç»ƒé›†ï¼š100%å‡†ç¡®ç‡
æµ‹è¯•é›†ï¼š60%å‡†ç¡®ç‡  â† ä¸¥é‡è¿‡æ‹Ÿåˆ
```

**æ¬ æ‹Ÿåˆ**ï¼ˆUnderfittingï¼‰ï¼š

```text
è®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®éƒ½å¾ˆå¤§
```

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Generalization Error](https://en.wikipedia.org/wiki/Generalization_error)
- [Shalev-Shwartz & Ben-David, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Understanding Machine Learning

---

## æ³›åŒ–çš„å½¢å¼åŒ–

### 1. åŸºæœ¬å®šä¹‰

#### æ³›åŒ–è¯¯å·®ï¼ˆGeneralization Errorï¼‰

**å®šä¹‰**ï¼š

å‡è®¾ h åœ¨åˆ†å¸ƒ ğ’Ÿ ä¸Šçš„**çœŸå®é£é™©**ï¼ˆTrue Riskï¼‰ï¼š

```text
R(h) = E_{(x,y)~ğ’Ÿ}[â„“(h(x), y)]
```

å…¶ä¸­ â„“ æ˜¯æŸå¤±å‡½æ•°ã€‚

#### ç»éªŒè¯¯å·®ï¼ˆEmpirical Errorï¼‰

åœ¨è®­ç»ƒé›† S = {(xâ‚,yâ‚), ..., (xâ‚˜,yâ‚˜)} ä¸Šçš„**ç»éªŒé£é™©**ï¼ˆEmpirical Riskï¼‰ï¼š

```text
RÌ‚_S(h) = (1/m) âˆ‘áµ¢â‚Œâ‚áµ â„“(h(xáµ¢), yáµ¢)
```

#### æ³›åŒ–å·®è·ï¼ˆGeneralization Gapï¼‰

```text
Gen(h, S) = R(h) - RÌ‚_S(h)
```

**ç›®æ ‡**ï¼š

ä½¿ |Gen(h, S)| å°½å¯èƒ½å°ã€‚

### 2. ä¸€è‡´æ”¶æ•›ï¼ˆUniform Convergenceï¼‰

**å®šä¹‰**ï¼š

å‡è®¾ç±» â„‹ æ»¡è¶³**ä¸€è‡´æ”¶æ•›**ï¼Œå¦‚æœï¼š

```text
sup_{hâˆˆâ„‹} |R(h) - RÌ‚_S(h)| â†’^{P} 0  å½“ m â†’ âˆ
```

**æ„ä¹‰**ï¼š

å¯¹äº â„‹ ä¸­çš„**æ‰€æœ‰**å‡è®¾ï¼Œç»éªŒé£é™©éƒ½æ”¶æ•›åˆ°çœŸå®é£é™©ã€‚

**å®šç†ï¼ˆVapnik & Chervonenkisï¼‰**ï¼š

å¦‚æœ VC-dim(â„‹) < âˆï¼Œåˆ™ â„‹ æ»¡è¶³ä¸€è‡´æ”¶æ•›ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Vapnik, 1998](https://link.springer.com/book/10.1007/978-1-4757-3264-1) - Statistical Learning Theory

### 3. æ³›åŒ–ç•Œï¼ˆGeneralization Boundsï¼‰

**ç›®æ ‡**ï¼š

ç»™å‡ºé«˜æ¦‚ç‡ä¸‹çš„æ³›åŒ–è¯¯å·®ç•Œï¼š

```text
Pr[R(h) â‰¤ RÌ‚_S(h) + Îµ] â‰¥ 1 - Î´
```

**åŸºæœ¬æ€æƒ³**ï¼š

- å¤æ‚åº¦è¶Šé«˜ â†’ Îµ è¶Šå¤§
- æ ·æœ¬æ•°è¶Šå¤š â†’ Îµ è¶Šå°

---

## ç»å…¸æ³›åŒ–ç•Œ

### 1. Hoeffdingç•Œ

**å®šç†ï¼ˆHoeffdingä¸ç­‰å¼ï¼‰**ï¼š

è®¾ h æ˜¯å›ºå®šå‡è®¾ï¼ˆä¸è®­ç»ƒé›†æ— å…³ï¼‰ï¼Œåˆ™ï¼š

```text
Pr[|R(h) - RÌ‚_S(h)| > Îµ] â‰¤ 2 exp(-2mÎµÂ²)
```

**æ¨è®º**ï¼š

ä»¥æ¦‚ç‡è‡³å°‘ 1-Î´ï¼š

```text
R(h) â‰¤ RÌ‚_S(h) + âˆš(log(2/Î´) / (2m))
```

**å±€é™æ€§**ï¼š

åªé€‚ç”¨äºå•ä¸ªå‡è®¾ï¼Œä¸é€‚ç”¨äºä»æ•°æ®ä¸­å­¦ä¹ çš„å‡è®¾ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Hoeffding's Inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality)

### 2. VCç»´æ³›åŒ–ç•Œ

**å®šç†**ï¼š

è®¾ VC-dim(â„‹) = dï¼Œåˆ™ä»¥æ¦‚ç‡è‡³å°‘ 1-Î´ï¼Œå¯¹æ‰€æœ‰ h âˆˆ â„‹ï¼š

```text
R(h) â‰¤ RÌ‚_S(h) + O(âˆš((d log(m/d) + log(1/Î´)) / m))
```

**è§£è¯»**ï¼š

- æ³›åŒ–è¯¯å·® â‰¤ è®­ç»ƒè¯¯å·® + å¤æ‚åº¦é¡¹
- å¤æ‚åº¦é¡¹éš VCç»´ d å¢åŠ 
- å¤æ‚åº¦é¡¹éšæ ·æœ¬æ•° m å‡å°‘ï¼ˆ~1/âˆšmï¼‰

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Vapnik & Chervonenkis, 1971](https://en.wikipedia.org/wiki/VC_dimension) - On the Uniform Convergence

### 3. Rademacherå¤æ‚åº¦ç•Œ

**å®šç†**ï¼š

ä»¥æ¦‚ç‡è‡³å°‘ 1-Î´ï¼Œå¯¹æ‰€æœ‰ h âˆˆ â„‹ï¼š

```text
R(h) â‰¤ RÌ‚_S(h) + 2â„›_m(â„‹) + 3âˆš(log(2/Î´) / (2m))
```

å…¶ä¸­ â„›_m(â„‹) æ˜¯ Rademacher å¤æ‚åº¦ã€‚

**ä¼˜åŠ¿**ï¼š

- æ•°æ®ä¾èµ–ï¼ˆä¸æ˜¯æœ€åæƒ…å†µï¼‰
- æ›´ç´§çš„ç•Œ

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Bartlett & Mendelson, 2002](https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf) - Rademacher and Gaussian Complexities

### 4. PAC-Bayesç•Œ

**å®šç†**ï¼š

è®¾ P æ˜¯å‡è®¾çš„å…ˆéªŒåˆ†å¸ƒï¼ŒQ æ˜¯åéªŒåˆ†å¸ƒï¼Œåˆ™ä»¥æ¦‚ç‡è‡³å°‘ 1-Î´ï¼š

```text
E_{h~Q}[R(h)] â‰¤ E_{h~Q}[RÌ‚_S(h)] + âˆš((KL(Qâ€–P) + log(2âˆšm/Î´)) / (2m))
```

**æ„ä¹‰**ï¼š

- å¦‚æœåéªŒæ¥è¿‘å…ˆéªŒï¼ˆKLå°ï¼‰ï¼Œæ³›åŒ–å¥½
- è§£é‡Šäº†ä¸ºä»€ä¹ˆè´å¶æ–¯æ–¹æ³•æ³›åŒ–å¥½

**å‚è€ƒæ–‡çŒ®**ï¼š

- [McAllester, 1999](https://www.sciencedirect.com/science/article/pii/S0890540198926247) - PAC-Bayesian Model Averaging

---

## åå·®-æ–¹å·®åˆ†è§£

### 1. åŸºæœ¬åˆ†è§£

**å®šç†ï¼ˆBias-Variance Decompositionï¼‰**ï¼š

å¯¹äºå¹³æ–¹æŸå¤±ï¼ŒæœŸæœ›æ³›åŒ–è¯¯å·®å¯ä»¥åˆ†è§£ä¸ºï¼š

```text
E_S[E_{(x,y)}[(h_S(x) - y)Â²]] = BiasÂ² + Variance + Noise
```

å…¶ä¸­ï¼š

1. **åå·®**ï¼ˆBiasï¼‰ï¼š

    ```text
    Bias = E_S[h_S(x)] - f*(x)
    ```

    å…¶ä¸­ f*(x) æ˜¯çœŸå®å‡½æ•°ã€‚

2. **æ–¹å·®**ï¼ˆVarianceï¼‰ï¼š

    ```text
    Variance = E_S[(h_S(x) - E_S[h_S(x)])Â²]
    ```

3. **å™ªå£°**ï¼ˆNoiseï¼‰ï¼š

    ```text
    Noise = E[(y - f*(x))Â²]
    ```

ï¼ˆä¸å¯çº¦è¯¯å·®ï¼Œä¸æ¨¡å‹æ— å…³ï¼‰

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)

### 2. åå·®-æ–¹å·®æƒè¡¡ï¼ˆTradeoffï¼‰

**æƒè¡¡å…³ç³»**ï¼š

| æ¨¡å‹å¤æ‚åº¦ | åå·® | æ–¹å·® | æ€»è¯¯å·® |
|----------|------|------|--------|
| **ä½**ï¼ˆç®€å•æ¨¡å‹ï¼‰ | é«˜ â†‘ | ä½ â†“ | å¯èƒ½é«˜ï¼ˆæ¬ æ‹Ÿåˆï¼‰ |
| **ä¸­ç­‰** | ä¸­ | ä¸­ | æœ€ä½ âœ“ |
| **é«˜**ï¼ˆå¤æ‚æ¨¡å‹ï¼‰ | ä½ â†“ | é«˜ â†‘ | å¯èƒ½é«˜ï¼ˆè¿‡æ‹Ÿåˆï¼‰ |

**å›¾ç¤º**ï¼š

```text
è¯¯å·®
  â†‘
  |         æ€»è¯¯å·®
  |        /  \
  |       /    \
  |      /      \___
  |  ___/           \
  | /æ–¹å·®            åå·®\
  |/____________________\___
  |                         
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ¨¡å‹å¤æ‚åº¦
  ç®€å•              å¤æ‚
```

### 3. æ·±åº¦å­¦ä¹ ä¸­çš„"ç ´è£‚"

**ä¼ ç»Ÿç†è®ºé¢„æµ‹**ï¼š

å¤æ‚æ¨¡å‹ï¼ˆè¿‡å‚æ•°åŒ–ï¼‰â†’ é«˜æ–¹å·® â†’ è¿‡æ‹Ÿåˆ

**å®é™…è§‚å¯Ÿ**ï¼ˆBelkin et al., 2019ï¼‰ï¼š

```text
è¯¯å·®
  â†‘
  |    \      /
  |     \    /
  |      \  /
  |       \/  â† ä¼ ç»Ÿæœ€ä¼˜ç‚¹
  |       /\
  |      /  \___
  |     /       \___
  |____/____________\___
  |   æ’å€¼é˜ˆå€¼        
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æ¨¡å‹å¤æ‚åº¦
```

**åŒä¸‹é™ç°è±¡**ï¼ˆDouble Descentï¼‰ï¼š

1. **ç»å…¸åŒºåŸŸ**ï¼šæ¬ æ‹Ÿåˆ â†’ æœ€ä¼˜ â†’ è¿‡æ‹Ÿåˆ
2. **æ’å€¼é˜ˆå€¼**ï¼šæ¨¡å‹åˆšå¥½èƒ½æ‹Ÿåˆæ‰€æœ‰è®­ç»ƒæ•°æ®
3. **ç°ä»£åŒºåŸŸ**ï¼šè¿‡å‚æ•°åŒ– â†’ è¯¯å·®å†æ¬¡ä¸‹é™ï¼

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Belkin et al., 2019](https://www.pnas.org/doi/10.1073/pnas.1903070116) - Reconciling Modern Machine Learning

---

## æ­£åˆ™åŒ–ç†è®º

### 1. æ­£åˆ™åŒ–çš„å®šä¹‰

**ç›®æ ‡å‡½æ•°**ï¼š

```text
min_h [RÌ‚_S(h) + Î»Î©(h)]
```

å…¶ä¸­ï¼š

- RÌ‚_S(h)ï¼šç»éªŒé£é™©ï¼ˆæ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼‰
- Î©(h)ï¼šæ­£åˆ™åŒ–é¡¹ï¼ˆæ§åˆ¶å¤æ‚åº¦ï¼‰
- Î»ï¼šæ­£åˆ™åŒ–å‚æ•°ï¼ˆæƒè¡¡ï¼‰

### 2. å¸¸è§æ­£åˆ™åŒ–æ–¹æ³•

#### L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰

```text
Î©(w) = â€–wâ€–â‚‚Â² = âˆ‘áµ¢ wáµ¢Â²
```

**æ•ˆæœ**ï¼š

- æƒé‡è¶‹å‘äºå°å€¼ï¼ˆä½†ä¸ä¸º0ï¼‰
- å¹³æ»‘è§£

#### L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰

```text
Î©(w) = â€–wâ€–â‚ = âˆ‘áµ¢ |wáµ¢|
```

**æ•ˆæœ**ï¼š

- äº§ç”Ÿç¨€ç–è§£ï¼ˆè®¸å¤šæƒé‡=0ï¼‰
- ç‰¹å¾é€‰æ‹©

#### Elastic Net

```text
Î©(w) = Î±â€–wâ€–â‚ + (1-Î±)â€–wâ€–â‚‚Â²
```

ç»“åˆ L1 å’Œ L2 çš„ä¼˜ç‚¹ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Wikipedia: Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics))

### 3. æ­£åˆ™åŒ–çš„æ³›åŒ–æ•ˆæœ

**å®šç†ï¼ˆNorm-basedæ³›åŒ–ç•Œï¼‰**ï¼š

å¯¹äºç¥ç»ç½‘ç»œï¼Œå¦‚æœæƒé‡çŸ©é˜µçš„è°±èŒƒæ•°ä¹‹ç§¯ä¸º Bï¼Œåˆ™ï¼š

```text
æ³›åŒ–è¯¯å·® = O(B / âˆšm)
```

**æ„ä¹‰**ï¼š

- å°èŒƒæ•° â†’ å¥½æ³›åŒ–
- è§£é‡Šäº†L2æ­£åˆ™åŒ–ä¸ºä»€ä¹ˆæœ‰æ•ˆ

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Bartlett et al., 2017](https://arxiv.org/abs/1706.08498) - Spectrally-normalized Margin Bounds for Neural Networks

### 4. æ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–

#### Dropout

**æ–¹æ³•**ï¼š

è®­ç»ƒæ—¶éšæœº"å…³é—­"éƒ¨åˆ†ç¥ç»å…ƒï¼ˆæ¦‚ç‡ pï¼‰ã€‚

**æ•ˆæœ**ï¼š

- é˜²æ­¢co-adaptationï¼ˆç¥ç»å…ƒäº’ç›¸ä¾èµ–ï¼‰
- ç›¸å½“äºé›†æˆå­¦ä¹ 

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Srivastava et al., 2014](https://jmlr.org/papers/v15/srivastava14a.html) - Dropout: A Simple Way to Prevent Neural Networks from Overfitting

#### Batch Normalization

**æ–¹æ³•**ï¼š

å½’ä¸€åŒ–æ¯å±‚çš„è¾“å…¥ï¼š

```text
xÌ‚ = (x - Î¼_batch) / âˆš(ÏƒÂ²_batch + Îµ)
```

**æ•ˆæœ**ï¼š

- åŠ é€Ÿè®­ç»ƒ
- éšå¼æ­£åˆ™åŒ–

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Ioffe & Szegedy, 2015](https://arxiv.org/abs/1502.03167) - Batch Normalization

#### Early Stopping

**æ–¹æ³•**ï¼š

ç›‘æ§éªŒè¯é›†è¯¯å·®ï¼Œåœ¨å¼€å§‹ä¸Šå‡æ—¶åœæ­¢è®­ç»ƒã€‚

**ç†è®º**ï¼š

- é™åˆ¶ä¼˜åŒ–æ­¥æ•° = éšå¼æ­£åˆ™åŒ–
- ç­‰ä»·äºL2æ­£åˆ™åŒ–ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹ï¼‰

#### Data Augmentation

**æ–¹æ³•**ï¼š

äººå·¥å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆæ—‹è½¬ã€ç¿»è½¬ã€è£å‰ªç­‰ï¼‰ã€‚

**æ•ˆæœ**ï¼š

- å¢åŠ æœ‰æ•ˆæ ·æœ¬æ•°
- æ³¨å…¥å…ˆéªŒçŸ¥è¯†ï¼ˆä¸å˜æ€§ï¼‰

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Shorten & Khoshgoftaar, 2019](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0) - A Survey on Image Data Augmentation

---

## æ·±åº¦å­¦ä¹ çš„æ³›åŒ–ä¹‹è°œ

### 1. ä¼ ç»Ÿç†è®ºçš„å›°æƒ‘

**è§‚å¯Ÿ**ï¼š

ç°ä»£æ·±åº¦ç½‘ç»œï¼š

- å‚æ•°æ•°é‡ W >> æ ·æœ¬æ•° m
- å¯ä»¥å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼ˆåŒ…æ‹¬éšæœºæ ‡ç­¾ï¼ï¼‰
- ä½†ä»èƒ½å¾ˆå¥½åœ°æ³›åŒ–

**ä¼ ç»Ÿç†è®ºé¢„æµ‹**ï¼š

æ ¹æ® VC ç»´ç†è®ºï¼š

```text
VC-dim â‰ˆ O(W log W)
æ³›åŒ–è¯¯å·® â‰ˆ O(âˆš(W log W / m))
```

å½“ W >> m æ—¶ï¼Œåº”è¯¥ä¸¥é‡è¿‡æ‹Ÿåˆã€‚

**å®é™…**ï¼š

æ³›åŒ–è‰¯å¥½ï¼

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Zhang et al., 2017](https://arxiv.org/abs/1611.03530) - Understanding Deep Learning Requires Rethinking Generalization

### 2. Zhangç­‰äººçš„å®éªŒ

**å…³é”®å®éªŒ**ï¼š

1. **éšæœºæ ‡ç­¾å®éªŒ**ï¼š

    ```text
    å°†è®­ç»ƒé›†æ ‡ç­¾éšæœºæ‰“ä¹±
    ç»“æœï¼šç½‘ç»œä»èƒ½è¾¾åˆ°0è®­ç»ƒè¯¯å·®ï¼ˆä½†æµ‹è¯•è¯¯å·®éšæœºï¼‰
    ```

    **ç»“è®º**ï¼šæ·±åº¦ç½‘ç»œæœ‰è¶³å¤Ÿå®¹é‡è®°å¿†ä»»æ„æ ‡ç­¾ã€‚

2. **éšæœºåƒç´ å®éªŒ**ï¼š

    ```text
    å°†è®­ç»ƒå›¾åƒæ›¿æ¢ä¸ºéšæœºå™ªå£°
    ç»“æœï¼šç½‘ç»œä»èƒ½æ‹Ÿåˆ
    ```

    **ç»“è®º**ï¼šå®¹é‡ä¸æ˜¯æ³›åŒ–çš„å”¯ä¸€è§£é‡Šã€‚

3. **æ˜¾å¼æ­£åˆ™åŒ–ä¸æ˜¯å¿…éœ€çš„**ï¼š

```text
å»é™¤æ‰€æœ‰æ­£åˆ™åŒ–ï¼ˆDropoutã€æ•°æ®å¢å¼ºã€æƒé‡è¡°å‡ï¼‰
ç»“æœï¼šæ³›åŒ–æ€§èƒ½ç•¥æœ‰ä¸‹é™ï¼Œä½†ä»ç„¶ä¸é”™
```

**ç»“è®º**ï¼šå¿…å®šå­˜åœ¨"éšå¼æ­£åˆ™åŒ–"æœºåˆ¶ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Zhang et al., 2017](https://arxiv.org/abs/1611.03530) - Understanding Deep Learning Requires Rethinking Generalization

### 3. è§£é‡Šå°è¯•

#### 1. éšå¼æ­£åˆ™åŒ–

**è§‚å¯Ÿ**ï¼š

SGD è®­ç»ƒçš„ç½‘ç»œå€¾å‘äºæ‰¾åˆ°"ç®€å•"çš„è§£ã€‚

**åŸå› **ï¼š

- SGD çš„å™ªå£°èµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨
- å¹³å¦æœ€å°å€¼ï¼ˆflat minimaï¼‰æ³›åŒ–æ›´å¥½

#### 2. è¿‡å‚æ•°åŒ–çš„ä¼˜åŠ¿

**ç†è®º**ï¼ˆNeural Tangent Kernelï¼‰ï¼š

æå®½ç½‘ç»œåœ¨è®­ç»ƒæ—¶å‡ ä¹æ˜¯çº¿æ€§çš„ï¼Œå®¹æ˜“ä¼˜åŒ–ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Jacot et al., 2018](https://arxiv.org/abs/1806.07572) - Neural Tangent Kernel

#### 3. å½’çº³åç½®

**CNNçš„å½’çº³åç½®**ï¼š

- å±€éƒ¨æ€§ï¼ˆå·ç§¯ï¼‰
- å¹³ç§»ä¸å˜æ€§ï¼ˆæƒé‡å…±äº«ï¼‰

è¿™äº›åç½®å¤§å¹…ç¼©å°æœ‰æ•ˆå‡è®¾ç©ºé—´ã€‚

#### 4. æ•°æ®ç»“æ„

**è‡ªç„¶æ•°æ®ä¸æ˜¯éšæœºçš„**ï¼š

- æœ‰ä½ç»´ç»“æ„ï¼ˆæµå½¢å‡è®¾ï¼‰
- æœ‰å¼ºç›¸å…³æ€§

ç½‘ç»œåˆ©ç”¨è¿™äº›ç»“æ„ï¼Œè€Œä¸æ˜¯è®°å¿†ã€‚

---

## éšå¼æ­£åˆ™åŒ–

### 1. SGDçš„éšå¼åç½®

**è§‚å¯Ÿ**ï¼š

SGD å€¾å‘äºæ‰¾åˆ°**æœ€å°èŒƒæ•°**çš„è§£ã€‚

**å®šç†ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰**ï¼š

å¯¹äºçº¿æ€§å¯åˆ†æ•°æ®ï¼ŒSGD æ”¶æ•›åˆ°**æœ€å¤§é—´éš”è§£**ï¼ˆç­‰ä»·äºSVMï¼‰ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Soudry et al., 2018](https://arxiv.org/abs/1710.10345) - The Implicit Bias of Gradient Descent on Separable Data

### 2. å¹³å¦æœ€å°å€¼ï¼ˆFlat Minimaï¼‰

**å‡è®¾ï¼ˆHochreiter & Schmidhuber, 1997ï¼‰**ï¼š

å¹³å¦çš„æœ€å°å€¼æ³›åŒ–æ›´å¥½ã€‚

**ç›´è§‰**ï¼š

```text
å°–é”æœ€å°å€¼ï¼š
  â†‘
  |\
  | \  â† å°æ‰°åŠ¨å¯¼è‡´å¤§è¯¯å·®å¢åŠ 
  |  \
  â””â”€â”€â”€

å¹³å¦æœ€å°å€¼ï¼š
  â†‘
  |
  |___ â† å¯¹æ‰°åŠ¨é²æ£’
  |
  â””â”€â”€â”€
```

**å¦‚ä½•è¡¡é‡å¹³å¦åº¦ï¼Ÿ**

HessiançŸ©é˜µçš„æœ€å¤§ç‰¹å¾å€¼ï¼š

```text
Î»_max(âˆ‡Â²L(w))
```

å° â†’ å¹³å¦ â†’ æ³›åŒ–å¥½

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Hochreiter & Schmidhuber, 1997](https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.1.1) - Flat Minima

### 3. æ‰¹é‡å¤§å°çš„å½±å“

**è§‚å¯Ÿ**ï¼š

å°æ‰¹é‡ â†’ å¥½æ³›åŒ–
å¤§æ‰¹é‡ â†’ å·®æ³›åŒ–

**è§£é‡Š**ï¼š

- å°æ‰¹é‡ï¼šSGDå™ªå£°å¤§ â†’ å€¾å‘äºå¹³å¦æœ€å°å€¼
- å¤§æ‰¹é‡ï¼šæ¥è¿‘æ¢¯åº¦ä¸‹é™ â†’ å¯èƒ½é™·å…¥å°–é”æœ€å°å€¼

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Keskar et al., 2017](https://arxiv.org/abs/1609.04836) - On Large-Batch Training for Deep Learning

### 4. å­¦ä¹ ç‡çš„å½±å“

**è§‚å¯Ÿ**ï¼š

é€‚å½“çš„å­¦ä¹ ç‡è°ƒåº¦ï¼ˆå¦‚å­¦ä¹ ç‡è¡°å‡ï¼‰æå‡æ³›åŒ–ã€‚

**è§£é‡Š**ï¼š

- åˆæœŸï¼šå¤§å­¦ä¹ ç‡å¿«é€Ÿæ¢ç´¢
- åæœŸï¼šå°å­¦ä¹ ç‡ç²¾ç»†ä¼˜åŒ–ï¼Œé¿å…è¿‡æ‹Ÿåˆ

---

## æ³›åŒ–çš„æ–°è§†è§’

### 1. å‹ç¼©ç•Œï¼ˆCompression Boundsï¼‰

**æ€æƒ³**ï¼š

å¦‚æœå¯ä»¥å°†æ¨¡å‹"å‹ç¼©"ï¼Œåˆ™æ³›åŒ–å¥½ã€‚

**å®šç†ï¼ˆArora et al., 2018ï¼‰**ï¼š

å¦‚æœç½‘ç»œå¯ä»¥å‹ç¼©åˆ° k ä½ï¼Œåˆ™æ³›åŒ–è¯¯å·®ï¼š

```text
O(âˆš(k / m))
```

**åº”ç”¨**ï¼š

- æƒé‡é‡åŒ–
- å‰ªæ
- çŸ¥è¯†è’¸é¦

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Arora et al., 2018](https://arxiv.org/abs/1802.05296) - Stronger Generalization Bounds for Deep Nets via a Compression Approach

### 2. ä¿¡æ¯è®ºè§†è§’

**ä¿¡æ¯ç“¶é¢ˆç†è®º**ï¼ˆTishby & Zaslavsky, 2015ï¼‰ï¼š

å¥½çš„è¡¨ç¤ºåº”è¯¥ï¼š

1. æœ€å¤§åŒ–ä¸æ ‡ç­¾ Y çš„äº’ä¿¡æ¯ I(T;Y)
2. æœ€å°åŒ–ä¸è¾“å…¥ X çš„äº’ä¿¡æ¯ I(T;X)

```text
min I(T;X) - Î²I(T;Y)
```

**æ„ä¹‰**ï¼š

- å‹ç¼©è¾“å…¥ï¼ˆå»é™¤æ— å…³ä¿¡æ¯ï¼‰
- ä¿ç•™é¢„æµ‹ç›¸å…³ä¿¡æ¯

**äº‰è®®**ï¼š

åç»­ç ”ç©¶è´¨ç–‘å…¶åœ¨æ·±åº¦ç½‘ç»œä¸­çš„æ™®éæ€§ã€‚

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Tishby & Zaslavsky, 2015](https://arxiv.org/abs/1503.02406) - Deep Learning and the Information Bottleneck Principle

### 3. æµå½¢å‡è®¾

**å‡è®¾**ï¼š

é«˜ç»´æ•°æ®å®é™…ä¸Šä½äºä½ç»´æµå½¢ä¸Šã€‚

**æ¨è®º**ï¼š

- æœ‰æ•ˆç»´åº¦ << è¾“å…¥ç»´åº¦
- è§£é‡Šäº†ä¸ºä»€ä¹ˆé«˜ç»´æ•°æ®å¯å­¦ä¹ 

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Fefferman et al., 2016](https://www.pnas.org/doi/10.1073/pnas.1602413113) - Testing the Manifold Hypothesis

### 4. åŒä¸‹é™ä¸æ’å€¼

**ç°ä»£æ³›åŒ–ç†è®º**ï¼ˆBelkin, 2021ï¼‰ï¼š

- ä¼ ç»Ÿï¼šé¿å…å®Œç¾æ‹Ÿåˆè®­ç»ƒæ•°æ®
- ç°ä»£ï¼šè¿‡å‚æ•°åŒ–æ¨¡å‹å¯ä»¥**æ’å€¼**ï¼ˆå®Œç¾æ‹Ÿåˆï¼‰ä¸”æ³›åŒ–

**å…³é”®æ¡ä»¶**ï¼š

- è¿‡å‚æ•°åŒ–
- å¥½çš„å½’çº³åç½®
- é€‚å½“çš„ä¼˜åŒ–ç®—æ³•

**å‚è€ƒæ–‡çŒ®**ï¼š

- [Belkin, 2021](https://arxiv.org/abs/2105.14368) - Fit without Fear: Remarkable Mathematical Phenomena

---

## å®è·µä¸­çš„æ³›åŒ–ç­–ç•¥

### 1. æ•°æ®å±‚é¢

#### 1.1 å¢åŠ æ•°æ®é‡

**æœ€ç›´æ¥æ–¹æ³•**ï¼š

æ›´å¤šæ•°æ® â†’ æ›´å¥½æ³›åŒ–

**æ•°æ®å¢å¼º**ï¼š

- å›¾åƒï¼šæ—‹è½¬ã€ç¿»è½¬ã€è£å‰ªã€é¢œè‰²å˜æ¢
- æ–‡æœ¬ï¼šåŒä¹‰è¯æ›¿æ¢ã€å›è¯‘

#### 1.2 æ•°æ®æ¸…æ´—

**å»é™¤å™ªå£°æ ‡ç­¾**ï¼š

```text
å™ªå£°æ ‡ç­¾ â†’ è¿‡æ‹Ÿåˆå™ªå£° â†’ å·®æ³›åŒ–
```

#### 1.3 è¿ç§»å­¦ä¹ 

**é¢„è®­ç»ƒæ¨¡å‹**ï¼š

- åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒ
- åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šå¾®è°ƒ

**ä¼˜åŠ¿**ï¼š

- æœ‰æ•ˆæ•°æ®é‡å¢åŠ 
- å­¦åˆ°é€šç”¨ç‰¹å¾

### 2. æ¨¡å‹å±‚é¢

#### 2.1 é€‰æ‹©åˆé€‚å¤æ‚åº¦

**åŸåˆ™**ï¼š

å¤æ‚åº¦ä¸æ•°æ®é‡åŒ¹é…ã€‚

**æ–¹æ³•**ï¼š

- äº¤å‰éªŒè¯é€‰æ‹©æ¨¡å‹å¤§å°
- ä»ç®€å•æ¨¡å‹å¼€å§‹

#### 2.2 æ­£åˆ™åŒ–

**æ˜¾å¼æ­£åˆ™åŒ–**ï¼š

- L1/L2 æƒé‡è¡°å‡
- Dropout
- Batch Normalization

**éšå¼æ­£åˆ™åŒ–**ï¼š

- æ—©åœ
- æ•°æ®å¢å¼º

#### 2.3 é›†æˆæ–¹æ³•

**Bagging**ï¼š

è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œå¹³å‡é¢„æµ‹ã€‚

**Boosting**ï¼š

è¿­ä»£è®­ç»ƒï¼Œå…³æ³¨éš¾æ ·æœ¬ã€‚

**æ•ˆæœ**ï¼š

- é™ä½æ–¹å·®
- æé«˜æ³›åŒ–

### 3. è®­ç»ƒå±‚é¢

#### 3.1 äº¤å‰éªŒè¯

**K-æŠ˜äº¤å‰éªŒè¯**ï¼š

```text
å°†æ•°æ®åˆ†æˆKä»½
æ¯æ¬¡ç”¨K-1ä»½è®­ç»ƒï¼Œ1ä»½éªŒè¯
å¹³å‡Kæ¬¡ç»“æœ
```

**ç”¨é€”**ï¼š

- æ¨¡å‹é€‰æ‹©
- è¶…å‚æ•°è°ƒä¼˜

#### 3.2 æ—©åœ

**ç›‘æ§éªŒè¯é›†**ï¼š

```text
éªŒè¯è¯¯å·®å¼€å§‹ä¸Šå‡ â†’ åœæ­¢è®­ç»ƒ
```

#### 3.3 å­¦ä¹ ç‡è°ƒåº¦

**ç­–ç•¥**ï¼š

- ä½™å¼¦é€€ç«
- åˆ†æ®µè¡°å‡
- warmup + decay

### 4. è¯„ä¼°å±‚é¢

#### 4.1 ç‹¬ç«‹æµ‹è¯•é›†

**é‡è¦æ€§**ï¼š

```text
è®­ç»ƒé›† â† è®­ç»ƒ
éªŒè¯é›† â† è°ƒå‚
æµ‹è¯•é›† â† æœ€ç»ˆè¯„ä¼°ï¼ˆåªç”¨ä¸€æ¬¡ï¼ï¼‰
```

#### 4.2 æ³›åŒ–å·®è·ç›‘æ§

**æŒ‡æ ‡**ï¼š

```text
æ³›åŒ–å·®è· = æµ‹è¯•è¯¯å·® - è®­ç»ƒè¯¯å·®
```

å¤§ â†’ è¿‡æ‹Ÿåˆ

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ³›åŒ–çš„å®šä¹‰**ï¼šæµ‹è¯•è¯¯å·® vs è®­ç»ƒè¯¯å·®

2. **ç»å…¸æ³›åŒ–ç•Œ**ï¼š
   - VCç»´ç•Œï¼šO(âˆš(d/m))
   - Rademacherç•Œï¼šæ•°æ®ä¾èµ–
   - PAC-Bayesç•Œï¼šå…ˆéªŒ-åéªŒ

3. **åå·®-æ–¹å·®æƒè¡¡**ï¼š
   - ç®€å•æ¨¡å‹ï¼šé«˜åå·®ï¼Œä½æ–¹å·®
   - å¤æ‚æ¨¡å‹ï¼šä½åå·®ï¼Œé«˜æ–¹å·®
   - åŒä¸‹é™ï¼šç°ä»£ç°è±¡

4. **æ­£åˆ™åŒ–**ï¼š
   - æ˜¾å¼ï¼šL1/L2ã€Dropout
   - éšå¼ï¼šSGDã€æ—©åœ

5. **æ·±åº¦å­¦ä¹ ä¹‹è°œ**ï¼š
   - è¿‡å‚æ•°åŒ–ä½†æ³›åŒ–å¥½
   - éšå¼æ­£åˆ™åŒ–æœºåˆ¶
   - å½’çº³åç½®çš„é‡è¦æ€§

6. **å®è·µç­–ç•¥**ï¼š
   - æ•°æ®å¢å¼º
   - è¿ç§»å­¦ä¹ 
   - äº¤å‰éªŒè¯
   - é›†æˆæ–¹æ³•

### ç†è®º vs å®è·µ

| ç»´åº¦ | ä¼ ç»Ÿç†è®º | æ·±åº¦å­¦ä¹ å®è·µ |
|------|---------|-------------|
| **å®¹é‡** | é¿å…è¿‡å¤§ | è¶Šå¤§è¶Šå¥½ï¼ˆè¿‡å‚æ•°åŒ–ï¼‰ |
| **æ‹Ÿåˆç¨‹åº¦** | é¿å…å®Œç¾æ‹Ÿåˆ | æ’å€¼ï¼ˆ0è®­ç»ƒè¯¯å·®ï¼‰OK |
| **æ­£åˆ™åŒ–** | å¿…éœ€ | æœ‰æ—¶å¯é€‰ |
| **è§£é‡Š** | VCç»´ã€æ ·æœ¬å¤æ‚åº¦ | éšå¼æ­£åˆ™åŒ–ã€å½’çº³åç½® |

### æœªè§£ä¹‹è°œ

1. **ä¸ºä»€ä¹ˆSGDæ‰¾åˆ°çš„è§£æ³›åŒ–å¥½ï¼Ÿ**
2. **å½’çº³åç½®å¦‚ä½•ç²¾ç¡®åœ°å½±å“æ³›åŒ–ï¼Ÿ**
3. **åŒä¸‹é™ç°è±¡çš„å®Œæ•´ç†è®ºï¼Ÿ**
4. **å¦‚ä½•é¢„å…ˆé¢„æµ‹æ³›åŒ–æ€§èƒ½ï¼Ÿ**

### å“²å­¦åæ€

> **æ³›åŒ–æ˜¯å­¦ä¹ çš„æœ¬è´¨ã€‚å®ƒæ­ç¤ºäº†ä¸€ä¸ªæ·±åˆ»çš„æ´å¯Ÿï¼šä¸–ç•Œæ˜¯æœ‰ç»“æ„çš„ã€å¯é¢„æµ‹çš„ã€‚å¦‚æœä¸–ç•Œå®Œå…¨éšæœºï¼Œæ³›åŒ–å°±ä¸å¯èƒ½ã€‚æœºå™¨å­¦ä¹ çš„æˆåŠŸï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨åˆ©ç”¨ä¸–ç•Œçš„è§„å¾‹æ€§ã€‚**

---

## å‚è€ƒæ–‡çŒ®

### åŸºç¡€ç†è®º

1. [Wikipedia: Generalization Error](https://en.wikipedia.org/wiki/Generalization_error)
2. [Shalev-Shwartz & Ben-David, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/) - Understanding Machine Learning
3. [Vapnik, 1998](https://link.springer.com/book/10.1007/978-1-4757-3264-1) - Statistical Learning Theory

### æ³›åŒ–ç•Œ

1. [Wikipedia: Hoeffding's Inequality](https://en.wikipedia.org/wiki/Hoeffding%27s_inequality)
2. [Bartlett & Mendelson, 2002](https://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf) - Rademacher and Gaussian Complexities
3. [McAllester, 1999](https://www.sciencedirect.com/science/article/pii/S0890540198926247) - PAC-Bayesian Model Averaging

### åå·®-æ–¹å·®

1. [Wikipedia: Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
2. [Belkin et al., 2019](https://www.pnas.org/doi/10.1073/pnas.1903070116) - Reconciling Modern Machine Learning
3. [Belkin, 2021](https://arxiv.org/abs/2105.14368) - Fit without Fear

### æ·±åº¦å­¦ä¹ æ³›åŒ–

1. [Zhang et al., 2017](https://arxiv.org/abs/1611.03530) - Understanding Deep Learning Requires Rethinking Generalization
2. [Bartlett et al., 2017](https://arxiv.org/abs/1706.08498) - Spectrally-normalized Margin Bounds
3. [Arora et al., 2018](https://arxiv.org/abs/1802.05296) - Stronger Generalization Bounds for Deep Nets

### éšå¼æ­£åˆ™åŒ–1

1. [Soudry et al., 2018](https://arxiv.org/abs/1710.10345) - The Implicit Bias of Gradient Descent
2. [Hochreiter & Schmidhuber, 1997](https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.1.1) - Flat Minima
3. [Keskar et al., 2017](https://arxiv.org/abs/1609.04836) - On Large-Batch Training

### ç¥ç»åˆ‡çº¿æ ¸

1. [Jacot et al., 2018](https://arxiv.org/abs/1806.07572) - Neural Tangent Kernel

### æ­£åˆ™åŒ–

1. [Wikipedia: Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics))
2. [Srivastava et al., 2014](https://jmlr.org/papers/v15/srivastava14a.html) - Dropout
3. [Ioffe & Szegedy, 2015](https://arxiv.org/abs/1502.03167) - Batch Normalization

### ä¿¡æ¯è®º

1. [Tishby & Zaslavsky, 2015](https://arxiv.org/abs/1503.02406) - Deep Learning and the Information Bottleneck

---

*æœ¬æ–‡æ¡£ç³»ç»Ÿé˜è¿°äº†æ³›åŒ–ç†è®ºçš„æ ¸å¿ƒæ¦‚å¿µã€ç»å…¸å®šç†å’Œç°ä»£æŒ‘æˆ˜ï¼Œä¸ºç†è§£æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æä¾›äº†å…¨é¢çš„ç†è®ºæ¡†æ¶ã€‚*
