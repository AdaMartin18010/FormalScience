# 2.5 流式编译 (Streaming Compilation)

## 核心命题

**流式编译定理**：
\[
\forall M : \text{Module}, \exists \text{Partition}(M) = [C_1, C_2, \ldots, C_n] : \text{Compile}(M) = \bigoplus_{i=1}^n \text{Compile}(C_i)
\]

**内存界限**：
\[
\text{Memory}_{\text{stream}}(M) = O(\max_{f \in M}(|f|)) \quad \text{vs} \quad \text{Memory}_{\text{batch}}(M) = O(|M|)
\]

**延迟优化**：
\[
\text{TTFB}_{\text{stream}} < \text{TTFB}_{\text{batch}} \quad \text{但} \quad \text{Throughput}_{\text{stream}} \leq \text{Throughput}_{\text{batch}}
\]

---

## 流式编译架构

### 管道阶段

**三阶段模型**：
\[
\text{Network} \xrightarrow{\text{Decode}} \text{Validate} \xrightarrow{\text{Compile}} \text{CodeGen}
\]

**并行性**：

```
Time ─────────────────────────────►
      ┌────────┐
Net   │ Func 1 │ Func 2 │ Func 3 │
      └────────┘
         ┌────────┐
Valid    │ Func 1 │ Func 2 │ Func 3 │
         └────────┘
            ┌────────┐
Compile     │ Func 1 │ Func 2 │ Func 3 │
            └────────┘
```

**吞吐量**：
\[
\text{Throughput} = \min(\text{BW}_{\text{net}}, \text{BW}_{\text{valid}}, \text{BW}_{\text{compile}})
\]

---

## Section 级流式

### 前置 Section 缓冲

**必须缓冲**：

- **Type Section**：后续所有引用需要
- **Import Section**：确定函数索引偏移
- **Function Section**：验证 Code Section 长度

**可流式**：

- **Code Section**：逐函数处理
- **Data Section**：延迟到实例化

**示例**：

```
接收：[Magic][Version][Type][Import][Function]
缓冲：Type, Import, Function
流式：开始接收 Code Section 第一个函数
```

### 延迟验证策略

**两阶段验证**：
\[
\begin{aligned}
\text{Stage 1} &: \text{结构完整性 + 引用完整性} \\
\text{Stage 2} &: \text{类型栈验证（逐函数）}
\end{aligned}
\]

**批判**：
> 延迟验证增加了状态复杂度。如果后期验证失败，前期已编译的函数需要回滚，但 JIT 代码可能已经分配了系统资源。

---

## 函数级流式

### Code Section 结构

**函数体格式**：
\[
\text{FunctionBody} := \langle \text{size}, \text{locals}, \text{instructions} \rangle
\]

**流式友好性**：

- `size` 前置：可分配精确缓冲区
- 自包含：函数间无依赖

### 增量解码

**算法**：

```python
def stream_decode_code_section(stream):
    count = read_uleb128(stream)
    for i in range(count):
        size = read_uleb128(stream)
        body = stream.read(size)  # 精确读取
        yield decode_function_body(body)
```

**内存管理**：
\[
\text{Buffer}_i = \text{size}(\text{Func}_i) \quad (\text{不需要保留整个 Code Section})
\]

---

## 编译策略

### Baseline JIT 优先

**快速编译路径**：

```
接收函数 → 寄存器分配 → 线性指令翻译 → 代码段写入
```

**时间约束**：
\[
T_{\text{baseline}} < T_{\text{download}} \quad (\text{保证零等待})
\]

**示例**（V8）：

```
下载速度：10 MB/s
基线 JIT：50 MB/s (编译速度)
流式条件：满足
```

### 分层编译

**Tier-up 机制**：

```
Baseline → 热点检测 → Optimizing JIT
```

**流式影响**：

- 基线编译必须在流式窗口内完成
- 优化编译可以在后台异步进行

**批判**：
> 分层编译的热点检测依赖运行时 profiling，但流式场景下首次执行可能在编译完成前触发。这导致早期函数无法被优化，性能不一致。

---

## 实现案例

### Chrome V8

**TurboFan 流式**：

```
1. Liftoff (baseline) 逐函数流式编译
2. TurboFan (optimizing) 后台编译热点
```

**内存限制**：

- 流式缓冲区：256 KB
- 单函数限制：128 KB（超出则回退批处理）

**数据**：

```
模块大小：10 MB
流式编译：1.2 秒
批处理编译：3.5 秒
内存峰值：流式 5 MB vs 批处理 15 MB
```

### Firefox SpiderMonkey

**Ion 流式**：

```
1. Baseline Interpreter：即时启动
2. Ion JIT：后台编译
```

**分块策略**：

- 64 KB 分块读取
- 跨分块函数需要缓冲拼接

**批判**：
> SpiderMonkey 的分块策略在大函数（>64KB）时性能退化。某些 Emscripten 输出的单函数可达 1MB，完全破坏了流式优势。

---

## 性能分析

### 延迟指标

**Time to First Byte (TTFB)**：
\[
\text{TTFB}_{\text{stream}} = T_{\text{network}}(\text{first function}) + T_{\text{compile}}(\text{first function})
\]

**Time to Interactive (TTI)**：
\[
\text{TTI}_{\text{stream}} \approx T_{\text{network}}(\text{last function}) + T_{\text{compile}}(\text{bottleneck function})
\]

### 吞吐量分析

**理想情况**：
\[
\text{Throughput} = \text{Network BW} \quad (\text{when } T_{\text{compile}} < T_{\text{network}})
\]

**实际情况**：

```
瓶颈分析：
- 网络：10 MB/s
- 验证：50 MB/s
- 基线 JIT：30 MB/s  ← 瓶颈
- 优化 JIT：5 MB/s
```

---

## 优化技术

### 预测解析

**前瞻缓冲**：

```python
class StreamParser:
    def __init__(self, stream):
        self.lookahead = stream.read(4096)  # 预读 4KB

    def peek_section_id(self):
        return self.lookahead[0]
```

**批判**：
> 预测解析增加了 CPU 缓存压力。如果预测错误（如遇到大 Custom Section），前瞻缓冲被浪费。

### 并行验证

**函数级并行**：

```rust
functions
    .par_iter()
    .map(|f| validate_function(f, &context))
    .collect()
```

**依赖处理**：

- Type Section：必须串行
- Code Section：可并行

**批判**：
> 并行验证在多核环境下有效，但移动设备（2-4 核）收益有限。功耗增加可能抵消性能提升。

---

## 错误处理

### 流式验证失败

**场景**：

```
已编译：Function 0-99
验证失败：Function 100（类型错误）
```

**回滚策略**：

1. **保守**：销毁所有已编译代码
2. **激进**：保留已编译部分（非标准）

**批判**：
> 规范未明确流式编译失败的回滚语义。不同 Runtime 行为不一致，导致跨平台调试困难。

### 网络中断

**断点续传**：

- HTTP Range Requests
- 缓存已解析 Section

**问题**：

```
已接收：[Magic][Version][Type][Import][50% of Code]
网络中断
重连：从 Code Section 50% 处继续？
```

**现状**：
> 大多数 Runtime 不支持断点续传，网络中断导致重新下载。缺乏 Section 级哈希校验。

---

## 流式编译的局限

### 无法流式的场景

**跨函数优化**：

- 内联（需要被调用函数完整代码）
- 全局逃逸分析
- 常量传播

**批判**：
> 流式编译牺牲了全局优化机会。对于计算密集型应用（如游戏物理引擎），峰值性能可能降低 20-30%。

### 函数顺序依赖

**问题**：

```wasm
(func $A call $B)  ;; Function 0
(func $B ...)      ;; Function 1
```

**流式挑战**：

- 编译 $A 时 $B 未就绪
- 需要占位符（placeholder）或延迟绑定

**解决方案**：

```
1. 生成间接调用
2. 后期 patch call 地址
```

---

## 未来方向

### Function-Level Caching

**提案**：

```
GET /module.wasm
Response:
  - Compiled cache for Function 0-50 (for this CPU)
  - Wasm bytecode for Function 51-100
```

**挑战**：

- CPU 架构多样性
- 安全性（缓存投毒）

### Adaptive Streaming

**动态调整**：
\[
\text{ChunkSize} = f(\text{NetworkSpeed}, \text{CompileSpeed}, \text{CPULoad})
\]

**机器学习**：

- 预测函数编译时间
- 动态调整分层编译阈值

---

## 批判性分析

### 适用场景

**优势明显**：

1. 大型模块（>1 MB）
2. 低延迟要求（首屏渲染）
3. 资源受限设备（低内存）

**不适用**：

1. 小型模块（<100 KB）
2. 计算密集型（需峰值性能）
3. 离线场景（无网络瓶颈）

### 设计权衡

**优势**：

- 降低首次交互延迟（50-70%）
- 减少内存峰值（3-5 倍）

**劣势**：

- 复杂度增加（实现难度）
- 峰值性能下降（缺少全局优化）
- 错误处理不明确

### 演化建议

1. **标准化回滚语义**：明确流式编译失败的状态
2. **Section 级哈希**：支持断点续传
3. **分层编译优先级**：热点函数优先优化

---

## 参考文献

1. **[Bastien18]** Bastien Amar et al. "WebAssembly Streaming." PLDI, 2018.
2. **[V8]** V8 Team. "Liftoff: WebAssembly Baseline Compiler." 2019.
3. **[SpiderMonkey]** Mozilla. "Ion Optimizing Compiler." 2020.

---

## 相关文档

- **[03.1 编译策略](../03_Runtime_Systems/03.1_Compilation_Strategies.md)** - JIT 编译细节
- **[02.3 模块结构](02.3_Module_Structure.md)** - Section 依赖关系
- **[03.5 性能分析](../03_Runtime_Systems/03.5_Performance_Analysis.md)** - 性能测量方法
