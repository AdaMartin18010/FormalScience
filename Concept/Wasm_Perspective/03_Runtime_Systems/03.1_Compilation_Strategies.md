# 3.1 编译策略 (Compilation Strategies)

## 目录

- [3.1 编译策略 (Compilation Strategies)](#31-编译策略-compilation-strategies)
  - [目录](#目录)
  - [核心命题](#核心命题)
    - [主定理](#主定理)
  - [解释执行](#解释执行)
    - [基本原理](#基本原理)
    - [优化技术](#优化技术)
    - [适用场景](#适用场景)
  - [基线编译](#基线编译)
    - [编译模型](#编译模型)
    - [性能分析](#性能分析)
    - [关键优化](#关键优化)
    - [案例：浏览器实现](#案例浏览器实现)
  - [优化编译](#优化编译)
    - [编译管道](#编译管道)
    - [核心优化](#核心优化)
    - [寄存器分配](#寄存器分配)
    - [性能数据](#性能数据)
  - [AOT 编译](#aot-编译)
    - [静态编译模型](#静态编译模型)
    - [优化机会](#优化机会)
    - [实践案例](#实践案例)
  - [分层编译](#分层编译)
    - [架构设计](#架构设计)
    - [热度计数](#热度计数)
    - [性能模型](#性能模型)
    - [案例：V8 策略](#案例v8-策略)
  - [批判性分析](#批判性分析)
    - [编译的本质权衡](#编译的本质权衡)
    - [JIT 的不确定性](#jit-的不确定性)
    - [编译器复杂度的代价](#编译器复杂度的代价)
  - [实证对比](#实证对比)
    - [性能基准（Shootout Benchmark）](#性能基准shootout-benchmark)
    - [实际应用数据](#实际应用数据)
  - [参考文献](#参考文献)

---

## 核心命题

### 主定理

**编译权衡不可能三角**：
\[
\neg \exists \text{Strategy} : \text{FastStartup} \wedge \text{PeakPerformance} \wedge \text{LowMemory}
\]
_无法同时优化启动速度、峰值性能和内存占用。_

**分层收敛定理**：
\[
\lim_{t \to \infty} \text{Perf}_{\text{tiered}}(t) = \text{Perf}_{\text{optimized}}
\]
_分层编译最终收敛到优化编译的性能。_

**编译时间下界**：
\[
T_{\text{compile}} \geq \Omega(n) \quad \text{（至少需遍历一次字节码）}
\]

---

## 解释执行

### 基本原理

**直接解释器**：
```python
def interpret(bytecode, state):
    pc = 0
    while pc < len(bytecode):
        opcode = bytecode[pc]
        match opcode:
            case 0x6A:  # i32.add
                b = state.stack.pop()
                a = state.stack.pop()
                state.stack.push((a + b) & 0xFFFFFFFF)
            case 0x10:  # call
                funcidx = read_varuint32(bytecode, pc+1)
                call_function(funcidx, state)
            ...
        pc += instr_length(opcode)
```

**性能特征**：
\[
\begin{align*}
\text{Overhead}_{\text{interpreter}} &\approx 10-100\times \\
T_{\text{startup}} &= 0 \text{ ms（无编译延迟）} \\
\text{Memory} &= O(n) \text{（字节码大小）}
\end{align*}
\]

### 优化技术

**1. 线程化代码（Threaded Code）**

**直接线程化**：
```c
void* dispatch_table[] = {
    &&op_i32_add,
    &&op_i32_mul,
    ...
};

void interpret(uint8_t* code) {
    goto *dispatch_table[*code++];

op_i32_add:
    int b = pop();
    int a = pop();
    push((a + b) & 0xFFFFFFFF);
    goto *dispatch_table[*code++];

op_i32_mul:
    ...
}
```

**性能提升**：
\[
\text{Speedup}_{\text{threaded}} \approx 2-3\times \text{ vs switch-case}
\]

**2. 栈缓存（Stack Caching）**

**策略**：将栈顶 N 个元素映射到寄存器
\[
\text{stack}[0..2] \mapsto \{\texttt{rax}, \texttt{rbx}, \texttt{rcx}\}
\]

**收益**：
\[
\text{减少内存访问} \approx 40-60\%
\]

### 适用场景

| 场景 | 优势 | 劣势 |
|------|------|------|
| **短生命周期函数** | 无编译开销 | 执行慢 |
| **冷代码路径** | 内存占用小 | 热路径性能差 |
| **资源受限设备** | 实现简单 | 不适合计算密集 |

**批判**：
> 纯解释器在现代场景中几乎不可接受，除非内存严重受限（< 1MB）。V8 已移除纯解释器，全面 JIT 化。

---

## 基线编译

### 编译模型

**单遍线性翻译**：
\[
\text{Wasm Instr} \xrightarrow{1:1} \text{Native Instr}
\]

**示例映射**：
```wasm
i32.const 42      →  MOV eax, 42
i32.const 10      →  PUSH eax
                     MOV eax, 10
i32.add           →  POP ebx
                     ADD eax, ebx
i32.store offset=0 → MOV [rcx], eax
```

**寄存器分配**：
- 栈顶 → `rax`
- 栈次顶 → `rbx`
- 溢出 → 物理栈

### 性能分析

**编译速度**：
\[
T_{\text{baseline}} = O(n) \approx 5-10 \text{ MB/s}
\]

**代码质量**：
\[
\text{Perf}_{\text{baseline}} \approx 0.5-0.7 \times \text{native}
\]

**内存占用**：
\[
\text{Code-size}_{\text{baseline}} \approx 3-5 \times \text{wasm-size}
\]

**实证数据（V8 Liftoff）**：

| 指标 | 值 |
|------|-----|
| 编译速度 | 8-10 MB/s |
| 冷启动延迟 | 1-2 ms/100KB |
| 峰值性能 | 50-70% native |
| 代码膨胀 | 4× |

### 关键优化

**1. 栈深度跟踪**
```c
struct StackState {
    int depth;
    Register top;  // 栈顶在哪个寄存器
};

void emit_i32_add(StackState* stack) {
    if (stack->depth >= 2 && stack->top == REG_RAX) {
        emit("pop rbx");
        emit("add rax, rbx");
    } else {
        // fallback to memory
    }
}
```

**2. 跳转表优化**
\[
\texttt{br\_table} \rightarrow \text{Jump Table（O(1) 跳转）}
\]

**3. 内联常量**
\[
\texttt{i32.const } n; \texttt{i32.add} \rightarrow \texttt{ADD eax, } n
\]

### 案例：浏览器实现

**V8 Liftoff**：
- 目标：<1ms 编译延迟
- 方法：单遍扫描 + 简单寄存器分配
- 性能：60-70% Turbofan 性能

**SpiderMonkey Baseline**：
- 特色：内联缓存（IC）支持
- 优化：类型反馈收集

**批判**：
> 基线编译是"够用"的艺术：不追求完美，但快速可用。Chrome 90% 代码由 Liftoff 执行，仅热点进入 Turbofan。

---

## 优化编译

### 编译管道

**多遍优化流程**：
```
Wasm → IR 构建 → SSA 转换 → 优化 Pass → 寄存器分配 → 代码生成
  ↓         ↓          ↓           ↓            ↓           ↓
 100ms    50ms      100ms       200ms        100ms       50ms
```

**总延迟**：
\[
T_{\text{opt}} = O(n \log n) \approx 30-100 \text{ ms/MB}
\]

### 核心优化

**1. 全局值编号（GVN）**

**定义**：
\[
e_1 \equiv e_2 \iff \text{structurally-equal}(e_1, e_2)
\]

**示例**：
```wasm
i32.const 10
i32.const 20
i32.add          ;; v1 = 10 + 20
...
i32.const 10
i32.const 20
i32.add          ;; v2 = 10 + 20  → reuse v1
```

**复杂度**：
\[
T_{\text{GVN}} = O(n \log n)
\]

**2. 循环不变代码外提（LICM）**

**转换**：
```wasm
;; Before
(loop $L
  local.get $base
  i32.const 100
  i32.mul         ;; invariant!
  ...
  br_if $L
)

;; After
local.get $base
i32.const 100
i32.mul
local.set $tmp
(loop $L
  local.get $tmp
  ...
  br_if $L
)
```

**收益**：
\[
\text{Iterations} \times C_{\text{mul}} \rightarrow 1 \times C_{\text{mul}}
\]

**3. 边界检查消除（BCE）**

**分析**：
\[
\forall i \in [0, n) : i < |\text{mem}| \implies \text{remove-check}(\text{load}(i))
\]

**示例**：
```wasm
;; 已知 len = memory.size
local.get $i
i32.const 1000
i32.lt_u
if
  local.get $i
  i32.load  ;; 边界检查可消除
end
```

**性能提升**：
\[
\text{Speedup}_{\text{BCE}} \approx 10-30\% \text{（数组密集代码）}
\]

**4. 内联（Inlining）**

**启发式**：
\[
\text{Inline}(f) \iff \text{Size}(f) < \theta \wedge \text{CallCount}(f) > k
\]

**权衡**：
\[
\begin{align*}
\text{Benefit} &= \text{消除调用开销} + \text{更多优化机会} \\
\text{Cost} &= \text{代码膨胀} + \text{缓存污染}
\end{align*}
\]

**典型阈值**：
- 小函数：< 20 字节 → 总是内联
- 中函数：20-200 字节 → 根据热度
- 大函数：> 200 字节 → 罕见内联

### 寄存器分配

**线性扫描算法**：
\[
\begin{align*}
&\text{按活跃区间起始排序} \\
&\text{贪心分配寄存器} \\
&\text{溢出 → 栈}
\end{align*}
\]

**复杂度**：
\[
T_{\text{linear-scan}} = O(n \log n)
\]

**图着色算法**（LLVM）：
\[
\text{Chromatic-Number}(\text{Interference-Graph}) \leq k
\]

**复杂度**：
\[
T_{\text{graph-coloring}} = O(n^2) \text{（近似算法）}
\]

### 性能数据

**V8 TurboFan**：

| 优化 | 编译时间 | 性能提升 |
|------|---------|---------|
| GVN | +20% | 5-10% |
| LICM | +15% | 10-20% |
| BCE | +10% | 5-15% |
| Inlining | +30% | 15-30% |
| **总计** | **+75%** | **50-80%** |

**最终性能**：
\[
\text{Perf}_{\text{TurboFan}} \approx 0.85-0.95 \times \text{native}
\]

---

## AOT 编译

### 静态编译模型

**工作流**：
```
Wasm → LLVM IR → 优化 → 原生二进制
  ↓        ↓         ↓         ↓
10ms    50ms     2000ms    100ms
```

**延迟分布**：
\[
T_{\text{AOT}} = O(n \log^2 n) \approx 2-10 \text{ s/MB}
\]

### 优化机会

**1. 全程序分析（WPA）**

**调用图构建**：
\[
\text{Call-Graph} = (V_{\text{funcs}}, E_{\text{calls}})
\]

**死代码消除**：
\[
\forall f : \neg \text{reachable}(f, \text{entry}) \implies \text{remove}(f)
\]

**2. 链接时优化（LTO）**

**跨模块内联**：
\[
\text{Inline across } \text{Module}_1 \text{ and } \text{Module}_2
\]

**去虚拟化**：
\[
\texttt{call\_indirect}(i) \xrightarrow{\text{devirt}} \texttt{call } f \quad \text{（若类型唯一）}
\]

**3. 配置文件引导优化（PGO）**

**热点数据**：
\[
\text{Profile} = \{(\text{BasicBlock}_i, \text{Count}_i)\}
\]

**优化策略**：
- 热路径优化
- 冷代码 outline
- 分支预测提示

**性能提升**：
\[
\text{Speedup}_{\text{PGO}} \approx 10-30\%
\]

### 实践案例

**Wasmer Native**：
- LLVM 后端
- 支持 LTO
- 编译时间：~5s/MB
- 性能：95-98% native

**批判**：
> AOT 是"空间换时间"：牺牲灵活性（静态链接）换取性能。适合生产环境，不适合开发调试。

---

## 分层编译

### 架构设计

**三层模型**：
```
Tier 1: Interpreter    (t=0, perf=10%)
  ↓ trigger: first execution
Tier 2: Baseline JIT   (t=2ms, perf=60%)
  ↓ trigger: hot counter > 1000
Tier 3: Optimizing JIT (t=50ms, perf=95%)
```

**触发策略**：
\[
\text{Tier-Up}(f) \iff \text{HotCount}(f) > \theta_{\text{tier}}
\]

### 热度计数

**计数器位置**：
- 函数入口
- 循环回边

**更新规则**：
\[
\text{Counter} \leftarrow \text{Counter} - 1 \quad \text{（倒计时）}
\]

**触发阈值**：
\[
\begin{align*}
\theta_{\text{baseline}} &= 0 \quad \text{（立即编译）} \\
\theta_{\text{opt}} &= 1000-10000 \quad \text{（热函数）}
\end{align*}
\]

### 性能模型

**启动延迟**：
\[
T_{\text{startup}} = T_{\text{baseline}} \approx 1-2 \text{ ms/100KB}
\]

**稳态性能**：
\[
\lim_{t \to \infty} \text{Perf}(t) = \text{Perf}_{\text{opt}}
\]

**ROI 模型**：
\[
\text{编译成本} < \text{执行收益} \iff T_{\text{compile}} < N_{\text{exec}} \times \Delta T
\]

**数值示例**：
\[
50\text{ms} < 1000 \times 0.1\text{ms} \implies \text{值得优化}
\]

### 案例：V8 策略

**Liftoff + TurboFan**：

| 阶段 | 触发条件 | 性能 |
|------|---------|------|
| Liftoff | 立即 | 60% |
| TurboFan | 调用 > 1k | 90% |
| TurboFan+PGO | 调用 > 10k | 95% |

**内存管理**：
- Liftoff 代码保留（快速反优化）
- TurboFan 代码共存
- 总膨胀：6-8×

---

## 批判性分析

### 编译的本质权衡

**不可能三角定理**：
\[
\begin{align*}
\text{Fast Compilation} &\iff \text{Simple Transforms} \\
\text{High Performance} &\iff \text{Complex Analysis} \\
\text{Low Memory} &\iff \text{No Code Duplication}
\end{align*}
\]

**证明**：
- Fast ∧ High ⇒ ¬Low（需多层代码共存）
- Fast ∧ Low ⇒ ¬High（无优化空间）
- High ∧ Low ⇒ ¬Fast（复杂分析耗时）

### JIT 的不确定性

**问题**：
\[
\text{Performance} = f(\text{Input}, \text{History}, \text{Resources})
\]

**来源**：
1. 热度计数器抖动
2. GC 触发时机
3. 代码缓存失效

**实践影响**：
- 基准测试不稳定（变异系数 5-10%）
- 性能回归难复现

**批判**：
> JIT 是"统计学意义上的优化"：大多数情况快，但无绝对保证。实时系统需 AOT。

### 编译器复杂度的代价

**V8 TurboFan 代码量**：
\[
\text{LOC} \approx 200k \text{ lines C++}
\]

**安全隐患**：
- Spectre 缓解代码
- JIT spraying 攻击
- 边界检查漏洞

**哲学反思**：
> 编译器是"信任的假设"：我们相信它生成正确代码，但 bug 难以避免。形式化验证编译器（CompCert）性能损失 20-30%。

---

## 实证对比

### 性能基准（Shootout Benchmark）

| 实现 | 策略 | 性能 | 启动 | 内存 |
|------|------|------|------|------|
| **Native C** | - | 1.00× | 0ms | 10MB |
| **V8** | Liftoff+TF | 0.92× | 5ms | 80MB |
| **Wasmer** | Baseline | 0.65× | 2ms | 40MB |
| **Wasmtime** | Cranelift | 0.88× | 8ms | 60MB |
| **WAVM** | LLVM | 0.95× | 2000ms | 30MB |

### 实际应用数据

**Figma（WebAssembly + V8）**：
- 启动延迟：< 50ms
- 渲染性能：95% native
- 代码体积：8MB wasm → 48MB JIT

---

## 参考文献

1. **[Chambers91]** Craig Chambers et al. "Adaptive Optimization for SELF." PLDI, 1991.
2. **[Hölzle94]** Urs Hölzle and David Ungar. "Optimizing Dynamically-Dispatched Calls." PLDI, 1994.
3. **[Gal09]** Andreas Gal et al. "Trace-based Just-in-Time Compilation." PLDI, 2009.
4. **[Haas17]** Andreas Haas et al. "Bringing the Web up to Speed with WebAssembly." PLDI, 2017.

---

**结论**：
> 编译策略是"延迟-性能-资源"三维空间的帕累托最优搜索。分层编译接近最优解：以适度内存换取快速启动与高峰值性能。但 JIT 的不确定性与复杂度，使其永远是工程妥协，而非理论完美。
