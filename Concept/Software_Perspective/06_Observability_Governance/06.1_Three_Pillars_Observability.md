# 6.1 三支柱可观测性

> **文档版本**: v1.0.0  
> **最后更新**: 2025-10-27  
> **文档规模**: 620行 | 可观测性三支柱详解  
> **阅读建议**: 本文深入对比Metrics/Logs/Traces三大支柱及其工具栈

---

## 目录 | Table of Contents

- [6.1 可观测性三支柱](#61-可观测性三支柱)
  - [目录 | Table of Contents](#目录--table-of-contents)
  - [📊 核心概念深度分析](#-核心概念深度分析)
  - [核心定义](#核心定义)
  - [三支柱框架](#三支柱框架)
  - [支柱 1：Metrics（指标）](#支柱-1metrics指标)
  - [支柱 2：Traces（追踪）](#支柱-2traces追踪)
  - [支柱 3：Logs（日志）](#支柱-3logs日志)
  - [三支柱对比与选择](#三支柱对比与选择)
  - [关键洞察](#关键洞察)
  - [相关主题](#相关主题)

---

## 📊 核心概念深度分析

<details>
<summary><b>👁️📊 点击展开：可观测性三支柱核心洞察</b></summary>

**终极洞察**: 可观测性≠监控。监控回答"是否正常？"（已知问题），可观测性回答"为什么不正常？"（未知问题）。三支柱（Honeycomb 2016提出）：①Metrics指标：时序数值（CPU/内存/QPS），聚合统计②Logs日志：离散事件（错误/审计/调试），全文搜索③Traces追踪：请求链路（分布式调用），因果关联。技术栈：①Metrics：Prometheus收集+Grafana可视化②Logs：ELK/Loki收集+全文索引③Traces：Jaeger/Zipkin/Tempo追踪+OpenTelemetry标准。三支柱互补：①Metrics发现异常（QPS下降）②Logs定位错误（具体报错）③Traces追踪链路（慢在哪个服务）。统一标准：OpenTelemetry（OTLP）统一采集SDK。高级能力：①关联分析：Traces→Logs→Metrics打通②AIOps：异常检测、根因分析③成本优化：采样策略、存储分层。关键：可观测性=通过外部输出推断内部状态的能力，三支柱缺一不可。

</details>

---

## 核心定义

**可观测性**（Observability）：通过系统外部输出（指标、日志、追踪）推断系统内部状态的能力。

**与监控的区别**：
- **监控**（Monitoring）：回答"系统是否正常？"（已知问题）
- **可观测性**（Observability）：回答"为什么不正常？"（未知问题）

## 三支柱框架

### 起源

**Honeycomb 于 2016 年提出**："可观测性的三支柱"
- Metrics（指标）
- Traces（追踪）
- Logs（日志）

### 完整对比

| 支柱 | 数据类型 | 问题域 | 示例 | 存储 |
|-----|---------|--------|------|------|
| **Metrics** | 时间序列数值 | "发生了什么" | CPU=80%, QPS=1000 | Prometheus |
| **Traces** | 分布式调用链 | "在哪慢/错" | 请求 A 耗时 2s | Jaeger, Tempo |
| **Logs** | 结构化/非结构化文本 | "详细信息" | ERROR: NullPointer | Loki, ES |

---

## 支柱 1：Metrics（指标）

### 定义
**聚合的数值型时间序列数据**，回答"系统整体健康度"

### 类型

| 类型 | 说明 | 示例 | 用途 |
|-----|------|------|------|
| **Counter** | 单调递增计数器 | http_requests_total | 总请求数 |
| **Gauge** | 可增可减的即时值 | memory_usage_bytes | 内存占用 |
| **Histogram** | 值的分布（桶） | http_request_duration_seconds | 延迟分布 |
| **Summary** | 分位数统计 | http_request_duration_seconds{quantile="0.95"} | P95 延迟 |

### 黄金信号（Golden Signals）

**Google SRE 提出的四大关键指标**：

1. **Latency**（延迟）
   ```promql
   # P95 延迟
   histogram_quantile(0.95, 
     rate(http_request_duration_seconds_bucket[5m])
   )
   ```

2. **Traffic**（流量）
   ```promql
   # QPS
   rate(http_requests_total[1m])
   ```

3. **Errors**（错误）
   ```promql
   # 错误率
   rate(http_requests_total{status=~"5.."}[5m]) /
   rate(http_requests_total[5m])
   ```

4. **Saturation**（饱和度）
   ```promql
   # CPU 使用率
   100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
   ```

### RED 方法（面向请求）

- **Rate**：请求速率
- **Errors**：错误数量
- **Duration**：延迟

### USE 方法（面向资源）

- **Utilization**：使用率
- **Saturation**：饱和度
- **Errors**：错误数

### Prometheus 示例

**暴露指标**（Go）：
```go
import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    httpRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total HTTP requests",
        },
        []string{"method", "endpoint", "status"},
    )
)

func handler(w http.ResponseWriter, r *http.Request) {
    httpRequestsTotal.WithLabelValues(
        r.Method, r.URL.Path, "200",
    ).Inc()
}
```

**查询指标**：
```promql
# 每秒请求数
rate(http_requests_total[5m])

# 按端点分组
sum by (endpoint) (rate(http_requests_total[5m]))

# 错误率
sum(rate(http_requests_total{status=~"5.."}[5m])) /
sum(rate(http_requests_total[5m]))
```

### 优点
- ✅ 聚合，数据量小
- ✅ 查询快（时序数据库优化）
- ✅ 适合告警（阈值判断）
- ✅ 长期趋势分析

### 缺点
- ❌ 丢失细节（聚合后无法还原单个请求）
- ❌ 高基数问题（label 太多会爆炸）

---

## 支柱 2：Traces（追踪）

### 定义
**分布式系统中单个请求的完整调用链**，回答"慢在哪里？错在哪里？"

### 核心概念

**Trace**：一个请求的完整生命周期
```
Trace ID: a1b2c3d4e5f6
```

**Span**：Trace 中的一个操作单元
```
Span: API Gateway → Auth Service
  - Start: 10:00:00.100
  - End: 10:00:00.150
  - Duration: 50ms
```

**Span 关系**：
```
Trace (总耗时 500ms)
├─ Span 1: API Gateway (500ms)
│  ├─ Span 2: Auth Service (50ms)
│  └─ Span 3: Order Service (400ms)
│     ├─ Span 4: Database Query (300ms) ← 瓶颈
│     └─ Span 5: Cache Check (50ms)
```

### OpenTelemetry Trace 示例

**创建 Trace**（Go）：
```go
import (
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/trace"
)

func processOrder(ctx context.Context, orderID string) error {
    tracer := otel.Tracer("order-service")
    ctx, span := tracer.Start(ctx, "processOrder")
    defer span.End()

    // 添加属性
    span.SetAttributes(
        attribute.String("order.id", orderID),
        attribute.Int("order.items", 3),
    )

    // 调用子服务（自动创建子 Span）
    err := validatePayment(ctx, orderID)
    if err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, "payment validation failed")
        return err
    }

    return nil
}
```

**Trace 传播**：
```
HTTP Header:
traceparent: 00-a1b2c3d4e5f6-1234567890ab-01
             └─ Trace ID ─┘ └─ Span ID ──┘

服务 A → 服务 B: 通过 HTTP Header 传递 Trace Context
```

### Jaeger UI 示例

```
Trace View (500ms total)
┌────────────────────────────────────────┐
│ API Gateway          [════════════════] 500ms
│   Auth Service       [═]                 50ms
│   Order Service      [════════════]     400ms
│     DB Query         [══════════]       300ms ← 慢
│     Cache Check      [═]                 50ms
└────────────────────────────────────────┘
```

### 关键用途

1. **性能分析**：找到慢的服务/函数
2. **依赖分析**：绘制服务调用拓扑
3. **根因定位**：从错误 Span 追溯到源头
4. **容量规划**：分析调用频率和延迟

### 优点
- ✅ 完整的调用链路
- ✅ 准确定位瓶颈
- ✅ 上下文丰富（携带业务信息）

### 缺点
- ❌ 数据量大（每个请求都生成）
- ❌ 采样策略复杂（不能 100% 采样）
- ❌ 跨语言/框架集成成本高

---

## 支柱 3：Logs（日志）

### 定义
**离散的事件记录**，回答"具体发生了什么？"

### 类型

| 类型 | 格式 | 示例 | 查询 |
|-----|------|------|------|
| **非结构化** | 纯文本 | `[ERROR] Connection timeout` | 正则/全文 |
| **半结构化** | Key-Value | `level=error msg="timeout" latency=5s` | 字段索引 |
| **结构化** | JSON | `{"level":"error","msg":"timeout"}` | 结构化查询 |

### 日志级别

```
TRACE   (最详细，调试用)
DEBUG   (调试信息)
INFO    (正常信息) ← 生产默认
WARN    (警告，需关注)
ERROR   (错误，影响功能)
FATAL   (致命，系统崩溃)
```

### 结构化日志示例

**传统日志**（难解析）：
```
2025-01-15 10:00:00 ERROR Failed to process order 12345 due to timeout after 5s
```

**结构化日志**（易查询）：
```json
{
  "timestamp": "2025-01-15T10:00:00Z",
  "level": "error",
  "message": "Failed to process order",
  "order_id": "12345",
  "error": "timeout",
  "latency_seconds": 5,
  "service": "order-service",
  "trace_id": "a1b2c3d4e5f6"
}
```

### OpenTelemetry Logs 集成

```go
import "go.opentelemetry.io/otel/log"

logger := log.Logger("order-service")

logger.Error(
    "Failed to process order",
    log.String("order_id", orderID),
    log.String("error", err.Error()),
    log.Int64("latency_ms", latency),
)
```

### 日志聚合工具

| 工具 | 特点 | 适用场景 |
|-----|------|---------|
| **Elasticsearch** | 全文搜索强 | 大规模日志 |
| **Loki** | K8s 原生，低成本 | 云原生环境 |
| **Splunk** | 商业，功能全 | 企业级 |
| **CloudWatch Logs** | AWS 原生 | AWS 环境 |

### Loki 查询示例

```logql
# 查询包含"error"的日志
{app="order-service"} |= "error"

# 统计错误数
count_over_time({app="order-service"} |= "error" [5m])

# 按 order_id 分组
sum by (order_id) (
  count_over_time({app="order-service"} | json [5m])
)
```

### 日志与 Trace 关联

```go
// 在日志中记录 Trace ID
span := trace.SpanFromContext(ctx)
logger.Error(
    "Database query failed",
    log.String("trace_id", span.SpanContext().TraceID().String()),
    log.String("span_id", span.SpanContext().SpanID().String()),
)
```

**效果**：
- 从日志跳转到 Trace
- 从 Trace 查看详细日志

### 优点
- ✅ 信息最详细
- ✅ 灵活（可记录任何信息）
- ✅ 适合调试

### 缺点
- ❌ 数据量巨大
- ❌ 查询慢（全文搜索）
- ❌ 存储成本高

---

## 三支柱协同

### 典型故障排查流程

```
1. Metrics 告警
   ↓
   "错误率 > 5%"
   
2. 查看 Metrics 趋势
   ↓
   "10:00 开始飙升"
   
3. 查询 Traces
   ↓
   "order-service → database 耗时 5s（正常 50ms）"
   
4. 查看 Logs
   ↓
   "ERROR: Connection pool exhausted"
   
5. 根因定位
   ↓
   "数据库连接池不足"
```

### 数据关联

**通过 Trace ID 关联**：
```
Metric ─┐
        ├─→ Trace ID: a1b2c3d4
Trace ──┤
        ├─→ Log: trace_id=a1b2c3d4
Log ────┘
```

**Grafana Tempo + Loki 示例**：
```
1. Grafana 看到错误率飙升（Metric）
2. 点击"查看 Traces" → 跳转到 Tempo
3. 在 Tempo 中选择一个慢 Trace
4. 点击"查看 Logs" → 跳转到 Loki
5. 看到详细错误堆栈
```

---

## OpenTelemetry：统一标准

### 为什么需要统一？

**过去的碎片化**：
- Metrics：Prometheus
- Traces：Jaeger, Zipkin
- Logs：Fluentd, Filebeat

每个都有自己的 SDK、协议、格式。

**OpenTelemetry 目标**：
- 统一 SDK
- 统一协议（OTLP）
- 统一语义约定

### 架构

```
┌────────────────────────────┐
│  应用（任何语言）          │
│  - 集成 OTel SDK            │
│  - 发送 Metrics/Traces/Logs │
└────────────┬───────────────┘
             │ OTLP (gRPC/HTTP)
             ↓
┌────────────────────────────┐
│  OTel Collector            │
│  - 接收数据                │
│  - 转换格式                │
│  - 路由到后端              │
└────────────┬───────────────┘
             │
       ┌─────┼─────┐
       ↓     ↓     ↓
   Prometheus Tempo Loki
```

### 核心组件

| 组件 | 作用 |
|-----|------|
| **SDK** | 应用端埋点 |
| **API** | 厂商无关接口 |
| **Collector** | 数据收集/转换/导出 |
| **OTLP** | 传输协议 |

### 示例配置

**应用端**（Go）：
```go
import (
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/sdk/trace"
)

// 配置 OTLP 导出器
exporter, _ := otlptracegrpc.New(ctx,
    otlptracegrpc.WithEndpoint("otel-collector:4317"),
)

tp := trace.NewTracerProvider(
    trace.WithBatcher(exporter),
)
otel.SetTracerProvider(tp)
```

**Collector 配置**：
```yaml
receivers:
  otlp:
    protocols:
      grpc:
      http:

processors:
  batch:

exporters:
  prometheus:
    endpoint: "0.0.0.0:8889"
  jaeger:
    endpoint: "jaeger:14250"
  loki:
    endpoint: "http://loki:3100/loki/api/v1/push"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [jaeger]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus]
```

---

## 最佳实践

### 1. 采样策略

**Metrics**：100%（聚合数据，量小）
**Traces**：
- 头部采样（Head-based）：1-10%（根据 Trace ID 哈希）
- 尾部采样（Tail-based）：智能（保留错误和慢请求）

**Logs**：
- DEBUG 级别：0%（生产禁用）
- ERROR 级别：100%

### 2. 高基数问题

**错误示例**（爆炸）：
```promql
http_requests_total{user_id="12345", session_id="abcd..."}
# user_id 有百万个 → 百万个时间序列
```

**正确做法**：
```promql
http_requests_total{endpoint="/api/orders", status="200"}
# endpoint 只有几十个 → 可控
```

**规则**：label 的唯一值数量应 < 100

### 3. 关联三支柱

**标准字段**：
```json
{
  "trace_id": "a1b2c3d4",
  "span_id": "1234567890ab",
  "service.name": "order-service",
  "service.version": "v1.2.3"
}
```

所有三支柱都包含这些字段，方便跳转。

### 4. 告警分层

```
L1: Metrics 告警（快速发现）
    ↓
L2: Trace 采样告警（异常请求）
    ↓
L3: Log 关键词告警（特定错误）
```

---

## 未来趋势

### 1. 持续性能分析（Continuous Profiling）

**第四支柱**：Profiling
- CPU 火焰图
- 内存分配
- 锁竞争

**工具**：Parca, Pyroscope

### 2. eBPF 加速

**内核级可观测性**：
- 无需修改代码
- 零性能损耗
- 全栈可见

**工具**：Pixie, Cilium

### 3. AI 驱动的异常检测

```
Metrics + Traces + Logs
    ↓ ML 模型
自动发现异常模式
    ↓ 自动关联
生成根因报告
```

---

## 关键洞察

### 洞察 1：三支柱互补

- **Metrics**：快速发现"什么地方"出问题
- **Traces**：精确定位"哪个环节"慢/错
- **Logs**：详细解释"具体原因"

### 洞察 2：可观测性 ≠ 监控

**监控**：预设仪表盘 + 告警（已知未知）
**可观测性**：随时探索（未知未知）

### 洞察 3：成本与价值的平衡

**全量采集**：
- 成本：高（存储、查询）
- 价值：高（完整信息）

**智能采样**：
- 成本：低
- 价值：中（可能丢失关键信息）

**最优策略**：尾部采样（只保留异常）

---

## 相关主题

- [4.2 OTLP 可观测性协议](../04_Self_Healing_Systems/04.2_OTLP_Observability.md)
- [4.1 自愈架构原理](../04_Self_Healing_Systems/04.1_Self_Healing_Architecture.md)

---


