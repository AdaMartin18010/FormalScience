# ç¥ç»ç½‘ç»œçš„é€’å½’æ¶æ„åˆ†æ

> **ä¸»é¢˜**: RNN/LSTM/Transformerçš„é€’å½’æ€§è´¨å¯¹æ¯”
> **åˆ›å»ºæ—¥æœŸ**: 2025-12-02
> **éš¾åº¦**: â­â­â­â­
> **å‰ç½®çŸ¥è¯†**: æ·±åº¦å­¦ä¹ ã€é€’å½’ç¥ç»ç½‘ç»œ

---

## ğŸ“‹ ç›®å½•

- [ç¥ç»ç½‘ç»œçš„é€’å½’æ¶æ„åˆ†æ](#ç¥ç»ç½‘ç»œçš„é€’å½’æ¶æ„åˆ†æ)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. RNNåŸºç¡€ä¸é€’å½’æ€§](#1-rnnåŸºç¡€ä¸é€’å½’æ€§)
    - [1.1 RNNæ–¹ç¨‹](#11-rnnæ–¹ç¨‹)
    - [1.2 å›¾çµå®Œå¤‡æ€§](#12-å›¾çµå®Œå¤‡æ€§)
  - [2. LSTMçš„é—¨æ§é€’å½’](#2-lstmçš„é—¨æ§é€’å½’)
    - [2.1 é—¨æœºåˆ¶](#21-é—¨æœºåˆ¶)
    - [2.2 é•¿æœŸä¾èµ–é—®é¢˜](#22-é•¿æœŸä¾èµ–é—®é¢˜)
  - [3. Transformer: é€’å½’çš„ç»ˆç»“?](#3-transformer-é€’å½’çš„ç»ˆç»“)
    - [3.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶](#31-è‡ªæ³¨æ„åŠ›æœºåˆ¶)
    - [3.2 vs RNNå¯¹æ¯”](#32-vs-rnnå¯¹æ¯”)
  - [4. ç”Ÿç‰©å¤§è„‘çš„é€’å½’è¿æ¥](#4-ç”Ÿç‰©å¤§è„‘çš„é€’å½’è¿æ¥)
    - [4.1 å‰é¦ˆvsåé¦ˆ](#41-å‰é¦ˆvsåé¦ˆ)
    - [4.2 é€’å½’æ·±åº¦ä¸æ„è¯†](#42-é€’å½’æ·±åº¦ä¸æ„è¯†)
  - [5. é€’å½’ç†è®ºåˆ†æ](#5-é€’å½’ç†è®ºåˆ†æ)
  - [6. ä¸»é¢˜-å­ä¸»é¢˜è®ºè¯é€»è¾‘å…³ç³»å›¾](#6-ä¸»é¢˜-å­ä¸»é¢˜è®ºè¯é€»è¾‘å…³ç³»å›¾)
    - [6.1 è®ºè¯ä¾èµ–å…³ç³»](#61-è®ºè¯ä¾èµ–å…³ç³»)
    - [6.2 æ¦‚å¿µä¾èµ–å…³ç³»](#62-æ¦‚å¿µä¾èµ–å…³ç³»)
  - [7. å‚è€ƒèµ„æº](#7-å‚è€ƒèµ„æº)
    - [7.1 ç»å…¸è®ºæ–‡](#71-ç»å…¸è®ºæ–‡)
    - [7.2 æ•™æ](#72-æ•™æ)
    - [7.3 åœ¨çº¿èµ„æº](#73-åœ¨çº¿èµ„æº)

---

## 1. RNNåŸºç¡€ä¸é€’å½’æ€§

### 1.1 RNNæ–¹ç¨‹

**æ ‡å‡†RNN**:

```text
h_t = tanh(W_h h_{t-1} + W_x x_t + b)
y_t = W_y h_t

é€’å½’æ€§è´¨:
âœ“ h_té€’å½’ä¾èµ–h_{t-1}
âœ“ å±•å¼€: h_t = f(x_t, x_{t-1}, ..., x_0)
âœ“ æ—¶é—´é€’å½’

é—®é¢˜:
âœ— æ¢¯åº¦æ¶ˆå¤± (tanhé¥±å’Œ)
âœ— æ¢¯åº¦çˆ†ç‚¸
âœ— é•¿æœŸä¾èµ–å›°éš¾
â†’ å®è·µå—é™ âš ï¸
```

---

### 1.2 å›¾çµå®Œå¤‡æ€§

**å®šç† (Siegelmann & Sontag 1995)**:

```text
RNN (å®æ•°æƒé‡) = å›¾çµå®Œå¤‡ âœ“

è¯æ˜æ€è·¯:
1. RNNå¯æ¨¡æ‹Ÿæ ˆ
2. æ ˆæœº = å›¾çµæœº
â†’ RNN = å›¾çµå®Œå¤‡

ä½†:
âš ï¸ éœ€è¦æ— é™ç²¾åº¦
âš ï¸ å®è·µä¸­æœ‰é™ç²¾åº¦
âš ï¸ è®­ç»ƒå›°éš¾

ç»“è®º:
âœ“ ç†è®ºä¸Šå›¾çµå®Œå¤‡
âœ— å®è·µå—é™
â†’ Transformeræ›¿ä»£è¶‹åŠ¿
```

---

## 2. LSTMçš„é—¨æ§é€’å½’

### 2.1 é—¨æœºåˆ¶

**LSTMæ–¹ç¨‹**:

```text
é—å¿˜é—¨: f_t = Ïƒ(W_fÂ·[h_{t-1}, x_t])
è¾“å…¥é—¨: i_t = Ïƒ(W_iÂ·[h_{t-1}, x_t])
è¾“å‡ºé—¨: o_t = Ïƒ(W_oÂ·[h_{t-1}, x_t])

ç»†èƒçŠ¶æ€:
C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ tanh(W_CÂ·[h_{t-1}, x_t])

éšçŠ¶æ€:
h_t = o_t âŠ™ tanh(C_t)

é€’å½’æ€§è´¨:
âœ“ h_té€’å½’
âœ“ C_té€’å½’ (é•¿æœŸè®°å¿†)
âœ“ é—¨æ§ä¿æŠ¤æ¢¯åº¦
```

---

### 2.2 é•¿æœŸä¾èµ–é—®é¢˜

**æ¢¯åº¦æµåŠ¨**:

```text
RNN:
âˆ‚L/âˆ‚h_0 = âˆ‚L/âˆ‚h_T Â· âˆ_{t=1}^T âˆ‚h_t/âˆ‚h_{t-1}
â†’ è¿ä¹˜ â†’ æ¶ˆå¤±/çˆ†ç‚¸ âœ—

LSTM:
C_t = f_t âŠ™ C_{t-1} + ...
â†’ åŠ æ³•è·¯å¾„ (éè¿ä¹˜)
â†’ æ¢¯åº¦ä¿æŠ¤ âœ“

æ•ˆæœ:
RNN: ~10æ­¥
LSTM: ~100æ­¥
â†’ æ”¹è¿›ä½†ä»æœ‰é™ âš ï¸
```

---

## 3. Transformer: é€’å½’çš„ç»ˆç»“?

### 3.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶

**éé€’å½’æ¶æ„**:

```text
Self-Attention:
Attention(Q,K,V) = softmax(QK^T/âˆšd)V

ç‰¹ç‚¹:
âœ“ å…¨è¿æ¥ (éé€’å½’)
âœ“ å¹¶è¡Œè®¡ç®—
âœ“ O(nÂ²)å¤æ‚åº¦

vs RNN:
RNN: ä¸²è¡Œé€’å½’ O(n)
Transformer: å¹¶è¡Œ O(1)å±‚
â†’ é€Ÿåº¦ä¼˜åŠ¿ â­â­â­â­â­
```

---

### 3.2 vs RNNå¯¹æ¯”

| ç»´åº¦ | RNN | LSTM | Transformer |
|------|-----|------|-------------|
| **æ¶æ„** | é€’å½’â†» | é€’å½’+é—¨æ§ | æ³¨æ„åŠ›â‡„ |
| **å¹¶è¡Œæ€§** | âœ—ä¸²è¡Œ | âœ—ä¸²è¡Œ | âœ“å¹¶è¡Œâ­ |
| **é•¿æœŸä¾èµ–** | âœ—å·® | âš ï¸ä¸­ç­‰ | âœ“ä¼˜ç§€ |
| **è®­ç»ƒé€Ÿåº¦** | æ…¢ | æ…¢ | å¿«âœ“ |
| **æ¨ç†é€Ÿåº¦** | å¿« | å¿« | ä¸­ |
| **å†…å­˜** | O(T) | O(T) | O(TÂ²) |
| **é€’å½’æ€§** | âœ“æ—¶é—´ | âœ“æ—¶é—´ | âœ—å±‚æ¬¡ |

**ç»“è®º**:
Transformerä¸»å¯¼ (2024)
RNNåº”ç”¨å‡å°‘
â†’ éé€’å½’æ¶æ„èƒœå‡º âš ï¸

---

## 4. ç”Ÿç‰©å¤§è„‘çš„é€’å½’è¿æ¥

### 4.1 å‰é¦ˆvsåé¦ˆ

**çš®å±‚è¿æ¥ç»Ÿè®¡**:

```text
äººè„‘è¿æ¥:
- å‰é¦ˆ: ~20%
- åé¦ˆ: ~40% â­â­â­
- ä¾§å‘: ~40%

â†’ å¤§è„‘ â‰  å‰é¦ˆç½‘ç»œ
â†’ å¤§è„‘ = é«˜åº¦é€’å½’ç³»ç»Ÿ

é€’å½’å›è·¯:
V1 â‡„ V2 â‡„ V4 â‡„ IT
 â†“     â†“     â†“     â†“
       PFC (æ‰§è¡Œæ§åˆ¶)

vs Transformer:
Transformer: å±‚æ¬¡åŒ– (å¼±é€’å½’)
å¤§è„‘: é«˜åº¦é€’å½’ â­
â†’ æ¶æ„å·®å¼‚å¤§
```

---

### 4.2 é€’å½’æ·±åº¦ä¸æ„è¯†

**å‡è®¾** (Dehaene, Lau & Rosenthal):

```text
æ„è¯† = é€’å½’å¤„ç†æ·±åº¦

Level 0: æ„ŸçŸ¥ (æ— é€’å½’)
  â””â”€ åå°„ååº”

Level 1: æ³¨æ„ (æµ…é€’å½’)
  â””â”€ ç‰¹å¾é€‰æ‹©

Level 2: å·¥ä½œè®°å¿† (ä¸­é€’å½’)
  â””â”€ ä¿¡æ¯ç»´æŒ

Level 3: å…ƒè®¤çŸ¥ (æ·±é€’å½’)
  â””â”€ æ€è€ƒæ€è€ƒ â­

Level 4: è‡ªæˆ‘æ„è¯† (æœ€æ·±é€’å½’)
  â””â”€ é€’å½’è‡ªæŒ‡ â­â­â­

é€’å½’æ·±åº¦ âˆ æ„è¯†æ°´å¹³ ?

æ‰¹åˆ¤:
âš ï¸ ç›¸å…³æ€§ â‰  å› æœ
âš ï¸ é€’å½’å¿…è¦ä½†å¯èƒ½ä¸å……åˆ†
```

---

## 5. é€’å½’ç†è®ºåˆ†æ

```text
ç¥ç»ç½‘ç»œ âˆˆ RE?

RNN/LSTM:
âœ“ å›¾çµå®Œå¤‡ (ç†è®º)
âœ“ âˆˆ RE
âœ— å®è·µå—é™ (æœ‰é™ç²¾åº¦)

Transformer:
âœ“ å›¾çµå®Œå¤‡ (PÃ©rez 2019)
âœ“ âˆˆ RE
âœ“ å®è·µå¼ºå¤§

ç”Ÿç‰©å¤§è„‘:
âœ“ ç‰©ç†ç³»ç»Ÿ â†’ å¯æ¨¡æ‹Ÿ
âœ“ âˆˆ RE (ç†è®º)
âœ— å®è·µä¸å¯è¡Œ (å¤æ‚åº¦)

ç»“è®º:
æ‰€æœ‰ç¥ç»æ¶æ„ âˆˆ RE âœ“
ä½†æ•ˆç‡å’Œå¯è®­ç»ƒæ€§å·®å¼‚å·¨å¤§ âš ï¸
â†’ é€’å½’ vs æ³¨æ„åŠ› = æ¶æ„é€‰æ‹©
```

---

## 6. ä¸»é¢˜-å­ä¸»é¢˜è®ºè¯é€»è¾‘å…³ç³»å›¾

### 6.1 è®ºè¯ä¾èµ–å…³ç³»

```mermaid
graph TD
    A[ç¥ç»ç½‘ç»œé€’å½’æ¶æ„] --> B[é—®é¢˜æå‡º]
    B --> C[RNNåŸºç¡€]

    C --> D[å®šä¹‰å»ºç«‹]
    D --> D1[RNNæ–¹ç¨‹]
    D --> D2[å›¾çµå®Œå¤‡æ€§]

    D1 --> E[æ€§è´¨æ¢ç´¢]
    D2 --> E
    E --> E1[LSTMé—¨æ§é€’å½’]
    E --> E2[Transformer]

    E1 --> F[è¯æ˜æ„é€ ]
    E2 --> F
    F --> F1[é—¨æœºåˆ¶]
    F --> F2[è‡ªæ³¨æ„åŠ›æœºåˆ¶]

    F1 --> G[åº”ç”¨å±•ç¤º]
    F2 --> G
    G --> G1[ç”Ÿç‰©å¤§è„‘é€’å½’]
    G --> G2[é€’å½’æ·±åº¦ä¸æ„è¯†]

    G1 --> H[æ‰¹åˆ¤åæ€]
    G2 --> H
    H --> H1[é€’å½’ç†è®ºåˆ†æ]
    H --> H2[æ¶æ„å¯¹æ¯”]

    style A fill:#ffcccc
    style D fill:#ccffcc
    style F fill:#ccccff
    style H fill:#ffffcc
```

### 6.2 æ¦‚å¿µä¾èµ–å…³ç³»

```mermaid
graph LR
    A[ç¥ç»ç½‘ç»œ] --> B[é€’å½’æ¶æ„]

    B --> C[RNN]
    B --> D[LSTM]
    B --> E[Transformer]

    C --> F[æ—¶é—´é€’å½’]
    D --> G[é—¨æ§é€’å½’]
    E --> H[è‡ªæ³¨æ„åŠ›]

    F --> I[é€’å½’ç†è®º]
    G --> I
    H --> I

    I --> J[å›¾çµå®Œå¤‡æ€§]
    I --> K[ç”Ÿç‰©å¤§è„‘]

    style A fill:#ffffcc
    style B fill:#ffcccc
    style I fill:#ccffcc
    style J fill:#ccccff
```

**è®ºè¯é€»è¾‘é“¾æ¡**ï¼š

1. **é—®é¢˜æå‡º** (1èŠ‚)ï¼š
   - RNNåŸºç¡€ä¸é€’å½’æ€§

2. **å®šä¹‰å»ºç«‹** (1.1-1.2èŠ‚)ï¼š
   - RNNæ–¹ç¨‹å’Œå›¾çµå®Œå¤‡æ€§

3. **æ€§è´¨æ¢ç´¢** (2-3èŠ‚)ï¼š
   - LSTMçš„é—¨æ§é€’å½’ï¼ˆ2èŠ‚ï¼‰
   - Transformerï¼ˆ3èŠ‚ï¼‰

4. **è¯æ˜æ„é€ ** (2.1, 3.1èŠ‚)ï¼š
   - é—¨æœºåˆ¶å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶

5. **åº”ç”¨å±•ç¤º** (4èŠ‚)ï¼š
   - ç”Ÿç‰©å¤§è„‘çš„é€’å½’è¿æ¥

6. **æ‰¹åˆ¤åæ€** (5èŠ‚)ï¼š
   - é€’å½’ç†è®ºåˆ†æ

---

## 7. å‚è€ƒèµ„æº

### 7.1 ç»å…¸è®ºæ–‡

1. **Siegelmann, H. T., & Sontag, E. D.** (1995). "On the Computational Power of Neural Nets"
   - _Journal of Computer and System Sciences_, 50(1), 132-150
   - RNNå›¾çµå®Œå¤‡æ€§è¯æ˜

2. **Hochreiter, S., & Schmidhuber, J.** (1997). "Long Short-Term Memory"
   - _Neural Computation_, 9(8), 1735-1780
   - LSTMåŸå§‹è®ºæ–‡ â­â­â­â­â­

3. **Vaswani, A., et al.** (2017). "Attention Is All You Need"
   - _NeurIPS 2017_. Advances in Neural Information Processing Systems 30
   - Transformeræ¶æ„ â­â­â­â­â­

### 7.2 æ•™æ

1. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016)
   - _Deep Learning_
   - MIT Press. ISBN 978-0262035613
   - æ·±åº¦å­¦ä¹ åŸºç¡€

2. **Graves, A.** (2012)
   - _Supervised Sequence Labelling with Recurrent Neural Networks_
   - Springer. ISBN 978-3642247965
   - RNNåºåˆ—æ ‡æ³¨

### 7.3 åœ¨çº¿èµ„æº

1. **PyTorch - RNN Tutorial**
   - https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html
   - RNNå®ç°æ•™ç¨‹

2. **The Illustrated Transformer**
   - https://jalammar.github.io/illustrated-transformer/
   - Transformerå¯è§†åŒ–

3. **Wikipedia - Recurrent neural network**
   - https://en.wikipedia.org/wiki/Recurrent_neural_network
   - RNNåŸºæœ¬æ¦‚å¿µ

---

**æœ€åæ›´æ–°**: 2025-12-04
**Tier**: 2 (ç§‘å­¦+å·¥ç¨‹)
**è¶‹åŠ¿**: Transformerä¸»å¯¼ âœ“
**é€’å½’æ€§**: RNNé€’å½’ï¼ŒTransformerå±‚æ¬¡
**çŠ¶æ€**: âœ… å·²æ·»åŠ ä¸»é¢˜-å­ä¸»é¢˜è®ºè¯é€»è¾‘å…³ç³»å›¾å’Œå‚è€ƒèµ„æºç« èŠ‚
