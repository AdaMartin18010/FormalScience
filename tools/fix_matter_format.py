#!/usr/bin/env python3
"""
ç»Ÿä¸€ Matter æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ Markdown æ–‡ä»¶çš„æ ¼å¼
- ç»Ÿä¸€ç›®å½•æ ¼å¼
- ç»Ÿä¸€æ ‡é¢˜ç¼–å·æ ¼å¼
- ä¿æŒåºå·ç»“æ„ä¸€è‡´æ€§
"""

import os
import re
import unicodedata
from pathlib import Path
from typing import List, Tuple, Dict, Optional

def slugify(text: str) -> str:
    """ç”Ÿæˆ GitHub é£æ ¼çš„é”šç‚¹ ID"""
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # ç§»é™¤ç¼–å·å‰ç¼€ï¼ˆç”¨äºç”Ÿæˆé”šç‚¹ï¼‰
    text = re.sub(r'^\d+[\.ã€]?\s*', '', text)
    # Unicode è§„èŒƒåŒ–
    text = unicodedata.normalize('NFKD', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fff-]', '', text)
    # å°†ç©ºæ ¼å’Œå¤šä¸ªè¿å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªè¿å­—ç¬¦
    text = re.sub(r'[-\s]+', '-', text)
    # ç§»é™¤é¦–å°¾è¿å­—ç¬¦
    text = text.strip('-')
    return text

def extract_headers(content: str) -> List[Tuple[int, str]]:
    """æå–æ‰€æœ‰æ ‡é¢˜ï¼Œè¿”å› (çº§åˆ«, åŸå§‹æ–‡æœ¬)"""
    headers = []
    lines = content.split('\n')
    
    for line in lines:
        # åŒ¹é… Markdown æ ‡é¢˜
        match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if match:
            level = len(match.group(1))
            text = match.group(2).strip()
            # è·³è¿‡"ç›®å½•"æ ‡é¢˜ï¼Œå®ƒä¸åº”è¯¥è¢«ç¼–å·
            if text.strip() in ['ç›®å½•', 'ğŸ“‹ ç›®å½•', 'ç›®å½• ğŸ“‹']:
                continue
            headers.append((level, text))
    
    return headers

def normalize_header_numbering(headers: List[Tuple[int, str]]) -> List[Tuple[int, str, str]]:
    """è§„èŒƒåŒ–æ ‡é¢˜ç¼–å·ï¼Œè¿”å› (çº§åˆ«, åŸå§‹æ–‡æœ¬, æ–°ç¼–å·æ–‡æœ¬)"""
    if not headers:
        return []
    
    normalized = []
    counters = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}
    last_level = 0
    
    for level, text in headers:
        # ç§»é™¤ç°æœ‰çš„ç¼–å·ï¼Œä½†ä¿ç•™å†…å®¹
        text_clean = text.strip()
        # ç§»é™¤æ•°å­—ç¼–å·ï¼ˆå¦‚ "1. ", "1.1 ", "1.1.1 " ç­‰ï¼‰
        text_clean = re.sub(r'^\d+(?:\.\d+)*[\.ã€]?\s*', '', text_clean)
        # ç§»é™¤ä¸­æ–‡ç¼–å·ï¼ˆå¦‚ "ç¬¬ä¸€éƒ¨åˆ†", "ç¬¬ä¸€ç« " ç­‰ï¼‰
        text_clean = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+[éƒ¨åˆ†ç« èŠ‚]\s*', '', text_clean)
        text_clean = text_clean.strip()
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬
        if not text_clean:
            text_clean = text.strip()
        
        # æ›´æ–°è®¡æ•°å™¨
        if level <= last_level:
            # é‡ç½®ä¸‹çº§è®¡æ•°å™¨
            for l in range(level + 1, 7):
                counters[l] = 0
        
        counters[level] += 1
        last_level = level
        
        # ç”Ÿæˆæ–°ç¼–å·
        if level == 1:
            new_text = f"{counters[1]}. {text_clean}"
        elif level == 2:
            new_text = f"{counters[1]}.{counters[2]} {text_clean}"
        elif level == 3:
            new_text = f"{counters[1]}.{counters[2]}.{counters[3]} {text_clean}"
        elif level == 4:
            new_text = f"{counters[1]}.{counters[2]}.{counters[3]}.{counters[4]} {text_clean}"
        elif level == 5:
            new_text = f"{counters[1]}.{counters[2]}.{counters[3]}.{counters[4]}.{counters[5]} {text_clean}"
        else:
            new_text = f"{counters[1]}.{counters[2]}.{counters[3]}.{counters[4]}.{counters[5]}.{counters[6]} {text_clean}"
        
        normalized.append((level, text, new_text))
    
    return normalized

def generate_toc(headers: List[Tuple[int, str, str]]) -> str:
    """ç”Ÿæˆæ ‡å‡†åŒ–çš„ç›®å½•"""
    if not headers:
        return ""
    
    toc_lines = ["## ç›®å½•", ""]
    
    for level, original, new_text in headers:
        # è·³è¿‡"ç›®å½•"æ ‡é¢˜æœ¬èº«
        text_clean = new_text.strip()
        if re.match(r'^\d+[\.ã€]?\s*ç›®å½•', text_clean, re.IGNORECASE):
            continue
        
        indent = "  " * (level - 1)
        # ç”Ÿæˆé”šç‚¹
        anchor = slugify(new_text)
        toc_lines.append(f"{indent}- [{new_text}](#{anchor})")
    
    return "\n".join(toc_lines) + "\n"

def update_content_headers(content: str, header_mapping: Dict[str, str]) -> str:
    """æ›´æ–°å†…å®¹ä¸­çš„æ ‡é¢˜"""
    lines = content.split('\n')
    result = []
    
    for line in lines:
        match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if match:
            level = len(match.group(1))
            original_text = match.group(2).strip()
            
            # è·³è¿‡"ç›®å½•"æ ‡é¢˜ï¼Œä¿æŒä¸º"## ç›®å½•"ï¼ˆä¸ç¼–å·ï¼‰
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•æ ‡é¢˜ï¼ˆå¯èƒ½å·²ç»ç¼–å·ï¼‰
            if (original_text.strip() in ['ç›®å½•', 'ğŸ“‹ ç›®å½•', 'ç›®å½• ğŸ“‹'] or 
                re.match(r'^\d+(?:\.\d+)*[\.ã€]?\s*[ğŸ“‹]*\s*ç›®å½•', original_text, re.IGNORECASE)):
                result.append(f"{'#' * level} ç›®å½•")
            # æŸ¥æ‰¾å¯¹åº”çš„æ–°æ ‡é¢˜
            elif original_text in header_mapping:
                result.append(f"{'#' * level} {header_mapping[original_text]}")
            else:
                result.append(line)
        else:
            result.append(line)
    
    return '\n'.join(result)

def find_and_replace_toc(content: str, new_toc: str) -> str:
    """æŸ¥æ‰¾å¹¶æ›¿æ¢ç›®å½•"""
    # åŒ¹é…å„ç§å¯èƒ½çš„ç›®å½•æ ¼å¼
    # åŒ¹é… ## ç›®å½• æˆ– ## ğŸ“‹ ç›®å½• ç­‰
    toc_start_pattern = r'##\s*[ğŸ“‹]*\s*ç›®å½•\s*\n'
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ç›®å½•ä½ç½®
    toc_matches = list(re.finditer(toc_start_pattern, content, re.IGNORECASE | re.MULTILINE))
    
    if toc_matches:
        # ä»ç¬¬ä¸€ä¸ªç›®å½•å¼€å§‹æ›¿æ¢ï¼ˆé€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯æ­£ç¡®çš„ï¼‰
        for toc_match in toc_matches:
            start_pos = toc_match.start()
            # æŸ¥æ‰¾ç›®å½•ç»“æŸä½ç½®
            remaining = content[start_pos + toc_match.end():]
            
            # åŒ¹é…ç›®å½•å†…å®¹ï¼ˆåˆ—è¡¨é¡¹ï¼Œå¯èƒ½è·¨å¤šè¡Œï¼‰
            # åŒ¹é…åˆ°ä¸‹ä¸€ä¸ª ## æ ‡é¢˜
            lines = remaining.split('\n')
            toc_end = 0
            found_list = False
            
            for i, line in enumerate(lines):
                # å¦‚æœæ˜¯åˆ—è¡¨é¡¹
                if re.match(r'^(?:  )*(?:-|\*|\d+\.)\s*\[.*?\]\(.*?\)', line):
                    found_list = True
                    toc_end = i + 1
                # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ª ## æ ‡é¢˜
                elif line.strip().startswith('##'):
                    break
                # å¦‚æœå·²ç»æ‰¾åˆ°åˆ—è¡¨é¡¹ï¼Œé‡åˆ°ç©ºè¡Œåè¿˜æœ‰éåˆ—è¡¨å†…å®¹ï¼Œåœæ­¢
                elif found_list and line.strip() and not re.match(r'^(?:  )*(?:-|\*|\d+\.)', line):
                    break
                # å¦‚æœè¿˜æ²¡æ‰¾åˆ°åˆ—è¡¨é¡¹ï¼Œä½†é‡åˆ°éåˆ—è¡¨å†…å®¹ï¼Œå¯èƒ½ä¸æ˜¯ç›®å½•
                elif not found_list and line.strip() and not re.match(r'^(?:  )*(?:-|\*|\d+\.)', line):
                    break
                elif found_list:
                    # ç»§ç»­æ”¶é›†åˆ—è¡¨é¡¹åçš„ç©ºè¡Œ
                    toc_end = i + 1
            
            if found_list:
                # æ‰¾åˆ°ç›®å½•å†…å®¹ï¼Œæ›¿æ¢
                toc_content = '\n'.join(lines[:toc_end])
                end_pos = start_pos + toc_match.end() + len(toc_content)
                # ç¡®ä¿åŒ…å«æœ«å°¾çš„æ¢è¡Œ
                if end_pos < len(content) and content[end_pos] == '\n':
                    end_pos += 1
                # æ›¿æ¢ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ç›®å½•ï¼Œç„¶åç»§ç»­å¤„ç†å…¶ä»–å¯èƒ½çš„æ—§ç›®å½•
                content = content[:start_pos] + new_toc + content[end_pos:]
                # ç»§ç»­æŸ¥æ‰¾å¹¶åˆ é™¤å…¶ä»–å¯èƒ½çš„æ—§ç›®å½•
                break
            else:
                # åªæ‰¾åˆ°ç›®å½•æ ‡é¢˜ï¼ŒæŸ¥æ‰¾ä¸‹ä¸€ä¸ª ## æ ‡é¢˜
                next_header = re.search(r'\n##\s+', remaining)
                if next_header:
                    end_pos = start_pos + toc_match.end() + next_header.start()
                    return content[:start_pos] + new_toc + content[end_pos:]
                else:
                    # æ²¡æœ‰ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼Œæ›¿æ¢åˆ°æ–‡ä»¶æœ«å°¾
                    return content[:start_pos] + new_toc + remaining
        
        # å¦‚æœæ‰€æœ‰åŒ¹é…éƒ½å¤„ç†äº†ï¼Œè¿”å›åŸå†…å®¹ï¼ˆä¸åº”è¯¥åˆ°è¿™é‡Œï¼‰
        return content
    else:
        # æ²¡æœ‰æ‰¾åˆ°ç›®å½•ï¼Œåœ¨ç¬¬ä¸€ä¸ªæ ‡é¢˜åæ’å…¥
        first_header_match = re.search(r'^(#\s+.+)$', content, re.MULTILINE)
        if first_header_match:
            pos = first_header_match.end()
            # æŸ¥æ‰¾æ˜¯å¦å·²ç»æœ‰ç©ºè¡Œ
            next_chars = content[pos:pos+2]
            if next_chars == '\n\n':
                return content[:pos+2] + new_toc + content[pos+2:]
            elif next_chars.startswith('\n'):
                return content[:pos+1] + '\n' + new_toc + content[pos+1:]
            else:
                return content[:pos] + '\n\n' + new_toc + content[pos:]
    
    return content

def fix_file(file_path: Path) -> bool:
    """ä¿®å¤å•ä¸ªæ–‡ä»¶çš„æ ¼å¼"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"  è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # æå–æ ‡é¢˜
    headers = extract_headers(content)
    
    if not headers:
        # æ²¡æœ‰æ ‡é¢˜ï¼Œè·³è¿‡
        return False
    
    # è§„èŒƒåŒ–ç¼–å·
    normalized = normalize_header_numbering(headers)
    
    if not normalized:
        return False
    
    # åˆ›å»ºæ ‡é¢˜æ˜ å°„ï¼ˆåŸå§‹æ–‡æœ¬ -> æ–°æ–‡æœ¬ï¼‰
    header_mapping = {orig: new for _, orig, new in normalized}
    
    # ç”Ÿæˆæ–°ç›®å½•
    new_toc = generate_toc(normalized)
    
    # æ›´æ–°å†…å®¹ä¸­çš„æ ‡é¢˜
    updated_content = update_content_headers(content, header_mapping)
    
    # æ›¿æ¢æˆ–æ’å…¥ç›®å½•
    updated_content = find_and_replace_toc(updated_content, new_toc)
    
    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ—§ç›®å½•ï¼ˆåªåˆ é™¤ç¼–å·çš„ç›®å½•æ ‡é¢˜ï¼Œä¸åˆ é™¤æ ‡å‡†çš„"## ç›®å½•"ï¼‰
    # åˆ é™¤æ‰€æœ‰ä»¥ ## æ•°å­—. ç›®å½• æˆ– ## æ•°å­— ğŸ“‹ ç›®å½• å¼€å¤´çš„å—
    old_toc_pattern = r'##\s+\d+(?:\.\d+)*[\.ã€]?\s*[ğŸ“‹]*\s*ç›®å½•\s*\n(?:(?:  )*(?:-|\*|\d+\.)\s*\[.*?\]\(.*?\)\s*\n?)+'
    updated_content = re.sub(old_toc_pattern, '', updated_content, flags=re.MULTILINE)
    
    # å†™å›æ–‡ä»¶
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        return True
    except Exception as e:
        print(f"  å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return False

def process_directory(root_dir: Path):
    """é€’å½’å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰ Markdown æ–‡ä»¶"""
    md_files = list(root_dir.rglob('*.md'))
    total = len(md_files)
    success = 0
    failed = []
    
    print(f"æ‰¾åˆ° {total} ä¸ª Markdown æ–‡ä»¶")
    
    for i, md_file in enumerate(md_files, 1):
        print(f"[{i}/{total}] å¤„ç†: {md_file.relative_to(root_dir)}")
        if fix_file(md_file):
            success += 1
        else:
            failed.append(md_file)
    
    print(f"\nå¤„ç†å®Œæˆ: æˆåŠŸ {success}/{total}")
    if failed:
        print(f"å¤±è´¥çš„æ–‡ä»¶ ({len(failed)}):")
        for f in failed:
            print(f"  - {f.relative_to(root_dir)}")

if __name__ == '__main__':
    # Matter ç›®å½•è·¯å¾„
    matter_dir = Path(__file__).parent.parent / 'docs' / 'Matter'
    
    if not matter_dir.exists():
        print(f"ç›®å½•ä¸å­˜åœ¨: {matter_dir}")
        exit(1)
    
    print(f"å¼€å§‹å¤„ç†ç›®å½•: {matter_dir}")
    process_directory(matter_dir)
    print("å®Œæˆï¼")
