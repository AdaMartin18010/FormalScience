#!/usr/bin/env python3
"""
æ–‡æ¡£ç»“æ„æ£€æŸ¥å’Œä¿®å¤å·¥å…·
æ£€æŸ¥æ‰€æœ‰Perspectiveä¸‹æ–‡æ¡£çš„ç»“æ„ä¸€è‡´æ€§
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
import sys

class DocumentChecker:
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.issues = []
        
    def check_file(self, filepath: Path) -> Dict:
        """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„ç»“æ„"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        issues = {}
        lines = content.split('\n')
        
        # æ£€æŸ¥æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
        if lines:
            first_line = lines[0].strip()
            if first_line.startswith('#'):
                # æ£€æŸ¥æ˜¯å¦æœ‰åºå·
                # åŒ¹é…æ¨¡å¼ï¼š# 1.1 æ ‡é¢˜ æˆ– # 01 æ ‡é¢˜
                has_number = bool(re.match(r'^#\s+(\d+\.?\d*)\s+', first_line))
                issues['has_numbered_title'] = has_number
                issues['title'] = first_line
            else:
                issues['has_numbered_title'] = False
                issues['title'] = None
        
        # æ£€æŸ¥å…ƒæ•°æ®å—
        metadata_pattern = r'>\s\*\*æ–‡æ¡£ç‰ˆæœ¬\*\*:'
        has_metadata = bool(re.search(metadata_pattern, content))
        issues['has_metadata'] = has_metadata
        
        # æ£€æŸ¥ç›®å½•
        toc_patterns = [
            r'##\s+ç›®å½•',
            r'##\s+Table of Contents',
            r'##\s+ç›®å½•\s+\|\s+Table of Contents'
        ]
        has_toc = any(re.search(pattern, content) for pattern in toc_patterns)
        issues['has_toc'] = has_toc
        
        # æ£€æŸ¥æ ¸å¿ƒæ¦‚å¿µæ·±åº¦åˆ†æ
        deep_analysis_pattern = r'##\s+ğŸ“Š\s+æ ¸å¿ƒæ¦‚å¿µæ·±åº¦åˆ†æ'
        has_deep_analysis = bool(re.search(deep_analysis_pattern, content))
        issues['has_deep_analysis'] = has_deep_analysis
        
        # æ£€æŸ¥å¯¼èˆª
        navigation_patterns = [
            r'\*\*å¯¼èˆª\*\*ï¼š',
            r'Navigation:',
            r'\[â† ä¸Šä¸€èŠ‚',
            r'\[ä¸‹ä¸€èŠ‚ â†’\]'
        ]
        has_navigation = any(re.search(pattern, content) for pattern in navigation_patterns)
        issues['has_navigation'] = has_navigation
        
        # æ£€æŸ¥ç›¸å…³ä¸»é¢˜
        related_topics_pattern = r'##\s+ç›¸å…³ä¸»é¢˜'
        has_related_topics = bool(re.search(related_topics_pattern, content))
        issues['has_related_topics'] = has_related_topics
        
        issues['line_count'] = len(lines)
        
        return issues
    
    def scan_perspective(self, perspective_name: str) -> List[Dict]:
        """æ‰«æä¸€ä¸ªPerspectiveä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        perspective_path = self.base_path / perspective_name
        if not perspective_path.exists():
            print(f"âš ï¸  è·¯å¾„ä¸å­˜åœ¨: {perspective_path}")
            return []
        
        results = []
        for md_file in perspective_path.rglob("*.md"):
            # è·³è¿‡READMEã€FAQç­‰ç‰¹æ®Šæ–‡ä»¶
            if md_file.name in ['README.md', 'FAQ.md', 'GLOSSARY.md', 
                                'QUICK_REFERENCE.md', 'LEARNING_PATHS.md',
                                '00_Master_Index.md', 'COMPLETION_SUMMARY.md',
                                'CONCEPT_ALIGNMENT_TABLE.md']:
                continue
            
            issues = self.check_file(md_file)
            issues['filepath'] = md_file.relative_to(self.base_path)
            issues['filename'] = md_file.name
            results.append(issues)
        
        return results
    
    def generate_report(self, perspective_name: str, results: List[Dict]) -> str:
        """ç”ŸæˆæŠ¥å‘Š"""
        report = [f"\n{'='*80}"]
        report.append(f"{perspective_name} ç»“æ„æ£€æŸ¥æŠ¥å‘Š")
        report.append(f"{'='*80}\n")
        
        total_files = len(results)
        
        # ç»Ÿè®¡å„é¡¹æŒ‡æ ‡
        stats = {
            'has_numbered_title': 0,
            'has_metadata': 0,
            'has_toc': 0,
            'has_deep_analysis': 0,
            'has_navigation': 0,
            'has_related_topics': 0
        }
        
        missing_toc_files = []
        missing_number_files = []
        missing_nav_files = []
        
        for result in results:
            for key in stats:
                if result.get(key, False):
                    stats[key] += 1
            
            if not result.get('has_toc', False):
                missing_toc_files.append(result['filepath'])
            if not result.get('has_numbered_title', False):
                missing_number_files.append(result['filepath'])
            if not result.get('has_navigation', False):
                missing_nav_files.append(result['filepath'])
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        report.append(f"æ€»æ–‡ä»¶æ•°: {total_files}")
        report.append(f"\nç»“æ„å…ƒç´ è¦†ç›–ç‡:")
        report.append(f"  âœ“ æ ‡é¢˜æœ‰åºå·:     {stats['has_numbered_title']:3d}/{total_files} ({stats['has_numbered_title']*100//total_files}%)")
        report.append(f"  âœ“ æœ‰å…ƒæ•°æ®å—:     {stats['has_metadata']:3d}/{total_files} ({stats['has_metadata']*100//total_files}%)")
        report.append(f"  âœ“ æœ‰ç›®å½•:         {stats['has_toc']:3d}/{total_files} ({stats['has_toc']*100//total_files}%)")
        report.append(f"  âœ“ æœ‰æ·±åº¦åˆ†æ:     {stats['has_deep_analysis']:3d}/{total_files} ({stats['has_deep_analysis']*100//total_files}%)")
        report.append(f"  âœ“ æœ‰å¯¼èˆª:         {stats['has_navigation']:3d}/{total_files} ({stats['has_navigation']*100//total_files}%)")
        report.append(f"  âœ“ æœ‰ç›¸å…³ä¸»é¢˜:     {stats['has_related_topics']:3d}/{total_files} ({stats['has_related_topics']*100//total_files}%)")
        
        # åˆ—å‡ºéœ€è¦ä¿®å¤çš„æ–‡ä»¶
        if missing_toc_files:
            report.append(f"\nâŒ ç¼ºå°‘ç›®å½•çš„æ–‡ä»¶ ({len(missing_toc_files)}ä¸ª):")
            for filepath in missing_toc_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report.append(f"   - {filepath}")
            if len(missing_toc_files) > 10:
                report.append(f"   ... è¿˜æœ‰ {len(missing_toc_files) - 10} ä¸ªæ–‡ä»¶")
        
        if missing_number_files:
            report.append(f"\nâŒ æ ‡é¢˜æ— åºå·çš„æ–‡ä»¶ ({len(missing_number_files)}ä¸ª):")
            for filepath in missing_number_files[:10]:
                report.append(f"   - {filepath}")
            if len(missing_number_files) > 10:
                report.append(f"   ... è¿˜æœ‰ {len(missing_number_files) - 10} ä¸ªæ–‡ä»¶")
        
        if missing_nav_files:
            report.append(f"\nâŒ ç¼ºå°‘å¯¼èˆªçš„æ–‡ä»¶ ({len(missing_nav_files)}ä¸ª):")
            for filepath in missing_nav_files[:10]:
                report.append(f"   - {filepath}")
            if len(missing_nav_files) > 10:
                report.append(f"   ... è¿˜æœ‰ {len(missing_nav_files) - 10} ä¸ªæ–‡ä»¶")
        
        return "\n".join(report)

def main():
    base_path = Path(__file__).parent.parent / "Concept"
    
    if len(sys.argv) > 1:
        base_path = Path(sys.argv[1])
    
    print(f"ğŸ” æ‰«æè·¯å¾„: {base_path}")
    
    checker = DocumentChecker(base_path)
    
    perspectives = [
        "Software_Perspective",
        "AI_model_Perspective",
        "FormalLanguage_Perspective",
        "Information_Theory_Perspective"
    ]
    
    all_reports = []
    all_results = {}
    
    for perspective in perspectives:
        print(f"\nğŸ“Š æ­£åœ¨æ‰«æ {perspective}...")
        results = checker.scan_perspective(perspective)
        all_results[perspective] = results
        report = checker.generate_report(perspective, results)
        all_reports.append(report)
        print(report)
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("æ€»ä½“ç»Ÿè®¡")
    print(f"{'='*80}\n")
    
    total_files = sum(len(results) for results in all_results.values())
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    
    for perspective, results in all_results.items():
        print(f"  {perspective}: {len(results)} ä¸ªæ–‡ä»¶")
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_file = base_path / "STRUCTURE_CHECK_REPORT.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# æ–‡æ¡£ç»“æ„æ£€æŸ¥æŠ¥å‘Š\n\n")
        f.write(f"> ç”Ÿæˆæ—¶é—´: 2025-10-27\n\n")
        for report in all_reports:
            f.write(report)
            f.write("\n\n")
    
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

if __name__ == "__main__":
    main()

