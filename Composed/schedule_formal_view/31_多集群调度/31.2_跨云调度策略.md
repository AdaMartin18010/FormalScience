# 31.2 è·¨äº‘è°ƒåº¦ç­–ç•¥

> **ä¸»é¢˜**: 31. å¤šé›†ç¾¤è°ƒåº¦ - 31.2 è·¨äº‘è°ƒåº¦ç­–ç•¥
> **è¦†ç›–**: è°ƒåº¦ç®—æ³•ã€è´Ÿè½½å‡è¡¡ã€æ•…éšœè½¬ç§»ã€æˆæœ¬ä¼˜åŒ–

---

## ğŸ“‹ ç›®å½•

- [31.2 è·¨äº‘è°ƒåº¦ç­–ç•¥](#312-è·¨äº‘è°ƒåº¦ç­–ç•¥)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1 è°ƒåº¦ç­–ç•¥åˆ†ç±»](#1-è°ƒåº¦ç­–ç•¥åˆ†ç±»)
    - [1.1 åŸºäºä½ç½®çš„è°ƒåº¦](#11-åŸºäºä½ç½®çš„è°ƒåº¦)
    - [1.2 åŸºäºèµ„æºçš„è°ƒåº¦](#12-åŸºäºèµ„æºçš„è°ƒåº¦)
    - [1.3 åŸºäºæˆæœ¬çš„è°ƒåº¦](#13-åŸºäºæˆæœ¬çš„è°ƒåº¦)
    - [1.4 åŸºäºSLOçš„è°ƒåº¦](#14-åŸºäºsloçš„è°ƒåº¦)
  - [2 è°ƒåº¦ç®—æ³•](#2-è°ƒåº¦ç®—æ³•)
    - [2.1 è´ªå¿ƒç®—æ³•](#21-è´ªå¿ƒç®—æ³•)
    - [2.2 åŠ æƒè½®è¯¢](#22-åŠ æƒè½®è¯¢)
    - [2.3 è´Ÿè½½æ„ŸçŸ¥è°ƒåº¦](#23-è´Ÿè½½æ„ŸçŸ¥è°ƒåº¦)
    - [2.4 æœºå™¨å­¦ä¹ è°ƒåº¦](#24-æœºå™¨å­¦ä¹ è°ƒåº¦)
  - [3 è´Ÿè½½å‡è¡¡ç­–ç•¥](#3-è´Ÿè½½å‡è¡¡ç­–ç•¥)
    - [3.1 é™æ€è´Ÿè½½å‡è¡¡](#31-é™æ€è´Ÿè½½å‡è¡¡)
    - [3.2 åŠ¨æ€è´Ÿè½½å‡è¡¡](#32-åŠ¨æ€è´Ÿè½½å‡è¡¡)
    - [3.3 è‡ªé€‚åº”å†å¹³è¡¡](#33-è‡ªé€‚åº”å†å¹³è¡¡)
  - [4 æ•…éšœè½¬ç§»ä¸é«˜å¯ç”¨](#4-æ•…éšœè½¬ç§»ä¸é«˜å¯ç”¨)
    - [4.1 é›†ç¾¤æ•…éšœæ£€æµ‹](#41-é›†ç¾¤æ•…éšœæ£€æµ‹)
    - [4.2 è‡ªåŠ¨æ•…éšœè½¬ç§»](#42-è‡ªåŠ¨æ•…éšœè½¬ç§»)
    - [4.3 ç¾éš¾æ¢å¤](#43-ç¾éš¾æ¢å¤)
  - [5 æˆæœ¬ä¼˜åŒ–ç­–ç•¥](#5-æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
    - [5.1 æˆæœ¬æ¨¡å‹](#51-æˆæœ¬æ¨¡å‹)
    - [5.2 æˆæœ¬æ„ŸçŸ¥è°ƒåº¦](#52-æˆæœ¬æ„ŸçŸ¥è°ƒåº¦)
    - [5.3 Spotå®ä¾‹è°ƒåº¦](#53-spotå®ä¾‹è°ƒåº¦)
  - [6 å½¢å¼åŒ–æ¨¡å‹](#6-å½¢å¼åŒ–æ¨¡å‹)
    - [6.1 è°ƒåº¦ç›®æ ‡å‡½æ•°](#61-è°ƒåº¦ç›®æ ‡å‡½æ•°)
    - [6.2 çº¦æŸæ¡ä»¶](#62-çº¦æŸæ¡ä»¶)
    - [6.3 ä¼˜åŒ–ç®—æ³•](#63-ä¼˜åŒ–ç®—æ³•)
  - [7 å®è·µæ¡ˆä¾‹](#7-å®è·µæ¡ˆä¾‹)
    - [7.1 Karmadaè°ƒåº¦ç­–ç•¥å®è·µ](#71-karmadaè°ƒåº¦ç­–ç•¥å®è·µ)
    - [7.2 æˆæœ¬ä¼˜åŒ–å®è·µ](#72-æˆæœ¬ä¼˜åŒ–å®è·µ)
  - [8 è·¨è§†è§’é“¾æ¥](#8-è·¨è§†è§’é“¾æ¥)

---

## 1 è°ƒåº¦ç­–ç•¥åˆ†ç±»

### 1.1 åŸºäºä½ç½®çš„è°ƒåº¦

**åœ°ç†ä½ç½®æ„ŸçŸ¥è°ƒåº¦**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: geo-aware-policy
spec:
  resourceSelectors:
  - apiVersion: apps/v1
    kind: Deployment
    name: web-service
  placement:
    clusterAffinity:
      clusterNames:
      - us-west-1   # ç¾å›½è¥¿æµ·å²¸
      - us-east-1   # ç¾å›½ä¸œæµ·å²¸
      - eu-west-1   # æ¬§æ´²è¥¿éƒ¨
    spreadConstraints:
    - spreadByField: region
      maxSkew: 1
      whenUnsatisfiable: DoNotSchedule
```

**å»¶è¿Ÿæ„ŸçŸ¥è°ƒåº¦**ï¼š

é€‰æ‹©å»¶è¿Ÿæœ€ä½çš„é›†ç¾¤ï¼š

$$
\text{cluster} = \arg\min_{c \in C} \text{Latency}(\text{user}, c)
$$

### 1.2 åŸºäºèµ„æºçš„è°ƒåº¦

**èµ„æºå¯ç”¨æ€§è°ƒåº¦**ï¼š

é€‰æ‹©èµ„æºå……è¶³çš„é›†ç¾¤ï¼š

$$
\text{Score}(c) = \alpha \cdot \frac{\text{CPU}_{available}}{\text{CPU}_{total}} + \beta \cdot \frac{\text{Memory}_{available}}{\text{Memory}_{total}}
$$

**ç¤ºä¾‹ï¼šèµ„æºæ„ŸçŸ¥è°ƒåº¦**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: resource-aware-policy
spec:
  resourceSelectors:
  - apiVersion: apps/v1
    kind: Deployment
    name: ml-training
  placement:
    clusterAffinity:
      clusterNames:
      - gpu-cluster-1
      - gpu-cluster-2
    clusterTolerations:
    - key: "resource.kubernetes.io/gpu"
      operator: "Exists"
```

### 1.3 åŸºäºæˆæœ¬çš„è°ƒåº¦

**æˆæœ¬ä¼˜åŒ–è°ƒåº¦**ï¼š

é€‰æ‹©æˆæœ¬æœ€ä½çš„é›†ç¾¤ï¼š

$$
\text{Cost}(c, w) = \text{ComputeCost}(c) \cdot \text{Duration}(w) + \text{NetworkCost}(c) \cdot \text{DataTransfer}(w)
$$

å…¶ä¸­ï¼š

- $\text{ComputeCost}(c)$ï¼šé›†ç¾¤ $c$ çš„è®¡ç®—æˆæœ¬ï¼ˆ$/å°æ—¶ï¼‰
- $\text{NetworkCost}(c)$ï¼šé›†ç¾¤ $c$ çš„ç½‘ç»œæˆæœ¬ï¼ˆ$/GBï¼‰
- $\text{Duration}(w)$ï¼šå·¥ä½œè´Ÿè½½ $w$ çš„è¿è¡Œæ—¶é•¿
- $\text{DataTransfer}(w)$ï¼šå·¥ä½œè´Ÿè½½ $w$ çš„æ•°æ®ä¼ è¾“é‡

### 1.4 åŸºäºSLOçš„è°ƒåº¦

**SLOä¿è¯è°ƒåº¦**ï¼š

ç¡®ä¿SLOæ»¡è¶³ï¼š

$$
\text{SLO\_Score}(c, w) = \begin{cases}
1 & \text{if SLO satisfied} \\
0 & \text{otherwise}
\end{cases}
$$

**ç¤ºä¾‹ï¼šå»¶è¿ŸSLO**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: slo-aware-policy
spec:
  resourceSelectors:
  - apiVersion: apps/v1
    kind: Deployment
    name: api-service
  placement:
    clusterAffinity:
      clusterNames:
      - us-west-1
      - us-west-2
    clusterTolerations:
    - key: "latency.kubernetes.io/p99"
      operator: "LessThan"
      value: "100ms"  # P99å»¶è¿Ÿ < 100ms
```

---

## 2 è°ƒåº¦ç®—æ³•

### 2.1 è´ªå¿ƒç®—æ³•

**æœ€ä½³é€‚é…ï¼ˆBest Fitï¼‰**ï¼š

é€‰æ‹©èµ„æºæœ€åŒ¹é…çš„é›†ç¾¤ï¼š

```python
def best_fit_schedule(workload, clusters):
    """
    æœ€ä½³é€‚é…è°ƒåº¦ç®—æ³•
    """
    best_cluster = None
    min_waste = float('inf')

    for cluster in clusters:
        if can_schedule(workload, cluster):
            waste = cluster.available_resources - workload.required_resources
            if waste < min_waste:
                min_waste = waste
                best_cluster = cluster

    return best_cluster
```

**é¦–æ¬¡é€‚é…ï¼ˆFirst Fitï¼‰**ï¼š

é€‰æ‹©ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„é›†ç¾¤ï¼š

```python
def first_fit_schedule(workload, clusters):
    """
    é¦–æ¬¡é€‚é…è°ƒåº¦ç®—æ³•
    """
    for cluster in clusters:
        if can_schedule(workload, cluster):
            return cluster

    return None
```

### 2.2 åŠ æƒè½®è¯¢

**åŠ æƒè½®è¯¢è°ƒåº¦**ï¼š

```python
class WeightedRoundRobinScheduler:
    def __init__(self, clusters, weights):
        self.clusters = clusters
        self.weights = weights
        self.current_index = 0
        self.current_weight = 0
        self.max_weight = max(weights)
        self.gcd_weight = self._gcd_all(weights)

    def schedule(self, workload):
        """
        åŠ æƒè½®è¯¢è°ƒåº¦
        """
        while True:
            self.current_index = (self.current_index + 1) % len(self.clusters)
            if self.current_index == 0:
                self.current_weight = self.current_weight - self.gcd_weight
                if self.current_weight <= 0:
                    self.current_weight = self.max_weight

            if self.weights[self.current_index] >= self.current_weight:
                return self.clusters[self.current_index]
```

### 2.3 è´Ÿè½½æ„ŸçŸ¥è°ƒåº¦

**è´Ÿè½½æ„ŸçŸ¥ç®—æ³•**ï¼š

```python
def load_aware_schedule(workload, clusters):
    """
    è´Ÿè½½æ„ŸçŸ¥è°ƒåº¦ç®—æ³•
    """
    scores = []

    for cluster in clusters:
        # è®¡ç®—é›†ç¾¤è´Ÿè½½
        cpu_utilization = cluster.cpu_used / cluster.cpu_total
        memory_utilization = cluster.memory_used / cluster.memory_total

        # è®¡ç®—è´Ÿè½½åˆ†æ•°ï¼ˆè´Ÿè½½è¶Šä½åˆ†æ•°è¶Šé«˜ï¼‰
        score = (1 - cpu_utilization) * 0.6 + (1 - memory_utilization) * 0.4

        scores.append((cluster, score))

    # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„é›†ç¾¤
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[0][0]
```

### 2.4 æœºå™¨å­¦ä¹ è°ƒåº¦

**å¼ºåŒ–å­¦ä¹ è°ƒåº¦**ï¼š

```python
import numpy as np
from tensorflow import keras

class RLScheduler:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.model = self._build_model()

    def _build_model(self):
        """
        æ„å»ºDQNæ¨¡å‹
        """
        model = keras.Sequential([
            keras.layers.Dense(128, activation='relu', input_shape=(self.state_dim,)),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dense(self.action_dim, activation='linear')
        ])
        model.compile(optimizer='adam', loss='mse')
        return model

    def schedule(self, state):
        """
        åŸºäºå½“å‰çŠ¶æ€é€‰æ‹©é›†ç¾¤
        """
        q_values = self.model.predict(np.array([state]))
        return np.argmax(q_values[0])

    def train(self, states, actions, rewards, next_states):
        """
        è®­ç»ƒæ¨¡å‹
        """
        # Q-learningæ›´æ–°
        targets = rewards + 0.99 * np.max(self.model.predict(next_states), axis=1)
        self.model.fit(states, targets, epochs=1, verbose=0)
```

---

## 3 è´Ÿè½½å‡è¡¡ç­–ç•¥

### 3.1 é™æ€è´Ÿè½½å‡è¡¡

**å›ºå®šæƒé‡åˆ†é…**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: static-balance-policy
spec:
  placement:
    replicaScheduling:
      replicaSchedulingType: Divided
      replicaDivisionPreference: Weighted
      weightPreference:
        staticWeightList:
        - targetCluster:
            clusterNames:
            - cluster-1
          weight: 3  # 50% (3/6)
        - targetCluster:
            clusterNames:
            - cluster-2
          weight: 2  # 33% (2/6)
        - targetCluster:
            clusterNames:
            - cluster-3
          weight: 1  # 17% (1/6)
```

### 3.2 åŠ¨æ€è´Ÿè½½å‡è¡¡

**åŠ¨æ€æƒé‡è°ƒæ•´**ï¼š

```python
class DynamicLoadBalancer:
    def __init__(self, clusters):
        self.clusters = clusters
        self.weights = [1.0] * len(clusters)

    def update_weights(self):
        """
        æ ¹æ®é›†ç¾¤è´Ÿè½½åŠ¨æ€æ›´æ–°æƒé‡
        """
        for i, cluster in enumerate(self.clusters):
            # è®¡ç®—é›†ç¾¤è´Ÿè½½
            load = cluster.cpu_used / cluster.cpu_total

            # è´Ÿè½½è¶Šé«˜ï¼Œæƒé‡è¶Šä½
            self.weights[i] = max(0.1, 1.0 - load)

    def schedule(self, workload):
        """
        åŠ æƒéšæœºé€‰æ‹©é›†ç¾¤
        """
        self.update_weights()
        total_weight = sum(self.weights)
        probabilities = [w / total_weight for w in self.weights]

        return np.random.choice(self.clusters, p=probabilities)
```

### 3.3 è‡ªé€‚åº”å†å¹³è¡¡

**è‡ªåŠ¨å†å¹³è¡¡ç­–ç•¥**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: rebalance-policy
spec:
  placement:
    replicaScheduling:
      replicaSchedulingType: Divided
      replicaDivisionPreference: Aggregated
      reschedulePolicy:
        rescheduleOnPolicyChange: true
        rescheduleOnClusterChange: true
```

**å†å¹³è¡¡è§¦å‘æ¡ä»¶**ï¼š

1. **é›†ç¾¤èµ„æºå˜åŒ–**ï¼šé›†ç¾¤å¯ç”¨èµ„æºæ˜¾è‘—å˜åŒ–
2. **è´Ÿè½½ä¸å‡**ï¼šé›†ç¾¤é—´è´Ÿè½½å·®å¼‚è¶…è¿‡é˜ˆå€¼
3. **ç­–ç•¥å˜æ›´**ï¼šPropagationPolicyæ›´æ–°

---

## 4 æ•…éšœè½¬ç§»ä¸é«˜å¯ç”¨

### 4.1 é›†ç¾¤æ•…éšœæ£€æµ‹

**å¥åº·æ£€æŸ¥æœºåˆ¶**ï¼š

```python
class ClusterHealthChecker:
    def __init__(self, clusters, check_interval=30):
        self.clusters = clusters
        self.check_interval = check_interval
        self.health_status = {}

    def check_cluster_health(self, cluster):
        """
        æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
        """
        try:
            # 1. API Serverå¯è¾¾æ€§æ£€æŸ¥
            response = requests.get(f"{cluster.api_server}/healthz", timeout=5)
            api_health = response.status_code == 200

            # 2. èµ„æºå¯ç”¨æ€§æ£€æŸ¥
            resource_health = cluster.available_cpu > 0 and cluster.available_memory > 0

            # 3. èŠ‚ç‚¹å¥åº·æ£€æŸ¥
            nodes = cluster.get_nodes()
            ready_nodes = [n for n in nodes if n.status == "Ready"]
            node_health = len(ready_nodes) / len(nodes) > 0.5

            return api_health and resource_health and node_health
        except Exception as e:
            return False

    def monitor(self):
        """
        æŒç»­ç›‘æ§é›†ç¾¤å¥åº·
        """
        while True:
            for cluster in self.clusters:
                self.health_status[cluster.name] = self.check_cluster_health(cluster)

            time.sleep(self.check_interval)
```

### 4.2 è‡ªåŠ¨æ•…éšœè½¬ç§»

**æ•…éšœè½¬ç§»ç­–ç•¥**ï¼š

```python
class FailoverScheduler:
    def __init__(self, primary_cluster, backup_clusters):
        self.primary_cluster = primary_cluster
        self.backup_clusters = backup_clusters

    def schedule_with_failover(self, workload):
        """
        å¸¦æ•…éšœè½¬ç§»çš„è°ƒåº¦
        """
        # 1. å°è¯•ä¸»é›†ç¾¤
        if self.is_cluster_healthy(self.primary_cluster):
            return self.primary_cluster

        # 2. æ•…éšœè½¬ç§»åˆ°å¤‡ä»½é›†ç¾¤
        for backup in self.backup_clusters:
            if self.is_cluster_healthy(backup):
                self.log_failover(workload, self.primary_cluster, backup)
                return backup

        # 3. æ— å¯ç”¨é›†ç¾¤
        raise NoAvailableClusterError()

    def log_failover(self, workload, from_cluster, to_cluster):
        """
        è®°å½•æ•…éšœè½¬ç§»äº‹ä»¶
        """
        print(f"Failover: {workload.name} from {from_cluster.name} to {to_cluster.name}")
```

### 4.3 ç¾éš¾æ¢å¤

**å¤šåŒºåŸŸç¾éš¾æ¢å¤**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: disaster-recovery-policy
spec:
  resourceSelectors:
  - apiVersion: apps/v1
    kind: StatefulSet
    name: database
  placement:
    clusterAffinity:
      clusterNames:
      - us-west-1      # ä¸»åŒºåŸŸ
      - us-east-1      # ç¾å¤‡åŒºåŸŸ
    replicaScheduling:
      replicaSchedulingType: Duplicated  # å¤åˆ¶æ¨¡å¼
  failover:
    application:
      decisionConditions:
        tolerationSeconds: 300  # å®¹å¿5åˆ†é’Ÿæ•…éšœ
      purgeMode: Graciously     # ä¼˜é›…æ¸…ç†
      gracePeriodSeconds: 600   # 10åˆ†é’Ÿä¼˜é›…æœŸ
```

---

## 5 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 5.1 æˆæœ¬æ¨¡å‹

**å¤šäº‘æˆæœ¬æ¨¡å‹**ï¼š

$$
\text{TotalCost} = \sum_{c \in C} \left( \text{ComputeCost}_c + \text{StorageCost}_c + \text{NetworkCost}_c \right)
$$

å…¶ä¸­ï¼š

- **è®¡ç®—æˆæœ¬**ï¼š$\text{ComputeCost}_c = \text{Price}_{CPU} \cdot \text{CPU}_c \cdot t + \text{Price}_{Memory} \cdot \text{Memory}_c \cdot t$
- **å­˜å‚¨æˆæœ¬**ï¼š$\text{StorageCost}_c = \text{Price}_{Storage} \cdot \text{Storage}_c \cdot t$
- **ç½‘ç»œæˆæœ¬**ï¼š$\text{NetworkCost}_c = \text{Price}_{Egress} \cdot \text{DataTransfer}_c$

### 5.2 æˆæœ¬æ„ŸçŸ¥è°ƒåº¦

**æˆæœ¬ä¼˜åŒ–ç®—æ³•**ï¼š

```python
class CostAwareScheduler:
    def __init__(self, clusters, cost_model):
        self.clusters = clusters
        self.cost_model = cost_model

    def schedule(self, workload):
        """
        æˆæœ¬æ„ŸçŸ¥è°ƒåº¦
        """
        costs = []

        for cluster in self.clusters:
            # è®¡ç®—åœ¨è¯¥é›†ç¾¤è¿è¡Œçš„æˆæœ¬
            compute_cost = self.cost_model.compute_cost(cluster, workload)
            network_cost = self.cost_model.network_cost(cluster, workload)
            total_cost = compute_cost + network_cost

            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³SLO
            slo_satisfied = self.check_slo(cluster, workload)

            if slo_satisfied:
                costs.append((cluster, total_cost))

        # é€‰æ‹©æˆæœ¬æœ€ä½çš„é›†ç¾¤
        costs.sort(key=lambda x: x[1])
        return costs[0][0] if costs else None
```

### 5.3 Spotå®ä¾‹è°ƒåº¦

**Spotå®ä¾‹æˆæœ¬ä¼˜åŒ–**ï¼š

```yaml
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: spot-instance-policy
spec:
  resourceSelectors:
  - apiVersion: batch/v1
    kind: Job
    name: batch-processing
  placement:
    clusterAffinity:
      clusterNames:
      - spot-cluster-1   # Spotå®ä¾‹é›†ç¾¤
      - spot-cluster-2
      - on-demand-cluster  # æŒ‰éœ€å®ä¾‹é›†ç¾¤ï¼ˆå¤‡ä»½ï¼‰
    clusterPriorities:
    - clusterName: spot-cluster-1
      priority: 100  # é«˜ä¼˜å…ˆçº§ï¼ˆä½æˆæœ¬ï¼‰
    - clusterName: spot-cluster-2
      priority: 90
    - clusterName: on-demand-cluster
      priority: 50   # ä½ä¼˜å…ˆçº§ï¼ˆé«˜æˆæœ¬ï¼Œä½†å¯é ï¼‰
```

---

## 6 å½¢å¼åŒ–æ¨¡å‹

### 6.1 è°ƒåº¦ç›®æ ‡å‡½æ•°

**å¤šç›®æ ‡ä¼˜åŒ–**ï¼š

$$
\min_{x_{ij}} \sum_{i=1}^{n} \sum_{j=1}^{m} \left( w_1 \cdot \text{Cost}_{ij} + w_2 \cdot \text{Latency}_{ij} - w_3 \cdot \text{Availability}_{ij} \right) x_{ij}
$$

å…¶ä¸­ï¼š

- $x_{ij} \in \{0, 1\}$ï¼šå·¥ä½œè´Ÿè½½ $j$ æ˜¯å¦åˆ†é…åˆ°é›†ç¾¤ $i$
- $w_1, w_2, w_3$ï¼šæƒé‡ç³»æ•°
- $\text{Cost}_{ij}$ï¼šæˆæœ¬
- $\text{Latency}_{ij}$ï¼šå»¶è¿Ÿ
- $\text{Availability}_{ij}$ï¼šå¯ç”¨æ€§

### 6.2 çº¦æŸæ¡ä»¶

**èµ„æºçº¦æŸ**ï¼š

$$
\sum_{j=1}^{m} \text{Resource}_{j} \cdot x_{ij} \leq \text{Capacity}_i, \quad \forall i \in \{1, \ldots, n\}
$$

**å·¥ä½œè´Ÿè½½çº¦æŸ**ï¼š

$$
\sum_{i=1}^{n} x_{ij} \geq 1, \quad \forall j \in \{1, \ldots, m\}
$$

ï¼ˆæ¯ä¸ªå·¥ä½œè´Ÿè½½è‡³å°‘åˆ†é…åˆ°ä¸€ä¸ªé›†ç¾¤ï¼‰

**äº²å’Œæ€§çº¦æŸ**ï¼š

$$
x_{ij} = 0, \quad \text{if } c_i \notin \text{Affinity}(w_j)
$$

### 6.3 ä¼˜åŒ–ç®—æ³•

**é—ä¼ ç®—æ³•**ï¼š

```python
import random

class GeneticScheduler:
    def __init__(self, clusters, workloads, population_size=100, generations=1000):
        self.clusters = clusters
        self.workloads = workloads
        self.population_size = population_size
        self.generations = generations

    def fitness(self, chromosome):
        """
        è®¡ç®—é€‚åº”åº¦ï¼ˆç›®æ ‡å‡½æ•°ï¼‰
        """
        cost = 0
        latency = 0
        availability = 0

        for i, cluster_idx in enumerate(chromosome):
            cluster = self.clusters[cluster_idx]
            workload = self.workloads[i]

            cost += self.calculate_cost(cluster, workload)
            latency += self.calculate_latency(cluster, workload)
            availability += cluster.availability

        # å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆåŠ æƒï¼‰
        return -(0.4 * cost + 0.4 * latency - 0.2 * availability)

    def evolve(self):
        """
        é—ä¼ ç®—æ³•è¿›åŒ–
        """
        # åˆå§‹åŒ–ç§ç¾¤
        population = [self.random_chromosome() for _ in range(self.population_size)]

        for generation in range(self.generations):
            # è¯„ä¼°é€‚åº”åº¦
            fitness_scores = [(chromo, self.fitness(chromo)) for chromo in population]
            fitness_scores.sort(key=lambda x: x[1], reverse=True)

            # é€‰æ‹©
            parents = [x[0] for x in fitness_scores[:self.population_size // 2]]

            # äº¤å‰å’Œå˜å¼‚
            offspring = []
            while len(offspring) < self.population_size // 2:
                parent1, parent2 = random.sample(parents, 2)
                child = self.crossover(parent1, parent2)
                child = self.mutate(child)
                offspring.append(child)

            population = parents + offspring

        return fitness_scores[0][0]  # è¿”å›æœ€ä¼˜è§£
```

---

## 7 å®è·µæ¡ˆä¾‹

### 7.1 Karmadaè°ƒåº¦ç­–ç•¥å®è·µ

**åœºæ™¯ï¼šå…¨çƒç”µå•†åº”ç”¨**

```yaml
# 1. åœ°ç†ä½ç½®æ„ŸçŸ¥è°ƒåº¦
apiVersion: policy.karmada.io/v1alpha1
kind: PropagationPolicy
metadata:
  name: ecommerce-web-policy
spec:
  resourceSelectors:
  - apiVersion: apps/v1
    kind: Deployment
    name: web-frontend
  placement:
    clusterAffinity:
      clusterNames:
      - us-west-1
      - eu-west-1
      - ap-southeast-1
    spreadConstraints:
    - spreadByField: region
      maxSkew: 1
    replicaScheduling:
      replicaSchedulingType: Divided
      replicaDivisionPreference: Weighted
      weightPreference:
        dynamicWeight: AvailableReplicas  # åŠ¨æ€æƒé‡ï¼ˆæ ¹æ®å¯ç”¨å‰¯æœ¬æ•°ï¼‰
```

### 7.2 æˆæœ¬ä¼˜åŒ–å®è·µ

**åœºæ™¯ï¼šæ‰¹é‡æ•°æ®å¤„ç†**

```python
class CostOptimizedBatchScheduler:
    def __init__(self, clusters):
        self.clusters = clusters
        self.cost_model = {
            'on-demand': 0.10,  # $/hour
            'spot': 0.03,       # $/hour (70% discount)
            'reserved': 0.06    # $/hour (40% discount)
        }

    def schedule_batch_job(self, job):
        """
        æ‰¹é‡ä½œä¸šæˆæœ¬ä¼˜åŒ–è°ƒåº¦
        """
        # 1. ä¼˜å…ˆä½¿ç”¨Spotå®ä¾‹
        spot_clusters = [c for c in self.clusters if c.instance_type == 'spot']
        for cluster in spot_clusters:
            if cluster.has_capacity(job):
                return cluster

        # 2. ä½¿ç”¨Reservedå®ä¾‹
        reserved_clusters = [c for c in self.clusters if c.instance_type == 'reserved']
        for cluster in reserved_clusters:
            if cluster.has_capacity(job):
                return cluster

        # 3. æœ€åä½¿ç”¨On-Demandå®ä¾‹
        on_demand_clusters = [c for c in self.clusters if c.instance_type == 'on-demand']
        for cluster in on_demand_clusters:
            if cluster.has_capacity(job):
                return cluster

        return None

# ä½¿ç”¨ç¤ºä¾‹
scheduler = CostOptimizedBatchScheduler(clusters)
cluster = scheduler.schedule_batch_job(batch_job)
print(f"Scheduled to {cluster.name} (type: {cluster.instance_type}, cost: ${scheduler.cost_model[cluster.instance_type]}/hour)")
```

---

## 8 è·¨è§†è§’é“¾æ¥

**å½¢å¼è¯­è¨€è§†è§’**ï¼š

- [09.4 åº”ç”¨èŒƒç•´è®º](../../formal_lang_view/09_å½¢å¼åŒ–ç†è®º/09.4_åº”ç”¨èŒƒç•´è®º.md) - è°ƒåº¦ç­–ç•¥çš„èŒƒç•´è®ºè¡¨ç¤º

**è°ƒåº¦è§†è§’**ï¼š

- [06.4 åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒåº¦](../06_è°ƒåº¦æ¨¡å‹/06.4_åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒåº¦.md) - åˆ†å¸ƒå¼è°ƒåº¦ç†è®º
- [10 AIé©±åŠ¨è°ƒåº¦](../10_AIé©±åŠ¨è°ƒåº¦/README.md) - æœºå™¨å­¦ä¹ è°ƒåº¦ç­–ç•¥

**Petriç½‘è§†è§’**ï¼š

- [PetriNetView/05 æ€§èƒ½åˆ†æ](../PetriNetView/05_æ€§èƒ½åˆ†æçš„Petriç½‘æ–¹æ³•/README.md) - è°ƒåº¦æ€§èƒ½åˆ†æ

---

**è¿”å›**: [31 å¤šé›†ç¾¤è°ƒåº¦](README.md)
