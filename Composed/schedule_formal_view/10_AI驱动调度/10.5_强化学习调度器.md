# 10.5 å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨

> **å­ä¸»é¢˜ç¼–å·**: 10.5
> **ä¸»é¢˜**: AIé©±åŠ¨è°ƒåº¦
> **æœ€åæ›´æ–°**: 2025-12-02
> **æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ

---

## ğŸ“‹ ç›®å½•

- [10.5 å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨](#105-å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1 æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 æ ¸å¿ƒæ´å¯Ÿ](#11-æ ¸å¿ƒæ´å¯Ÿ)
    - [1.2 RLè°ƒåº¦ç‰¹æ€§](#12-rlè°ƒåº¦ç‰¹æ€§)
    - [1.3 å½¢å¼åŒ–å®šä¹‰](#13-å½¢å¼åŒ–å®šä¹‰)
  - [2 æ€ç»´å¯¼å›¾](#2-æ€ç»´å¯¼å›¾)
  - [3 RLè°ƒåº¦æ¡†æ¶](#3-rlè°ƒåº¦æ¡†æ¶)
    - [3.1 æ•´ä½“æ¶æ„](#31-æ•´ä½“æ¶æ„)
    - [3.2 è®­ç»ƒå¾ªç¯](#32-è®­ç»ƒå¾ªç¯)
  - [4 çŠ¶æ€ä¸åŠ¨ä½œè®¾è®¡](#4-çŠ¶æ€ä¸åŠ¨ä½œè®¾è®¡)
    - [4.1 çŠ¶æ€ç©ºé—´è®¾è®¡](#41-çŠ¶æ€ç©ºé—´è®¾è®¡)
    - [4.2 åŠ¨ä½œç©ºé—´è®¾è®¡](#42-åŠ¨ä½œç©ºé—´è®¾è®¡)
  - [5 å¥–åŠ±å‡½æ•°è®¾è®¡](#5-å¥–åŠ±å‡½æ•°è®¾è®¡)
    - [5.1 å¤šç›®æ ‡å¥–åŠ±](#51-å¤šç›®æ ‡å¥–åŠ±)
  - [6 è®­ç»ƒä¸éƒ¨ç½²](#6-è®­ç»ƒä¸éƒ¨ç½²)
    - [6.1 PPOè®­ç»ƒ](#61-ppoè®­ç»ƒ)
    - [6.2 ç”Ÿäº§éƒ¨ç½²](#62-ç”Ÿäº§éƒ¨ç½²)
  - [7 å®è·µæ¡ˆä¾‹](#7-å®è·µæ¡ˆä¾‹)
    - [7.1 Google Borg RL](#71-google-borg-rl)
    - [7.2 Alibaba PAI](#72-alibaba-pai)
  - [8 è·¨è§†è§’é“¾æ¥](#8-è·¨è§†è§’é“¾æ¥)
    - [8.1 è°ƒåº¦è§†è§’å…³è”](#81-è°ƒåº¦è§†è§’å…³è”)
    - [8.2 å½¢å¼è¯­è¨€è§†è§’å…³è”](#82-å½¢å¼è¯­è¨€è§†è§’å…³è”)

---

## 1 æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ´å¯Ÿ

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è°ƒåº¦å™¨é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜è°ƒåº¦ç­–ç•¥ã€‚
ç›¸æ¯”ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ï¼ŒRLè°ƒåº¦å™¨èƒ½å¤Ÿ**è‡ªé€‚åº”å¤æ‚åŠ¨æ€ç¯å¢ƒ**ï¼Œåœ¨é•¿æœŸç›®æ ‡ä¼˜åŒ–ä¸Šå±•ç°ä¼˜åŠ¿ã€‚

### 1.2 RLè°ƒåº¦ç‰¹æ€§

| ç‰¹æ€§ | æè¿° | è°ƒåº¦ä¼˜åŠ¿ |
|------|------|---------|
| **è‡ªé€‚åº”** | ä»ç»éªŒå­¦ä¹  | é€‚åº”è´Ÿè½½å˜åŒ– |
| **é•¿æœŸä¼˜åŒ–** | è€ƒè™‘æœªæ¥å¥–åŠ± | å…¨å±€æœ€ä¼˜ |
| **æ¢ç´¢åˆ©ç”¨** | å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ | å‘ç°æ–°ç­–ç•¥ |
| **ç«¯åˆ°ç«¯** | ç›´æ¥ä»çŠ¶æ€åˆ°åŠ¨ä½œ | å‡å°‘äººå·¥ç‰¹å¾ |
| **å¯è¿ç§»** | ç­–ç•¥å¯è¿ç§» | è·¨é›†ç¾¤éƒ¨ç½² |

### 1.3 å½¢å¼åŒ–å®šä¹‰

```text
RLè°ƒåº¦é—®é¢˜å»ºæ¨¡ä¸ºMDP: M = (S, A, P, R, Î³)

å…¶ä¸­ï¼š
  S: çŠ¶æ€ç©ºé—´ï¼ˆé›†ç¾¤çŠ¶æ€ã€ä½œä¸šé˜Ÿåˆ—ï¼‰
  A: åŠ¨ä½œç©ºé—´ï¼ˆè°ƒåº¦å†³ç­–ï¼‰
  P: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s, a)
  R: å¥–åŠ±å‡½æ•° R(s, a, s')
  Î³: æŠ˜æ‰£å› å­

ç›®æ ‡ï¼š
  æ‰¾åˆ°ç­–ç•¥ Ï€*: S â†’ A æœ€å¤§åŒ–æœŸæœ›ç´¯è®¡å¥–åŠ±
  Ï€* = argmax_Ï€ E[Î£_t Î³^t R(s_t, a_t, s_{t+1}) | Ï€]
```

---

## 2 æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((RLè°ƒåº¦å™¨))
    çŠ¶æ€è®¾è®¡
      é›†ç¾¤çŠ¶æ€
        èµ„æºåˆ©ç”¨ç‡
        é˜Ÿåˆ—é•¿åº¦
        èŠ‚ç‚¹å¥åº·
      ä½œä¸šçŠ¶æ€
        èµ„æºéœ€æ±‚
        ä¼˜å…ˆçº§
        ç­‰å¾…æ—¶é—´
    åŠ¨ä½œè®¾è®¡
      è°ƒåº¦åŠ¨ä½œ
        ä½œä¸šé€‰æ‹©
        èŠ‚ç‚¹é€‰æ‹©
        èµ„æºåˆ†é…
      æŠ¢å åŠ¨ä½œ
        é©±é€ç­–ç•¥
        è¿ç§»ç­–ç•¥
    å¥–åŠ±è®¾è®¡
      æ€§èƒ½æŒ‡æ ‡
        ååé‡
        å»¶è¿Ÿ
        å…¬å¹³æ€§
      èµ„æºæ•ˆç‡
        åˆ©ç”¨ç‡
        ç¢ç‰‡åŒ–
      SLAæ»¡è¶³
    ç®—æ³•é€‰æ‹©
      å€¼å‡½æ•°
        DQN
        Dueling DQN
      ç­–ç•¥æ¢¯åº¦
        A2C/A3C
        PPO
        SAC
```

---

## 3 RLè°ƒåº¦æ¡†æ¶

### 3.1 æ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph "ç¯å¢ƒ"
        CLUSTER[é›†ç¾¤çŠ¶æ€]
        QUEUE[ä½œä¸šé˜Ÿåˆ—]
        METRICS[æ€§èƒ½æŒ‡æ ‡]
    end

    subgraph "RL Agent"
        STATE[çŠ¶æ€ç¼–ç å™¨]
        POLICY[ç­–ç•¥ç½‘ç»œ]
        VALUE[ä»·å€¼ç½‘ç»œ]
    end

    subgraph "æ‰§è¡Œ"
        ACTION[è°ƒåº¦åŠ¨ä½œ]
        EXECUTOR[è°ƒåº¦æ‰§è¡Œå™¨]
    end

    CLUSTER --> STATE
    QUEUE --> STATE
    STATE --> POLICY
    STATE --> VALUE
    POLICY --> ACTION
    ACTION --> EXECUTOR
    EXECUTOR --> CLUSTER
    METRICS --> REWARD[å¥–åŠ±è®¡ç®—]
    REWARD --> POLICY
```

### 3.2 è®­ç»ƒå¾ªç¯

```python
# RLè°ƒåº¦å™¨è®­ç»ƒæ¡†æ¶
class RLSchedulerTrainer:
    def __init__(self, env, agent, config):
        self.env = env  # è°ƒåº¦ç¯å¢ƒ
        self.agent = agent  # RL agent
        self.config = config
        self.replay_buffer = ReplayBuffer(config.buffer_size)

    def train(self, num_episodes):
        """è®­ç»ƒä¸»å¾ªç¯"""
        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            done = False

            while not done:
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.select_action(state)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.env.step(action)

                # å­˜å‚¨ç»éªŒ
                self.replay_buffer.push(
                    state, action, reward, next_state, done
                )

                # æ›´æ–°ç­–ç•¥
                if len(self.replay_buffer) >= self.config.batch_size:
                    batch = self.replay_buffer.sample(self.config.batch_size)
                    loss = self.agent.update(batch)

                state = next_state
                episode_reward += reward

            # æ—¥å¿—
            self.log_episode(episode, episode_reward)

    def evaluate(self, num_episodes=10):
        """è¯„ä¼°ç­–ç•¥"""
        total_reward = 0
        metrics = defaultdict(list)

        for _ in range(num_episodes):
            state = self.env.reset()
            done = False

            while not done:
                action = self.agent.select_action(state, explore=False)
                next_state, reward, done, info = self.env.step(action)

                total_reward += reward
                for k, v in info['metrics'].items():
                    metrics[k].append(v)

                state = next_state

        return {
            'mean_reward': total_reward / num_episodes,
            'metrics': {k: np.mean(v) for k, v in metrics.items()}
        }
```

---

## 4 çŠ¶æ€ä¸åŠ¨ä½œè®¾è®¡

### 4.1 çŠ¶æ€ç©ºé—´è®¾è®¡

```python
# çŠ¶æ€ç©ºé—´å®šä¹‰
class SchedulerState:
    def __init__(self, cluster, queue):
        self.cluster = cluster
        self.queue = queue

    def encode(self):
        """ç¼–ç çŠ¶æ€ä¸ºå‘é‡"""
        features = []

        # é›†ç¾¤çº§ç‰¹å¾
        features.extend([
            self.cluster.cpu_utilization,
            self.cluster.memory_utilization,
            self.cluster.gpu_utilization,
            self.cluster.network_utilization,
            len(self.cluster.nodes),
            self.cluster.healthy_nodes_ratio,
        ])

        # èŠ‚ç‚¹çº§ç‰¹å¾ (èšåˆ)
        node_features = []
        for node in self.cluster.nodes:
            node_features.append([
                node.cpu_available,
                node.memory_available,
                node.gpu_available,
                len(node.running_pods),
                node.load_average,
            ])

        # èšåˆèŠ‚ç‚¹ç‰¹å¾
        node_features = np.array(node_features)
        features.extend(node_features.mean(axis=0))
        features.extend(node_features.std(axis=0))
        features.extend(node_features.min(axis=0))
        features.extend(node_features.max(axis=0))

        # é˜Ÿåˆ—ç‰¹å¾
        features.extend([
            len(self.queue),
            self.queue.total_cpu_requested,
            self.queue.total_memory_requested,
            self.queue.avg_wait_time,
            self.queue.max_wait_time,
        ])

        # ä½œä¸šç‰¹å¾ (top-k)
        for job in self.queue.top_k(10):
            features.extend([
                job.cpu_request,
                job.memory_request,
                job.gpu_request,
                job.priority,
                job.wait_time,
            ])

        return np.array(features, dtype=np.float32)

# å›¾ç¥ç»ç½‘ç»œçŠ¶æ€ç¼–ç 
class GNNStateEncoder(nn.Module):
    def __init__(self, node_dim, edge_dim, hidden_dim):
        super().__init__()
        self.node_encoder = nn.Linear(node_dim, hidden_dim)
        self.edge_encoder = nn.Linear(edge_dim, hidden_dim)
        self.gnn_layers = nn.ModuleList([
            GATConv(hidden_dim, hidden_dim) for _ in range(3)
        ])
        self.readout = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, node_features, edge_index, edge_features):
        """å›¾ç¥ç»ç½‘ç»œç¼–ç """
        x = self.node_encoder(node_features)

        for gnn in self.gnn_layers:
            x = F.relu(gnn(x, edge_index))

        # å…¨å±€æ± åŒ–
        graph_embedding = x.mean(dim=0)

        return self.readout(graph_embedding)
```

### 4.2 åŠ¨ä½œç©ºé—´è®¾è®¡

```python
# åŠ¨ä½œç©ºé—´å®šä¹‰
class SchedulerActionSpace:
    def __init__(self, config):
        self.config = config

    def get_actions(self, state):
        """è·å–å¯ç”¨åŠ¨ä½œ"""
        actions = []

        # å¯¹æ¯ä¸ªå¾…è°ƒåº¦ä½œä¸š
        for job_idx, job in enumerate(state.queue):
            # å¯¹æ¯ä¸ªå¯ç”¨èŠ‚ç‚¹
            for node_idx, node in enumerate(state.cluster.nodes):
                if self._can_schedule(job, node):
                    actions.append(ScheduleAction(
                        job_idx=job_idx,
                        node_idx=node_idx,
                        job_id=job.id,
                        node_id=node.id
                    ))

        # æ·»åŠ "ä¸è°ƒåº¦"åŠ¨ä½œ
        actions.append(NoOpAction())

        return actions

    def _can_schedule(self, job, node):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥è°ƒåº¦"""
        return (
            node.cpu_available >= job.cpu_request and
            node.memory_available >= job.memory_request and
            node.gpu_available >= job.gpu_request and
            node.is_healthy and
            self._check_affinity(job, node) and
            self._check_taints(job, node)
        )

# å‚æ•°åŒ–åŠ¨ä½œç©ºé—´
class ParameterizedActionSpace:
    """å‚æ•°åŒ–åŠ¨ä½œï¼šè¿ç»­é€‰æ‹©ä½œä¸šå’ŒèŠ‚ç‚¹"""

    def __init__(self, num_jobs_embed, num_nodes_embed):
        self.job_selector = nn.Linear(num_jobs_embed, 1)
        self.node_selector = nn.Linear(num_nodes_embed, 1)

    def select_action(self, job_embeddings, node_embeddings):
        """é€‰æ‹©ä½œä¸šå’ŒèŠ‚ç‚¹"""
        # ä½œä¸šé€‰æ‹©æ¦‚ç‡
        job_scores = self.job_selector(job_embeddings).squeeze(-1)
        job_probs = F.softmax(job_scores, dim=-1)
        job_idx = torch.multinomial(job_probs, 1)

        # èŠ‚ç‚¹é€‰æ‹©æ¦‚ç‡
        node_scores = self.node_selector(node_embeddings).squeeze(-1)
        node_probs = F.softmax(node_scores, dim=-1)
        node_idx = torch.multinomial(node_probs, 1)

        return job_idx, node_idx, job_probs, node_probs
```

---

## 5 å¥–åŠ±å‡½æ•°è®¾è®¡

### 5.1 å¤šç›®æ ‡å¥–åŠ±

```python
# å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
class SchedulerReward:
    def __init__(self, weights):
        self.weights = weights

    def compute(self, state, action, next_state, info):
        """è®¡ç®—ç»¼åˆå¥–åŠ±"""
        rewards = {}

        # èµ„æºåˆ©ç”¨ç‡å¥–åŠ±
        rewards['utilization'] = self._utilization_reward(next_state)

        # ä½œä¸šå®Œæˆå¥–åŠ±
        rewards['completion'] = self._completion_reward(info)

        # ç­‰å¾…æ—¶é—´æƒ©ç½š
        rewards['wait_time'] = self._wait_time_penalty(next_state)

        # ç¢ç‰‡åŒ–æƒ©ç½š
        rewards['fragmentation'] = self._fragmentation_penalty(next_state)

        # SLAæ»¡è¶³å¥–åŠ±
        rewards['sla'] = self._sla_reward(info)

        # å…¬å¹³æ€§å¥–åŠ±
        rewards['fairness'] = self._fairness_reward(next_state)

        # åŠ æƒæ±‚å’Œ
        total_reward = sum(
            self.weights.get(k, 1.0) * v
            for k, v in rewards.items()
        )

        return total_reward, rewards

    def _utilization_reward(self, state):
        """èµ„æºåˆ©ç”¨ç‡å¥–åŠ±"""
        util = state.cluster.overall_utilization
        # ç›®æ ‡åˆ©ç”¨ç‡70-80%
        if 0.7 <= util <= 0.8:
            return 1.0
        elif util < 0.7:
            return util / 0.7
        else:
            return max(0, 1 - (util - 0.8) * 5)  # è¿‡è½½æƒ©ç½š

    def _completion_reward(self, info):
        """ä½œä¸šå®Œæˆå¥–åŠ±"""
        if info.get('job_completed'):
            job = info['job']
            # æå‰å®Œæˆé¢å¤–å¥–åŠ±
            bonus = max(0, (job.deadline - job.completion_time) / job.deadline)
            return 1.0 + 0.5 * bonus
        return 0

    def _wait_time_penalty(self, state):
        """ç­‰å¾…æ—¶é—´æƒ©ç½š"""
        avg_wait = state.queue.avg_wait_time
        max_wait = state.queue.max_wait_time

        # å½’ä¸€åŒ–æƒ©ç½š
        return -0.1 * (avg_wait / 60) - 0.2 * (max_wait / 300)

    def _fragmentation_penalty(self, state):
        """èµ„æºç¢ç‰‡åŒ–æƒ©ç½š"""
        frag_score = 0
        for node in state.cluster.nodes:
            # è®¡ç®—ç¢ç‰‡åŒ–ç¨‹åº¦
            cpu_frag = node.cpu_available / node.cpu_total
            mem_frag = node.memory_available / node.memory_total

            # ä¸å‡è¡¡çš„ç¢ç‰‡æ›´ä¸¥é‡
            frag_score += abs(cpu_frag - mem_frag)

        return -0.1 * frag_score / len(state.cluster.nodes)

    def _sla_reward(self, info):
        """SLAæ»¡è¶³å¥–åŠ±"""
        if info.get('job_completed'):
            job = info['job']
            if job.completion_time <= job.deadline:
                return 0.5
            else:
                # SLAè¿åæƒ©ç½š
                violation = (job.completion_time - job.deadline) / job.deadline
                return -violation
        return 0

    def _fairness_reward(self, state):
        """å…¬å¹³æ€§å¥–åŠ± (Jain's fairness index)"""
        user_allocations = defaultdict(float)
        user_requests = defaultdict(float)

        for pod in state.cluster.all_pods:
            user = pod.owner
            user_allocations[user] += pod.resources

        for job in state.queue:
            user = job.owner
            user_requests[user] += job.resources

        if not user_allocations:
            return 0

        # è®¡ç®—å…¬å¹³æ€§æŒ‡æ•°
        shares = list(user_allocations.values())
        n = len(shares)
        jain = (sum(shares) ** 2) / (n * sum(s**2 for s in shares))

        return 0.5 * (jain - 0.5)  # å½’ä¸€åŒ–åˆ°[-0.25, 0.25]
```

---

## 6 è®­ç»ƒä¸éƒ¨ç½²

### 6.1 PPOè®­ç»ƒ

```python
# PPOè°ƒåº¦å™¨å®ç°
class PPOScheduler(nn.Module):
    def __init__(self, state_dim, action_dim, config):
        super().__init__()

        # å…±äº«ç‰¹å¾æå–
        self.feature = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
        )

        # Actor (ç­–ç•¥ç½‘ç»œ)
        self.actor = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )

        # Critic (ä»·å€¼ç½‘ç»œ)
        self.critic = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

        self.config = config

    def forward(self, state):
        features = self.feature(state)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def update(self, rollouts):
        """PPOæ›´æ–°"""
        states = rollouts['states']
        actions = rollouts['actions']
        old_log_probs = rollouts['log_probs']
        returns = rollouts['returns']
        advantages = rollouts['advantages']

        for _ in range(self.config.ppo_epochs):
            # è·å–å½“å‰ç­–ç•¥
            action_probs, values = self(states)
            dist = Categorical(action_probs)
            new_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()

            # PPOæ¯”ç‡
            ratio = torch.exp(new_log_probs - old_log_probs)

            # Clipped surrogate
            surr1 = ratio * advantages
            surr2 = torch.clamp(
                ratio,
                1 - self.config.clip_epsilon,
                1 + self.config.clip_epsilon
            ) * advantages

            # æŸå¤±å‡½æ•°
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = F.mse_loss(values.squeeze(), returns)
            entropy_loss = -entropy

            loss = (
                actor_loss +
                self.config.value_coef * critic_loss +
                self.config.entropy_coef * entropy_loss
            )

            # æ›´æ–°
            self.optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(self.parameters(), self.config.max_grad_norm)
            self.optimizer.step()

        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item()
        }
```

### 6.2 ç”Ÿäº§éƒ¨ç½²

```python
# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
class ProductionRLScheduler:
    def __init__(self, model_path, config):
        self.model = self._load_model(model_path)
        self.config = config
        self.fallback_scheduler = DefaultScheduler()

        # ç›‘æ§
        self.metrics = SchedulerMetrics()
        self.anomaly_detector = AnomalyDetector()

    def schedule(self, state):
        """ç”Ÿäº§è°ƒåº¦å†³ç­–"""
        try:
            # æ£€æŸ¥æ¨¡å‹å¥åº·
            if not self._is_model_healthy():
                return self.fallback_scheduler.schedule(state)

            # RLå†³ç­–
            with torch.no_grad():
                state_tensor = self._encode_state(state)
                action_probs, value = self.model(state_tensor)

                # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨greedy
                action = action_probs.argmax()

            # éªŒè¯å†³ç­–
            if not self._validate_action(action, state):
                return self.fallback_scheduler.schedule(state)

            # è®°å½•æŒ‡æ ‡
            self.metrics.record_decision(state, action, value)

            return self._decode_action(action, state)

        except Exception as e:
            self.metrics.record_error(e)
            return self.fallback_scheduler.schedule(state)

    def _is_model_healthy(self):
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€"""
        # æ£€æŸ¥æœ€è¿‘å†³ç­–è´¨é‡
        recent_metrics = self.metrics.get_recent(window=100)

        if recent_metrics['error_rate'] > 0.05:
            return False

        if recent_metrics['avg_reward'] < self.config.min_reward_threshold:
            return False

        return True

    def _validate_action(self, action, state):
        """éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§"""
        if action.type == 'schedule':
            job = state.queue[action.job_idx]
            node = state.cluster.nodes[action.node_idx]

            # èµ„æºæ£€æŸ¥
            if not node.has_resources(job.resources):
                return False

            # çº¦æŸæ£€æŸ¥
            if not self._check_constraints(job, node):
                return False

        return True
```

---

## 7 å®è·µæ¡ˆä¾‹

### 7.1 Google Borg RL

```text
Google Borgå¼ºåŒ–å­¦ä¹ è°ƒåº¦:
- ç›®æ ‡: å‡å°‘ä½œä¸šç­‰å¾…æ—¶é—´
- çŠ¶æ€: é›†ç¾¤èµ„æºã€ä½œä¸šé˜Ÿåˆ—
- åŠ¨ä½œ: ä½œä¸š-æœºå™¨åˆ†é…
- å¥–åŠ±: è´Ÿç­‰å¾…æ—¶é—´

ç»“æœ:
- ç­‰å¾…æ—¶é—´å‡å°‘20%+
- èµ„æºåˆ©ç”¨ç‡æå‡
- åœ¨çº¿å­¦ä¹ æŒç»­ä¼˜åŒ–
```

### 7.2 Alibaba PAI

```text
é˜¿é‡ŒPAI RLè°ƒåº¦å™¨:
- åœºæ™¯: æ·±åº¦å­¦ä¹ è®­ç»ƒä½œä¸š
- æŒ‘æˆ˜: GPUèµ„æºç¨€ç¼º
- æ–¹æ³•: Multi-agent RL

ç‰¹ç‚¹:
- åˆ†å¸ƒå¼è®­ç»ƒæ„ŸçŸ¥
- ç½‘ç»œæ‹“æ‰‘æ„ŸçŸ¥
- å…¬å¹³æ€§ä¿è¯
```

---

## 8 è·¨è§†è§’é“¾æ¥

### 8.1 è°ƒåº¦è§†è§’å…³è”

| ç›¸å…³ä¸»é¢˜ | å…³è”å†…å®¹ | é“¾æ¥ |
|---------|---------|------|
| è°ƒåº¦æ¨¡å‹ | åŸºç¡€è°ƒåº¦ç®—æ³• | [06_è°ƒåº¦æ¨¡å‹](../06_è°ƒåº¦æ¨¡å‹/) |
| GPUè°ƒåº¦ | å¼‚æ„èµ„æº | [16_GPUä¸åŠ é€Ÿå™¨è°ƒåº¦](../16_GPUä¸åŠ é€Ÿå™¨è°ƒåº¦/) |
| LLMè°ƒåº¦ | æ¨ç†è°ƒåº¦ | [25_LLMæ¨ç†è°ƒåº¦](../25_LLMæ¨ç†è°ƒåº¦/) |

### 8.2 å½¢å¼è¯­è¨€è§†è§’å…³è”

| å½¢å¼è¯­è¨€æ¦‚å¿µ | RLè°ƒåº¦å¯¹åº” | æ˜ å°„è¯´æ˜ |
|------------|------------|---------|
| **ç±»å‹å®‰å…¨** | åŠ¨ä½œéªŒè¯ | çº¦æŸæ£€æŸ¥ |
| **æ•ˆåº”ç³»ç»Ÿ** | çŠ¶æ€è½¬ç§» | å‰¯ä½œç”¨è¿½è¸ª |
| **æ³›å‹** | ç­–ç•¥å‚æ•°åŒ– | å¯è¿ç§»ç­–ç•¥ |

---

**è¿”å›**: [AIé©±åŠ¨è°ƒåº¦ä¸»ç´¢å¼•](./README.md) | [è°ƒåº¦è§†è§’ä¸»ç´¢å¼•](../README.md)
