# 25.1 Transformeræ¨ç†è°ƒåº¦

> **å­ä¸»é¢˜ç¼–å·**: 25.1
> **ä¸»é¢˜**: LLMæ¨ç†è°ƒåº¦
> **æœ€åæ›´æ–°**: 2025-12-02
> **æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ

---

## ğŸ“‹ ç›®å½•

- [1 æ¦‚è¿°](#1-æ¦‚è¿°)
- [2 æ€ç»´å¯¼å›¾](#2-æ€ç»´å¯¼å›¾)
- [3 Transformeræ¨ç†é˜¶æ®µ](#3-transformeræ¨ç†é˜¶æ®µ)
- [4 æ³¨æ„åŠ›æœºåˆ¶è°ƒåº¦](#4-æ³¨æ„åŠ›æœºåˆ¶è°ƒåº¦)
- [5 é¢„å¡«å……ä¸è§£ç åˆ†ç¦»](#5-é¢„å¡«å……ä¸è§£ç åˆ†ç¦»)
- [6 çŸ¥è¯†çŸ©é˜µ](#6-çŸ¥è¯†çŸ©é˜µ)
- [7 å½¢å¼åŒ–æ¨¡å‹](#7-å½¢å¼åŒ–æ¨¡å‹)
- [8 ä¼˜åŒ–ç­–ç•¥](#8-ä¼˜åŒ–ç­–ç•¥)
- [9 è·¨è§†è§’é“¾æ¥](#9-è·¨è§†è§’é“¾æ¥)

---

## 1 æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ´å¯Ÿ

Transformeræ¨ç†è°ƒåº¦çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºè‡ªå›å½’ç”Ÿæˆçš„ç‰¹æ€§ï¼šæ¯ä¸ªæ–°Tokençš„ç”Ÿæˆéƒ½ä¾èµ–äºä¹‹å‰æ‰€æœ‰Tokençš„æ³¨æ„åŠ›è®¡ç®—ï¼Œè¿™å¯¼è‡´äº†ç‹¬ç‰¹çš„è°ƒåº¦æ¨¡å¼ã€‚

### 1.2 æ¨ç†ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | é¢„å¡«å……é˜¶æ®µ (Prefill) | è§£ç é˜¶æ®µ (Decode) |
|------|---------------------|------------------|
| **è®¡ç®—æ¨¡å¼** | è®¡ç®—å¯†é›†å‹ | å†…å­˜å¯†é›†å‹ |
| **æ‰¹å¤„ç†** | é«˜æ•ˆ | å—KV-Cacheé™åˆ¶ |
| **GPUåˆ©ç”¨ç‡** | é«˜ | é€šå¸¸è¾ƒä½ |
| **å»¶è¿Ÿç»„æˆ** | é¦–Tokenå»¶è¿Ÿ | Tokené—´å»¶è¿Ÿ |
| **å¹¶è¡Œåº¦** | é«˜ï¼ˆTokené—´å¹¶è¡Œï¼‰ | ä½ï¼ˆåºåˆ—ä¾èµ–ï¼‰ |

---

## 2 æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((Transformeræ¨ç†è°ƒåº¦))
    æ¨ç†é˜¶æ®µ
      é¢„å¡«å……
        è¾“å…¥å¤„ç†
        KV-Cacheåˆå§‹åŒ–
        é¦–Tokenç”Ÿæˆ
      è§£ç 
        è‡ªå›å½’ç”Ÿæˆ
        KV-Cacheæ›´æ–°
        é€Tokenè¾“å‡º
    æ³¨æ„åŠ›è°ƒåº¦
      è‡ªæ³¨æ„åŠ›
        Q/K/VæŠ•å½±
        æ³¨æ„åŠ›åˆ†æ•°
        è¾“å‡ºæŠ•å½±
      Flash Attention
        åˆ†å—è®¡ç®—
        å†…å­˜ä¼˜åŒ–
        IOæ„ŸçŸ¥
    è°ƒåº¦ç­–ç•¥
      é˜¶æ®µåˆ†ç¦»
        ç‹¬ç«‹è°ƒåº¦
        èµ„æºéš”ç¦»
      æ··åˆè°ƒåº¦
        åŠ¨æ€åˆ‡æ¢
        è´Ÿè½½å‡è¡¡
    æ€§èƒ½ä¼˜åŒ–
      è®¡ç®—ä¼˜åŒ–
        ç®—å­èåˆ
        é‡åŒ–æ¨ç†
      å†…å­˜ä¼˜åŒ–
        KV-Cacheå‹ç¼©
        ç¨€ç–æ³¨æ„åŠ›
```

---

## 3 Transformeræ¨ç†é˜¶æ®µ

### 3.1 é˜¶æ®µè¯¦è§£

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·è¯·æ±‚
    participant P as é¢„å¡«å……é˜¶æ®µ
    participant D as è§£ç é˜¶æ®µ
    participant O as è¾“å‡º

    U->>P: è¾“å…¥Prompt
    Note over P: å¹¶è¡Œå¤„ç†æ‰€æœ‰è¾“å…¥Token
    P->>P: è®¡ç®—Q/K/V
    P->>P: åˆå§‹åŒ–KV-Cache
    P->>D: é¦–Token + KV-Cache

    loop è‡ªå›å½’ç”Ÿæˆ
        D->>D: ç”Ÿæˆä¸‹ä¸€Token
        D->>D: æ›´æ–°KV-Cache
        D->>O: è¾“å‡ºToken
    end

    D->>O: EOS/æœ€å¤§é•¿åº¦
```

### 3.2 è®¡ç®—å¤æ‚åº¦åˆ†æ

```text
é¢„å¡«å……é˜¶æ®µ (è¾“å…¥é•¿åº¦ n):
  æ—¶é—´å¤æ‚åº¦: O(nÂ² Â· d)  [æ³¨æ„åŠ›è®¡ç®—]
  ç©ºé—´å¤æ‚åº¦: O(n Â· d)   [KV-Cache]

è§£ç é˜¶æ®µ (ç”Ÿæˆ m ä¸ªToken):
  æ¯æ­¥æ—¶é—´: O(n Â· d)     [ä¸å†å²æ³¨æ„åŠ›]
  æ€»æ—¶é—´: O(m Â· n Â· d)
  ç©ºé—´å¢é•¿: O(m Â· d)     [KV-Cacheå¢é‡]

å…¶ä¸­ d = æ¨¡å‹éšè—ç»´åº¦
```

### 3.3 é˜¶æ®µèµ„æºéœ€æ±‚å¯¹æ¯”

```mermaid
graph LR
    subgraph "é¢„å¡«å……é˜¶æ®µ"
        P1[é«˜è®¡ç®—éœ€æ±‚]
        P2[GPUè®¡ç®—å•å…ƒ]
        P3[ä¸€æ¬¡æ€§å®Œæˆ]
    end

    subgraph "è§£ç é˜¶æ®µ"
        D1[é«˜å†…å­˜å¸¦å®½éœ€æ±‚]
        D2[GPUå†…å­˜]
        D3[æŒç»­è¿›è¡Œ]
    end

    P1 --> P2
    D1 --> D2

    style P1 fill:#ffcccc
    style D1 fill:#ccccff
```

---

## 4 æ³¨æ„åŠ›æœºåˆ¶è°ƒåº¦

### 4.1 æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—

```text
æ ‡å‡†è‡ªæ³¨æ„åŠ›:
  Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Â· V

è°ƒåº¦æ­¥éª¤:
  1. Q, K, V çº¿æ€§æŠ•å½± (å¹¶è¡Œ)
  2. æ³¨æ„åŠ›åˆ†æ•°è®¡ç®— (O(nÂ²))
  3. Softmaxå½’ä¸€åŒ–
  4. åŠ æƒæ±‚å’Œ
  5. è¾“å‡ºæŠ•å½±
```

### 4.2 Flash Attentionä¼˜åŒ–

Flash Attentioné€šè¿‡åˆ†å—è®¡ç®—å’ŒIOæ„ŸçŸ¥ä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜è®¿é—®ï¼š

```mermaid
graph TB
    subgraph "æ ‡å‡†æ³¨æ„åŠ›"
        S1[å®Œæ•´QK^TçŸ©é˜µ]
        S2[O(nÂ²) HBMè¯»å†™]
        S3[å†…å­˜ç“¶é¢ˆ]
    end

    subgraph "Flash Attention"
        F1[åˆ†å—è®¡ç®—]
        F2[SRAMå¤ç”¨]
        F3[åœ¨çº¿Softmax]
        F4[å•æ¬¡HBMå†™å…¥]
    end

    S1 --> S2 --> S3
    F1 --> F2 --> F3 --> F4

    style S3 fill:#ffcccc
    style F4 fill:#ccffcc
```

### 4.3 Flash Attentionç®—æ³•

```python
# Flash Attention ä¼ªä»£ç 
def flash_attention(Q, K, V, block_size):
    """
    åˆ†å—è®¡ç®—æ³¨æ„åŠ›ï¼Œå‡å°‘HBMè®¿é—®
    """
    N = Q.shape[0]
    output = zeros_like(Q)

    for i in range(0, N, block_size):
        # åŠ è½½Qå—åˆ°SRAM
        Q_block = Q[i:i+block_size]

        # åœ¨çº¿SoftmaxçŠ¶æ€
        m_i = -inf  # æœ€å¤§å€¼
        l_i = 0     # å½’ä¸€åŒ–å› å­
        O_i = 0     # ç´¯ç§¯è¾“å‡º

        for j in range(0, N, block_size):
            # åŠ è½½K, Vå—åˆ°SRAM
            K_block = K[j:j+block_size]
            V_block = V[j:j+block_size]

            # è®¡ç®—å±€éƒ¨æ³¨æ„åŠ›åˆ†æ•°
            S_ij = Q_block @ K_block.T / sqrt(d)

            # åœ¨çº¿Softmaxæ›´æ–°
            m_new = max(m_i, S_ij.max())
            l_new = exp(m_i - m_new) * l_i + exp(S_ij - m_new).sum()
            O_i = exp(m_i - m_new) * O_i + exp(S_ij - m_new) @ V_block

            m_i, l_i = m_new, l_new

        output[i:i+block_size] = O_i / l_i

    return output
```

---

## 5 é¢„å¡«å……ä¸è§£ç åˆ†ç¦»

### 5.1 åˆ†ç¦»è°ƒåº¦æ¶æ„

```mermaid
graph TB
    subgraph "è¯·æ±‚é˜Ÿåˆ—"
        RQ[æ–°è¯·æ±‚é˜Ÿåˆ—]
    end

    subgraph "é¢„å¡«å……å®ä¾‹"
        PI[é¢„å¡«å……GPUæ± ]
        PC[é¢„å¡«å……è®¡ç®—]
    end

    subgraph "è§£ç å®ä¾‹"
        DI[è§£ç GPUæ± ]
        DC[è§£ç è®¡ç®—]
    end

    subgraph "KV-Cacheä¼ è¾“"
        KV[KV-Cacheè¿ç§»]
    end

    RQ --> PI
    PI --> PC
    PC --> KV
    KV --> DI
    DI --> DC
    DC -->|å®Œæˆ| OUT[è¾“å‡º]
    DC -->|ç»§ç»­| DC
```

### 5.2 Splitwise/DistServeæ¶æ„

```text
åˆ†ç¦»å¼éƒ¨ç½²ä¼˜åŠ¿:
  1. èµ„æºä¸“é—¨åŒ–: é¢„å¡«å……GPUå¯é€‰æ‹©è®¡ç®—ä¼˜åŒ–å‹ï¼Œè§£ç GPUé€‰æ‹©å†…å­˜ä¼˜åŒ–å‹
  2. ç‹¬ç«‹æ‰©å±•: æ ¹æ®è´Ÿè½½ç‹¬ç«‹æ‰©å±•é¢„å¡«å……æˆ–è§£ç å®ä¾‹
  3. SLOä¼˜åŒ–: åˆ†åˆ«ä¼˜åŒ–é¦–Tokenå»¶è¿Ÿå’Œååé‡

è°ƒåº¦ç­–ç•¥:
  é¢„å¡«å……è°ƒåº¦: FCFSæˆ–ä¼˜å…ˆçº§é˜Ÿåˆ—
  è§£ç è°ƒåº¦: æ‰¹æ¬¡åˆå¹¶ï¼ŒContinuous Batching
  KV-Cache: é«˜é€Ÿäº’è”ä¼ è¾“ (NVLink/InfiniBand)
```

### 5.3 åˆ†å—é¢„å¡«å…… (Chunked Prefill)

```mermaid
graph LR
    subgraph "ä¼ ç»Ÿé¢„å¡«å……"
        T1[é•¿Promptä¸€æ¬¡å¤„ç†]
        T2[é˜»å¡è§£ç è¯·æ±‚]
        T3[å»¶è¿Ÿæ³¢åŠ¨å¤§]
    end

    subgraph "åˆ†å—é¢„å¡«å……"
        C1[Promptåˆ†å—]
        C2[ä¸è§£ç äº¤é”™]
        C3[å»¶è¿Ÿç¨³å®š]
    end

    T1 --> T2 --> T3
    C1 --> C2 --> C3
```

**åˆ†å—é¢„å¡«å……è°ƒåº¦ç®—æ³•**:

```python
def chunked_prefill_schedule(prefill_requests, decode_batch, chunk_size):
    """
    åˆ†å—é¢„å¡«å……è°ƒåº¦ï¼Œä¸è§£ç è¯·æ±‚äº¤é”™æ‰§è¡Œ
    """
    schedule = []

    while prefill_requests or decode_batch:
        # å¤„ç†ä¸€ä¸ªé¢„å¡«å……å—
        if prefill_requests:
            req = prefill_requests[0]
            chunk = req.get_next_chunk(chunk_size)
            schedule.append(('prefill', chunk))

            if req.is_complete():
                prefill_requests.pop(0)
                decode_batch.append(req)

        # å¤„ç†è§£ç æ‰¹æ¬¡
        if decode_batch:
            schedule.append(('decode', decode_batch))

            # ç§»é™¤å®Œæˆçš„è¯·æ±‚
            decode_batch = [r for r in decode_batch if not r.is_done()]

    return schedule
```

---

## 6 çŸ¥è¯†çŸ©é˜µ

### 6.1 æ¨ç†ä¼˜åŒ–æŠ€æœ¯çŸ©é˜µ

| æŠ€æœ¯ | ä¼˜åŒ–ç›®æ ‡ | é€‚ç”¨é˜¶æ®µ | åŠ é€Ÿæ¯” | å†…å­˜èŠ‚çœ |
|------|---------|---------|-------|---------|
| **Flash Attention** | è®¡ç®—+å†…å­˜ | ä¸¤é˜¶æ®µ | 2-4x | 10-20x |
| **Flash Attention 2** | è®¡ç®— | ä¸¤é˜¶æ®µ | 2x vs FA1 | - |
| **PagedAttention** | å†…å­˜ | è§£ç  | 1.5x | 2-4x |
| **åˆ†å—é¢„å¡«å……** | å»¶è¿Ÿ | é¢„å¡«å…… | - | - |
| **æ¨æµ‹è§£ç ** | å»¶è¿Ÿ | è§£ç  | 2-3x | - |
| **é‡åŒ–æ¨ç†** | è®¡ç®—+å†…å­˜ | ä¸¤é˜¶æ®µ | 1.5-2x | 2-4x |
| **ç¨€ç–æ³¨æ„åŠ›** | è®¡ç®— | ä¸¤é˜¶æ®µ | 2-4x | 2-4x |

### 6.2 ç³»ç»Ÿå¯¹æ¯”çŸ©é˜µ

| ç³»ç»Ÿ | é¢„å¡«å……ä¼˜åŒ– | è§£ç ä¼˜åŒ– | æ‰¹å¤„ç† | åˆ†å¸ƒå¼ | ç‰¹è‰² |
|------|-----------|---------|-------|-------|------|
| **vLLM** | Flash Attn | PagedAttn | Continuous | âœ… | å†…å­˜æ•ˆç‡ |
| **TensorRT-LLM** | CUDAä¼˜åŒ– | é‡åŒ–æ¨ç† | In-flight | âœ… | ä½å»¶è¿Ÿ |
| **TGI** | Flash Attn | æ ‡å‡† | Continuous | âœ… | æ˜“éƒ¨ç½² |
| **SGLang** | RadixAttn | RadixAttn | Continuous | âœ… | å‰ç¼€å…±äº« |
| **DeepSpeed** | Flash Attn | ZeRO | é™æ€ | âœ… | å¤§æ¨¡å‹ |

---

## 7 å½¢å¼åŒ–æ¨¡å‹

### 7.1 æ¨ç†è°ƒåº¦å½¢å¼åŒ–

```text
å®šä¹‰: Transformeræ¨ç†è°ƒåº¦ç³»ç»Ÿ

ç»“æ„ TransformerScheduler:
  State = {
    prefill_queue: Queue<Request>,
    decode_batch: Set<Request>,
    kv_cache: Map<RequestId, KVCache>,
    gpu_memory: MemoryPool
  }

  æ¨ç†é˜¶æ®µç±»å‹ Phase = Prefill | Decode

  è°ƒåº¦å‡½æ•° schedule: State â†’ Action
  å…¶ä¸­ Action =
    | RunPrefill(requests: List<Request>)
    | RunDecode(batch: Set<Request>)
    | Idle

çº¦æŸæ¡ä»¶:
  âˆ€r âˆˆ decode_batch: kv_cache[r.id] â‰  âˆ…
  Î£(kv_cache[r].size for r in decode_batch) â‰¤ gpu_memory.capacity
```

### 7.2 å»¶è¿Ÿæ¨¡å‹

```text
æ€»å»¶è¿Ÿåˆ†è§£:

TTFT (Time To First Token):
  T_TTFT = T_queue + T_prefill
  T_prefill = f(input_length, batch_size, model_size)

TPOT (Time Per Output Token):
  T_TPOT = T_decode_step
  T_decode_step = g(batch_size, seq_length, memory_bandwidth)

ç«¯åˆ°ç«¯å»¶è¿Ÿ:
  T_total = T_TTFT + (output_length - 1) Ã— T_TPOT
```

### 7.3 ååé‡æ¨¡å‹

```text
ååé‡è®¡ç®—:

æ‰¹å¤„ç†ååé‡:
  Throughput = batch_size Ã— tokens_per_second

GPUåˆ©ç”¨ç‡çº¦æŸ:
  Utilization = min(compute_bound, memory_bound)

  compute_bound = FLOPs_required / GPU_FLOPs
  memory_bound = memory_access / memory_bandwidth

æœ€ä¼˜æ‰¹å¤§å°:
  batch_size_opt = argmax_b { Throughput(b) | Memory(b) â‰¤ GPU_Memory }
```

---

## 8 ä¼˜åŒ–ç­–ç•¥

### 8.1 è®¡ç®—ä¼˜åŒ–

```mermaid
graph TB
    subgraph "ç®—å­èåˆ"
        OF1[Q/K/VæŠ•å½±èåˆ]
        OF2[æ³¨æ„åŠ›+Softmaxèåˆ]
        OF3[MLPå±‚èåˆ]
    end

    subgraph "å¹¶è¡Œç­–ç•¥"
        PS1[å¤šå¤´å¹¶è¡Œ]
        PS2[åºåˆ—å¹¶è¡Œ]
        PS3[å¼ é‡å¹¶è¡Œ]
    end

    subgraph "é‡åŒ–åŠ é€Ÿ"
        QA1[INT8æ¨ç†]
        QA2[FP16æ··åˆç²¾åº¦]
        QA3[INT4é‡åŒ–]
    end

    OF1 --> PS1
    OF2 --> PS2
    OF3 --> PS3
    PS1 --> QA1
    PS2 --> QA2
    PS3 --> QA3
```

### 8.2 å†…å­˜ä¼˜åŒ–

| ä¼˜åŒ–æŠ€æœ¯ | åŸç† | å†…å­˜èŠ‚çœ | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|---------|
| **KV-Cacheé‡åŒ–** | INT8/FP8å­˜å‚¨ | 50-75% | é•¿åºåˆ— |
| **PagedAttention** | æŒ‰éœ€åˆ†é… | 50-90% | é€šç”¨ |
| **å¤šæŸ¥è¯¢æ³¨æ„åŠ›(MQA)** | å…±äº«KV | 50-90% | æ¨¡å‹è®¾è®¡ |
| **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)** | KVåˆ†ç»„ | 30-70% | æ¨¡å‹è®¾è®¡ |
| **ç¨€ç–æ³¨æ„åŠ›** | ç¨€ç–æ¨¡å¼ | 50-80% | ç‰¹å®šä»»åŠ¡ |

### 8.3 è°ƒåº¦ä¼˜åŒ–å†³ç­–æ ‘

```mermaid
graph TD
    A[æ¨ç†è¯·æ±‚] --> B{è¯·æ±‚ç±»å‹?}

    B -->|æ–°è¯·æ±‚| C{Prompté•¿åº¦?}
    B -->|è¿›è¡Œä¸­| D[åŠ å…¥è§£ç æ‰¹æ¬¡]

    C -->|çŸ­ <512| E[ç«‹å³é¢„å¡«å……]
    C -->|ä¸­ 512-2048| F[åˆ†å—é¢„å¡«å……]
    C -->|é•¿ >2048| G[ä¸“ç”¨é¢„å¡«å……GPU]

    E --> H{è§£ç æ‰¹æ¬¡ç©ºé—²?}
    F --> H
    G --> I[KV-Cacheä¼ è¾“]

    H -->|æ˜¯| J[åˆå¹¶åˆ°è§£ç æ‰¹æ¬¡]
    H -->|å¦| K[ç­‰å¾…æ‰¹æ¬¡ç©ºä½]

    I --> J
    K --> J

    D --> L{æ‰¹å¤§å°è¾¾é˜ˆå€¼?}
    L -->|æ˜¯| M[æ‰§è¡Œè§£ç æ­¥]
    L -->|å¦| N[ç´¯ç§¯ç­‰å¾…]

    N --> L
```

---

## 9 è·¨è§†è§’é“¾æ¥

### 9.1 è°ƒåº¦è§†è§’å…³è”

- [GPUä»»åŠ¡è°ƒåº¦](../16_GPUä¸åŠ é€Ÿå™¨è°ƒåº¦/16.1_GPUä»»åŠ¡è°ƒåº¦.md) - GPUè®¡ç®—è°ƒåº¦åŸºç¡€
- [å¼‚æ„è®¡ç®—è°ƒåº¦](../16_GPUä¸åŠ é€Ÿå™¨è°ƒåº¦/16.4_å¼‚æ„è®¡ç®—è°ƒåº¦.md) - CPU-GPUååŒ
- [åˆ†å¸ƒå¼è°ƒåº¦](../06_è°ƒåº¦æ¨¡å‹/06.4_åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒåº¦.md) - é›†ç¾¤çº§è°ƒåº¦

### 9.2 å½¢å¼è¯­è¨€è§†è§’å…³è”

| å½¢å¼è¯­è¨€æ¦‚å¿µ | LLMè°ƒåº¦å¯¹åº” | æ˜ å°„è¯´æ˜ |
|------------|------------|---------|
| **çº¿æ€§ç±»å‹** | KV-Cacheç”Ÿå‘½å‘¨æœŸ | èµ„æºæ‰€æœ‰æƒç®¡ç† |
| **æ•ˆåº”ç³»ç»Ÿ** | æ¨ç†çŠ¶æ€è¿½è¸ª | å‰¯ä½œç”¨ç®¡ç† |
| **ä¾èµ–ç±»å‹** | åŠ¨æ€åºåˆ—é•¿åº¦ | è¿è¡Œæ—¶ç±»å‹ä¾èµ– |

---

**è¿”å›**: [LLMæ¨ç†è°ƒåº¦ä¸»ç´¢å¼•](./README.md) | [è°ƒåº¦è§†è§’ä¸»ç´¢å¼•](../README.md)
