# 25.3 æ‰¹å¤„ç†è°ƒåº¦ç­–ç•¥

> **å­ä¸»é¢˜ç¼–å·**: 25.3
> **ä¸»é¢˜**: LLMæ¨ç†è°ƒåº¦
> **æœ€åæ›´æ–°**: 2025-12-02
> **æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ

---

## ğŸ“‹ ç›®å½•

- [1 æ¦‚è¿°](#1-æ¦‚è¿°)
- [2 æ€ç»´å¯¼å›¾](#2-æ€ç»´å¯¼å›¾)
- [3 é™æ€æ‰¹å¤„ç†](#3-é™æ€æ‰¹å¤„ç†)
- [4 è¿ç»­æ‰¹å¤„ç†](#4-è¿ç»­æ‰¹å¤„ç†)
- [5 è°ƒåº¦ç®—æ³•](#5-è°ƒåº¦ç®—æ³•)
- [6 çŸ¥è¯†çŸ©é˜µ](#6-çŸ¥è¯†çŸ©é˜µ)
- [7 å½¢å¼åŒ–æ¨¡å‹](#7-å½¢å¼åŒ–æ¨¡å‹)
- [8 é«˜çº§ç­–ç•¥](#8-é«˜çº§ç­–ç•¥)
- [9 è·¨è§†è§’é“¾æ¥](#9-è·¨è§†è§’é“¾æ¥)

---

## 1 æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ´å¯Ÿ

æ‰¹å¤„ç†æ˜¯æå‡LLMæ¨ç†ååé‡çš„å…³é”®æŠ€æœ¯ã€‚ç”±äºGPUå…·æœ‰é«˜åº¦å¹¶è¡Œæ€§ï¼Œå°†å¤šä¸ªè¯·æ±‚åˆå¹¶æˆæ‰¹æ¬¡å¯ä»¥æ˜¾è‘—æé«˜è®¡ç®—æ•ˆç‡ã€‚ç„¶è€Œï¼ŒLLMçš„è‡ªå›å½’ç‰¹æ€§ä½¿å¾—ä¼ ç»Ÿæ‰¹å¤„ç†ç­–ç•¥é¢ä¸´æŒ‘æˆ˜ã€‚

### 1.2 æ‰¹å¤„ç†æŒ‘æˆ˜

| æŒ‘æˆ˜ | æè¿° | å½±å“ |
|------|------|------|
| **é•¿åº¦ä¸ä¸€** | è¯·æ±‚è¾“å…¥/è¾“å‡ºé•¿åº¦å·®å¼‚å¤§ | çŸ­è¯·æ±‚ç­‰å¾…é•¿è¯·æ±‚ |
| **åŠ¨æ€åˆ°è¾¾** | è¯·æ±‚åœ¨çº¿åˆ°è¾¾ | æ‰¹æ¬¡ç»„è£…å»¶è¿Ÿ |
| **èµ„æºäº‰ç”¨** | å†…å­˜éšæ‰¹å¤§å°å¢é•¿ | OOMé£é™© |
| **SLOçº¦æŸ** | å»¶è¿Ÿè¦æ±‚ä¸åŒ | è°ƒåº¦å¤æ‚ |

---

## 2 æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((æ‰¹å¤„ç†è°ƒåº¦))
    é™æ€æ‰¹å¤„ç†
      ç­‰å¾…ç´¯ç§¯
      ç»Ÿä¸€å¤„ç†
      æ•ˆç‡ä½ä¸‹
        çŸ­è¯·æ±‚ç­‰å¾…
        èµ„æºæµªè´¹
    è¿ç»­æ‰¹å¤„ç†
      åŠ¨æ€åŠ å…¥
      è¿­ä»£çº§è°ƒåº¦
      é«˜åå
        å³æ—¶å“åº”
        GPUåˆ©ç”¨ç‡é«˜
    è°ƒåº¦ç­–ç•¥
      FCFS
      ä¼˜å…ˆçº§
      SLOæ„ŸçŸ¥
      å…¬å¹³è°ƒåº¦
    ä¼˜åŒ–æŠ€æœ¯
      é€‰æ‹©æ€§æ‰¹å¤„ç†
      åˆ†å—é¢„å¡«å……
      æ¨æµ‹è§£ç 
```

---

## 3 é™æ€æ‰¹å¤„ç†

### 3.1 é™æ€æ‰¹å¤„ç†æµç¨‹

```mermaid
sequenceDiagram
    participant Q as è¯·æ±‚é˜Ÿåˆ—
    participant B as æ‰¹å¤„ç†å™¨
    participant G as GPU
    participant O as è¾“å‡º

    Note over Q,B: ç­‰å¾…ç´¯ç§¯è¯·æ±‚
    Q->>B: è¯·æ±‚1
    Q->>B: è¯·æ±‚2
    Q->>B: è¯·æ±‚3

    Note over B: è¾¾åˆ°æ‰¹å¤§å°æˆ–è¶…æ—¶
    B->>G: æ‰¹å¤„ç†è¯·æ±‚[1,2,3]

    loop ç›´åˆ°æ‰€æœ‰å®Œæˆ
        G->>G: å¹¶è¡Œç”ŸæˆToken
        Note over G: çŸ­è¯·æ±‚å®Œæˆåpaddingç­‰å¾…
    end

    G->>O: æ‰¹é‡è¾“å‡º
```

### 3.2 é™æ€æ‰¹å¤„ç†é—®é¢˜

```text
é—®é¢˜ç¤ºä¾‹:

è¯·æ±‚é•¿åº¦: [100, 500, 50] tokens
æœ€é•¿è¯·æ±‚: 500 tokens

é™æ€æ‰¹å¤„ç†æ‰§è¡Œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Req1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ 100 tokens + 400 padding
â”‚ Req2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ 500 tokens (å…¨é•¿)
â”‚ Req3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ 50 tokens + 450 padding
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å®é™…è®¡ç®—: 500 Ã— 3 = 1500 token-steps
æœ‰æ•ˆè®¡ç®—: 100 + 500 + 50 = 650 token-steps
æ•ˆç‡: 650/1500 = 43.3%
```

### 3.3 é™æ€æ‰¹å¤„ç†ä»£ç 

```python
class StaticBatcher:
    """é™æ€æ‰¹å¤„ç†å™¨"""

    def __init__(self, batch_size: int, max_wait_time: float):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests: List[Request] = []
        self.last_batch_time = time.time()

    def add_request(self, request: Request):
        self.pending_requests.append(request)

    def should_process(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†æ‰¹æ¬¡"""
        return (
            len(self.pending_requests) >= self.batch_size or
            time.time() - self.last_batch_time > self.max_wait_time
        )

    def process_batch(self, model):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
        if not self.pending_requests:
            return []

        batch = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]

        # Paddingåˆ°æœ€å¤§é•¿åº¦
        max_len = max(r.input_length for r in batch)
        padded_inputs = [pad_to_length(r.input, max_len) for r in batch]

        # æ‰¹é‡æ¨ç†
        outputs = []
        finished = [False] * len(batch)

        while not all(finished):
            next_tokens = model.forward(padded_inputs)
            for i, token in enumerate(next_tokens):
                if not finished[i]:
                    batch[i].output.append(token)
                    if token == EOS or len(batch[i].output) >= batch[i].max_length:
                        finished[i] = True

        self.last_batch_time = time.time()
        return batch
```

---

## 4 è¿ç»­æ‰¹å¤„ç†

### 4.1 æ ¸å¿ƒæ€æƒ³

è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batching / In-flight Batchingï¼‰å…è®¸åœ¨æ¯ä¸ªè§£ç è¿­ä»£ååŠ¨æ€è°ƒæ•´æ‰¹æ¬¡ï¼šå®Œæˆçš„è¯·æ±‚ç«‹å³é€€å‡ºï¼Œæ–°è¯·æ±‚éšæ—¶åŠ å…¥ã€‚

```mermaid
graph TB
    subgraph "è¿ç»­æ‰¹å¤„ç†æµç¨‹"
        I1[è¿­ä»£1]
        I2[è¿­ä»£2]
        I3[è¿­ä»£3]
        I4[è¿­ä»£4]
    end

    subgraph "æ‰¹æ¬¡ç»„æˆ"
        B1["[A,B,C]"]
        B2["[A,B,C,D]"]
        B3["[A,B,D,E]"]
        B4["[B,D,E,F]"]
    end

    I1 --> B1
    I2 --> B2
    I3 --> B3
    I4 --> B4

    B1 -->|"DåŠ å…¥"| B2
    B2 -->|"Cå®Œæˆ,EåŠ å…¥"| B3
    B3 -->|"Aå®Œæˆ,FåŠ å…¥"| B4
```

### 4.2 è¿ç»­æ‰¹å¤„ç†æ‰§è¡Œç¤ºä¾‹

```text
è¿ç»­æ‰¹å¤„ç†æ‰§è¡Œ:

æ—¶é—´  æ‰¹æ¬¡çŠ¶æ€              äº‹ä»¶
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T0    [A(0)]               Aå¼€å§‹
T1    [A(1),B(0)]          BåŠ å…¥
T2    [A(2),B(1),C(0)]     CåŠ å…¥
T3    [A(3),B(2),C(1)]
T4    [B(3),C(2),D(0)]     Aå®Œæˆ(4 tokens), DåŠ å…¥
T5    [B(4),C(3),D(1)]
T6    [C(4),D(2),E(0)]     Bå®Œæˆ(5 tokens), EåŠ å…¥
...

vs é™æ€æ‰¹å¤„ç†:
- é™æ€: A,B,Cå¿…é¡»ç­‰æœ€é•¿çš„å®Œæˆ
- è¿ç»­: å®Œæˆå³é€€å‡ºï¼Œæ–°è¯·æ±‚å³åŠ å…¥
```

### 4.3 è¿ç»­æ‰¹å¤„ç†å®ç°

```python
class ContinuousBatcher:
    """è¿ç»­æ‰¹å¤„ç†è°ƒåº¦å™¨"""

    def __init__(self, max_batch_size: int, kv_cache_manager: KVCacheManager):
        self.max_batch_size = max_batch_size
        self.kv_cache = kv_cache_manager
        self.running_requests: List[Request] = []
        self.waiting_queue: Queue[Request] = Queue()

    def add_request(self, request: Request):
        """æ·»åŠ æ–°è¯·æ±‚åˆ°ç­‰å¾…é˜Ÿåˆ—"""
        self.waiting_queue.put(request)

    def _can_add_request(self, request: Request) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ·»åŠ æ–°è¯·æ±‚"""
        # æ£€æŸ¥æ‰¹å¤§å°é™åˆ¶
        if len(self.running_requests) >= self.max_batch_size:
            return False
        # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
        estimated_blocks = self._estimate_blocks(request)
        return self.kv_cache.has_free_blocks(estimated_blocks)

    def _schedule_iteration(self):
        """è°ƒåº¦ä¸€ä¸ªè¿­ä»£"""
        # 1. å°è¯•æ·»åŠ ç­‰å¾…ä¸­çš„è¯·æ±‚
        while not self.waiting_queue.empty():
            request = self.waiting_queue.queue[0]
            if self._can_add_request(request):
                request = self.waiting_queue.get()
                self._prefill_request(request)
                self.running_requests.append(request)
            else:
                break

        # 2. æ‰§è¡Œè§£ç æ­¥
        if self.running_requests:
            outputs = self._decode_step(self.running_requests)

            # 3. å¤„ç†å®Œæˆçš„è¯·æ±‚
            finished = []
            for i, (req, token) in enumerate(zip(self.running_requests, outputs)):
                req.output.append(token)
                if self._is_finished(req, token):
                    finished.append(i)

            # 4. ç§»é™¤å®Œæˆçš„è¯·æ±‚ï¼ˆé€†åºé¿å…ç´¢å¼•é—®é¢˜ï¼‰
            for i in reversed(finished):
                req = self.running_requests.pop(i)
                self.kv_cache.free(req.id)
                self._return_result(req)

    def _prefill_request(self, request: Request):
        """é¢„å¡«å……æ–°è¯·æ±‚"""
        # åˆ†é…KV-Cache
        self.kv_cache.allocate(request.id, request.input_length)
        # è¿è¡Œé¢„å¡«å……
        self._run_prefill(request)

    def _decode_step(self, requests: List[Request]) -> List[int]:
        """æ‰¹é‡è§£ç ä¸€æ­¥"""
        # æ„å»ºæ‰¹æ¬¡è¾“å…¥
        batch_input = self._prepare_batch(requests)
        # GPUå‰å‘ä¼ æ’­
        logits = self.model.forward(batch_input)
        # é‡‡æ ·
        tokens = self._sample(logits, requests)
        return tokens
```

### 4.4 æ€§èƒ½å¯¹æ¯”

```mermaid
graph LR
    subgraph "ååé‡å¯¹æ¯”"
        S[é™æ€æ‰¹å¤„ç†]
        C[è¿ç»­æ‰¹å¤„ç†]
    end

    S -->|"~100 req/s"| SR[é™æ€åå]
    C -->|"~250 req/s"| CR[è¿ç»­åå]

    subgraph "å»¶è¿Ÿå¯¹æ¯”"
        SL["P50: 2s, P99: 10s"]
        CL["P50: 1s, P99: 3s"]
    end

    S --> SL
    C --> CL
```

---

## 5 è°ƒåº¦ç®—æ³•

### 5.1 FCFSï¼ˆå…ˆæ¥å…ˆæœåŠ¡ï¼‰

```python
class FCFSScheduler:
    """å…ˆæ¥å…ˆæœåŠ¡è°ƒåº¦å™¨"""

    def select_requests(self, waiting: List[Request], max_batch: int) -> List[Request]:
        """æŒ‰åˆ°è¾¾é¡ºåºé€‰æ‹©è¯·æ±‚"""
        return waiting[:max_batch]
```

### 5.2 SJFï¼ˆæœ€çŸ­ä½œä¸šä¼˜å…ˆï¼‰

```python
class SJFScheduler:
    """æœ€çŸ­ä½œä¸šä¼˜å…ˆè°ƒåº¦å™¨"""

    def select_requests(self, waiting: List[Request], max_batch: int) -> List[Request]:
        """é€‰æ‹©é¢„è®¡è¾“å‡ºæœ€çŸ­çš„è¯·æ±‚"""
        # æŒ‰é¢„ä¼°è¾“å‡ºé•¿åº¦æ’åº
        sorted_requests = sorted(waiting, key=lambda r: r.estimated_output_length)
        return sorted_requests[:max_batch]
```

### 5.3 SLOæ„ŸçŸ¥è°ƒåº¦

```python
class SLOAwareScheduler:
    """SLOæ„ŸçŸ¥è°ƒåº¦å™¨"""

    def __init__(self):
        self.slo_targets = {
            'realtime': 0.5,    # 500ms TTFT
            'interactive': 2.0,  # 2s TTFT
            'batch': 10.0,       # 10s TTFT
        }

    def select_requests(self, waiting: List[Request], max_batch: int) -> List[Request]:
        """åŸºäºSLOç´§è¿«åº¦é€‰æ‹©è¯·æ±‚"""
        now = time.time()

        def urgency(r: Request) -> float:
            deadline = r.arrival_time + self.slo_targets[r.slo_class]
            slack = deadline - now
            return -slack  # è¶Šç´§è¿«è¶Šé«˜ä¼˜å…ˆçº§

        sorted_requests = sorted(waiting, key=urgency)
        return sorted_requests[:max_batch]
```

### 5.4 å…¬å¹³è°ƒåº¦

```python
class FairScheduler:
    """å…¬å¹³è°ƒåº¦å™¨ - åŸºäºè™šæ‹Ÿæ—¶é—´"""

    def __init__(self):
        self.virtual_time: Dict[str, float] = defaultdict(float)  # user -> vtime

    def select_requests(self, waiting: List[Request], max_batch: int) -> List[Request]:
        """åŸºäºè™šæ‹Ÿæ—¶é—´çš„å…¬å¹³è°ƒåº¦"""
        # æŒ‰ç”¨æˆ·è™šæ‹Ÿæ—¶é—´æ’åº
        sorted_requests = sorted(waiting, key=lambda r: self.virtual_time[r.user])

        selected = []
        for req in sorted_requests:
            if len(selected) >= max_batch:
                break
            selected.append(req)
            # æ›´æ–°è™šæ‹Ÿæ—¶é—´
            self.virtual_time[req.user] += req.estimated_cost

        return selected
```

### 5.5 è°ƒåº¦ç®—æ³•å†³ç­–æ ‘

```mermaid
graph TD
    A[æ–°è¯·æ±‚åˆ°è¾¾] --> B{ç³»ç»Ÿè´Ÿè½½?}

    B -->|ä½è´Ÿè½½| C[ç«‹å³å¤„ç†]
    B -->|ä¸­è´Ÿè½½| D{è¯·æ±‚ç±»å‹?}
    B -->|é«˜è´Ÿè½½| E{ä¼˜å…ˆçº§?}

    D -->|å®æ—¶| F[ä¼˜å…ˆè°ƒåº¦]
    D -->|äº¤äº’| G[æ­£å¸¸é˜Ÿåˆ—]
    D -->|æ‰¹é‡| H[å»¶è¿Ÿé˜Ÿåˆ—]

    E -->|é«˜ä¼˜å…ˆçº§| I[æŠ¢å èµ„æº]
    E -->|æ™®é€šä¼˜å…ˆçº§| J[ç­‰å¾…ç©ºä½]
    E -->|ä½ä¼˜å…ˆçº§| K[åå°å¤„ç†]

    C --> L[æ‰§è¡Œæ¨ç†]
    F --> L
    G --> L
    I --> L
```

---

## 6 çŸ¥è¯†çŸ©é˜µ

### 6.1 æ‰¹å¤„ç†ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | ååé‡ | å»¶è¿Ÿ | å®ç°å¤æ‚åº¦ | é€‚ç”¨åœºæ™¯ |
|------|-------|------|-----------|---------|
| **é™æ€æ‰¹å¤„ç†** | ä½ | é«˜ | ä½ | ç¦»çº¿æ¨ç† |
| **è¿ç»­æ‰¹å¤„ç†** | é«˜ | ä½ | ä¸­ | åœ¨çº¿æœåŠ¡ |
| **é€‰æ‹©æ€§æ‰¹å¤„ç†** | æé«˜ | æä½ | é«˜ | æ··åˆè´Ÿè½½ |
| **åˆ†å—é¢„å¡«å……** | é«˜ | ç¨³å®š | é«˜ | é•¿prompt |

### 6.2 è°ƒåº¦ç®—æ³•å¯¹æ¯”

| ç®—æ³• | å…¬å¹³æ€§ | ååé‡ | å°¾å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ |
|------|-------|-------|-------|---------|
| **FCFS** | ä¸­ | ä¸­ | é«˜ | ç®€å•åœºæ™¯ |
| **SJF** | ä½ | é«˜ | ä½ | æ‰¹å¤„ç† |
| **SLOæ„ŸçŸ¥** | é«˜ | ä¸­ | å¯æ§ | ç”Ÿäº§ç¯å¢ƒ |
| **å…¬å¹³è°ƒåº¦** | æé«˜ | ä¸­ | ä¸­ | å¤šç§Ÿæˆ· |

---

## 7 å½¢å¼åŒ–æ¨¡å‹

### 7.1 è¿ç»­æ‰¹å¤„ç†å½¢å¼åŒ–

```text
å®šä¹‰: è¿ç»­æ‰¹å¤„ç†ç³»ç»Ÿ

çŠ¶æ€ç©ºé—´:
  State = {
    running: Set<Request>,      // è¿è¡Œä¸­è¯·æ±‚
    waiting: Queue<Request>,    // ç­‰å¾…é˜Ÿåˆ—
    memory: MemoryState,        // å†…å­˜çŠ¶æ€
    time: Timestamp             // å½“å‰æ—¶é—´
  }

è½¬ç§»å‡½æ•°:
  transition: State Ã— Event â†’ State

  Event =
    | Arrival(request)          // æ–°è¯·æ±‚åˆ°è¾¾
    | IterationComplete(tokens) // è¿­ä»£å®Œæˆ
    | RequestFinish(request_id) // è¯·æ±‚å®Œæˆ

è°ƒåº¦ç­–ç•¥:
  schedule: State â†’ Action
  Action =
    | AddToRunning(requests: List<Request>)
    | RemoveFromRunning(request_ids: List<RequestId>)
    | Preempt(request_id)
```

### 7.2 ååé‡åˆ†æ

```text
å®šç†: è¿ç»­æ‰¹å¤„ç†ååé‡ä¸Šç•Œ

ç»™å®š:
  - GPUè®¡ç®—èƒ½åŠ›: C FLOPs/s
  - å•Tokenè®¡ç®—é‡: F FLOPs
  - å¹³å‡åºåˆ—é•¿åº¦: L
  - æ‰¹å¤§å°: B

ç†è®ºæœ€å¤§ååé‡:
  Throughput_max = C / (F Ã— L) tokens/s

å®é™…ååé‡:
  Throughput = min(
    Throughput_max,
    Memory_Bandwidth / (2 Ã— KV_Cache_size),
    B / decode_latency
  )

è¿ç»­æ‰¹å¤„ç†å¢ç›Š:
  Gain = Throughput_continuous / Throughput_static
       = 1 / (padding_ratio Ã— wait_ratio)

å…¶ä¸­:
  padding_ratio: é™æ€æ‰¹å¤„ç†çš„paddingå¼€é”€
  wait_ratio: é™æ€æ‰¹å¤„ç†çš„ç­‰å¾…å¼€é”€
```

### 7.3 SLOæ»¡è¶³ç‡åˆ†æ

```text
å®šç†: SLOæ„ŸçŸ¥è°ƒåº¦çš„æ»¡è¶³ç‡ä¿è¯

è®¾SLOç›®æ ‡ä¸º T_sloï¼Œåˆ°è¾¾ç‡ä¸º Î»

è‹¥ä½¿ç”¨SLOæ„ŸçŸ¥è°ƒåº¦ï¼Œä¸”ï¼š
  - å¹³å‡æœåŠ¡æ—¶é—´ E[S] < T_slo
  - ç³»ç»Ÿåˆ©ç”¨ç‡ Ï = Î» Ã— E[S] < 1

åˆ™SLOæ»¡è¶³ç‡:
  P(Response_Time â‰¤ T_slo) â‰¥ 1 - Ï^(T_slo/E[S])

è¯æ˜æ€è·¯: åŸºäºM/G/1é˜Ÿåˆ—åˆ†æ
```

---

## 8 é«˜çº§ç­–ç•¥

### 8.1 æ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰

```mermaid
graph TB
    subgraph "æ¨æµ‹è§£ç æµç¨‹"
        D[å°æ¨¡å‹æ¨æµ‹]
        V[å¤§æ¨¡å‹éªŒè¯]
        A[æ¥å—/æ‹’ç»]
    end

    D -->|"æ¨æµ‹Kä¸ªtoken"| V
    V -->|"å¹¶è¡ŒéªŒè¯"| A
    A -->|"æ¥å—nä¸ª"| O[è¾“å‡ºnä¸ªtoken]
    A -->|"æ‹’ç»"| R[å›é€€]
    R --> D
```

```python
class SpeculativeDecoder:
    """æ¨æµ‹è§£ç å™¨"""

    def __init__(self, draft_model, target_model, speculation_length: int = 4):
        self.draft_model = draft_model      # å°æ¨¡å‹
        self.target_model = target_model    # å¤§æ¨¡å‹
        self.k = speculation_length

    def decode_step(self, prefix: List[int]) -> List[int]:
        """æ¨æµ‹è§£ç ä¸€æ­¥"""
        # 1. å°æ¨¡å‹æ¨æµ‹Kä¸ªtoken
        draft_tokens = []
        draft_probs = []
        for _ in range(self.k):
            logits = self.draft_model.forward(prefix + draft_tokens)
            prob = softmax(logits)
            token = sample(prob)
            draft_tokens.append(token)
            draft_probs.append(prob[token])

        # 2. å¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯
        all_positions = prefix + draft_tokens
        target_logits = self.target_model.forward(all_positions)
        target_probs = softmax(target_logits, dim=-1)

        # 3. æ‹’ç»é‡‡æ ·éªŒè¯
        accepted = []
        for i, token in enumerate(draft_tokens):
            p_target = target_probs[len(prefix) + i - 1, token]
            p_draft = draft_probs[i]

            # æ¥å—æ¦‚ç‡
            accept_prob = min(1, p_target / p_draft)
            if random.random() < accept_prob:
                accepted.append(token)
            else:
                # ä»ä¿®æ­£åˆ†å¸ƒé‡‡æ ·
                corrected_prob = max(0, target_probs[len(prefix) + i - 1] - draft_probs[i])
                corrected_prob = corrected_prob / corrected_prob.sum()
                new_token = sample(corrected_prob)
                accepted.append(new_token)
                break

        return accepted
```

### 8.2 é€‰æ‹©æ€§æ‰¹å¤„ç†

```python
class SelectiveBatcher:
    """é€‰æ‹©æ€§æ‰¹å¤„ç† - åˆ†ç¦»é¢„å¡«å……å’Œè§£ç """

    def __init__(self):
        self.prefill_batch: List[Request] = []
        self.decode_batch: List[Request] = []

    def schedule(self):
        """é€‰æ‹©æ€§è°ƒåº¦ç­–ç•¥"""
        # ä¼˜å…ˆå¤„ç†çŸ­é¢„å¡«å……
        short_prefills = [r for r in self.prefill_batch if r.input_length < 512]
        long_prefills = [r for r in self.prefill_batch if r.input_length >= 512]

        if short_prefills and len(self.decode_batch) < self.max_decode_batch:
            # çŸ­é¢„å¡«å……ä¸è§£ç æ··åˆ
            return self._mixed_iteration(short_prefills, self.decode_batch)
        elif self.decode_batch:
            # çº¯è§£ç è¿­ä»£
            return self._decode_iteration(self.decode_batch)
        elif long_prefills:
            # é•¿é¢„å¡«å……å•ç‹¬å¤„ç†
            return self._prefill_iteration(long_prefills[:1])
        else:
            return None
```

### 8.3 åˆ†å—é¢„å¡«å……è°ƒåº¦

```mermaid
graph TB
    subgraph "é•¿Promptå¤„ç†"
        LP[é•¿Prompt: 4096 tokens]
        C1[Chunk 1: 0-1024]
        C2[Chunk 2: 1024-2048]
        C3[Chunk 3: 2048-3072]
        C4[Chunk 4: 3072-4096]
    end

    subgraph "äº¤é”™è°ƒåº¦"
        I1[Iter: C1]
        I2[Iter: Decode Batch]
        I3[Iter: C2]
        I4[Iter: Decode Batch]
    end

    LP --> C1 --> C2 --> C3 --> C4
    C1 --> I1
    I1 --> I2
    C2 --> I3
    I3 --> I4
```

---

## 9 è·¨è§†è§’é“¾æ¥

### 9.1 è°ƒåº¦è§†è§’å…³è”

- [æ‰¹å¤„ç†è°ƒåº¦åŸç†](../06_è°ƒåº¦æ¨¡å‹/06.2_OSå†…æ ¸è°ƒåº¦.md) - æ“ä½œç³»ç»Ÿæ‰¹å¤„ç†
- [GPUæ‰¹å¤„ç†](../16_GPUä¸åŠ é€Ÿå™¨è°ƒåº¦/16.1_GPUä»»åŠ¡è°ƒåº¦.md) - GPUæ‰¹å¤„ç†ä¼˜åŒ–
- [åˆ†å¸ƒå¼æ‰¹å¤„ç†](../06_è°ƒåº¦æ¨¡å‹/06.4_åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒåº¦.md) - åˆ†å¸ƒå¼æ‰¹å¤„ç†

### 9.2 å½¢å¼è¯­è¨€è§†è§’å…³è”

| å½¢å¼è¯­è¨€æ¦‚å¿µ | æ‰¹å¤„ç†å¯¹åº” | æ˜ å°„è¯´æ˜ |
|------------|-----------|---------|
| **ç±»å‹æ¨æ–­** | æ‰¹æ¬¡ç»„è£… | å…¼å®¹æ€§æ£€æŸ¥ |
| **æƒ°æ€§æ±‚å€¼** | å»¶è¿Ÿæ‰¹å¤„ç† | æŒ‰éœ€è®¡ç®— |
| **æµå¼å¤„ç†** | è¿ç»­æ‰¹å¤„ç† | å¢é‡å¤„ç† |

---

**è¿”å›**: [LLMæ¨ç†è°ƒåº¦ä¸»ç´¢å¼•](./README.md) | [è°ƒåº¦è§†è§’ä¸»ç´¢å¼•](../README.md)
