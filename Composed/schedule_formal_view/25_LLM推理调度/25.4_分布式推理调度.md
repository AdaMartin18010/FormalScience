# 25.4 åˆ†å¸ƒå¼æ¨ç†è°ƒåº¦

> **å­ä¸»é¢˜ç¼–å·**: 25.4
> **ä¸»é¢˜**: LLMæ¨ç†è°ƒåº¦
> **æœ€åæ›´æ–°**: 2025-12-02
> **æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ

---

## ğŸ“‹ ç›®å½•

- [1 æ¦‚è¿°](#1-æ¦‚è¿°)
- [2 æ€ç»´å¯¼å›¾](#2-æ€ç»´å¯¼å›¾)
- [3 å¼ é‡å¹¶è¡Œ](#3-å¼ é‡å¹¶è¡Œ)
- [4 æµæ°´çº¿å¹¶è¡Œ](#4-æµæ°´çº¿å¹¶è¡Œ)
- [5 æ··åˆå¹¶è¡Œç­–ç•¥](#5-æ··åˆå¹¶è¡Œç­–ç•¥)
- [6 çŸ¥è¯†çŸ©é˜µ](#6-çŸ¥è¯†çŸ©é˜µ)
- [7 å½¢å¼åŒ–æ¨¡å‹](#7-å½¢å¼åŒ–æ¨¡å‹)
- [8 å®è·µä¼˜åŒ–](#8-å®è·µä¼˜åŒ–)
- [9 è·¨è§†è§’é“¾æ¥](#9-è·¨è§†è§’é“¾æ¥)

---

## 1 æ¦‚è¿°

### 1.1 æ ¸å¿ƒæ´å¯Ÿ

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ã€LLaMA-70B+ï¼‰çš„å‚æ•°é‡è¿œè¶…å•GPUå†…å­˜å®¹é‡ï¼Œå¿…é¡»é‡‡ç”¨åˆ†å¸ƒå¼æ¨ç†ã€‚åˆ†å¸ƒå¼LLMæ¨ç†é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼šéœ€è¦åœ¨ä¿æŒä½å»¶è¿Ÿçš„åŒæ—¶ï¼Œé«˜æ•ˆåè°ƒå¤šGPUé—´çš„é€šä¿¡å’Œè®¡ç®—ã€‚

### 1.2 åˆ†å¸ƒå¼æ¨ç†æŒ‘æˆ˜

| æŒ‘æˆ˜ | æè¿° | å½±å“ |
|------|------|------|
| **æ¨¡å‹åˆ‡åˆ†** | å¦‚ä½•å°†æ¨¡å‹åˆ†å¸ƒåˆ°å¤šGPU | é€šä¿¡å¼€é”€ |
| **KV-CacheåŒæ­¥** | åˆ†å¸ƒå¼KV-Cacheç®¡ç† | å†…å­˜æ•ˆç‡ |
| **é€šä¿¡ç“¶é¢ˆ** | GPUé—´æ•°æ®ä¼ è¾“ | å»¶è¿Ÿå¢åŠ  |
| **è´Ÿè½½å‡è¡¡** | å„GPUè´Ÿè½½ä¸å‡ | èµ„æºæµªè´¹ |
| **æ•…éšœæ¢å¤** | èŠ‚ç‚¹æ•…éšœå¤„ç† | å¯é æ€§ |

### 1.3 æ¨¡å‹è§„æ¨¡ä¸GPUéœ€æ±‚

```text
æ¨¡å‹å‚æ•°é‡ vs GPUéœ€æ±‚ (FP16):

| æ¨¡å‹       | å‚æ•°é‡  | æ¨¡å‹å†…å­˜  | æ¨èGPUé…ç½®      |
|------------|---------|----------|-----------------|
| LLaMA-7B   | 7B      | ~14GB    | 1Ã— A100 80GB    |
| LLaMA-13B  | 13B     | ~26GB    | 1Ã— A100 80GB    |
| LLaMA-70B  | 70B     | ~140GB   | 2Ã— A100 80GB    |
| GPT-3      | 175B    | ~350GB   | 8Ã— A100 80GB    |
| GPT-4*     | ~1.8T   | ~3.6TB   | å¤šèŠ‚ç‚¹é›†ç¾¤       |

*GPT-4ä¸ºä¼°è®¡å€¼
```

---

## 2 æ€ç»´å¯¼å›¾

```mermaid
mindmap
  root((åˆ†å¸ƒå¼æ¨ç†è°ƒåº¦))
    å¹¶è¡Œç­–ç•¥
      å¼ é‡å¹¶è¡Œ
        å±‚å†…åˆ‡åˆ†
        AllReduceé€šä¿¡
        ä½å»¶è¿Ÿ
      æµæ°´çº¿å¹¶è¡Œ
        å±‚é—´åˆ‡åˆ†
        å¾®æ‰¹å¤„ç†
        é«˜åå
      æ··åˆå¹¶è¡Œ
        å¼ é‡+æµæ°´çº¿
        æœ€ä¼˜é…ç½®
    é€šä¿¡ä¼˜åŒ–
      é€šä¿¡é‡å 
        è®¡ç®—-é€šä¿¡é‡å 
        å¼‚æ­¥ä¼ è¾“
      é€šä¿¡å‹ç¼©
        é‡åŒ–æ¢¯åº¦
        ç¨€ç–é€šä¿¡
      æ‹“æ‰‘æ„ŸçŸ¥
        NVLinkåˆ©ç”¨
        è·¨èŠ‚ç‚¹ä¼˜åŒ–
    KV-Cacheç®¡ç†
      åˆ†å¸ƒå¼å­˜å‚¨
        åˆ†ç‰‡å­˜å‚¨
        å¤åˆ¶ç­–ç•¥
      ä¼ è¾“ä¼˜åŒ–
        é¢„å–
        å‹ç¼©
    æ•…éšœå¤„ç†
      æ£€æŸ¥ç‚¹
      å‰¯æœ¬å†—ä½™
      åŠ¨æ€æ¢å¤
```

---

## 3 å¼ é‡å¹¶è¡Œ

### 3.1 å¼ é‡å¹¶è¡ŒåŸç†

å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelism, TPï¼‰å°†å•å±‚çš„è®¡ç®—åˆ†å¸ƒåˆ°å¤šä¸ªGPUä¸Šï¼Œé€‚åˆä½å»¶è¿Ÿåœºæ™¯ã€‚

```mermaid
graph TB
    subgraph "å•GPUè®¡ç®—"
        A1[è¾“å…¥ X]
        W1[æƒé‡ W]
        O1[è¾“å‡º Y = XW]
        A1 --> W1 --> O1
    end

    subgraph "2-wayå¼ é‡å¹¶è¡Œ"
        A2[è¾“å…¥ X]
        W2a[Wâ‚ on GPU0]
        W2b[Wâ‚‚ on GPU1]
        O2a[Yâ‚ = XWâ‚]
        O2b[Yâ‚‚ = XWâ‚‚]
        AR[AllReduce]
        O2[Y = Yâ‚ + Yâ‚‚]

        A2 --> W2a --> O2a
        A2 --> W2b --> O2b
        O2a --> AR
        O2b --> AR
        AR --> O2
    end
```

### 3.2 æ³¨æ„åŠ›å±‚å¼ é‡å¹¶è¡Œ

```text
å¤šå¤´æ³¨æ„åŠ›å¼ é‡å¹¶è¡Œ:

åŸå§‹: Attention = Concat(headâ‚, ..., head_h) Ã— W_O
      head_i = Softmax(Q_i Ã— K_i^T / âˆšd) Ã— V_i

å¼ é‡å¹¶è¡Œ (TP=2):
  GPU0: headâ‚, headâ‚‚, ..., head_{h/2}
  GPU1: head_{h/2+1}, ..., head_h

  è®¡ç®—æµç¨‹:
  1. [å¹¶è¡Œ] å„GPUè®¡ç®—è´Ÿè´£çš„å¤´
  2. [é€šä¿¡] AllReduceåˆå¹¶è¾“å‡ºæŠ•å½±ç»“æœ
```

```python
class TensorParallelAttention:
    """å¼ é‡å¹¶è¡Œæ³¨æ„åŠ›å±‚"""

    def __init__(self, hidden_size: int, num_heads: int, tp_size: int, rank: int):
        self.tp_size = tp_size
        self.rank = rank
        self.num_heads_per_rank = num_heads // tp_size

        # æ¯ä¸ªrankåªæŒæœ‰éƒ¨åˆ†å¤´çš„å‚æ•°
        self.q_proj = Linear(hidden_size, hidden_size // tp_size)
        self.k_proj = Linear(hidden_size, hidden_size // tp_size)
        self.v_proj = Linear(hidden_size, hidden_size // tp_size)
        self.o_proj = Linear(hidden_size // tp_size, hidden_size)

    def forward(self, x: Tensor, kv_cache: Optional[KVCache]) -> Tensor:
        # 1. è®¡ç®—æœ¬åœ°Q, K, V
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # 2. æ›´æ–°KV-Cache
        if kv_cache is not None:
            k, v = kv_cache.update(k, v)

        # 3. è®¡ç®—æ³¨æ„åŠ›ï¼ˆåªæ¶‰åŠæœ¬åœ°å¤´ï¼‰
        attn_output = scaled_dot_product_attention(q, k, v)

        # 4. è¾“å‡ºæŠ•å½±
        output = self.o_proj(attn_output)

        # 5. AllReduceåˆå¹¶æ‰€æœ‰rankçš„ç»“æœ
        output = all_reduce(output)

        return output
```

### 3.3 MLPå±‚å¼ é‡å¹¶è¡Œ

```text
MLPå¼ é‡å¹¶è¡Œ (TP=2):

åŸå§‹: Y = GeLU(X Ã— Wâ‚) Ã— Wâ‚‚

åˆ—å¹¶è¡Œ + è¡Œå¹¶è¡Œ:
  Wâ‚ æŒ‰åˆ—åˆ‡åˆ†: Wâ‚ = [Wâ‚á´¬ | Wâ‚á´®]
  Wâ‚‚ æŒ‰è¡Œåˆ‡åˆ†: Wâ‚‚ = [Wâ‚‚á´¬]
                    [Wâ‚‚á´®]

GPU0: Yâ‚€ = GeLU(X Ã— Wâ‚á´¬) Ã— Wâ‚‚á´¬
GPU1: Yâ‚ = GeLU(X Ã— Wâ‚á´®) Ã— Wâ‚‚á´®

AllReduce: Y = Yâ‚€ + Yâ‚
```

### 3.4 é€šä¿¡åˆ†æ

```text
å¼ é‡å¹¶è¡Œé€šä¿¡å¼€é”€:

æ¯å±‚é€šä¿¡é‡:
  - æ³¨æ„åŠ›å±‚: 2 Ã— batch Ã— seq_len Ã— hidden_size Ã— dtype_size
  - MLPå±‚: 2 Ã— batch Ã— seq_len Ã— hidden_size Ã— dtype_size

æ€»è®¡æ¯å±‚: 4 Ã— B Ã— S Ã— H Ã— sizeof(dtype)

ç¤ºä¾‹ (LLaMA-70B, B=1, S=2048, H=8192, FP16):
  æ¯å±‚é€šä¿¡: 4 Ã— 1 Ã— 2048 Ã— 8192 Ã— 2 = 128MB
  æ€»é€šä¿¡ (80å±‚): 80 Ã— 128MB = 10GB

é€šä¿¡æ—¶é—´ (NVLink 300GB/s):
  T_comm = 10GB / 300GB/s â‰ˆ 33ms
```

---

## 4 æµæ°´çº¿å¹¶è¡Œ

### 4.1 æµæ°´çº¿å¹¶è¡ŒåŸç†

æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelism, PPï¼‰å°†æ¨¡å‹æŒ‰å±‚åˆ‡åˆ†ï¼Œä¸åŒstageå¤„ç†ä¸åŒçš„å±‚ã€‚

```mermaid
graph LR
    subgraph "Stage 0 (GPU 0)"
        L1[Layers 0-19]
    end

    subgraph "Stage 1 (GPU 1)"
        L2[Layers 20-39]
    end

    subgraph "Stage 2 (GPU 2)"
        L3[Layers 40-59]
    end

    subgraph "Stage 3 (GPU 3)"
        L4[Layers 60-79]
    end

    L1 -->|æ¿€æ´»å€¼| L2 -->|æ¿€æ´»å€¼| L3 -->|æ¿€æ´»å€¼| L4
```

### 4.2 æµæ°´çº¿æ°”æ³¡é—®é¢˜

```text
æœ´ç´ æµæ°´çº¿ (4 stages, 4 microbatches):

æ—¶é—´ â†’
      Stage0: [MB0][MB1][MB2][MB3][   ][   ][   ]
      Stage1: [   ][MB0][MB1][MB2][MB3][   ][   ]
      Stage2: [   ][   ][MB0][MB1][MB2][MB3][   ]
      Stage3: [   ][   ][   ][MB0][MB1][MB2][MB3]
              â””â”€â”€â”€â”€â”€æ°”æ³¡å¼€é”€â”€â”€â”€â”€â”€â”˜

æ°”æ³¡æ¯”ä¾‹ = (p-1) / (m+p-1)
  å…¶ä¸­ p=stagesæ•°, m=microbatchæ•°

ç¤ºä¾‹: p=4, m=4 â†’ æ°”æ³¡ = 3/7 = 42.9%
```

### 4.3 1F1Bè°ƒåº¦

```text
1F1B (One Forward One Backward) è°ƒåº¦:

æ—¶é—´ â†’
      Stage0: [F0][F1][F2][F3][B3][B2][B1][B0]
      Stage1: [  ][F0][F1][F2][B2][B3][B1][B0][B]
      Stage2: [  ][  ][F0][F1][B1][B2][B3][B0][B][B]
      Stage3: [  ][  ][  ][F0][B0][B1][B2][B3][B][B][B]

æ¨ç†ç®€åŒ–ç‰ˆ (ä»…å‰å‘):
      Stage0: [F0][F1][F2][F3]...
      Stage1: [  ][F0][F1][F2][F3]...
      Stage2: [  ][  ][F0][F1][F2][F3]...
      Stage3: [  ][  ][  ][F0][F1][F2][F3]...

ç¨³æ€åæ— æ°”æ³¡!
```

### 4.4 æµæ°´çº¿å¹¶è¡Œå®ç°

```python
class PipelineStage:
    """æµæ°´çº¿å¹¶è¡ŒStage"""

    def __init__(self, layers: List[Layer], stage_id: int, num_stages: int):
        self.layers = layers
        self.stage_id = stage_id
        self.num_stages = num_stages
        self.is_first = (stage_id == 0)
        self.is_last = (stage_id == num_stages - 1)

    def forward(self, hidden_states: Optional[Tensor] = None) -> Tensor:
        # ä»ä¸Šä¸€stageæ¥æ”¶æ¿€æ´»å€¼
        if not self.is_first:
            hidden_states = recv_tensor(src=self.stage_id - 1)

        # è®¡ç®—æœ¬stageçš„å±‚
        for layer in self.layers:
            hidden_states = layer(hidden_states)

        # å‘é€åˆ°ä¸‹ä¸€stage
        if not self.is_last:
            send_tensor(hidden_states, dst=self.stage_id + 1)
            return None
        else:
            return hidden_states


class PipelineScheduler:
    """æµæ°´çº¿è°ƒåº¦å™¨"""

    def __init__(self, num_stages: int, num_microbatches: int):
        self.num_stages = num_stages
        self.num_microbatches = num_microbatches

    def generate_schedule(self) -> List[Tuple[int, int]]:
        """ç”Ÿæˆ1F1Bè°ƒåº¦"""
        schedule = []

        # Warm-up: å¡«å……æµæ°´çº¿
        for mb in range(min(self.num_stages, self.num_microbatches)):
            for stage in range(self.num_stages):
                if mb >= stage:
                    schedule.append(('forward', stage, mb - stage))

        # Steady state: 1F1B
        for mb in range(self.num_stages, self.num_microbatches):
            for stage in range(self.num_stages):
                schedule.append(('forward', stage, mb - stage))

        return schedule
```

---

## 5 æ··åˆå¹¶è¡Œç­–ç•¥

### 5.1 TP + PPæ··åˆ

```mermaid
graph TB
    subgraph "èŠ‚ç‚¹0"
        subgraph "TP Group 0"
            G00[GPU 0]
            G01[GPU 1]
        end
    end

    subgraph "èŠ‚ç‚¹1"
        subgraph "TP Group 1"
            G10[GPU 2]
            G11[GPU 3]
        end
    end

    G00 <-->|NVLink| G01
    G10 <-->|NVLink| G11

    G00 -->|IB| G10
    G01 -->|IB| G11
```

```text
æ··åˆå¹¶è¡Œé…ç½®ç¤ºä¾‹ (LLaMA-70B, 8 GPUs):

é…ç½®1: TP=4, PP=2
  - Stage 0 (Layers 0-39): GPU 0-3 (å¼ é‡å¹¶è¡Œ)
  - Stage 1 (Layers 40-79): GPU 4-7 (å¼ é‡å¹¶è¡Œ)

é…ç½®2: TP=2, PP=4
  - Stage 0: GPU 0-1
  - Stage 1: GPU 2-3
  - Stage 2: GPU 4-5
  - Stage 3: GPU 6-7

é€‰æ‹©ä¾æ®:
  - èŠ‚ç‚¹å†…GPU: ä¼˜å…ˆTP (NVLinké«˜å¸¦å®½)
  - èŠ‚ç‚¹é—´: ä¼˜å…ˆPP (é€šä¿¡é‡å°)
```

### 5.2 é…ç½®ä¼˜åŒ–

```python
def optimize_parallel_config(
    model_size: int,
    num_layers: int,
    hidden_size: int,
    num_gpus: int,
    gpu_memory: int,
    intra_node_bandwidth: float,  # NVLink
    inter_node_bandwidth: float,  # InfiniBand
) -> Tuple[int, int]:
    """
    ä¼˜åŒ–å¹¶è¡Œé…ç½®
    Returns: (tp_size, pp_size)
    """
    best_config = None
    best_latency = float('inf')

    for tp in range(1, num_gpus + 1):
        if num_gpus % tp != 0:
            continue
        pp = num_gpus // tp

        if num_layers % pp != 0:
            continue

        # ä¼°ç®—å†…å­˜
        model_memory_per_gpu = model_size / tp / pp
        kv_cache_per_gpu = estimate_kv_cache(hidden_size, num_layers // pp)
        total_memory = model_memory_per_gpu + kv_cache_per_gpu

        if total_memory > gpu_memory:
            continue

        # ä¼°ç®—å»¶è¿Ÿ
        tp_comm = 4 * hidden_size * 2 / intra_node_bandwidth  # AllReduce
        pp_comm = hidden_size * 2 / inter_node_bandwidth  # P2P
        compute_time = estimate_compute(model_size, tp, pp)

        total_latency = compute_time + (num_layers // pp) * tp_comm + (pp - 1) * pp_comm

        if total_latency < best_latency:
            best_latency = total_latency
            best_config = (tp, pp)

    return best_config
```

### 5.3 ä¸“å®¶å¹¶è¡Œï¼ˆMoEï¼‰

å¯¹äºMixture of Expertsæ¨¡å‹ï¼Œè¿˜éœ€è¦ä¸“å®¶å¹¶è¡Œï¼ˆExpert Parallelism, EPï¼‰ï¼š

```mermaid
graph TB
    subgraph "MoEå±‚è°ƒåº¦"
        I[è¾“å…¥Token]
        R[Router]

        subgraph "Expert Parallel"
            E0[Expert 0]
            E1[Expert 1]
            E2[Expert 2]
            E3[Expert 3]
        end

        C[Combine]
        O[è¾“å‡º]
    end

    I --> R
    R -->|è·¯ç”±| E0
    R -->|è·¯ç”±| E1
    R -->|è·¯ç”±| E2
    R -->|è·¯ç”±| E3
    E0 --> C
    E1 --> C
    E2 --> C
    E3 --> C
    C --> O
```

```text
MoEè°ƒåº¦æŒ‘æˆ˜:
1. è´Ÿè½½ä¸å‡: Tokenè·¯ç”±ä¸å‡åŒ€
2. é€šä¿¡: All-to-Allé€šä¿¡
3. ä¸“å®¶å®¹é‡: é™åˆ¶æ¯ä¸ªä¸“å®¶çš„Tokenæ•°

è§£å†³æ–¹æ¡ˆ:
- è¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤±
- ä¸“å®¶å®¹é‡é™åˆ¶
- Top-kè·¯ç”±è°ƒä¼˜
```

---

## 6 çŸ¥è¯†çŸ©é˜µ

### 6.1 å¹¶è¡Œç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | é€šä¿¡æ¨¡å¼ | é€šä¿¡é‡ | å»¶è¿Ÿ | ååé‡ | é€‚ç”¨åœºæ™¯ |
|------|---------|-------|------|-------|---------|
| **å¼ é‡å¹¶è¡Œ** | AllReduce | é«˜ | ä½ | ä¸­ | èŠ‚ç‚¹å†… |
| **æµæ°´çº¿å¹¶è¡Œ** | P2P | ä½ | é«˜ | é«˜ | èŠ‚ç‚¹é—´ |
| **ä¸“å®¶å¹¶è¡Œ** | All-to-All | ä¸­ | ä¸­ | é«˜ | MoEæ¨¡å‹ |
| **æ··åˆå¹¶è¡Œ** | æ··åˆ | å¯æ§ | å¯æ§ | é«˜ | å¤§è§„æ¨¡ |

### 6.2 ç³»ç»Ÿå®ç°å¯¹æ¯”

| ç³»ç»Ÿ | TP | PP | EP | ç‰¹è‰² |
|------|----|----|----|----|
| **Megatron-LM** | âœ… | âœ… | âœ… | NVIDIAå®˜æ–¹ |
| **DeepSpeed** | âœ… | âœ… | âœ… | ZeROä¼˜åŒ– |
| **vLLM** | âœ… | âœ… | âŒ | PagedAttention |
| **TensorRT-LLM** | âœ… | âœ… | âœ… | ä½å»¶è¿Ÿ |
| **Alpa** | âœ… | âœ… | âŒ | è‡ªåŠ¨å¹¶è¡Œ |

---

## 7 å½¢å¼åŒ–æ¨¡å‹

### 7.1 åˆ†å¸ƒå¼æ¨ç†å½¢å¼åŒ–

```text
å®šä¹‰: åˆ†å¸ƒå¼LLMæ¨ç†ç³»ç»Ÿ

ç»“æ„ DistributedLLM:
  Model = List<Layer>
  Partition: Model â†’ Map<DeviceId, List<Layer>>

å¹¶è¡Œé…ç½®:
  Config = {
    tp_size: Nat,      // å¼ é‡å¹¶è¡Œåº¦
    pp_size: Nat,      // æµæ°´çº¿å¹¶è¡Œåº¦
    ep_size: Nat,      // ä¸“å®¶å¹¶è¡Œåº¦
  }

çº¦æŸ:
  tp_size Ã— pp_size Ã— ep_size = num_devices
  âˆ€ stage âˆˆ [0, pp_size): |layers(stage)| = num_layers / pp_size
```

### 7.2 å»¶è¿Ÿæ¨¡å‹

```text
ç«¯åˆ°ç«¯æ¨ç†å»¶è¿Ÿ:

T_total = T_prefill + T_decode Ã— output_length

é¢„å¡«å……å»¶è¿Ÿ (TP=t, PP=p):
  T_prefill = T_compute(prefill) / t + (p-1) Ã— T_bubble + T_comm_tp Ã— num_layers

è§£ç å»¶è¿Ÿ:
  T_decode = T_compute(decode) / t + T_comm_tp Ã— num_layers / p + T_comm_pp

å…¶ä¸­:
  T_comm_tp = 2 Ã— H Ã— dtype_size / BW_nvlink  (AllReduce)
  T_comm_pp = B Ã— S Ã— H Ã— dtype_size / BW_ib  (P2P)
  T_bubble = T_compute(decode) Ã— (p-1) / m    (æµæ°´çº¿æ°”æ³¡)
```

### 7.3 æœ€ä¼˜é…ç½®å®šç†

```text
å®šç†: ç»™å®šç¡¬ä»¶æ‹“æ‰‘ï¼Œå­˜åœ¨æœ€ä¼˜å¹¶è¡Œé…ç½®

ç»™å®š:
  - n ä¸ªGPUï¼Œåˆ†å¸ƒåœ¨ k ä¸ªèŠ‚ç‚¹
  - èŠ‚ç‚¹å†…å¸¦å®½ BW_intra
  - èŠ‚ç‚¹é—´å¸¦å®½ BW_inter
  - BW_intra >> BW_inter

æœ€ä¼˜é…ç½®:
  tp_size = gpus_per_node  (åˆ©ç”¨èŠ‚ç‚¹å†…é«˜å¸¦å®½)
  pp_size = num_nodes      (æœ€å°åŒ–èŠ‚ç‚¹é—´é€šä¿¡)

è¯æ˜æ€è·¯:
  å¼ é‡å¹¶è¡Œé€šä¿¡é‡å¤§ä½†å¯ç”¨NVLink
  æµæ°´çº¿å¹¶è¡Œé€šä¿¡é‡å°é€‚åˆè·¨èŠ‚ç‚¹
```

---

## 8 å®è·µä¼˜åŒ–

### 8.1 é€šä¿¡ä¼˜åŒ–

```mermaid
graph TB
    subgraph "é€šä¿¡ä¼˜åŒ–æŠ€æœ¯"
        CO1[è®¡ç®—-é€šä¿¡é‡å ]
        CO2[æ¢¯åº¦å‹ç¼©]
        CO3[æ‹“æ‰‘æ„ŸçŸ¥]
    end

    CO1 --> O1[éšè—é€šä¿¡å»¶è¿Ÿ]
    CO2 --> O2[å‡å°‘é€šä¿¡é‡]
    CO3 --> O3[ä¼˜åŒ–é€šä¿¡è·¯å¾„]
```

```python
class OverlappedTensorParallel:
    """è®¡ç®—-é€šä¿¡é‡å çš„å¼ é‡å¹¶è¡Œ"""

    def forward(self, x: Tensor) -> Tensor:
        # åˆ†å‰²è¾“å…¥ç”¨äºæµæ°´
        chunks = x.chunk(num_chunks, dim=0)
        outputs = []

        # å¼‚æ­¥é€šä¿¡å¥æŸ„
        handles = []

        for i, chunk in enumerate(chunks):
            # è®¡ç®—å½“å‰å—
            local_out = self.compute(chunk)

            # å¼‚æ­¥å¯åŠ¨AllReduce
            handle = all_reduce_async(local_out)
            handles.append(handle)

            # ç­‰å¾…ä¸Šä¸€ä¸ªå—çš„é€šä¿¡å®Œæˆ
            if i > 0:
                handles[i-1].wait()
                outputs.append(handles[i-1].result())

        # ç­‰å¾…æœ€åä¸€ä¸ª
        handles[-1].wait()
        outputs.append(handles[-1].result())

        return torch.cat(outputs, dim=0)
```

### 8.2 KV-Cacheåˆ†å¸ƒå¼ç®¡ç†

```python
class DistributedKVCache:
    """åˆ†å¸ƒå¼KV-Cacheç®¡ç†"""

    def __init__(self, tp_size: int, pp_size: int):
        self.tp_size = tp_size
        self.pp_size = pp_size
        # æ¯ä¸ªstageç»´æŠ¤è‡ªå·±å±‚çš„KV-Cache
        self.local_cache = LocalKVCache()

    def update(self, layer_id: int, k: Tensor, v: Tensor):
        """æ›´æ–°KV-Cache"""
        # K,Vå·²ç»æ˜¯å¼ é‡å¹¶è¡Œåˆ‡åˆ†åçš„
        self.local_cache.update(layer_id, k, v)

    def migrate(self, request_id: int, target_device: int):
        """è·¨è®¾å¤‡è¿ç§»KV-Cache"""
        cache_data = self.local_cache.get(request_id)
        # å¼‚æ­¥ä¼ è¾“
        send_async(cache_data, target_device)
```

### 8.3 æ•…éšœæ¢å¤

```text
æ•…éšœæ¢å¤ç­–ç•¥:

1. æ£€æŸ¥ç‚¹ä¿å­˜:
   - å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€
   - KV-Cacheæ£€æŸ¥ç‚¹(å¯é€‰)
   - è¯·æ±‚é˜Ÿåˆ—çŠ¶æ€

2. å‰¯æœ¬å†—ä½™:
   - å…³é”®æ•°æ®å¤šå‰¯æœ¬
   - å¿«é€Ÿæ•…éšœæ£€æµ‹

3. åŠ¨æ€æ¢å¤:
   - æ£€æµ‹æ•…éšœèŠ‚ç‚¹
   - é‡æ–°åˆ†é…ä»»åŠ¡
   - ä»æ£€æŸ¥ç‚¹æ¢å¤
```

---

## 9 è·¨è§†è§’é“¾æ¥

### 9.1 è°ƒåº¦è§†è§’å…³è”

- [åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒåº¦](../06_è°ƒåº¦æ¨¡å‹/06.4_åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒåº¦.md) - åˆ†å¸ƒå¼è°ƒåº¦åŸºç¡€
- [GPUé›†ç¾¤è°ƒåº¦](../16_GPUä¸åŠ é€Ÿå™¨è°ƒåº¦/16.4_å¼‚æ„è®¡ç®—è°ƒåº¦.md) - å¼‚æ„é›†ç¾¤
- [ç½‘ç»œè°ƒåº¦](../15_ç½‘ç»œè°ƒåº¦ç³»ç»Ÿ/README.md) - é€šä¿¡è°ƒåº¦

### 9.2 å½¢å¼è¯­è¨€è§†è§’å…³è”

| å½¢å¼è¯­è¨€æ¦‚å¿µ | åˆ†å¸ƒå¼æ¨ç†å¯¹åº” | æ˜ å°„è¯´æ˜ |
|------------|--------------|---------|
| **åˆ†å¸ƒå¼ç±»å‹** | å¼ é‡åˆ‡åˆ† | æ•°æ®åˆ†å¸ƒ |
| **é€šé“ç±»å‹** | GPUé—´é€šä¿¡ | æ¶ˆæ¯ä¼ é€’ |
| **å¹¶è¡Œç»„åˆ** | æ··åˆå¹¶è¡Œ | å¹¶è¡Œç»„åˆ |

---

**è¿”å›**: [LLMæ¨ç†è°ƒåº¦ä¸»ç´¢å¼•](./README.md) | [è°ƒåº¦è§†è§’ä¸»ç´¢å¼•](../README.md)
