# 02.5 å¹¶å‘è°ƒåº¦å®é™…æ¡ˆä¾‹é›†

> **å­ä¸»é¢˜**: 02.5
> **æ‰€å±ä¸»é¢˜**: 02 å¹¶å‘è°ƒåº¦çš„Petriç½‘å»ºæ¨¡
> **æœ€åæ›´æ–°**: 2025-12-02

---

## ğŸ“‹ ç›®å½•

- [02.5 å¹¶å‘è°ƒåº¦å®é™…æ¡ˆä¾‹é›†](#025-å¹¶å‘è°ƒåº¦å®é™…æ¡ˆä¾‹é›†)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1 æ¡ˆä¾‹1ï¼šPostgreSQLäº‹åŠ¡å¹¶å‘æ§åˆ¶](#1-æ¡ˆä¾‹1postgresqläº‹åŠ¡å¹¶å‘æ§åˆ¶)
  - [2 æ¡ˆä¾‹2ï¼šApache Sparkä»»åŠ¡è°ƒåº¦](#2-æ¡ˆä¾‹2apache-sparkä»»åŠ¡è°ƒåº¦)
  - [3 æ¡ˆä¾‹3ï¼šKuberneteså¤šPodå¹¶å‘è°ƒåº¦](#3-æ¡ˆä¾‹3kuberneteså¤špodå¹¶å‘è°ƒåº¦)
  - [4 æ¡ˆä¾‹4ï¼šåˆ¶é€ ä¸šæŸ”æ€§ç”Ÿäº§çº¿è°ƒåº¦](#4-æ¡ˆä¾‹4åˆ¶é€ ä¸šæŸ”æ€§ç”Ÿäº§çº¿è°ƒåº¦)

---

## 1 æ¡ˆä¾‹1ï¼šPostgreSQLäº‹åŠ¡å¹¶å‘æ§åˆ¶

**ç³»ç»Ÿæè¿°**ï¼š

PostgreSQLä½¿ç”¨MVCCï¼ˆå¤šç‰ˆæœ¬å¹¶å‘æ§åˆ¶ï¼‰å®ç°äº‹åŠ¡å¹¶å‘ã€‚

**Petriç½‘å»ºæ¨¡**ï¼š

```python
def create_postgresql_mvcc_model():
    """PostgreSQL MVCCçš„Petriç½‘æ¨¡å‹"""
    pn = PetriNet()

    # æ•°æ®è¡Œç‰ˆæœ¬åº“æ‰€
    pn.add_place('p_row_v1', tokens=1)
    pn.add_place('p_row_v2', tokens=0)

    # äº‹åŠ¡1ï¼šè¯»å–
    pn.add_transition('t_tx1_read')
    pn.add_arc('p_row_v1', 't_tx1_read', weight=0)  # è¯»ä¸æ¶ˆè€—
    pn.add_place('p_tx1_data', tokens=0)
    pn.add_arc('t_tx1_read', 'p_tx1_data')

    # äº‹åŠ¡2ï¼šæ›´æ–°ï¼ˆåˆ›å»ºæ–°ç‰ˆæœ¬ï¼‰
    pn.add_transition('t_tx2_update')
    pn.add_arc('p_row_v1', 't_tx2_update', weight=0)  # è¯»æ—§ç‰ˆæœ¬
    pn.add_arc('t_tx2_update', 'p_row_v2')  # åˆ›å»ºæ–°ç‰ˆæœ¬

    return pn
```

**å…³é”®æ´å¯Ÿ**ï¼šMVCCé€šè¿‡ç‰ˆæœ¬åŒ–é¿å…è¯»å†™å†²çªï¼Œåœ¨Petriç½‘ä¸­ä½“ç°ä¸ºå¤šä¸ªç‰ˆæœ¬åº“æ‰€ã€‚

---

## 2 æ¡ˆä¾‹2ï¼šApache Sparkä»»åŠ¡è°ƒåº¦

**ç³»ç»Ÿæè¿°**ï¼š

Sparkçš„Stageå¹¶è¡Œæ‰§è¡Œï¼Œæ¯ä¸ªStageå†…ä»»åŠ¡å¹¶å‘ã€‚

**Petriç½‘å»ºæ¨¡**ï¼š

```python
def create_spark_dag_scheduler():
    """Spark DAGè°ƒåº¦å™¨çš„Petriç½‘æ¨¡å‹"""
    pn = PetriNet()

    # Stage 1ï¼šMapé˜¶æ®µï¼ˆ4ä¸ªå¹¶è¡Œä»»åŠ¡ï¼‰
    pn.add_place('p_stage1_start', tokens=1)
    pn.add_transition('t_fork_stage1')
    pn.add_arc('p_stage1_start', 't_fork_stage1')

    map_tasks = [f'Map{i}' for i in range(4)]
    for task in map_tasks:
        pn.add_place(f'p_{task}_ready', tokens=0)
        pn.add_arc('t_fork_stage1', f'p_{task}_ready')

        pn.add_transition(f't_{task}')
        pn.add_arc(f'p_{task}_ready', f't_{task}')

        pn.add_place(f'p_{task}_done', tokens=0)
        pn.add_arc(f't_{task}', f'p_{task}_done')

    # Shuffleå±éšœ
    pn.add_transition('t_shuffle')
    for task in map_tasks:
        pn.add_arc(f'p_{task}_done', 't_shuffle')

    # Stage 2ï¼šReduceé˜¶æ®µï¼ˆ2ä¸ªå¹¶è¡Œä»»åŠ¡ï¼‰
    pn.add_place('p_stage2_start', tokens=0)
    pn.add_arc('t_shuffle', 'p_stage2_start')

    pn.add_transition('t_fork_stage2')
    pn.add_arc('p_stage2_start', 't_fork_stage2')

    reduce_tasks = [f'Reduce{i}' for i in range(2)]
    for task in reduce_tasks:
        pn.add_place(f'p_{task}_ready', tokens=0)
        pn.add_arc('t_fork_stage2', f'p_{task}_ready')

        pn.add_transition(f't_{task}')
        pn.add_arc(f'p_{task}_ready', f't_{task}')

        pn.add_place(f'p_{task}_done', tokens=0)
        pn.add_arc(f't_{task}', f'p_{task}_done')

    # æœ€ç»ˆæ±‡æ€»
    pn.add_transition('t_collect')
    for task in reduce_tasks:
        pn.add_arc(f'p_{task}_done', 't_collect')

    pn.add_place('p_output', tokens=0)
    pn.add_arc('t_collect', 'p_output')

    return pn

# åˆ†æ
spark_pn = create_spark_dag_scheduler()
analyzer = ConcurrencyAnalyzer(spark_pn, spark_pn.marking)
results = analyzer.analyze()

print("Spark DAGè°ƒåº¦åˆ†æï¼š")
print(f"  Stage 1æœ€å¤§å¹¶å‘: 4ï¼ˆMapä»»åŠ¡ï¼‰")
print(f"  Stage 2æœ€å¤§å¹¶å‘: 2ï¼ˆReduceä»»åŠ¡ï¼‰")
print(f"  ç“¶é¢ˆ: ShuffleåŒæ­¥ç‚¹")
```

---

## 3 æ¡ˆä¾‹3ï¼šKuberneteså¤šPodå¹¶å‘è°ƒåº¦

**ç³»ç»Ÿæè¿°**ï¼š

Kubernetesè°ƒåº¦å™¨åŒæ—¶è°ƒåº¦å¤šä¸ªPodåˆ°ä¸åŒèŠ‚ç‚¹ã€‚

**Petriç½‘å»ºæ¨¡**ï¼ˆå®Œæ•´å®ç°è§01.2ï¼‰ï¼š

- èŠ‚ç‚¹èµ„æºï¼šåº“æ‰€
- Podè°ƒåº¦ï¼šå˜è¿
- å¹¶å‘è°ƒåº¦ï¼šå¤šä¸ªè°ƒåº¦å˜è¿åŒæ—¶ç‚¹ç«

---

## 4 æ¡ˆä¾‹4ï¼šåˆ¶é€ ä¸šæŸ”æ€§ç”Ÿäº§çº¿è°ƒåº¦

**ç³»ç»Ÿæè¿°**ï¼š

æŸ”æ€§åˆ¶é€ ç³»ç»Ÿï¼ˆFMSï¼‰ä¸­ï¼Œå¤šä¸ªå·¥ä»¶åœ¨å¤šå°æœºå™¨ä¸Šå¹¶å‘åŠ å·¥ã€‚

**Petriç½‘å»ºæ¨¡**ï¼š

```python
def create_flexible_manufacturing_system():
    """æŸ”æ€§åˆ¶é€ ç³»ç»Ÿçš„Petriç½‘æ¨¡å‹"""
    pn = PetriNet()

    # 3å°æœºå™¨èµ„æº
    machines = ['M1', 'M2', 'M3']
    for machine in machines:
        pn.add_place(f'p_{machine}_idle', tokens=1)

    # 5ä¸ªå·¥ä»¶
    workpieces = ['W1', 'W2', 'W3', 'W4', 'W5']

    for workpiece in workpieces:
        # å·¥ä»¶å°±ç»ª
        pn.add_place(f'p_{workpiece}_ready', tokens=1)

        # æ¯å°æœºå™¨éƒ½å¯ä»¥åŠ å·¥ï¼ˆé€‰æ‹©ï¼‰
        for machine in machines:
            # åŠ å·¥å˜è¿
            pn.add_transition(f't_{workpiece}_on_{machine}')
            pn.add_arc(f'p_{workpiece}_ready', f't_{workpiece}_on_{machine}')
            pn.add_arc(f'p_{machine}_idle', f't_{workpiece}_on_{machine}')

            # åŠ å·¥ä¸­
            pn.add_place(f'p_{workpiece}_processing_on_{machine}', tokens=0)
            pn.add_arc(f't_{workpiece}_on_{machine}',
                       f'p_{workpiece}_processing_on_{machine}')

            # å®Œæˆå˜è¿
            pn.add_transition(f't_{workpiece}_{machine}_done')
            pn.add_arc(f'p_{workpiece}_processing_on_{machine}',
                       f't_{workpiece}_{machine}_done')
            pn.add_arc(f't_{workpiece}_{machine}_done', f'p_{machine}_idle')

        # å·¥ä»¶å®Œæˆ
        pn.add_place(f'p_{workpiece}_done', tokens=0)
        for machine in machines:
            pn.add_arc(f't_{workpiece}_{machine}_done', f'p_{workpiece}_done')

    return pn

# åˆ†æ
fms_pn = create_flexible_manufacturing_system()
analyzer = ConcurrencyAnalyzer(fms_pn, fms_pn.marking)
results = analyzer.analyze()

print("æŸ”æ€§åˆ¶é€ ç³»ç»Ÿåˆ†æï¼š")
print(f"  æœºå™¨æ•°: 3")
print(f"  å·¥ä»¶æ•°: 5")
print(f"  æœ€å¤§å¹¶å‘: {results['static_concurrency']}")
print(f"  é¢„æœŸï¼šæœ€å¤š3ä¸ªå·¥ä»¶åŒæ—¶åŠ å·¥ï¼ˆå—æœºå™¨æ•°é™åˆ¶ï¼‰")
```

---

**è¿”å›**: [02 å¹¶å‘è°ƒåº¦çš„Petriç½‘å»ºæ¨¡](README.md)

**å‚è€ƒæ–‡çŒ®**ï¼š

1. Zhou, M., & Venkatesh, K. (1998). "Modeling, Simulation, and Control of Flexible Manufacturing Systems: A Petri Net Approach"
2. Zaharia, M., et al. (2012). "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing" (Spark)
