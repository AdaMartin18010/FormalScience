# Rust 2025åˆ†å¸ƒå¼é«˜ååé›†ç¾¤æœåŠ¡å®ç°æŒ‡å—

```text
å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºä½¿ç”¨Rustå®ç°åˆ†å¸ƒå¼é«˜ååé›†ç¾¤æœåŠ¡çš„é—®é¢˜ã€‚
ç”¨æˆ·æœŸæœ›äº†è§£2025å¹´çš„Rustç”Ÿæ€ç³»ç»Ÿä¸­çš„è‘—åå¼€æºåº“å’Œè¯­è¨€ç‰¹æ€§ï¼Œç”¨äºæ„å»ºå‡ ä¸ªå…³é”®ç»„ä»¶ã€‚
æˆ‘éœ€è¦æä¾›ä¸€ä¸ªå‰ç»æ€§çš„å›ç­”ï¼ŒåŸºäºå½“å‰Rustç”Ÿæ€ç³»ç»Ÿçš„å‘å±•è¶‹åŠ¿ã€‚

é¦–å…ˆï¼Œè®©æˆ‘æ¢³ç†ä¸€ä¸‹éœ€è¦è¦†ç›–çš„å†…å®¹ï¼š

1. åˆ†å¸ƒå¼æ—¥å¿—ç›‘æ§å’Œæ£€ç´¢ç³»ç»Ÿ
   - æœ¬åœ°æ—¥å¿—å¤„ç†
   - æ—¥æœŸå’Œå®¹é‡æ»šåŠ¨
   - traceã€logã€metricçš„å¤„ç†

2. åˆ†å¸ƒå¼æ¶ˆæ¯æœåŠ¡
   - å‘å¸ƒ-è®¢é˜…æ¨¡å¼
   - HTTPå’Œæ¶ˆæ¯é˜Ÿåˆ—(MQ)ä¹‹é—´çš„è½¬æ¢

3. é›†ä¸­å¼é…ç½®ç®¡ç†
   - é…ç½®è®¢é˜…ä¸æ›´æ–°

4. å¤§æ•°æ®å¤„ç†å’ŒSQLé›†æˆ
   - å·¥ä½œæµ
   - å„ç§ä¸šåŠ¡æµç¨‹ï¼šè®¢å•ã€æ”¯ä»˜ã€ç‰©æµç­‰

è™½ç„¶æˆ‘ä¸èƒ½å‡†ç¡®é¢„æµ‹2025å¹´çš„å…·ä½“åº“ï¼Œä½†æˆ‘å¯ä»¥åŸºäºå½“å‰å‘å±•è¶‹åŠ¿é¢„æµ‹å¯èƒ½çš„æ¼”è¿›æ–¹å‘ã€‚
æˆ‘ä¼šå°½é‡ç»™å‡ºåˆç†çš„é¢„æµ‹ã€‚
```

## ğŸ“‹ ç›®å½•

- [1 æ€»ä½“æ¶æ„è®¾è®¡](#1-æ€»ä½“æ¶æ„è®¾è®¡)
- [2 ä¸€ã€åˆ†å¸ƒå¼æ—¥å¿—ç›‘æ§å’Œæ£€ç´¢](#2-ä¸€åˆ†å¸ƒå¼æ—¥å¿—ç›‘æ§å’Œæ£€ç´¢)
  - [2.1 æ ¸å¿ƒåº“ä¸æ¡†æ¶](#21-æ ¸å¿ƒåº“ä¸æ¡†æ¶)
  - [2.2 æœ¬åœ°æ—¥å¿—ä¸æ»šåŠ¨å®ç°](#22-æœ¬åœ°æ—¥å¿—ä¸æ»šåŠ¨å®ç°)
  - [2.3 åˆ†å¸ƒå¼æ—¥å¿—èšåˆä¸æ£€ç´¢](#23-åˆ†å¸ƒå¼æ—¥å¿—èšåˆä¸æ£€ç´¢)
- [3 äºŒã€åˆ†å¸ƒå¼æ¶ˆæ¯æœåŠ¡](#3-äºŒåˆ†å¸ƒå¼æ¶ˆæ¯æœåŠ¡)
  - [3.1 -æ ¸å¿ƒåº“ä¸æ¡†æ¶](#31--æ ¸å¿ƒåº“ä¸æ¡†æ¶)
  - [3.2 æ¶ˆæ¯å‘å¸ƒè®¢é˜…æœåŠ¡å®ç°](#32-æ¶ˆæ¯å‘å¸ƒè®¢é˜…æœåŠ¡å®ç°)
- [4 ä¸‰ã€é›†ä¸­å¼é…ç½®ç®¡ç†å’Œè®¢é˜…æ›´æ–°](#4-ä¸‰é›†ä¸­å¼é…ç½®ç®¡ç†å’Œè®¢é˜…æ›´æ–°)
  - [4.1 -æ ¸å¿ƒåº“ä¸æ¡†æ¶-](#41--æ ¸å¿ƒåº“ä¸æ¡†æ¶)
  - [4.2 é…ç½®ç®¡ç†æœåŠ¡å®ç°](#42-é…ç½®ç®¡ç†æœåŠ¡å®ç°)
- [5 å››ã€å¤§æ•°æ®å¤„ç†å’Œæ•°æ®åº“é›†æˆ](#5-å››å¤§æ•°æ®å¤„ç†å’Œæ•°æ®åº“é›†æˆ)
  - [5.1 -æ ¸å¿ƒåº“ä¸æ¡†æ¶-](#51--æ ¸å¿ƒåº“ä¸æ¡†æ¶)
  - [5.2 æ•°æ®å¤„ç†å’Œå·¥ä½œæµå®ç°](#52-æ•°æ®å¤„ç†å’Œå·¥ä½œæµå®ç°)
- [6 éƒ¨ç½²ä¸è¿ç»´](#6-éƒ¨ç½²ä¸è¿ç»´)
- [7 æ€»ç»“](#7-æ€»ç»“)

---

## 1 æ€»ä½“æ¶æ„è®¾è®¡

ä»¥ä¸‹æ–¹æ¡ˆåŸºäºRustè¯­è¨€ç”Ÿæ€ä¸­é¢„è®¡åœ¨2025å¹´æœ€æˆç†Ÿå’Œå¹¿æ³›é‡‡ç”¨çš„æ¡†æ¶å’Œåº“ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„åˆ†å¸ƒå¼é«˜ååé›†ç¾¤æœåŠ¡ã€‚

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            å‰ç«¯æœåŠ¡é›†ç¾¤                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         APIç½‘å…³ (Envoy + Rust)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ ¸å¿ƒæœåŠ¡é›†ç¾¤                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ—¥å¿—ç›‘æ§æœåŠ¡ â”‚  æ¶ˆæ¯æœåŠ¡   â”‚ é…ç½®ç®¡ç†æœåŠ¡   â”‚ å¤§æ•°æ®å¤„ç†æœåŠ¡   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“                 â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenTelemetryâ”‚    Kafka    â”‚   Consul/etcd  â”‚    ClickHouse   â”‚
â”‚  Elasticsearchâ”‚    NATS     â”‚                â”‚    PostgreSQL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2 ä¸€ã€åˆ†å¸ƒå¼æ—¥å¿—ç›‘æ§å’Œæ£€ç´¢

### 2.1 æ ¸å¿ƒåº“ä¸æ¡†æ¶

```rust
// Cargo.toml ä¾èµ–ç¤ºä¾‹
[dependencies]
# å¼‚æ­¥è¿è¡Œæ—¶
tokio = { version = "2.0", features = ["full"] }
# é«˜æ€§èƒ½æ—¥å¿—æ¡†æ¶ (tracingæ¼”è¿›ç‰ˆ)
opentelemetry-tracing = "2.0"
tracing-subscriber = "1.0"
# æ—¥å¿—èšåˆä¸å‘é€
opentelemetry-otlp = "2.0"
# é«˜æ€§èƒ½æ—¥å¿—æ»šåŠ¨
tracing-appender = "1.0"
# Elasticsearchå®¢æˆ·ç«¯
elasticsearch = "3.0"
# æŒ‡æ ‡ç›‘æ§
metrics = "1.0"
metrics-exporter-prometheus = "1.0"
```

### 2.2 æœ¬åœ°æ—¥å¿—ä¸æ»šåŠ¨å®ç°

```rust
use std::path::Path;
use tokio::time::{Duration, interval};
use tracing::{info, error, warn, debug, span, Level};
use tracing_appender::rolling::{RollingFileAppender, Rotation, LogFile};
use tracing_subscriber::{fmt, layer::SubscriberExt, util::SubscriberInitExt};
use opentelemetry::trace::TraceError;
use opentelemetry_otlp::WithExportConfig;
use opentelemetry_sdk::{trace, Resource};
use opentelemetry_semantic_conventions as semcov;

// æ—¥å¿—æœåŠ¡åˆå§‹åŒ–
async fn init_logging(service_name: &str, log_dir: &Path) -> Result<(), TraceError> {
    // 1. æœ¬åœ°æ–‡ä»¶æ—¥å¿—é…ç½® - æŒ‰æ—¥æœŸå’Œå¤§å°æ»šåŠ¨
    let file_appender = RollingFileAppender::builder()
        .rotation(Rotation::DAILY) // æ¯æ—¥æ»šåŠ¨
        .filename_prefix(service_name)
        .filename_suffix("log")
        .max_log_files(30) // ä¿ç•™30å¤©æ—¥å¿—
        .build(log_dir)
        .expect("Failed to create rolling file appender");
    
    // 2. è®¾ç½®æ—¥å¿—æ ¼å¼å’Œè¿‡æ»¤çº§åˆ«
    let file_layer = fmt::layer()
        .with_writer(file_appender)
        .with_ansi(false) // æ–‡ä»¶æ—¥å¿—ä¸éœ€è¦ANSIé¢œè‰²
        .with_filter(tracing_subscriber::filter::EnvFilter::new(
            std::env::var("RUST_LOG").unwrap_or_else(|_| "info".into())
        ));
    
    // 3. æ§åˆ¶å°æ—¥å¿—
    let stdout_layer = fmt::layer()
        .with_filter(tracing_subscriber::filter::EnvFilter::new(
            std::env::var("RUST_LOG").unwrap_or_else(|_| "debug".into())
        ));
    
    // 4. OpenTelemetryé…ç½® - åˆ†å¸ƒå¼è¿½è¸ª
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint("http://otel-collector:4317")
        )
        .with_trace_config(
            trace::config()
                .with_resource(
                    Resource::new(vec![
                        semcov::resource::SERVICE_NAME.string(service_name.to_string()),
                        semcov::resource::SERVICE_VERSION.string(env!("CARGO_PKG_VERSION").to_string()),
                    ])
                )
        )
        .install_batch(opentelemetry_sdk::runtime::Tokio)?;
    
    let telemetry_layer = tracing_opentelemetry::layer().with_tracer(tracer);
    
    // 5. ç»„åˆæ‰€æœ‰layer
    tracing_subscriber::registry()
        .with(file_layer)
        .with(stdout_layer)
        .with(telemetry_layer)
        .init();
    
    // 6. é…ç½®æŒ‡æ ‡æ”¶é›†
    let metrics_recorder = metrics_exporter_prometheus::PrometheusBuilder::new()
        .with_http_listener(([0, 0, 0, 0], 9000))
        .build()
        .expect("Failed to create Prometheus metrics exporter");
    
    metrics::set_boxed_recorder(Box::new(metrics_recorder))
        .expect("Failed to set metrics recorder");
    
    Ok(())
}

// ç¤ºä¾‹ä½¿ç”¨
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let log_dir = Path::new("/var/log/myservice");
    init_logging("order-service", log_dir).await?;
    
    // è®°å½•ä¸åŒçº§åˆ«æ—¥å¿—
    let root_span = span!(Level::INFO, "request_processing", 
                           service = "order-service", 
                           request_id = format!("req-{}", uuid::Uuid::new_v4()));
    
    let _enter = root_span.enter();
    
    info!(target: "api", "å¤„ç†æ–°è®¢å•è¯·æ±‚");
    
    let process_span = span!(Level::DEBUG, "order_validation");
    let _process_guard = process_span.enter();
    
    debug!("éªŒè¯è®¢å•æ•°æ®");
    
    // è®°å½•æŒ‡æ ‡
    metrics::counter!("orders_processed_total", 1);
    metrics::gauge!("active_users", 42.0);
    metrics::histogram!("order_processing_time", 157.0);
    
    // å‘¨æœŸæ€§ä»»åŠ¡ - æ—¥å¿—è½®è½¬æ£€æŸ¥
    let mut interval = interval(Duration::from_secs(3600));
    tokio::spawn(async move {
        loop {
            interval.tick().await;
            info!("æ‰§è¡Œæ¯å°æ—¶æ—¥å¿—çŠ¶æ€æ£€æŸ¥");
            // è¿™é‡Œå¯ä»¥æ·»åŠ æ—¥å¿—æ¸…ç†æˆ–çŠ¶æ€æ£€æŸ¥é€»è¾‘
        }
    });
    
    Ok(())
}
```

### 2.3 åˆ†å¸ƒå¼æ—¥å¿—èšåˆä¸æ£€ç´¢

```rust
use elasticsearch::{
    http::transport::Transport,
    Elasticsearch, 
    SearchParts,
    indices::IndicesCreateParts,
};
use serde_json::{json, Value};
use chrono::{Utc, Duration};

struct LogSearchService {
    client: Elasticsearch,
    index_prefix: String,
}

impl LogSearchService {
    async fn new(elasticsearch_url: &str, index_prefix: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let transport = Transport::single_node(elasticsearch_url)?;
        let client = Elasticsearch::new(transport);
        
        Ok(Self {
            client,
            index_prefix: index_prefix.to_string(),
        })
    }
    
    // åˆ›å»ºæ—¥å¿—ç´¢å¼•
    async fn create_index(&self, service_name: &str) -> Result<(), Box<dyn std::error::Error>> {
        let index_name = format!("{}-{}-{}", 
                                 self.index_prefix, 
                                 service_name,
                                 Utc::now().format("%Y-%m-%d"));
        
        let response = self.client
            .indices()
            .create(IndicesCreateParts::Index(&index_name))
            .body(json!({
                "settings": {
                    "number_of_shards": 3,
                    "number_of_replicas": 2
                },
                "mappings": {
                    "properties": {
                        "@timestamp": { "type": "date" },
                        "service": { "type": "keyword" },
                        "level": { "type": "keyword" },
                        "message": { "type": "text" },
                        "trace_id": { "type": "keyword" },
                        "span_id": { "type": "keyword" },
                        "metadata": { "type": "object", "dynamic": true }
                    }
                }
            }))
            .send()
            .await?;
            
        if !response.status_code().is_success() {
            error!("åˆ›å»ºç´¢å¼•å¤±è´¥: {:?}", response.text().await?);
            return Err("åˆ›å»ºç´¢å¼•å¤±è´¥".into());
        }
        
        Ok(())
    }
    
    // æœç´¢æ—¥å¿—
    async fn search_logs(&self, 
                        service_name: &str,
                        query: &str, 
                        level: Option<&str>,
                        time_range_hours: i64,
                        limit: usize) -> Result<Vec<Value>, Box<dyn std::error::Error>> {
        
        let now = Utc::now();
        let start_time = now - Duration::hours(time_range_hours);
        
        // æ„å»ºç´¢å¼•åç§°é€šé…ç¬¦ï¼Œä¾‹å¦‚ "logs-order-service-*" 
        let index_pattern = format!("{}-{}-*", self.index_prefix, service_name);
        
        // æ„å»ºæŸ¥è¯¢
        let mut query_obj = json!({
            "bool": {
                "must": [
                    { "query_string": { "query": query } },
                    {
                        "range": {
                            "@timestamp": {
                                "gte": start_time.to_rfc3339(),
                                "lte": now.to_rfc3339()
                            }
                        }
                    }
                ]
            }
        });
        
        // å¦‚æœæŒ‡å®šäº†æ—¥å¿—çº§åˆ«ï¼Œæ·»åŠ è¿‡æ»¤æ¡ä»¶
        if let Some(log_level) = level {
            if let Some(bool_query) = query_obj.get_mut("bool") {
                if let Some(must) = bool_query.get_mut("must").and_then(|m| m.as_array_mut()) {
                    must.push(json!({
                        "term": { "level": log_level }
                    }));
                }
            }
        }
        
        let response = self.client
            .search(SearchParts::Index(&[&index_pattern]))
            .body(json!({
                "query": query_obj,
                "sort": [
                    { "@timestamp": { "order": "desc" } }
                ],
                "size": limit
            }))
            .send()
            .await?;
        
        let response_body = response.json::<Value>().await?;
        
        let hits = response_body["hits"]["hits"]
            .as_array()
            .ok_or("Invalid response format")?;
        
        let mut results = Vec::with_capacity(hits.len());
        for hit in hits {
            if let Some(source) = hit["_source"].as_object() {
                results.push(hit["_source"].clone());
            }
        }
        
        Ok(results)
    }
}
```

## 3 äºŒã€åˆ†å¸ƒå¼æ¶ˆæ¯æœåŠ¡

### 3.1 -æ ¸å¿ƒåº“ä¸æ¡†æ¶

```rust
// Cargo.toml
[dependencies]
# å¼‚æ­¥è¿è¡Œæ—¶
tokio = { version = "2.0", features = ["full"] }
# NATSæ¶ˆæ¯é˜Ÿåˆ—å®¢æˆ·ç«¯
async-nats = "1.0"
# Apache Kafkaå®¢æˆ·ç«¯
rdkafka = { version = "1.0", features = ["ssl", "sasl"] }
# HTTPæœåŠ¡å™¨
axum = "1.0"
# åºåˆ—åŒ–
serde = { version = "2.0", features = ["derive"] }
serde_json = "2.0"
# åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ
opentelemetry = "2.0"
tracing = "1.0"
```

### 3.2 æ¶ˆæ¯å‘å¸ƒè®¢é˜…æœåŠ¡å®ç°

```rust
use async_nats::{Client, Message, Subscriber};
use axum::{
    routing::{get, post},
    Router, Extension,
    extract::{Path, Json},
    response::IntoResponse,
    http::StatusCode,
};
use rdkafka::{
    config::ClientConfig,
    producer::{FutureProducer, FutureRecord},
    consumer::{StreamConsumer, Consumer},
    message::{Message as KafkaMessage, OwnedHeaders},
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{RwLock, Mutex};
use tracing::{info, error, warn, instrument};
use opentelemetry::trace::{Span, TraceContextExt};

#[derive(Serialize, Deserialize, Clone, Debug)]
struct MessagePayload {
    topic: String,
    content_type: String,
    headers: HashMap<String, String>,
    data: serde_json::Value,
    trace_id: Option<String>,
}

struct MessageBrokerService {
    nats_client: Client,
    kafka_producer: FutureProducer,
    // ç”¨äºä¿å­˜æ¶ˆæ¯è½¬æ¢å’Œè·¯ç”±è§„åˆ™
    routing_rules: Arc<RwLock<HashMap<String, Vec<String>>>>,
}

impl MessageBrokerService {
    async fn new(
        nats_url: &str,
        kafka_brokers: &str,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        // è¿æ¥NATS
        let nats_client = async_nats::connect(nats_url).await?;
        
        // é…ç½®Kafkaç”Ÿäº§è€…
        let kafka_producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", kafka_brokers)
            .set("message.timeout.ms", "5000")
            .set("queue.buffering.max.messages", "100000")
            .set("compression.type", "lz4")
            .create()?;
            
        Ok(Self {
            nats_client,
            kafka_producer,
            routing_rules: Arc::new(RwLock::new(HashMap::new())),
        })
    }
    
    // æ·»åŠ è·¯ç”±è§„åˆ™
    async fn add_routing_rule(&self, source: String, destinations: Vec<String>) {
        let mut rules = self.routing_rules.write().await;
        rules.insert(source, destinations);
    }
    
    // ä»HTTPå‘å¸ƒåˆ°æ¶ˆæ¯é˜Ÿåˆ—
    #[instrument(skip(self, payload), fields(trace_id = %payload.trace_id.clone().unwrap_or_default()))]
    async fn publish_from_http(
        &self, 
        payload: MessagePayload
    ) -> Result<(), Box<dyn std::error::Error>> {
        info!(topic = %payload.topic, "æ¥æ”¶åˆ°HTTPæ¶ˆæ¯å‘å¸ƒè¯·æ±‚");
        
        // æ£€æŸ¥æ­¤ä¸»é¢˜æ˜¯å¦æœ‰è·¯ç”±è§„åˆ™
        let rules = self.routing_rules.read().await;
        let destinations = rules.get(&payload.topic).cloned().unwrap_or_else(|| {
            vec![payload.topic.clone()] // é»˜è®¤å‘é€åˆ°åŒåä¸»é¢˜
        });
        
        // è·å–å½“å‰traceä¸Šä¸‹æ–‡
        let span = tracing::Span::current();
        let context = span.context();
        let trace_id = context.span().span_context().trace_id().to_string();
        
        for dest in destinations {
            if dest.starts_with("nats.") {
                // å‘å¸ƒåˆ°NATS
                let nats_topic = dest.trim_start_matches("nats.");
                let data_bytes = serde_json::to_vec(&payload.data)?;
                
                // æ·»åŠ è·Ÿè¸ªIDåˆ°æ¶ˆæ¯å¤´
                let mut headers = HashMap::new();
                for (k, v) in &payload.headers {
                    headers.insert(k.clone(), v.clone());
                }
                headers.insert("trace-id".to_string(), trace_id.clone());
                
                let header_json = serde_json::to_string(&headers)?;
                
                // å‘å¸ƒæ¶ˆæ¯åˆ°NATS
                self.nats_client.publish(
                    nats_topic,
                    payload.content_type.into(),
                    header_json.into(),
                    data_bytes.into()
                ).await?;
                
                info!(nats_topic = %nats_topic, "æ¶ˆæ¯å‘å¸ƒåˆ°NATS");
            } else if dest.starts_with("kafka.") {
                // å‘å¸ƒåˆ°Kafka
                let kafka_topic = dest.trim_start_matches("kafka.");
                let data_bytes = serde_json::to_vec(&payload.data)?;
                
                // å‡†å¤‡Kafkaæ¶ˆæ¯å¤´
                let mut headers = OwnedHeaders::new();
                for (k, v) in &payload.headers {
                    headers = headers.add(k, v.as_bytes());
                }
                headers = headers.add("trace-id", trace_id.as_bytes());
                headers = headers.add("content-type", payload.content_type.as_bytes());
                
                // å‘å¸ƒæ¶ˆæ¯åˆ°Kafka
                let record = FutureRecord::to(kafka_topic)
                    .headers(headers)
                    .payload(&data_bytes);
                
                self.kafka_producer.send(record, std::time::Duration::from_secs(5)).await?;
                info!(kafka_topic = %kafka_topic, "æ¶ˆæ¯å‘å¸ƒåˆ°Kafka");
            }
        }
        
        Ok(())
    }
    
    // è®¢é˜…æ¶ˆæ¯å¹¶è½¬å‘åˆ°HTTP webhook
    async fn subscribe_and_forward_to_http(
        &self,
        topic: &str,
        webhook_url: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        if topic.starts_with("nats.") {
            // ä»NATSè®¢é˜…
            let nats_topic = topic.trim_start_matches("nats.");
            let mut subscription = self.nats_client.subscribe(nats_topic.into()).await?;
            
            let client = reqwest::Client::new();
            let webhook = webhook_url.to_string();
            
            tokio::spawn(async move {
                while let Some(msg) = subscription.next().await {
                    let payload = process_nats_message(msg);
                    
                    // è½¬å‘åˆ°HTTP webhook
                    if let Err(e) = client.post(&webhook)
                        .json(&payload)
                        .send()
                        .await {
                        error!(error = %e, "è½¬å‘NATSæ¶ˆæ¯åˆ°webhookå¤±è´¥");
                    }
                }
            });
            
        } else if topic.starts_with("kafka.") {
            // ä»Kafkaè®¢é˜…
            let kafka_topic = topic.trim_start_matches("kafka.");
            
            let consumer: StreamConsumer = ClientConfig::new()
                .set("group.id", "http-forwarder")
                .set("bootstrap.servers", "kafka:9092")
                .set("enable.auto.commit", "true")
                .set("auto.offset.reset", "latest")
                .create()?;
                
            consumer.subscribe(&[kafka_topic])?;
            
            let client = reqwest::Client::new();
            let webhook = webhook_url.to_string();
            
            tokio::spawn(async move {
                loop {
                    match consumer.recv().await {
                        Ok(msg) => {
                            if let Some(payload) = process_kafka_message(&msg) {
                                // è½¬å‘åˆ°HTTP webhook
                                if let Err(e) = client.post(&webhook)
                                    .json(&payload)
                                    .send()
                                    .await {
                                    error!(error = %e, "è½¬å‘Kafkaæ¶ˆæ¯åˆ°webhookå¤±è´¥");
                                }
                            }
                        }
                        Err(e) => {
                            error!(error = %e, "æ¥æ”¶Kafkaæ¶ˆæ¯å¤±è´¥");
                            tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
                        }
                    }
                }
            });
        }
        
        Ok(())
    }
}

// å¤„ç†NATSæ¶ˆæ¯
fn process_nats_message(msg: Message) -> MessagePayload {
    // å¤„ç†æ¶ˆæ¯å¤´
    let headers_str = String::from_utf8_lossy(&msg.headers.unwrap_or_default());
    let headers: HashMap<String, String> = 
        serde_json::from_str(&headers_str).unwrap_or_default();
    
    // å¤„ç†æ¶ˆæ¯æ•°æ®
    let data: serde_json::Value = 
        serde_json::from_slice(&msg.payload).unwrap_or(serde_json::Value::Null);
    
    // è·å–è¿½è¸ªID
    let trace_id = headers.get("trace-id").cloned();
    
    MessagePayload {
        topic: msg.subject.to_string(),
        content_type: msg.description.unwrap_or_default(),
        headers,
        data,
        trace_id,
    }
}

// å¤„ç†Kafkaæ¶ˆæ¯
fn process_kafka_message<'a, K: KafkaMessage>(msg: &'a K) -> Option<MessagePayload> {
    // å¤„ç†æ¶ˆæ¯å¤´
    let mut headers = HashMap::new();
    if let Some(kafka_headers) = msg.headers() {
        for i in 0..kafka_headers.count() {
            if let Some((name, value)) = kafka_headers.get(i) {
                if let Some(value_str) = value.map(|v| String::from_utf8_lossy(v).to_string()) {
                    headers.insert(name.to_string(), value_str);
                }
            }
        }
    }
    
    // å¤„ç†æ¶ˆæ¯æ•°æ®
    let payload = msg.payload()?;
    let data: serde_json::Value = 
        serde_json::from_slice(payload).unwrap_or(serde_json::Value::Null);
    
    // è·å–è·Ÿè¸ªIDå’Œå†…å®¹ç±»å‹
    let trace_id = headers.get("trace-id").cloned();
    let content_type = headers.get("content-type")
        .cloned()
        .unwrap_or_else(|| "application/json".to_string());
    
    Some(MessagePayload {
        topic: msg.topic().to_string(),
        content_type,
        headers,
        data,
        trace_id,
    })
}

// HTTP APIè·¯ç”±
async fn setup_api_routes(message_service: Arc<MessageBrokerService>) -> Router {
    Router::new()
        .route("/api/v1/messages", post(publish_message))
        .route("/api/v1/subscriptions", post(create_subscription))
        .route("/api/v1/routes", post(create_route))
        .layer(Extension(message_service))
}

// HTTPå¤„ç†ç¨‹åº
async fn publish_message(
    Extension(service): Extension<Arc<MessageBrokerService>>,
    Json(payload): Json<MessagePayload>,
) -> impl IntoResponse {
    match service.publish_from_http(payload).await {
        Ok(_) => StatusCode::ACCEPTED,
        Err(e) => {
            error!(error = %e, "å‘å¸ƒæ¶ˆæ¯å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR
        }
    }
}

#[derive(Deserialize)]
struct SubscriptionRequest {
    topic: String,
    webhook_url: String,
}

async fn create_subscription(
    Extension(service): Extension<Arc<MessageBrokerService>>,
    Json(req): Json<SubscriptionRequest>,
) -> impl IntoResponse {
    match service.subscribe_and_forward_to_http(&req.topic, &req.webhook_url).await {
        Ok(_) => StatusCode::CREATED,
        Err(e) => {
            error!(error = %e, "åˆ›å»ºè®¢é˜…å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR
        }
    }
}

#[derive(Deserialize)]
struct RouteRequest {
    source: String,
    destinations: Vec<String>,
}

async fn create_route(
    Extension(service): Extension<Arc<MessageBrokerService>>,
    Json(req): Json<RouteRequest>,
) -> impl IntoResponse {
    service.add_routing_rule(req.source, req.destinations).await;
    StatusCode::CREATED
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–æ—¥å¿—å’Œè¿½è¸ª
    init_logging("message-broker", std::path::Path::new("/var/log/message-broker")).await?;
    
    // åˆ›å»ºæ¶ˆæ¯æœåŠ¡
    let message_service = Arc::new(
        MessageBrokerService::new(
            "nats://nats:4222",
            "kafka:9092"
        ).await?
    );
    
    // è®¾ç½®é»˜è®¤è·¯ç”±è§„åˆ™
    message_service.add_routing_rule(
        "http.orders".to_string(), 
        vec!["kafka.orders".to_string(), "nats.orders.new".to_string()]
    ).await;
    
    // å¯åŠ¨HTTP API
    let app = setup_api_routes(message_service).await;
    let listener = tokio::net::TcpListener::bind("0.0.0.0:8080").await?;
    info!("æ¶ˆæ¯æœåŠ¡HTTP APIå·²å¯åŠ¨åœ¨ 0.0.0.0:8080");
    axum::serve(listener, app).await?;
    
    Ok(())
}
```

## 4 ä¸‰ã€é›†ä¸­å¼é…ç½®ç®¡ç†å’Œè®¢é˜…æ›´æ–°

### 4.1 -æ ¸å¿ƒåº“ä¸æ¡†æ¶-

```rust
// Cargo.toml
[dependencies]
# å¼‚æ­¥è¿è¡Œæ—¶
tokio = { version = "2.0", features = ["full"] }
# HTTPæœåŠ¡
axum = "1.0"
# Consulå®¢æˆ·ç«¯
consul-rs = "1.0"
# ETCDå®¢æˆ·ç«¯
etcd-client = "1.0"
# åºåˆ—åŒ–
serde = { version = "2.0", features = ["derive"] }
serde_json = "2.0"
# é…ç½®ç®¡ç†
config = "1.0"
# è§‚å¯Ÿè€…æ¨¡å¼å®ç°
tokio-watch = "1.0"
```

### 4.2 é…ç½®ç®¡ç†æœåŠ¡å®ç°

```rust
use axum::{
    routing::{get, post, put},
    Router, Extension, Json,
    extract::{Path, Query},
    response::{IntoResponse, Json as JsonResponse},
    http::StatusCode,
};
use config::{Config, ConfigError, File, Environment};
use etcd_client::{Client as EtcdClient, GetOptions, PutOptions, WatchOptions};
use consul_rs::{Client as ConsulClient, KV, QueryOptions, QueryMeta};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{broadcast, RwLock, watch};
use tokio::time::{Duration, interval};
use tracing::{info, error, warn, instrument};

// é…ç½®æ•°æ®æ¨¡å‹
#[derive(Clone, Debug, Serialize, Deserialize)]
struct ConfigItem {
    key: String,
    value: serde_json::Value,
    version: u64,
    last_updated: chrono::DateTime<chrono::Utc>,
    metadata: HashMap<String, String>,
}

// é…ç½®å˜æ›´äº‹ä»¶
#[derive(Clone, Debug, Serialize, Deserialize)]
struct ConfigChangeEvent {
    key: String,
    old_value: Option<serde_json::Value>,
    new_value: Option<serde_json::Value>,
    version: u64,
    timestamp: chrono::DateTime<chrono::Utc>,
}

// é…ç½®æä¾›è€…ç‰¹å¾ - æ”¯æŒä¸åŒé…ç½®åç«¯
#[async_trait::async_trait]
trait ConfigProvider: Send + Sync {
    async fn get(&self, key: &str) -> Result<Option<ConfigItem>, Box<dyn std::error::Error>>;
    async fn put(&self, item: ConfigItem) -> Result<(), Box<dyn std::error::Error>>;
    async fn delete(&self, key: &str) -> Result<(), Box<dyn std::error::Error>>;
    async fn list(&self, prefix: &str) -> Result<Vec<ConfigItem>, Box<dyn std::error::Error>>;
    async fn watch(&self, prefix: &str, tx: broadcast::Sender<ConfigChangeEvent>) -> Result<(), Box<dyn std::error::Error>>;
}

// etcdé…ç½®æä¾›è€…å®ç°
struct EtcdConfigProvider {
    client: EtcdClient,
    namespace: String,
}

#[async_trait::async_trait]
impl ConfigProvider for EtcdConfigProvider {
    async fn get(&self, key: &str) -> Result<Option<ConfigItem>, Box<dyn std::error::Error>> {
        let full_key = format!("{}/{}", self.namespace, key);
        let resp = self.client.get(full_key, None).await?;
        
        if let Some(kv) = resp.kvs().first() {
            let value: serde_json::Value = serde_json::from_slice(kv.value())?;
            let config_item = ConfigItem {
                key: String::from_utf8(kv.key().to_vec())?,
                value,
                version: kv.mod_revision(),
                last_updated: chrono::Utc::now(),
                metadata: HashMap::new(),
            };
            Ok(Some(config_item))
        } else {
            Ok(None)
        }
    }
    
    async fn put(&self, item: ConfigItem) -> Result<(), Box<dyn std::error::Error>> {
        let full_key = format!("{}/{}", self.namespace, item.key);
        let value = serde_json::to_vec(&item.value)?;
        self.client.put(full_key, value, None).await?;
        Ok(())
    }
    
    async fn delete(&self, key: &str) -> Result<(), Box<dyn std::error::Error>> {
        let full_key = format!("{}/{}", self.namespace, key);
        self.client.delete(full_key, None).await?;
        Ok(())
    }
    
    async fn list(&self, prefix: &str) -> Result<Vec<ConfigItem>, Box<dyn std::error::Error>> {
        let full_prefix = format!("{}/{}", self.namespace, prefix);
        let resp = self.client
            .get(full_prefix, Some(GetOptions::new().with_prefix()))
            .await?;
        
        let mut items = Vec::new();
        for kv in resp.kvs() {
            let key = String::from_utf8(kv.key().to_vec())?;
            let value: serde_json::Value = serde_json::from_slice(kv.value())?;
            
            items.push(ConfigItem {
                key,
                value,
                version: kv.mod_revision(),
                last_updated: chrono::Utc::now(),
                metadata: HashMap::new(),
            });
        }
        
        Ok(items)
    }
    
    async fn watch(&self, prefix: &str, tx: broadcast::Sender<ConfigChangeEvent>) -> Result<(), Box<dyn std::error::Error>> {
        let full_prefix = format!("{}/{}", self.namespace, prefix);
        let (mut watcher, mut stream) = self.client
            .watch(full_prefix, Some(WatchOptions::new().with_prefix()))
            .await?;
        
        tokio::spawn(async move {
            while let Some(resp) = stream.message().await.unwrap() {
                for event in resp.events() {
                    let event_type = event.event_type();
                    if let Some(kv) = event.kv() {
                        let key = String::from_utf8_lossy(kv.key()).to_string();
                        let new_value = if event_type == etcd_client::EventType::Delete {
                            None
                        } else {
                            Some(serde_json::from_slice(kv.value()).unwrap_or(serde_json::Value::Null))
                        };
                        
                        let change_event = ConfigChangeEvent {
                            key,
                            old_value: None, // etcdä¸æä¾›æ—§å€¼
                            new_value,
                            version: kv.mod_revision(),
                            timestamp: chrono::Utc::now(),
                        };
                        
                        let _ = tx.send(change_event);
                    }
                }
            }
        });
        
        Ok(())
    }
}

// é…ç½®ç®¡ç†æœåŠ¡
struct ConfigService {
    provider: Box<dyn ConfigProvider>,
    cache: Arc<RwLock<HashMap<String, ConfigItem>>>,
    change_tx: broadcast::Sender<ConfigChangeEvent>,
}

impl ConfigService {
    fn new(provider: Box<dyn ConfigProvider>) -> Self {
        let (change_tx, _) = broadcast::channel(1000);
        
        Self {
            provider,
            cache: Arc::new(RwLock::new(HashMap::new())),
            change_tx,
        }
    }
    
    async fn init(&self) -> Result<(), Box<dyn std::error::Error>> {
        // åŠ è½½æ‰€æœ‰é…ç½®åˆ°ç¼“å­˜
        let items = self.provider.list("").await?;
        let mut cache = self.cache.write().await;
        
        for item in items {
            cache.insert(item.key.clone(), item);
        }
        
        // è®¾ç½®é…ç½®å˜æ›´ç›‘è§†
        let tx = self.change_tx.clone();
        let provider = &self.provider;
        provider.watch("", tx).await?;
        
        // ç¼“å­˜æ›´æ–°å¤„ç†
        let cache_ref = self.cache.clone();
        let mut rx = self.change_tx.subscribe();
        
        tokio::spawn(async move {
            while let Ok(event) = rx.recv().await {
                let mut cache = cache_ref.write().await;
                
                if let Some(new_value) = &event.new_value {
                    // æ›´æ–°æˆ–æ·»åŠ 
                    cache.insert(event.key.clone(), ConfigItem {
                        key: event.key.clone(),
                        value: new_value.clone(),
                        version: event.version,
                        last_updated: event.timestamp,
                        metadata: HashMap::new(),
                    });
                } else {
                    // åˆ é™¤
                    cache.remove(&event.key);
                }
            }
        });
        
        Ok(())
    }
    
    // è·å–é…ç½®é¡¹
    async fn get_config(&self, key: &str) -> Result<Option<ConfigItem>, Box<dyn std::error::Error>> {
        // é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        {
            let cache = self.cache.read().await;
            if let Some(item) = cache.get(key) {
                return Ok(Some(item.clone()));
            }
        }
        
        // ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æºè·å–
        let result = self.provider.get(key).await?;
        
        // æ›´æ–°ç¼“å­˜
        if let Some(item) = &result {
            let mut cache = self.cache.write().await;
            cache.insert(key.to_string(), item.clone());
        }
        
        Ok(result)
    }
    
    // è®¾ç½®é…ç½®é¡¹
    async fn set_config(&self, item: ConfigItem) -> Result<(), Box<dyn std::error::Error>> {
        // è·å–æ—§å€¼ç”¨äºäº‹ä»¶é€šçŸ¥
        let old_value = {
            let cache = self.cache.read().await;
            cache.get(&item.key).map(|i| i.value.clone())
        };
        
        // æ›´æ–°å­˜å‚¨
        self.provider.put(item.clone()).await?;

//è®©æˆ‘ç»§ç»­å®Œæˆé…ç½®ç®¡ç†æœåŠ¡çš„å®ç°éƒ¨åˆ†ï¼Œç„¶åå†è¿›å…¥å¤§æ•°æ®å¤„ç†å’Œæ•°æ®åº“é›†æˆéƒ¨åˆ†ã€‚
//æˆ‘éœ€è¦å®ŒæˆConfigServiceçš„å…¶ä½™æ–¹æ³•ï¼Œç„¶åæ·»åŠ HTTP APIè·¯ç”±å’Œå¤„ç†ç¨‹åºã€‚

        // æ›´æ–°ç¼“å­˜
        {
            let mut cache = self.cache.write().await;
            cache.insert(item.key.clone(), item.clone());
        }
        
        // å‘é€å˜æ›´äº‹ä»¶
        let change_event = ConfigChangeEvent {
            key: item.key,
            old_value,
            new_value: Some(item.value),
            version: item.version,
            timestamp: chrono::Utc::now(),
        };
        
        let _ = self.change_tx.send(change_event);
        
        Ok(())
    }
    
    // åˆ é™¤é…ç½®é¡¹
    async fn delete_config(&self, key: &str) -> Result<(), Box<dyn std::error::Error>> {
        // è·å–æ—§å€¼ç”¨äºäº‹ä»¶é€šçŸ¥
        let old_value = {
            let cache = self.cache.read().await;
            cache.get(key).map(|i| i.value.clone())
        };
        
        // ä»å­˜å‚¨ä¸­åˆ é™¤
        self.provider.delete(key).await?;
        
        // æ›´æ–°ç¼“å­˜
        {
            let mut cache = self.cache.write().await;
            cache.remove(key);
        }
        
        // å‘é€å˜æ›´äº‹ä»¶
        let change_event = ConfigChangeEvent {
            key: key.to_string(),
            old_value,
            new_value: None,
            version: 0,
            timestamp: chrono::Utc::now(),
        };
        
        let _ = self.change_tx.send(change_event);
        
        Ok(())
    }
    
    // åˆ—å‡ºé…ç½®é¡¹
    async fn list_configs(&self, prefix: &str) -> Result<Vec<ConfigItem>, Box<dyn std::error::Error>> {
        // ä»ç¼“å­˜ä¸­è¯»å–åŒ¹é…å‰ç¼€çš„é…ç½®
        let cache = self.cache.read().await;
        let items: Vec<ConfigItem> = cache
            .iter()
            .filter(|(k, _)| k.starts_with(prefix))
            .map(|(_, v)| v.clone())
            .collect();
            
        Ok(items)
    }
    
    // è·å–é…ç½®å˜æ›´é€šçŸ¥
    fn subscribe_changes(&self) -> broadcast::Receiver<ConfigChangeEvent> {
        self.change_tx.subscribe()
    }
}

// HTTP API è¯·æ±‚/å“åº”æ¨¡å‹
#[derive(Deserialize)]
struct SetConfigRequest {
    value: serde_json::Value,
    metadata: Option<HashMap<String, String>>,
}

#[derive(Deserialize)]
struct QueryParams {
    watch: Option<bool>,
}

// HTTP APIå¤„ç†ç¨‹åº
async fn setup_config_api(config_service: Arc<ConfigService>) -> Router {
    Router::new()
        .route("/api/v1/configs/:key", get(get_config))
        .route("/api/v1/configs/:key", put(set_config))
        .route("/api/v1/configs/:key", axum::routing::delete(delete_config))
        .route("/api/v1/configs", get(list_configs))
        .route("/api/v1/watch", get(watch_configs))
        .layer(Extension(config_service))
}

async fn get_config(
    Extension(service): Extension<Arc<ConfigService>>,
    Path(key): Path<String>,
    Query(params): Query<QueryParams>,
) -> impl IntoResponse {
    if params.watch.unwrap_or(false) {
        // é•¿è½®è¯¢ï¼Œç­‰å¾…é…ç½®å˜æ›´
        let mut rx = service.subscribe_changes();
        let target_key = key.clone();
        
        let config_future = service.get_config(&key);
        let watch_future = async {
            while let Ok(event) = rx.recv().await {
                if event.key == target_key {
                    return event;
                }
            }
            panic!("å¹¿æ’­é€šé“å·²å…³é—­"); // åº”è¯¥ä¸ä¼šå‘ç”Ÿ
        };
        
        tokio::select! {
            config_result = config_future => {
                match config_result {
                    Ok(Some(item)) => (StatusCode::OK, JsonResponse(item)).into_response(),
                    Ok(None) => StatusCode::NOT_FOUND.into_response(),
                    Err(e) => {
                        error!(error = %e, "è·å–é…ç½®å¤±è´¥");
                        StatusCode::INTERNAL_SERVER_ERROR.into_response()
                    }
                }
            }
            event = watch_future => {
                let response = ConfigChangeEvent {
                    key: event.key,
                    old_value: event.old_value,
                    new_value: event.new_value,
                    version: event.version,
                    timestamp: event.timestamp,
                };
                (StatusCode::OK, JsonResponse(response)).into_response()
            }
        }
    } else {
        // ç«‹å³è¿”å›å½“å‰é…ç½®
        match service.get_config(&key).await {
            Ok(Some(item)) => (StatusCode::OK, JsonResponse(item)).into_response(),
            Ok(None) => StatusCode::NOT_FOUND.into_response(),
            Err(e) => {
                error!(error = %e, "è·å–é…ç½®å¤±è´¥");
                StatusCode::INTERNAL_SERVER_ERROR.into_response()
            }
        }
    }
}

async fn set_config(
    Extension(service): Extension<Arc<ConfigService>>,
    Path(key): Path<String>,
    Json(req): Json<SetConfigRequest>,
) -> impl IntoResponse {
    let config_item = ConfigItem {
        key,
        value: req.value,
        version: chrono::Utc::now().timestamp() as u64,
        last_updated: chrono::Utc::now(),
        metadata: req.metadata.unwrap_or_default(),
    };
    
    match service.set_config(config_item).await {
        Ok(_) => StatusCode::OK.into_response(),
        Err(e) => {
            error!(error = %e, "è®¾ç½®é…ç½®å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR.into_response()
        }
    }
}

async fn delete_config(
    Extension(service): Extension<Arc<ConfigService>>,
    Path(key): Path<String>,
) -> impl IntoResponse {
    match service.delete_config(&key).await {
        Ok(_) => StatusCode::NO_CONTENT.into_response(),
        Err(e) => {
            error!(error = %e, "åˆ é™¤é…ç½®å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR.into_response()
        }
    }
}

async fn list_configs(
    Extension(service): Extension<Arc<ConfigService>>,
    Query(params): Query<HashMap<String, String>>,
) -> impl IntoResponse {
    let prefix = params.get("prefix").cloned().unwrap_or_default();
    
    match service.list_configs(&prefix).await {
        Ok(items) => (StatusCode::OK, JsonResponse(items)).into_response(),
        Err(e) => {
            error!(error = %e, "åˆ—å‡ºé…ç½®å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR.into_response()
        }
    }
}

async fn watch_configs(
    Extension(service): Extension<Arc<ConfigService>>,
    Query(params): Query<HashMap<String, String>>,
) -> impl IntoResponse {
    let mut rx = service.subscribe_changes();
    let filter_prefix = params.get("prefix").cloned().unwrap_or_default();
    
    // ä½¿ç”¨SSEæµè¿”å›é…ç½®å˜æ›´
    let stream = async_stream::stream! {
        while let Ok(event) = rx.recv().await {
            if event.key.starts_with(&filter_prefix) {
                let data = serde_json::to_string(&event).unwrap();
                yield format!("data: {}\n\n", data);
            }
        }
    };
    
    axum::response::sse::Sse::new(stream)
        .keep_alive(axum::response::sse::KeepAlive::new()
            .interval(Duration::from_secs(15))
            .text("ping"))
}

// å®¢æˆ·ç«¯åº“ç¤ºä¾‹ - åº”ç”¨ç¨‹åºå¯ä»¥ä½¿ç”¨è¿™ä¸ªåº“æ¥è®¿é—®é…ç½®
struct ConfigClient {
    http_client: reqwest::Client,
    base_url: String,
    cache: HashMap<String, ConfigItem>,
    // æ¯ä¸ªé”®çš„watché€šé“
    watches: HashMap<String, watch::Sender<ConfigItem>>,
}

impl ConfigClient {
    fn new(base_url: &str) -> Self {
        Self {
            http_client: reqwest::Client::new(),
            base_url: base_url.to_string(),
            cache: HashMap::new(),
            watches: HashMap::new(),
        }
    }
    
    async fn start_watching(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        let watch_url = format!("{}/api/v1/watch", self.base_url);
        
        let response = self.http_client.get(&watch_url)
            .header("Accept", "text/event-stream")
            .send()
            .await?;
            
        let mut stream = response.bytes_stream();
        
        tokio::spawn(async move {
            while let Some(chunk_result) = stream.next().await {
                if let Ok(chunk) = chunk_result {
                    let text = String::from_utf8_lossy(&chunk);
                    if text.starts_with("data: ") {
                        let data = &text["data: ".len()..];
                        if let Ok(event) = serde_json::from_str::<ConfigChangeEvent>(data) {
                            // æ›´æ–°ç¼“å­˜å’Œé€šçŸ¥è§‚å¯Ÿè€…
                            // (è¿™é‡Œçš„å®ç°ä¼šç¨å¾®å¤æ‚ä¸€äº›)
                        }
                    }
                }
            }
        });
        
        Ok(())
    }
    
    async fn get_config(&mut self, key: &str) -> Result<Option<ConfigItem>, Box<dyn std::error::Error>> {
        // æ£€æŸ¥ç¼“å­˜
        if let Some(item) = self.cache.get(key) {
            return Ok(Some(item.clone()));
        }
        
        // ä»æœåŠ¡å™¨è·å–
        let url = format!("{}/api/v1/configs/{}", self.base_url, key);
        let response = self.http_client.get(&url).send().await?;
        
        if response.status() == StatusCode::NOT_FOUND {
            return Ok(None);
        }
        
        let config_item = response.json::<ConfigItem>().await?;
        
        // æ›´æ–°ç¼“å­˜
        self.cache.insert(key.to_string(), config_item.clone());
        
        Ok(Some(config_item))
    }
    
    // è®¢é˜…é…ç½®å˜æ›´
    fn watch_config(&mut self, key: &str) -> watch::Receiver<ConfigItem> {
        if let Some(sender) = self.watches.get(key) {
            return sender.subscribe();
        }
        
        let (tx, rx) = watch::channel(ConfigItem {
            key: key.to_string(),
            value: serde_json::Value::Null,
            version: 0,
            last_updated: chrono::Utc::now(),
            metadata: HashMap::new(),
        });
        
        self.watches.insert(key.to_string(), tx);
        rx
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–æ—¥å¿—
    init_logging("config-service", std::path::Path::new("/var/log/config-service")).await?;
    
    // åˆ›å»ºetcdå®¢æˆ·ç«¯
    let etcd_client = EtcdClient::connect(["etcd:2379"], None).await?;
    
    // åˆ›å»ºé…ç½®æä¾›è€…
    let provider = Box::new(EtcdConfigProvider {
        client: etcd_client,
        namespace: "app/config".to_string(),
    });
    
    // åˆ›å»ºé…ç½®æœåŠ¡
    let config_service = Arc::new(ConfigService::new(provider));
    config_service.init().await?;
    
    // å¯åŠ¨HTTP API
    let app = setup_config_api(config_service).await;
    let listener = tokio::net::TcpListener::bind("0.0.0.0:8080").await?;
    info!("é…ç½®ç®¡ç†æœåŠ¡HTTP APIå·²å¯åŠ¨åœ¨ 0.0.0.0:8080");
    axum::serve(listener, app).await?;
    
    Ok(())
}
```

## 5 å››ã€å¤§æ•°æ®å¤„ç†å’Œæ•°æ®åº“é›†æˆ

### 5.1 -æ ¸å¿ƒåº“ä¸æ¡†æ¶-

```rust
// Cargo.toml
[dependencies]
# å¼‚æ­¥è¿è¡Œæ—¶
tokio = { version = "2.0", features = ["full"] }
# æ•°æ®åº“è¿æ¥æ± 
sqlx = { version = "0.8", features = ["runtime-tokio-rustls", "postgres", "uuid", "chrono", "json"] }
# æ•°æ®å¤„ç†
polars = { version = "1.0", features = ["lazy", "parquet", "csv", "json", "random"] }
datafusion = "20.0"
# Kafkaæµå¤„ç†
rdkafka = { version = "1.0", features = ["ssl", "sasl"] }
# æµå¤„ç†æ¡†æ¶
timely = "0.12"
differential-dataflow = "0.13"
# å·¥ä½œæµå¼•æ“
temporal-sdk-core = "1.0"
# HTTPæœåŠ¡
axum = "1.0"
# åºåˆ—åŒ–
serde = { version = "2.0", features = ["derive"] }
serde_json = "2.0"
# ClickHouseå®¢æˆ·ç«¯
clickhouse-rs = "1.0"
```

### 5.2 æ•°æ®å¤„ç†å’Œå·¥ä½œæµå®ç°

```rust
use axum::{
    routing::{get, post},
    Router, Extension, Json,
    extract::{Path, Query},
    response::{IntoResponse, Json as JsonResponse},
    http::StatusCode,
};
use chrono::{DateTime, Utc};
use clickhouse_rs::{Pool, types::Block};
use datafusion::{
    prelude::*,
    arrow::datatypes::{DataType, Field, Schema},
    arrow::record_batch::RecordBatch,
};
use differential_dataflow::input::InputSession;
use polars::{
    prelude::*,
    frame::DataFrame,
    io::{csv::CsvReader, parquet::ParquetReader},
};
use rdkafka::{
    consumer::{StreamConsumer, Consumer},
    config::ClientConfig,
    message::Message,
    producer::{FutureProducer, FutureRecord},
};
use serde::{Deserialize, Serialize};
use sqlx::{postgres::PgPoolOptions, PgPool, Row};
use std::collections::HashMap;
use std::sync::Arc;
use temporal_sdk_core::{
    WorkflowClient, WorkflowOptions, WorkflowResult, 
    activity::{ActivityOptions, ActivityResult},
};
use tokio::sync::{RwLock, Mutex};
use tracing::{info, error, warn, instrument};
use uuid::Uuid;

// æ•°æ®æ¨¡å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
struct Order {
    id: Uuid,
    customer_id: String,
    order_date: DateTime<Utc>,
    total_amount: f64,
    status: String,
    items: Vec<OrderItem>,
    metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct OrderItem {
    product_id: String,
    quantity: i32,
    price: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Payment {
    id: Uuid,
    order_id: Uuid,
    amount: f64,
    payment_date: DateTime<Utc>,
    payment_method: String,
    status: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Shipment {
    id: Uuid,
    order_id: Uuid,
    tracking_number: String,
    carrier: String,
    ship_date: Option<DateTime<Utc>>,
    delivery_date: Option<DateTime<Utc>>,
    status: String,
}

// æ•°æ®åº“æœåŠ¡
struct DatabaseService {
    pg_pool: PgPool,
    clickhouse_pool: Pool,
}

impl DatabaseService {
    async fn new(pg_url: &str, clickhouse_url: &str) -> Result<Self, Box<dyn std::error::Error>> {
        // åˆå§‹åŒ–PostgreSQLè¿æ¥æ± 
        let pg_pool = PgPoolOptions::new()
            .max_connections(20)
            .connect(pg_url)
            .await?;
            
        // åˆå§‹åŒ–ClickHouseè¿æ¥æ± 
        let clickhouse_pool = Pool::new(clickhouse_url);
        
        Ok(Self {
            pg_pool,
            clickhouse_pool,
        })
    }
    
    // ä¿å­˜è®¢å•åˆ°PostgreSQL
    async fn save_order(&self, order: &Order) -> Result<(), sqlx::Error> {
        // å¼€å¯äº‹åŠ¡
        let mut tx = self.pg_pool.begin().await?;
        
        // æ’å…¥è®¢å•
        let order_id = sqlx::query(
            "INSERT INTO orders (id, customer_id, order_date, total_amount, status, metadata) 
             VALUES ($1, $2, $3, $4, $5, $6)
             RETURNING id"
        )
        .bind(order.id)
        .bind(&order.customer_id)
        .bind(order.order_date)
        .bind(order.total_amount)
        .bind(&order.status)
        .bind(serde_json::to_value(&order.metadata)?)
        .fetch_one(&mut *tx)
        .await?
        .get::<Uuid, _>(0);
        
        // æ’å…¥è®¢å•é¡¹
        for item in &order.items {
            sqlx::query(
                "INSERT INTO order_items (order_id, product_id, quantity, price) 
                 VALUES ($1, $2, $3, $4)"
            )
            .bind(order_id)
            .bind(&item.product_id)
            .bind(item.quantity)
            .bind(item.price)
            .execute(&mut *tx)
            .await?;
        }
        
        // æäº¤äº‹åŠ¡
        tx.commit().await?;
        
        Ok(())
    }
    
    // è·å–è®¢å•
    async fn get_order(&self, id: Uuid) -> Result<Option<Order>, sqlx::Error> {
        // æŸ¥è¯¢è®¢å•
        let order_row = sqlx::query(
            "SELECT id, customer_id, order_date, total_amount, status, metadata 
             FROM orders WHERE id = $1"
        )
        .bind(id)
        .fetch_optional(&self.pg_pool)
        .await?;
        
        let order = match order_row {
            Some(row) => {
                let order_id: Uuid = row.get("id");
                
                // æŸ¥è¯¢è®¢å•é¡¹
                let items = sqlx::query_as!(
                    OrderItem,
                    "SELECT product_id, quantity, price FROM order_items WHERE order_id = $1",
                    order_id
                )
                .fetch_all(&self.pg_pool)
                .await?;
                
                let metadata: serde_json::Value = row.get("metadata");
                let metadata: HashMap<String, String> = serde_json::from_value(metadata)?;
                
                Some(Order {
                    id: order_id,
                    customer_id: row.get("customer_id"),
                    order_date: row.get("order_date"),
                    total_amount: row.get("total_amount"),
                    status: row.get("status"),
                    items,
                    metadata,
                })
            }
            None => None,
        };
        
        Ok(order)
    }
    
    // å°†æ•°æ®å†™å…¥ClickHouseç”¨äºåˆ†æ
    async fn write_to_clickhouse<T: Serialize>(&self, table: &str, data: &[T]) -> Result<(), Box<dyn std::error::Error>> {
        let mut block = Block::new();
        
        // è¿™é‡Œåº”è¯¥æ ¹æ®Tç±»å‹åŠ¨æ€æ„å»ºClickHouseçš„Block
        // ä¸ºç®€æ´èµ·è§ï¼Œè¿™é‡Œçœç•¥äº†å…·ä½“å®ç°
        
        let mut client = self.clickhouse_pool.get_handle().await?;
        client.insert(table, block).await?;
        
        Ok(())
    }
    
    // æ‰§è¡ŒClickHouseåˆ†ææŸ¥è¯¢
    async fn execute_analytics_query(&self, query: &str) -> Result<DataFrame, Box<dyn std::error::Error>> {
        let mut client = self.clickhouse_pool.get_handle().await?;
        let block = client.query(query).fetch_all().await?;
        
        // å°†ClickHouse Blockè½¬æ¢ä¸ºPolars DataFrame
        // è¿™é‡Œç®€åŒ–äº†å…·ä½“å®ç°
        let df = DataFrame::default();
        
        Ok(df)
    }
}

// Kafkaæµå¤„ç†æœåŠ¡
struct StreamProcessingService {
    kafka_consumer: StreamConsumer,
    kafka_producer: FutureProducer,
    db_service: Arc<DatabaseService>,
}

impl StreamProcessingService {
    async fn new(
        kafka_brokers: &str,
        consumer_group: &str,
        db_service: Arc<DatabaseService>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        // é…ç½®Kafkaæ¶ˆè´¹è€…
        let consumer: StreamConsumer = ClientConfig::new()
            .set("group.id", consumer_group)
            .set("bootstrap.servers", kafka_brokers)
            .set("enable.auto.commit", "true")
            .set("auto.offset.reset", "earliest")
            .create()?;
            
        // é…ç½®Kafkaç”Ÿäº§è€…
        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", kafka_brokers)
            .set("message.timeout.ms", "5000")
            .create()?;
            
        Ok(Self {
            kafka_consumer: consumer,
            kafka_producer: producer,
            db_service,
        })
    }
    
    // è®¢é˜…ä¸»é¢˜å¹¶å¤„ç†æ¶ˆæ¯
    async fn subscribe_and_process(&self, topics: &[&str]) -> Result<(), Box<dyn std::error::Error>> {
        self.kafka_consumer.subscribe(topics)?;
        
        let consumer = &self.kafka_consumer;
        let producer = self.kafka_producer.clone();
        let db_service = self.db_service.clone();
        
        // å¯åŠ¨æ¶ˆæ¯å¤„ç†å¾ªç¯
        tokio::spawn(async move {
            loop {
                match consumer.recv().await {
                    Ok(msg) => {
                        let topic = msg.topic();
                        let payload = match msg.payload() {
                            Some(data) => data,
                            None => continue,
                        };
                        
                        info!(topic = %topic, "æ”¶åˆ°Kafkaæ¶ˆæ¯");
                        
                        // æ ¹æ®ä¸»é¢˜å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
                        match topic {
                            "orders" => {
                                if let Ok(order) = serde_json::from_slice::<Order>(payload) {
                                    // ä¿å­˜è®¢å•åˆ°æ•°æ®åº“
                                    if let Err(e) = db_service.save_order(&order).await {
                                        error!(error = %e, order_id = %order.id, "ä¿å­˜è®¢å•å¤±è´¥");
                                        continue;
                                    }
                                    
                                    // å‘é€åˆ°ä¸‹æ¸¸ä¸»é¢˜
                                    let record = FutureRecord::to("order_processed")
                                        .payload(payload)
                                        .key(&order.id.to_string());
                                        
                                    if let Err((e, _)) = producer.send(record, std::time::Duration::from_secs(0)).await {
                                        error!(error = %e, "å‘é€å¤„ç†åçš„è®¢å•å¤±è´¥");
                                    }
                                }
                            },
                            "payments" => {
                                if let Ok(payment) = serde_json::from_slice::<Payment>(payload) {
                                    // å¤„ç†æ”¯ä»˜æ¶ˆæ¯
                                    // ...
                                    
                                    // å¦‚æœæ”¯ä»˜æˆåŠŸï¼Œæ›´æ–°è®¢å•çŠ¶æ€
                                    if payment.status == "completed" {
                                        let order_update = json!({
                                            "id": payment.order_id,
                                            "status": "paid"
                                        });
                                        
                                        let record = FutureRecord::to("order_updates")
                                            .payload(&serde_json::to_vec(&order_update).unwrap())
                                            .key(&payment.order_id.to_string());
                                            
                                        let _ = producer.send(record, std::time::Duration::from_secs(0)).await;
                                    }
                                }
                            },
                            "shipments" => {
                                if let Ok(shipment) = serde_json::from_slice::<Shipment>(payload) {
                                    // å¤„ç†å‘è´§æ¶ˆæ¯
                                    // ...
                                    
                                    // æ›´æ–°è®¢å•çŠ¶æ€
                                    let status = match shipment.status.as_str() {
                                        "shipped" => "shipped",
                                        "delivered" => "completed",
                                        _ => continue,
                                    };
                                    
                                    let order_update = json!({
                                        "id": shipment.order_id,
                                        "status": status
                                    });
                                    
                                    let record = FutureRecord::to("order_updates")
                                        .payload(&serde_json::to_vec(&order_update).unwrap())
                                        .key(&shipment.order_id.to_string());
                                        
                                    let _ = producer.send(record, std::time::Duration::from_secs(0)).await;
                                }
                            },
                            _ => {
                                warn!(topic = %topic, "æœªå¤„ç†çš„Kafkaä¸»é¢˜");
                            }
                        }
                    },
                    Err(e) => {
                        error!(error = %e, "æ¥æ”¶Kafkaæ¶ˆæ¯å¤±è´¥");
                        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
                    }
                }
            }
        });
        
        Ok(())
    }
    
    // æµå¼åˆ†æç¤ºä¾‹ - ä½¿ç”¨DataFusion
    async fn run_streaming_analytics(&self) -> Result<(), Box<dyn std::error::Error>> {
        // åˆ›å»ºDataFusionä¸Šä¸‹æ–‡
        let ctx = SessionContext::new();
        
        // åˆ›å»ºå†…å­˜è¡¨æ¥å­˜å‚¨è®¢å•æ•°æ®
        let schema = Arc::new(Schema::new(vec![
            Field::new("id", DataType::Utf8, false),
            Field::new("customer_id", DataType::Utf8, false),
            Field::new("total_amount", DataType::Float64, false),
            Field::new("status", DataType::Utf8, false),
            Field::new("order_date", DataType::Timestamp(TimeUnit::Microsecond, None), false),
        ]));
        
        ctx.register_table(
            "orders",
            Arc::new(MemTable::try_new(schema.clone(), vec![])?),
        )?;
        
        // åˆ›å»ºä¸€ä¸ªå¤„ç†å¾ªç¯ï¼Œå®šæœŸä»Kafkaè¯»å–è®¢å•æ•°æ®å¹¶è¿›è¡Œåˆ†æ
        let consumer = self.kafka_consumer.clone();
        let db_service = self.db_service.clone();
        
        tokio::spawn(async move {
            let mut batch_orders = Vec::new();
            let mut interval = tokio::time::interval(tokio::time::Duration::from_secs(60));
            
            loop {
                interval.tick().await;
                
                if !batch_orders.is_empty() {
                    // æ‰§è¡Œèšåˆåˆ†æ
                    let df = ctx.sql(
                        "SELECT 
                            DATE_TRUNC('hour', order_date) as hour, 
                            COUNT(*) as order_count, 
                            SUM(total_amount) as total_sales,
                            AVG(total_amount) as avg_order_value
                         FROM orders 
                         GROUP BY DATE_TRUNC('hour', order_date)
                         ORDER BY hour DESC
                         LIMIT 24"
                    ).await.unwrap();
                    
                    let result = df.collect().await.unwrap();
                    
                    // å°†åˆ†æç»“æœå†™å…¥ClickHouse
                    // (ç®€åŒ–äº†å®ç°)
                    
                    // æ¸…ç©ºæ‰¹æ¬¡
                    batch_orders.clear();
                }
            }
        });
        
        Ok(())
    }
}

// å·¥ä½œæµæœåŠ¡ - åŸºäºTemporal
struct WorkflowService {
    temporal_client: Arc<WorkflowClient>,
    db_service: Arc<DatabaseService>,
}

impl WorkflowService {
    async fn new(
        temporal_url: &str,
        temporal_namespace: &str,
        db_service: Arc<DatabaseService>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        // åˆå§‹åŒ–Temporalå®¢æˆ·ç«¯
        let temporal_client = Arc::new(WorkflowClient::connect(temporal_url, temporal_namespace).await?);
        
        Ok(Self {
            temporal_client,
            db_service,
        })
    }
    
    // å¯åŠ¨è®¢å•å¤„ç†å·¥ä½œæµ
    async fn start_order_workflow(&self, order: Order) -> Result<String, Box<dyn std::error::Error>> {
        // è®¾ç½®å·¥ä½œæµé€‰é¡¹
        let options = WorkflowOptions::default()
            .workflow_id(format!("order-{}", order.id))
            .task_queue("order-processing")
            .workflow_execution_timeout(std::time::Duration::from_secs(86400)); // 24å°æ—¶
            
        // å¯åŠ¨å·¥ä½œæµ
        let workflow_run = self.temporal_client
            .start_workflow("OrderProcessingWorkflow", order, options)
            .await?;
            
        Ok(workflow_run.workflow_id)
    }
    
    // æ³¨å†Œå·¥ä½œæµå’Œæ´»åŠ¨
    async fn register_workflows(&self) -> Result<(), Box<dyn std::error::Error>> {
        // æ³¨å†Œè®¢å•å¤„ç†å·¥ä½œæµ
        self.temporal_client.register_workflow::<Order, WorkflowResult<()>>(
            "OrderProcessingWorkflow",
            |ctx, input| async move {
                // å·¥ä½œæµå®ç°
                let order = input;
                
                // ç¬¬ä¸€æ­¥ï¼šéªŒè¯è®¢å•
                let validate_result = ctx.activity(
                    "ValidateOrderActivity",
                    order.clone(),
                    ActivityOptions::default()
                        .start_to_close_timeout(std::time::Duration::from_secs(5))
                ).await?;
                
                if !validate_result {
                    // è®¢å•éªŒè¯å¤±è´¥
                    ctx.activity(
                        "CancelOrderActivity",
                        order.id,
                        ActivityOptions::default()
                    ).await?;
                    return Ok(());
                }
                
                // ç¬¬äºŒæ­¥ï¼šå¤„ç†æ”¯ä»˜
                let payment_result = ctx.activity(
                    "ProcessPaymentActivity",
                    order.clone(),
                    ActivityOptions::default()
                        .start_to_close_timeout(std::time::Duration::from_secs(30))
                ).await?;
                
                if !payment_result {
                    // æ”¯ä»˜å¤±è´¥
                    ctx.activity(
                        "FailOrderActivity",
                        order.id,
                        ActivityOptions::default()
                    ).await?;
                    return Ok(());
                }
                
                // ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡å‘è´§
                ctx.activity(
                    "PrepareShipmentActivity",
                    order.clone(),
                    ActivityOptions::default()
                ).await?;
                
                // ç¬¬å››æ­¥ï¼šé€šçŸ¥å®¢æˆ·
                ctx.activity(
                    "NotifyCustomerActivity",
                    order.clone(),
                    ActivityOptions::default()
                ).await?;
                
                Ok(())
            }
        );
        
        // æ³¨å†Œæ´»åŠ¨
        let db_service = self.db_service.clone();
        
        // éªŒè¯è®¢å•æ´»åŠ¨
        self.temporal_client.register_activity(
            "ValidateOrderActivity",
            move |order: Order| {
                let db = db_service.clone();
                async move {
                    // éªŒè¯è®¢å•é€»è¾‘
                    Ok(true)
                }
            }
        );
        
        // å…¶ä»–æ´»åŠ¨æ³¨å†Œ
        // ...
        
        Ok(())
    }
}

// APIæœåŠ¡
async fn setup_api_routes(
    db_service: Arc<DatabaseService>,
    workflow_service: Arc<WorkflowService>,
) -> Router {
    Router::new()
        .route("/api/v1/orders", post(create_order))
        .route("/api/v1/orders/:id", get(get_order))
        .route("/api/v1/analytics/sales", get(get_sales_analytics))
        .layer(Extension(db_service))
        .layer(Extension(workflow_service))
}

async fn create_order(
    Extension(db_service): Extension<Arc<DatabaseService>>,
    Extension(workflow_service): Extension<Arc<WorkflowService>>,
    Json(mut order): Json<Order>,
) -> impl IntoResponse {
    // ç”Ÿæˆè®¢å•ID
    if order.id == Uuid::nil() {
        order.id = Uuid::new_v4();
    }
    
    // ä¿å­˜è®¢å•
    match db_service.save_order(&order).await {
        Ok(_) => {
            // å¯åŠ¨è®¢å•å¤„ç†å·¥ä½œæµ
            match workflow_service.start_order_workflow(order.clone()).await {
                Ok(workflow_id) => {
                    let response = json!({
                        "order_id": order.id,
                        "workflow_id": workflow_id,
                        "status": "processing"
                    });
                    (StatusCode::CREATED, JsonResponse(response)).into_response()
                }
                Err(e) => {
                    error!(error = %e, "å¯åŠ¨è®¢å•å·¥ä½œæµå¤±è´¥");
                    StatusCode::INTERNAL_SERVER_ERROR.into_response()
                }
            }
        }
        Err(e) => {
            error!(error = %e, "ä¿å­˜è®¢å•å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR.into_response()
        }
    }
}

async fn get_order(
    Extension(db_service): Extension<Arc<DatabaseService>>,
    Path(id): Path<Uuid>,
) -> impl IntoResponse {
    match db_service.get_order(id).await {
        Ok(Some(order)) => (StatusCode::OK, JsonResponse(order)).into_response(),
        Ok(None) => StatusCode::NOT_FOUND.into_response(),
        Err(e) => {
            error!(error = %e, "è·å–è®¢å•å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR.into_response()
        }
    }
}

async fn get_sales_analytics(
    Extension(db_service): Extension<Arc<DatabaseService>>,
    Query(params): Query<HashMap<String, String>>,
) -> impl IntoResponse {
    let period = params.get("period").cloned().unwrap_or_else(|| "day".to_string());
    let days = params.get("days").and_then(|d| d.parse::<i32>().ok()).unwrap_or(7);
    
    // æ„å»ºåˆ†ææŸ¥è¯¢
    let query = match period.as_str() {
        "hour" => format!(
            "SELECT 
                toStartOfHour(order_date) as period, 
                count() as order_count,
                sum(total_amount) as total_sales,
                avg(total_amount) as avg_order_value
             FROM orders
             WHERE order_date >= now() - INTERVAL {} DAY
             GROUP BY period
             ORDER BY period DESC",
            days
        ),
        "day" => format!(
            "SELECT 
                toDate(order_date) as period, 
                count() as order_count,
                sum(total_amount) as total_sales,
                avg(total_amount) as avg_order_value
             FROM orders
             WHERE order_date >= now() - INTERVAL {} DAY
             GROUP BY period
             ORDER BY period DESC",
            days
        ),
        "month" => format!(
            "SELECT 
                toStartOfMonth(order_date) as period, 
                count() as order_count,
                sum(total_amount) as total_sales,
                avg(total_amount) as avg_order_value
             FROM orders
             WHERE order_date >= now() - INTERVAL {} MONTH
             GROUP BY period
             ORDER BY period DESC",
            days / 30
        ),
        _ => format!(
            "SELECT 
                toDate(order_date) as period, 
                count() as order_count,
                sum(total_amount) as total_sales,
                avg(total_amount) as avg_order_value
             FROM orders
             WHERE order_date >= now() - INTERVAL {} DAY
             GROUP BY period
             ORDER BY period DESC",
            days
        ),
    };
    
    match db_service.execute_analytics_query(&query).await {
        Ok(df) => {
            // å°†DataFrameè½¬æ¢ä¸ºJSON
            let json = df.to_json(None).unwrap();
            (StatusCode::OK, JsonResponse(json)).into_response()
        }
        Err(e) => {
            error!(error = %e, "æ‰§è¡Œåˆ†ææŸ¥è¯¢å¤±è´¥");
            StatusCode::INTERNAL_SERVER_ERROR.into_response()
        }
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–æ—¥å¿—
    init_logging("data-service", std::path::Path::new("/var/log/data-service")).await?

//I need to continue the code for the data processing and workflow service. 
//This will complete the main function and 
//include any additional implementations needed for the data processing service.

    // åˆå§‹åŒ–æ•°æ®åº“æœåŠ¡
    let db_service = Arc::new(
        DatabaseService::new(
            "postgres://user:password@postgres:5432/orderdb",
            "tcp://clickhouse:9000?database=analytics"
        ).await?
    );
    
    // åˆå§‹åŒ–å·¥ä½œæµæœåŠ¡
    let workflow_service = Arc::new(
        WorkflowService::new(
            "nats://nats:4222",
            "kafka:9092",
            db_service.clone()
        ).await?
    );
    
    // å¯åŠ¨Kafkaæ¶ˆè´¹è€…ï¼Œå¤„ç†æµæ•°æ®
    let consumer_service = StreamProcessingService::new(
        "kafka:9092",
        vec!["orders", "payments", "shipments"],
        db_service.clone()
    ).await?;
    
    // å¯åŠ¨æµå¤„ç†
    tokio::spawn(async move {
        if let Err(e) = consumer_service.start().await {
            error!(error = %e, "æµå¤„ç†æœåŠ¡å‡ºé”™");
        }
    });
    
    // è®¾ç½®HTTP APIè·¯ç”±
    let app = Router::new()
        .route("/api/v1/orders", post(create_order))
        .route("/api/v1/orders/:id", get(get_order))
        .route("/api/v1/analytics/sales", get(get_sales_analytics))
        .route("/api/v1/workflows/:id/status", get(get_workflow_status))
        .layer(Extension(db_service))
        .layer(Extension(workflow_service));
    
    // å¯åŠ¨HTTPæœåŠ¡å™¨
    let listener = tokio::net::TcpListener::bind("0.0.0.0:8080").await?;
    info!("æ•°æ®å¤„ç†æœåŠ¡HTTP APIå·²å¯åŠ¨åœ¨ 0.0.0.0:8080");
    axum::serve(listener, app).await?;
    
    Ok(())
}

// æµå¤„ç†æœåŠ¡å®ç°
struct StreamProcessingService {
    kafka_consumer: StreamConsumer,
    db_service: Arc<DatabaseService>,
    event_processors: HashMap<String, Box<dyn EventProcessor>>,
}

// äº‹ä»¶å¤„ç†å™¨ç‰¹å¾
#[async_trait::async_trait]
trait EventProcessor: Send + Sync {
    async fn process(&self, topic: &str, payload: &[u8]) -> Result<(), Box<dyn std::error::Error>>;
}

struct OrderEventProcessor {
    db_service: Arc<DatabaseService>,
}

#[async_trait::async_trait]
impl EventProcessor for OrderEventProcessor {
    async fn process(&self, topic: &str, payload: &[u8]) -> Result<(), Box<dyn std::error::Error>> {
        // è§£æè®¢å•äº‹ä»¶
        let order: Order = serde_json::from_slice(payload)?;
        
        // ä¿å­˜åˆ°PostgreSQL
        self.db_service.save_order(&order).await?;
        
        // æ·»åŠ åˆ°å®æ—¶åˆ†æ
        self.db_service.add_to_analytics("orders", &order).await?;
        
        info!(order_id = %order.id, "å¤„ç†è®¢å•äº‹ä»¶");
        Ok(())
    }
}

struct PaymentEventProcessor {
    db_service: Arc<DatabaseService>,
}

#[async_trait::async_trait]
impl EventProcessor for PaymentEventProcessor {
    async fn process(&self, topic: &str, payload: &[u8]) -> Result<(), Box<dyn std::error::Error>> {
        // è§£ææ”¯ä»˜äº‹ä»¶
        let payment: Payment = serde_json::from_slice(payload)?;
        
        // æ›´æ–°æ”¯ä»˜çŠ¶æ€
        self.db_service.update_payment_status(payment.order_id, &payment).await?;
        
        info!(payment_id = %payment.id, order_id = %payment.order_id, "å¤„ç†æ”¯ä»˜äº‹ä»¶");
        Ok(())
    }
}

struct ShipmentEventProcessor {
    db_service: Arc<DatabaseService>,
}

#[async_trait::async_trait]
impl EventProcessor for ShipmentEventProcessor {
    async fn process(&self, topic: &str, payload: &[u8]) -> Result<(), Box<dyn std::error::Error>> {
        // è§£æç‰©æµäº‹ä»¶
        let shipment: Shipment = serde_json::from_slice(payload)?;
        
        // æ›´æ–°ç‰©æµçŠ¶æ€
        self.db_service.update_shipment_status(shipment.order_id, &shipment).await?;
        
        info!(shipment_id = %shipment.id, order_id = %shipment.order_id, "å¤„ç†ç‰©æµäº‹ä»¶");
        Ok(())
    }
}

impl StreamProcessingService {
    async fn new(
        kafka_brokers: &str, 
        topics: Vec<&str>,
        db_service: Arc<DatabaseService>
    ) -> Result<Self, Box<dyn std::error::Error>> {
        // é…ç½®Kafkaæ¶ˆè´¹è€…
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", kafka_brokers)
            .set("group.id", "data-processing-service")
            .set("enable.auto.commit", "true")
            .set("auto.offset.reset", "earliest")
            .create()?;
            
        // è®¢é˜…ä¸»é¢˜
        consumer.subscribe(&topics)?;
        
        // åˆ›å»ºäº‹ä»¶å¤„ç†å™¨
        let mut event_processors: HashMap<String, Box<dyn EventProcessor>> = HashMap::new();
        
        event_processors.insert(
            "orders".to_string(), 
            Box::new(OrderEventProcessor { db_service: db_service.clone() })
        );
        
        event_processors.insert(
            "payments".to_string(), 
            Box::new(PaymentEventProcessor { db_service: db_service.clone() })
        );
        
        event_processors.insert(
            "shipments".to_string(), 
            Box::new(ShipmentEventProcessor { db_service: db_service.clone() })
        );
        
        Ok(Self {
            kafka_consumer: consumer,
            db_service,
            event_processors,
        })
    }
    
    async fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        info!("å¯åŠ¨æµå¤„ç†æœåŠ¡");
        
        // æ— é™å¾ªç¯å¤„ç†äº‹ä»¶
        loop {
            match self.kafka_consumer.recv().await {
                Ok(msg) => {
                    // è·å–ä¸»é¢˜å’Œè´Ÿè½½
                    let topic = msg.topic();
                    let payload = msg.payload().unwrap_or_default();
                    
                    // æŸ¥æ‰¾å¯¹åº”çš„å¤„ç†å™¨
                    if let Some(processor) = self.event_processors.get(topic) {
                        match processor.process(topic, payload).await {
                            Ok(_) => {
                                // æˆåŠŸå¤„ç†
                                debug!(topic = %topic, "äº‹ä»¶å¤„ç†æˆåŠŸ");
                            }
                            Err(e) => {
                                // å¤„ç†å¤±è´¥
                                error!(topic = %topic, error = %e, "äº‹ä»¶å¤„ç†å¤±è´¥");
                                // è¿™é‡Œå¯ä»¥æ·»åŠ é‡è¯•é€»è¾‘æˆ–é”™è¯¯å¤„ç†
                            }
                        }
                    } else {
                        warn!(topic = %topic, "æ²¡æœ‰å¯¹åº”çš„å¤„ç†å™¨");
                    }
                }
                Err(e) => {
                    error!(error = %e, "ä»Kafkaæ¥æ”¶æ¶ˆæ¯å¤±è´¥");
                    // çŸ­æš‚å»¶è¿Ÿåé‡è¯•
                    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
                }
            }
        }
    }
}

// æ‰¹å¤„ç†ä»»åŠ¡å®ç°
struct BatchProcessingService {
    db_service: Arc<DatabaseService>,
}

impl BatchProcessingService {
    fn new(db_service: Arc<DatabaseService>) -> Self {
        Self { db_service }
    }
    
    // æ¯æ—¥é”€å”®æŠ¥è¡¨ç”Ÿæˆ
    async fn generate_daily_sales_report(&self, date: DateTime<Utc>) -> Result<DataFrame, Box<dyn std::error::Error>> {
        info!(date = %date, "ç”Ÿæˆæ¯æ—¥é”€å”®æŠ¥è¡¨");
        
        // æ„å»ºClickHouseæŸ¥è¯¢
        let query = format!(
            "SELECT 
                toDate(order_date) as day,
                count() as total_orders,
                sum(total_amount) as total_sales,
                avg(total_amount) as avg_order_value,
                count(DISTINCT customer_id) as unique_customers
             FROM orders
             WHERE toDate(order_date) = '{}'
             GROUP BY day",
            date.format("%Y-%m-%d")
        );
        
        // æ‰§è¡ŒæŸ¥è¯¢
        let df = self.db_service.execute_analytics_query(&query).await?;
        
        // ä¿å­˜åˆ°CSVæ–‡ä»¶
        let file_path = format!("/reports/daily_sales_{}.csv", date.format("%Y-%m-%d"));
        df.clone().write_csv(&file_path)?;
        
        // å‘é€åˆ°æ•°æ®ä»“åº“æˆ–æ•°æ®æ¹–
        self.db_service.export_to_data_warehouse("daily_sales", &df).await?;
        
        Ok(df)
    }
    
    // åº“å­˜åˆ†æ
    async fn analyze_inventory(&self) -> Result<(), Box<dyn std::error::Error>> {
        info!("æ‰§è¡Œåº“å­˜åˆ†æ");
        
        // æ‰§è¡Œåº“å­˜åˆ†ææŸ¥è¯¢
        let query = "
            SELECT 
                products.id as product_id,
                products.name as product_name,
                inventory.quantity as stock_quantity,
                inventory.last_updated,
                SUM(order_items.quantity) as sold_quantity_30d
            FROM
                products
                JOIN inventory ON products.id = inventory.product_id
                LEFT JOIN order_items ON products.id = order_items.product_id
                LEFT JOIN orders ON order_items.order_id = orders.id
                    AND orders.order_date >= now() - INTERVAL 30 DAY
            GROUP BY
                products.id, products.name, inventory.quantity, inventory.last_updated
            ORDER BY
                sold_quantity_30d DESC
        ";
        
        let df = self.db_service.execute_query(query).await?;
        
        // è®¡ç®—åº“å­˜å‘¨è½¬ç‡å’Œç¼ºè´§é£é™©
        let df_with_metrics = self.calculate_inventory_metrics(df).await?;
        
        // ä¿å­˜åˆ†æç»“æœ
        self.db_service.save_inventory_analysis(&df_with_metrics).await?;
        
        Ok(())
    }
    
    async fn calculate_inventory_metrics(&self, df: DataFrame) -> Result<DataFrame, Box<dyn std::error::Error>> {
        // ä½¿ç”¨Polarsè®¡ç®—åº“å­˜æŒ‡æ ‡
        let mut lazy_df = df.lazy();
        
        // è®¡ç®—åº“å­˜å‘¨è½¬ç‡å’Œç¼ºè´§é£é™©
        lazy_df = lazy_df.with_column(
            (col("sold_quantity_30d") / col("stock_quantity"))
                .alias("turnover_rate_30d")
        ).with_column(
            when(col("stock_quantity") / col("sold_quantity_30d").div(30) < lit(7))
                .then(lit("high"))
                .when(col("stock_quantity") / col("sold_quantity_30d").div(30) < lit(14))
                .then(lit("medium"))
                .otherwise(lit("low"))
                .alias("stockout_risk")
        );
        
        // æ‰§è¡Œè®¡ç®—å¹¶è·å–ç»“æœ
        let result_df = lazy_df.collect()?;
        
        Ok(result_df)
    }
}

// è®¢å•å·¥ä½œæµå®ç°
#[derive(Debug, Clone, Serialize, Deserialize)]
enum OrderWorkflowState {
    Created,
    PaymentPending,
    PaymentCompleted,
    Processing,
    Shipping,
    Delivered,
    Canceled,
    Completed,
}

struct OrderWorkflow {
    order_id: Uuid,
    current_state: OrderWorkflowState,
    db_service: Arc<DatabaseService>,
    kafka_producer: FutureProducer,
}

impl OrderWorkflow {
    fn new(
        order_id: Uuid, 
        db_service: Arc<DatabaseService>,
        kafka_producer: FutureProducer,
    ) -> Self {
        Self {
            order_id,
            current_state: OrderWorkflowState::Created,
            db_service,
            kafka_producer,
        }
    }
    
    async fn execute(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // è·å–è®¢å•ä¿¡æ¯
        let order = match self.db_service.get_order(self.order_id).await? {
            Some(order) => order,
            None => return Err(format!("è®¢å•ä¸å­˜åœ¨: {}", self.order_id).into()),
        };
        
        // æ‰§è¡Œå·¥ä½œæµçŠ¶æ€æœº
        loop {
            match self.current_state {
                OrderWorkflowState::Created => {
                    info!(order_id = %self.order_id, "å·¥ä½œæµå¼€å§‹: åˆ›å»ºè®¢å•");
                    // æ›´æ–°çŠ¶æ€
                    self.update_state(OrderWorkflowState::PaymentPending).await?;
                }
                OrderWorkflowState::PaymentPending => {
                    info!(order_id = %self.order_id, "ç­‰å¾…æ”¯ä»˜");
                    // æ£€æŸ¥æ”¯ä»˜çŠ¶æ€
                    let payment = self.db_service.get_payment_by_order_id(self.order_id).await?;
                    
                    if let Some(payment) = payment {
                        if payment.status == "completed" {
                            self.update_state(OrderWorkflowState::PaymentCompleted).await?;
                        } else if payment.status == "failed" {
                            self.update_state(OrderWorkflowState::Canceled).await?;
                            break;
                        } else {
                            // ä»åœ¨ç­‰å¾…æ”¯ä»˜ï¼Œæš‚åœå·¥ä½œæµ
                            break;
                        }
                    } else {
                        // æ²¡æœ‰æ”¯ä»˜è®°å½•ï¼Œæš‚åœå·¥ä½œæµ
                        break;
                    }
                }
                OrderWorkflowState::PaymentCompleted => {
                    info!(order_id = %self.order_id, "æ”¯ä»˜å®Œæˆï¼Œå¤„ç†è®¢å•");
                    // å‘é€åº“å­˜æ£€æŸ¥æ¶ˆæ¯
                    self.send_event("inventory_check", json!({
                        "order_id": self.order_id,
                        "items": order.items
                    })).await?;
                    
                    self.update_state(OrderWorkflowState::Processing).await?;
                    // æš‚åœå·¥ä½œæµï¼Œç­‰å¾…åº“å­˜ç¡®è®¤
                    break;
                }
                OrderWorkflowState::Processing => {
                    info!(order_id = %self.order_id, "è®¢å•å¤„ç†ä¸­");
                    // æ£€æŸ¥åº“å­˜ç¡®è®¤
                    let inventory_confirmed = self.db_service
                        .is_inventory_confirmed(self.order_id).await?;
                        
                    if inventory_confirmed {
                        // å‘é€å‘è´§è¯·æ±‚
                        self.send_event("shipping_request", json!({
                            "order_id": self.order_id,
                            "customer_id": order.customer_id,
                            "items": order.items
                        })).await?;
                        
                        self.update_state(OrderWorkflowState::Shipping).await?;
                    } else {
                        // ç­‰å¾…åº“å­˜ç¡®è®¤
                        break;
                    }
                }
                OrderWorkflowState::Shipping => {
                    info!(order_id = %self.order_id, "è®¢å•é…é€ä¸­");
                    // æ£€æŸ¥ç‰©æµçŠ¶æ€
                    let shipment = self.db_service.get_shipment_by_order_id(self.order_id).await?;
                    
                    if let Some(shipment) = shipment {
                        if shipment.status == "delivered" {
                            self.update_state(OrderWorkflowState::Delivered).await?;
                        } else {
                            // ç­‰å¾…ç‰©æµæ›´æ–°
                            break;
                        }
                    } else {
                        // ç­‰å¾…ç‰©æµä¿¡æ¯
                        break;
                    }
                }
                OrderWorkflowState::Delivered => {
                    info!(order_id = %self.order_id, "è®¢å•å·²é€è¾¾");
                    // å‘é€è®¢å•å®Œæˆäº‹ä»¶
                    self.send_event("order_completed", json!({
                        "order_id": self.order_id,
                        "customer_id": order.customer_id,
                        "completed_date": chrono::Utc::now()
                    })).await?;
                    
                    self.update_state(OrderWorkflowState::Completed).await?;
                    break;
                }
                OrderWorkflowState::Canceled => {
                    info!(order_id = %self.order_id, "è®¢å•å·²å–æ¶ˆ");
                    // å‘é€è®¢å•å–æ¶ˆäº‹ä»¶
                    self.send_event("order_canceled", json!({
                        "order_id": self.order_id,
                        "customer_id": order.customer_id,
                        "canceled_date": chrono::Utc::now()
                    })).await?;
                    
                    break;
                }
                OrderWorkflowState::Completed => {
                    info!(order_id = %self.order_id, "è®¢å•å¤„ç†å®Œæˆ");
                    break;
                }
            }
        }
        
        Ok(())
    }
    
    async fn update_state(&mut self, new_state: OrderWorkflowState) -> Result<(), Box<dyn std::error::Error>> {
        // æ›´æ–°æ•°æ®åº“ä¸­çš„è®¢å•çŠ¶æ€
        self.db_service.update_order_workflow_state(self.order_id, &new_state).await?;
        
        // æ›´æ–°å½“å‰çŠ¶æ€
        self.current_state = new_state;
        
        // å‘é€çŠ¶æ€å˜æ›´äº‹ä»¶
        self.send_event("workflow_state_changed", json!({
            "order_id": self.order_id,
            "old_state": format!("{:?}", self.current_state),
            "new_state": format!("{:?}", new_state),
            "timestamp": chrono::Utc::now()
        })).await?;
        
        Ok(())
    }
    
    async fn send_event(&self, event_type: &str, payload: serde_json::Value) -> Result<(), Box<dyn std::error::Error>> {
        // åˆ›å»ºå®Œæ•´çš„äº‹ä»¶æ•°æ®
        let event_data = json!({
            "event_type": event_type,
            "order_id": self.order_id.to_string(),
            "timestamp": chrono::Utc::now(),
            "payload": payload
        });
        
        // å‘é€åˆ°Kafka
        let topic = format!("order.events.{}", event_type);
        self.kafka_producer.send(
            FutureRecord::to(&topic)
                .payload(&serde_json::to_string(&event_data)?)
                .key(&self.order_id.to_string()),
            tokio::time::Duration::from_secs(5),
        ).await?;
        
        Ok(())
    }
}
```

## 6 éƒ¨ç½²ä¸è¿ç»´

ä¸ºäº†å®Œæˆè¿™ä¸ªåˆ†å¸ƒå¼é«˜ååé›†ç¾¤æœåŠ¡çš„å®ç°ï¼Œè¿˜éœ€è¦æä¾›éƒ¨ç½²å’Œè¿ç»´é…ç½®ã€‚
ä»¥ä¸‹æ˜¯ä½¿ç”¨Docker Composeçš„éƒ¨ç½²ç¤ºä¾‹ï¼š

```yaml
version: '3.8'

services:
  # APIç½‘å…³
  api-gateway:
    image: envoyproxy/envoy:v1.28.0
    ports:
      - "8000:8000"
    volumes:
      - ./envoy.yaml:/etc/envoy/envoy.yaml
    networks:
      - backend-network

  # æ—¥å¿—ç›‘æ§æœåŠ¡
  logging-service:
    build:
      context: ./logging-service
      dockerfile: Dockerfile
    environment:
      - RUST_LOG=info
    volumes:
      - log-data:/var/log
    depends_on:
      - elasticsearch
      - otel-collector
    networks:
      - backend-network

  # æ¶ˆæ¯æœåŠ¡
  message-service:
    build:
      context: ./message-service
      dockerfile: Dockerfile
    environment:
      - RUST_LOG=info
      - NATS_URL=nats://nats:4222
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - nats
      - kafka
      - otel-collector
    networks:
      - backend-network

  # é…ç½®ç®¡ç†æœåŠ¡
  config-service:
    build:
      context: ./config-service
      dockerfile: Dockerfile
    environment:
      - RUST_LOG=info
      - ETCD_ENDPOINTS=http://etcd:2379
    depends_on:
      - etcd
      - otel-collector
    networks:
      - backend-network

  # æ•°æ®å¤„ç†æœåŠ¡
  data-service:
    build:
      context: ./data-service
      dockerfile: Dockerfile
    environment:
      - RUST_LOG=info
      - POSTGRES_URL=postgres://user:password@postgres:5432/orderdb
      - CLICKHOUSE_URL=tcp://clickhouse:9000?database=analytics
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - postgres
      - clickhouse
      - kafka
      - otel-collector
    networks:
      - backend-network

  # åŸºç¡€è®¾æ–½æœåŠ¡
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elastic-data:/usr/share/elasticsearch/data
    networks:
      - backend-network

  nats:
    image: nats:2.10
    ports:
      - "4222:4222"
    networks:
      - backend-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    depends_on:
      - zookeeper
    networks:
      - backend-network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - backend-network

  etcd:
    image: quay.io/coreos/etcd:v3.5.0
    command: etcd --advertise-client-urls http://0.0.0.0:2379 --listen-client-urls http://0.0.0.0:2379
    networks:
      - backend-network

  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: orderdb
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - backend-network

  clickhouse:
    image: clickhouse/clickhouse-server:23.8
    volumes:
      - clickhouse-data:/var/lib/clickhouse
    networks:
      - backend-network

  otel-collector:
    image: otel/opentelemetry-collector:0.96.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    networks:
      - backend-network

networks:
  backend-network:
    driver: bridge

volumes:
  log-data:
  elastic-data:
  postgres-data:
  clickhouse-data:
```

## 7 æ€»ç»“

ä»¥ä¸Šä»£ç å’Œæ¶æ„å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Rust 2025å¹´é¢„æœŸçš„æœ€æˆç†Ÿæ¡†æ¶å’Œåº“æ¥å®ç°åˆ†å¸ƒå¼é«˜ååé›†ç¾¤æœåŠ¡ã€‚ä¸»è¦åŒ…æ‹¬ï¼š

1. **åˆ†å¸ƒå¼æ—¥å¿—ç›‘æ§å’Œæ£€ç´¢**ï¼šä½¿ç”¨OpenTelemetryå’ŒElasticsearchå®ç°æ—¥å¿—æ”¶é›†ã€è¿½è¸ªå’ŒæŸ¥è¯¢ã€‚

2. **åˆ†å¸ƒå¼æ¶ˆæ¯æœåŠ¡**ï¼šç»“åˆNATSå’ŒKafkaå®ç°é«˜æ€§èƒ½çš„æ¶ˆæ¯å‘å¸ƒ/è®¢é˜…å’Œè½¬æ¢æœåŠ¡ã€‚

3. **é›†ä¸­å¼é…ç½®ç®¡ç†**ï¼šä½¿ç”¨etcdä½œä¸ºé…ç½®å­˜å‚¨ï¼Œå®ç°é…ç½®çš„å®æ—¶æ›´æ–°å’Œè§‚å¯Ÿè€…æ¨¡å¼ã€‚

4. **å¤§æ•°æ®å¤„ç†å’Œå·¥ä½œæµ**ï¼šä½¿ç”¨PostgreSQLã€ClickHouseå’ŒPolars/DataFusionå®ç°é«˜æ€§èƒ½æ•°æ®å¤„ç†å’Œåˆ†æã€‚

è¿™ä¸ªæ¶æ„å…·æœ‰é«˜å¯æ‰©å±•æ€§ã€é«˜å¯ç”¨æ€§å’Œé«˜æ€§èƒ½ç‰¹ç‚¹ï¼Œé€‚åˆå¤„ç†å¤§è§„æ¨¡æœåŠ¡çš„éœ€æ±‚ã€‚
éšç€Rustç”Ÿæ€çš„ä¸æ–­å‘å±•ï¼Œé¢„è®¡åˆ°2025å¹´ï¼Œè¿™äº›åº“å’Œæ¡†æ¶ä¼šæ›´åŠ æˆç†Ÿå’Œå®Œå–„ï¼Œä¸ºåˆ†å¸ƒå¼ç³»ç»Ÿå¼€å‘æä¾›æ›´å¥½çš„æ”¯æŒã€‚
