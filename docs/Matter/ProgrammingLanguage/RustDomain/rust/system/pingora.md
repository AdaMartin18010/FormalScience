# ä½¿ç”¨Pingoraå®ç°åˆ†å¸ƒå¼HTTPé›†ç¾¤æœåŠ¡ (Rust 2025)

```text
Let me organize my thoughts on implementing a distributed HTTP cluster service using Pingora (Rust) in 2025. This is a comprehensive request covering several key aspects:

1. **Distributed HTTP Cluster with Pingora**:
   - Pingora is Cloudflare's Rust-based framework for building fast, reliable network services
   - By 2025, it's reasonable to expect significant evolution in its feature set

2. **Logging and Monitoring Integration**:
   - Tracing, logging, metrics integration
   - Observability features

3. **gRPC Backend Integration**:
   - Protocol generation
   - Bidirectional streaming support
   - Rate limiting, retries, idempotency, proxying/transformation

4. **HTTP Protocol Support**:
   - HTTP/1.x, HTTP/2, HTTP/3 (QUIC)
   - Authentication (session, JWT)
   - Complete HTTP semantics (redirects, etc.)

5. **MQ Integration**:
   - Sidecars for user behavior data
   - Event subscription and processing

For a 2025 version, I'll need to extrapolate current Rust ecosystem trends, especially for Pingora which is still evolving. I'll base this on realistic projections of where these technologies might be by 2025.
```

## ğŸ“‹ ç›®å½•

- [ä½¿ç”¨Pingoraå®ç°åˆ†å¸ƒå¼HTTPé›†ç¾¤æœåŠ¡ (Rust 2025)](#ä½¿ç”¨pingoraå®ç°åˆ†å¸ƒå¼httpé›†ç¾¤æœåŠ¡-rust-2025)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#1-ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
  - [2 ä¸€ã€æ ¸å¿ƒHTTPç½‘å…³æœåŠ¡å®ç°](#2-ä¸€æ ¸å¿ƒhttpç½‘å…³æœåŠ¡å®ç°)
    - [2.1 ä¸»åº”ç”¨æœåŠ¡å™¨å®ç°](#21-ä¸»åº”ç”¨æœåŠ¡å™¨å®ç°)
  - [3 äºŒã€é…ç½®ã€ç›‘æ§å’Œæ—¥å¿—æ¨¡å—](#3-äºŒé…ç½®ç›‘æ§å’Œæ—¥å¿—æ¨¡å—)
  - [4 ä¸‰ã€è®¤è¯å’Œæˆæƒä¸­é—´ä»¶](#4-ä¸‰è®¤è¯å’Œæˆæƒä¸­é—´ä»¶)
  - [5 å››ã€ä¸­é—´ä»¶å®ç°](#5-å››ä¸­é—´ä»¶å®ç°)
  - [6 äº”ã€gRPCæœåŠ¡é›†æˆ](#6-äº”grpcæœåŠ¡é›†æˆ)
  - [7 å…­ã€æ¶ˆæ¯é˜Ÿåˆ—é›†æˆå’Œç”¨æˆ·è¡Œä¸ºè·Ÿè¸ª](#7-å…­æ¶ˆæ¯é˜Ÿåˆ—é›†æˆå’Œç”¨æˆ·è¡Œä¸ºè·Ÿè¸ª)
  - [8 ä¸ƒã€é…ç½®æ–‡ä»¶ç¤ºä¾‹](#8-ä¸ƒé…ç½®æ–‡ä»¶ç¤ºä¾‹)
  - [9 å…«ã€éƒ¨ç½²ä¸Dockeré…ç½®](#9-å…«éƒ¨ç½²ä¸dockeré…ç½®)
  - [10 ä¹ã€æ€»ç»“ä¸åŠŸèƒ½æ¦‚è§ˆ](#10-ä¹æ€»ç»“ä¸åŠŸèƒ½æ¦‚è§ˆ)

---

## 1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å®¢æˆ·ç«¯ (Web/Mobile)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Pingora HTTPé›†ç¾¤ç½‘å…³                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ HTTP/1,2,3  â”‚  â”‚  è®¤è¯ä¸­é—´ä»¶  â”‚  â”‚  é™æµ/è´Ÿè½½   â”‚            â”‚
â”‚  â”‚  QUICæ”¯æŒ   â”‚  â”‚ JWT/Session â”‚  â”‚   å‡è¡¡å™¨     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  è¯·æ±‚è·¯ç”±   â”‚  â”‚ åè®®è½¬æ¢å±‚  â”‚  â”‚ è§‚æµ‹ä¸ç›‘æ§  â”‚            â”‚
â”‚  â”‚  ä¸è§„åˆ™     â”‚  â”‚HTTP<->gRPC  â”‚  â”‚Trace/Log/Metricâ”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     gRPC æœåŠ¡é›†ç¾¤         â”‚     â”‚       æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿ         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ä¸šåŠ¡æœåŠ¡A (åŒå‘æµ) â”‚  â”‚     â”‚  â”‚     Kafka/NATS       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ä¸šåŠ¡æœåŠ¡B          â”‚  â”‚     â”‚  â”‚  ç”¨æˆ·è¡Œä¸ºè·Ÿè¸ªå¤„ç†    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2 ä¸€ã€æ ¸å¿ƒHTTPç½‘å…³æœåŠ¡å®ç°

é¦–å…ˆï¼Œåˆ›å»ºé¡¹ç›®å¹¶æ·»åŠ ä¾èµ–ï¼š

```rust
// Cargo.toml
[package]
name = "pingora-gateway"
version = "0.1.0"
edition = "2021"

[dependencies]
# Pingora 2025ç‰ˆæœ¬æ ¸å¿ƒç»„ä»¶
pingora = { version = "0.10", features = ["full"] }
pingora-core = "0.10"
pingora-http = "0.10"
pingora-proxy = "0.10"
pingora-load-balancing = "0.10"
pingora-limiters = "0.10"
pingora-observability = "0.10"
pingora-tls = "0.10"
pingora-quic = "0.10"
pingora-grpc = "0.10"  # gRPCæ”¯æŒç»„ä»¶

# å¼‚æ­¥è¿è¡Œæ—¶
tokio = { version = "2.0", features = ["full", "tracing"] }

# è§‚æµ‹å’Œç›‘æ§
opentelemetry = { version = "2.0", features = ["trace", "metrics"] }
opentelemetry-otlp = { version = "1.0", features = ["trace", "metrics"] }
tracing = "1.0"
tracing-subscriber = { version = "1.0", features = ["env-filter"] }
tracing-opentelemetry = "1.0"
metrics = "1.0"
metrics-exporter-prometheus = "1.0"

# gRPC å’Œåè®®
tonic = "1.0"
tonic-build = "1.0"
prost = "1.0"
prost-types = "1.0"

# è®¤è¯ä¸ä¼šè¯
jsonwebtoken = "10.0"
session = "1.0"
uuid = { version = "2.0", features = ["v4", "serde"] }

# æ¶ˆæ¯é˜Ÿåˆ—
rdkafka = { version = "1.0", features = ["ssl", "sasl", "dynamic-linking"] }
async-nats = "1.0"

# å®ç”¨å·¥å…·
serde = { version = "2.0", features = ["derive"] }
serde_json = "2.0"
futures = "1.0"
async-trait = "1.0"
thiserror = "2.0"
anyhow = "2.0"
clap = { version = "4.0", features = ["derive"] }
config = "1.0"
tower = "1.0"
http = "1.0"
bytes = "2.0"
```

### 2.1 ä¸»åº”ç”¨æœåŠ¡å™¨å®ç°

```rust
use anyhow::Result;
use clap::Parser;
use pingora::{
    apps::health_check::HealthCheck,
    proxy::{
        ProxyHttp, HttpPipelineHandler, ProxyHttpService, ProxyHttpAction,
        http_proxy_service, ProxyHttp2Tls, ProxyHttpConfiguration,
    },
    services::{
        background::{BackgroundService, BackgroundTask},
        listening::QuicListener,
        Service, ServiceBuilder,
    },
    server::{configuration::ServerConf, Server},
    tls::ServerTlsConfig,
    upstreams::{
        peer::HttpPeer, upstream::UpstreamConfig,
        balance::LoadBalancer,
        health_check::TcpHealthCheck,
    },
};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf, sync::Arc, time::Duration};
use tokio::sync::Mutex;
use tracing::{info, error, warn};

mod auth;
mod config;
mod grpc;
mod logging;
mod metrics;
mod middleware;
mod mq;
mod handlers;

use crate::{
    auth::{AuthManager, AuthMode},
    config::AppConfig,
    grpc::GrpcServiceManager,
    logging::setup_logging,
    metrics::setup_metrics,
    middleware::{
        RateLimiter, RequestMiddleware, ResponseMiddleware,
        SessionManager, SecurityMiddleware, MiddlewareChain
    },
    mq::EventPublisher,
};

// å‘½ä»¤è¡Œå‚æ•°å®šä¹‰
#[derive(Parser, Debug)]
#[clap(author, version, about)]
struct Args {
    #[clap(short, long, value_parser, default_value = "config/default.toml")]
    config: PathBuf,
}

// ä¸»HTTPå¤„ç†å™¨ï¼Œå®ç°æ ¸å¿ƒä»£ç†é€»è¾‘
struct MainProxyHandler {
    upstreams: Arc<HashMap<String, Arc<LoadBalancer<HttpPeer>>>>,
    grpc_services: Arc<GrpcServiceManager>,
    auth_manager: Arc<AuthManager>,
    event_publisher: Arc<EventPublisher>,
    middleware_chain: Arc<MiddlewareChain>,
    config: Arc<AppConfig>,
}

// å®ç°Pingoraçš„HTTPç®¡é“å¤„ç†æ¥å£
#[async_trait::async_trait]
impl HttpPipelineHandler for MainProxyHandler {
    async fn handle_request(
        &self,
        mut req: http::Request<hyper::Body>,
        ctx: &mut HttpContext,
    ) -> Result<ProxyHttpAction, pingora::Error> {
        // 1. æ‰§è¡Œä¸­é—´ä»¶é“¾å¤„ç†è¯·æ±‚
        let middleware_result = self.middleware_chain.process_request(&mut req, ctx).await?;
        if let Some(response) = middleware_result {
            return Ok(ProxyHttpAction::RespondWith(response));
        }

        // 2. è®¤è¯å¤„ç†
        let auth_result = self.auth_manager.authenticate(&req).await;
        if let Err(e) = auth_result {
            let status = match e {
                auth::AuthError::Unauthorized => http::StatusCode::UNAUTHORIZED,
                auth::AuthError::Forbidden => http::StatusCode::FORBIDDEN,
                _ => http::StatusCode::INTERNAL_SERVER_ERROR,
            };

            // æ„å»ºé”™è¯¯å“åº”
            let response = http::Response::builder()
                .status(status)
                .body(hyper::Body::from(e.to_string()))?;

            return Ok(ProxyHttpAction::RespondWith(response));
        }

        // 3. è§£æè·¯ç”±ä»¥ç¡®å®šç›®æ ‡æœåŠ¡
        let path = req.uri().path();
        let service_name = self.extract_service_name(path);

        // 4. åˆ¤æ–­æ˜¯å¦æ˜¯gRPCè¯·æ±‚
        if req.headers().get("content-type")
            .map(|v| v.to_str().unwrap_or(""))
            .unwrap_or("")
            .contains("application/grpc")
        {
            // gRPCè¯·æ±‚å¤„ç†
            return self.handle_grpc_request(req, service_name, ctx).await;
        }

        // 5. æ ‡å‡†HTTPè¯·æ±‚å¤„ç†
        if let Some(upstream) = self.upstreams.get(&service_name) {
            // è®°å½•è·Ÿè¸ªä¿¡æ¯
            let trace_id = ctx.get_tracing_id().unwrap_or_default();
            info!(trace_id = %trace_id, service = %service_name, "Proxying HTTP request");

            // è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„å¯¹ç­‰ç‚¹ï¼Œæ‰§è¡Œè´Ÿè½½å‡è¡¡
            let peer = upstream.get_peer().await?;

            // æ·»åŠ è‡ªå®šä¹‰HTTPå¤´ - è·Ÿè¸ªä¿¡æ¯
            req.headers_mut().insert(
                "X-Trace-ID",
                http::header::HeaderValue::from_str(&trace_id.to_string())?
            );

            // å‘å¸ƒäº‹ä»¶åˆ°æ¶ˆæ¯é˜Ÿåˆ—
            let user_id = self.auth_manager.extract_user_id(&req).unwrap_or_default();
            self.event_publisher.publish_request_event(
                &req,
                &service_name,
                &user_id,
                &trace_id.to_string()
            ).await?;

            // ä»£ç†åˆ°åç«¯æœåŠ¡
            Ok(ProxyHttpAction::Proxy(peer, req))
        } else {
            // æ‰¾ä¸åˆ°æœåŠ¡è¿”å›404
            let resp = http::Response::builder()
                .status(http::StatusCode::NOT_FOUND)
                .body(hyper::Body::from(format!("Service '{}' not found", service_name)))?;

            Ok(ProxyHttpAction::RespondWith(resp))
        }
    }

    // å¤„ç†å“åº”çš„ä¸­é—´ä»¶é’©å­
    async fn handle_response(
        &self,
        res: &mut http::Response<hyper::Body>,
        ctx: &mut HttpContext,
    ) -> Result<(), pingora::Error> {
        // æ‰§è¡Œå“åº”å¤„ç†ä¸­é—´ä»¶é“¾
        self.middleware_chain.process_response(res, ctx).await?;
        Ok(())
    }
}

impl MainProxyHandler {
    // ä»è¯·æ±‚è·¯å¾„æå–æœåŠ¡åç§°
    fn extract_service_name(&self, path: &str) -> String {
        // ç®€å•å®ç°ï¼šæŒ‰ç…§ç¬¬ä¸€æ®µè·¯å¾„æå–æœåŠ¡å
        let parts: Vec<&str> = path.split('/').filter(|p| !p.is_empty()).collect();
        parts.first().map(|s| s.to_string()).unwrap_or_else(|| "default".to_string())
    }

    // å¤„ç†gRPCè¯·æ±‚
    async fn handle_grpc_request(
        &self,
        req: http::Request<hyper::Body>,
        service_name: String,
        ctx: &mut HttpContext,
    ) -> Result<ProxyHttpAction, pingora::Error> {
        let trace_id = ctx.get_tracing_id().unwrap_or_default();

        // æ£€æŸ¥gRPCæœåŠ¡æ˜¯å¦å·²æ³¨å†Œ
        if self.grpc_services.has_service(&service_name) {
            // è®©gRPCæœåŠ¡ç®¡ç†å™¨å¤„ç†è¯·æ±‚
            let grpc_result = self.grpc_services.handle_request(req, &service_name).await;

            match grpc_result {
                Ok(response) => {
                    // å‘å¸ƒgRPCäº‹ä»¶åˆ°æ¶ˆæ¯é˜Ÿåˆ—
                    let user_id = self.auth_manager.extract_user_id(&req).unwrap_or_default();
                    self.event_publisher.publish_grpc_event(
                        &service_name,
                        &user_id,
                        &trace_id.to_string(),
                        true,
                    ).await?;

                    Ok(ProxyHttpAction::RespondWith(response))
                },
                Err(e) => {
                    error!(error = %e, service = %service_name, "gRPC request handling error");

                    // æ„å»ºé”™è¯¯å“åº”
                    let mut response = http::Response::builder()
                        .status(http::StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/grpc")
                        .header("grpc-status", "2") // UNKNOWN
                        .body(hyper::Body::empty())?;

                    self.event_publisher.publish_grpc_event(
                        &service_name,
                        &self.auth_manager.extract_user_id(&req).unwrap_or_default(),
                        &trace_id.to_string(),
                        false,
                    ).await?;

                    Ok(ProxyHttpAction::RespondWith(response))
                }
            }
        } else if let Some(upstream) = self.upstreams.get(&service_name) {
            // å¦‚æœæˆ‘ä»¬æœ‰upstreamé…ç½®ä½†æ²¡æœ‰ç‰¹å®šçš„gRPCå¤„ç†å™¨ï¼Œåˆ™ç›´æ¥ä»£ç†
            let peer = upstream.get_peer().await?;
            Ok(ProxyHttpAction::Proxy(peer, req))
        } else {
            // æœªæ‰¾åˆ°gRPCæœåŠ¡
            let resp = http::Response::builder()
                .status(http::StatusCode::NOT_FOUND)
                .header("content-type", "application/grpc")
                .header("grpc-status", "5") // NOT_FOUND
                .body(hyper::Body::empty())?;

            Ok(ProxyHttpAction::RespondWith(resp))
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    // è§£æå‘½ä»¤è¡Œå‚æ•°
    let args = Args::parse();

    // åŠ è½½é…ç½®
    let config = config::load_config(args.config).await?;
    let config = Arc::new(config);

    // è®¾ç½®æ—¥å¿—å’Œç›‘æ§
    setup_logging(&config.logging)?;
    setup_metrics(&config.metrics)?;

    info!("Starting Pingora HTTP Gateway v{}", env!("CARGO_PKG_VERSION"));

    // åˆ›å»ºPingoraæœåŠ¡å™¨
    let mut server = Server::new(Some(config.server.clone().into()))?;

    // åˆå§‹åŒ–å„ç»„ä»¶
    let auth_manager = Arc::new(AuthManager::new(&config.auth).await?);
    let event_publisher = Arc::new(EventPublisher::new(&config.messaging).await?);
    let grpc_services = Arc::new(GrpcServiceManager::new(&config.grpc).await?);

    // åˆ›å»ºä¸­é—´ä»¶é“¾
    let middleware_chain = Arc::new(MiddlewareChain::new(vec![
        Box::new(SecurityMiddleware::new(&config.security)),
        Box::new(RateLimiter::new(&config.rate_limit)),
        Box::new(SessionManager::new(&config.session)),
        // æ·»åŠ æ›´å¤šä¸­é—´ä»¶...
    ]));

    // åˆå§‹åŒ–ä¸Šæ¸¸æœåŠ¡é…ç½®
    let mut upstreams: HashMap<String, Arc<LoadBalancer<HttpPeer>>> = HashMap::new();

    for (service_name, upstream_config) in &config.upstreams {
        let mut lb_config = LoadBalancerConfig::default();
        lb_config.set_health_check(Box::new(TcpHealthCheck {}));
        lb_config.set_health_check_interval(Duration::from_secs(upstream_config.health_check_interval_secs));

        let mut balancer = LoadBalancer::new(lb_config);

        // æ·»åŠ ä¸Šæ¸¸æœåŠ¡å™¨
        for server in &upstream_config.servers {
            let upstream_addr: SocketAddr = server.parse()?;
            let peer = HttpPeer::new(upstream_addr, UpstreamConfig::default());
            balancer.add_peer(peer).await;
        }

        upstreams.insert(service_name.clone(), Arc::new(balancer));
    }

    // åˆ›å»ºä¸»HTTPå¤„ç†å™¨
    let handler = MainProxyHandler {
        upstreams: Arc::new(upstreams),
        grpc_services,
        auth_manager,
        event_publisher,
        middleware_chain,
        config: config.clone(),
    };

    // é…ç½®HTTPä»£ç†æœåŠ¡
    let mut http_proxy_conf = ProxyHttpConfiguration::default();
    http_proxy_conf.http2_only = false; // æ”¯æŒHTTP/1.xå’ŒHTTP/2

    // åˆ›å»ºHTTPä»£ç†æœåŠ¡
    let mut service = ServiceBuilder::new(ProxyHttp::new(handler, http_proxy_conf));

    // é…ç½®TLS(å¦‚æœå¯ç”¨)
    if config.tls.enabled {
        let tls_config = ServerTlsConfig::new()
            .cert_path(&config.tls.cert_path)
            .key_path(&config.tls.key_path);

        service.add_tcp(config.server.listen_addr)?
              .add_tls(config.server.listen_addr_tls, tls_config)?;

        // æ·»åŠ HTTP/3 (QUIC) æ”¯æŒ
        if config.http3.enabled {
            service.add_quic(
                config.http3.listen_addr,
                tls_config,
                QuicListener::default(),
            )?;
        }
    } else {
        // ä»…æ™®é€šHTTP
        service.add_tcp(config.server.listen_addr)?;
    }

    // æ·»åŠ å¥åº·æ£€æŸ¥æœåŠ¡
    let health_service = ServiceBuilder::new(HealthCheck::new());
    health_service.add_tcp(config.health.listen_addr)?;

    // æ³¨å†ŒæœåŠ¡åˆ°æœåŠ¡å™¨
    server.add_service(service);
    server.add_service(health_service);

    // æ·»åŠ åå°ä»»åŠ¡
    let background_tasks = BackgroundService::new();
    // æ·»åŠ æ‚¨éœ€è¦çš„åå°ä»»åŠ¡ï¼š
    // background_tasks.add_task(Box::new(YourBackgroundTask::new()));
    server.add_service(background_tasks);

    // å¯åŠ¨æœåŠ¡å™¨
    info!("HTTP Gateway started, listening on: {}", config.server.listen_addr);
    server.run_forever();

    Ok(())
}
```

## 3 äºŒã€é…ç½®ã€ç›‘æ§å’Œæ—¥å¿—æ¨¡å—

```rust
// src/config.rs
use std::path::Path;
use serde::Deserialize;
use tokio::fs;
use anyhow::Result;

#[derive(Debug, Clone, Deserialize)]
pub struct AppConfig {
    pub server: ServerConfig,
    pub health: HealthConfig,
    pub tls: TlsConfig,
    pub http3: Http3Config,
    pub auth: AuthConfig,
    pub session: SessionConfig,
    pub security: SecurityConfig,
    pub rate_limit: RateLimitConfig,
    pub logging: LoggingConfig,
    pub metrics: MetricsConfig,
    pub grpc: GrpcConfig,
    pub messaging: MessagingConfig,
    pub upstreams: std::collections::HashMap<String, UpstreamConfig>,
}

#[derive(Debug, Clone, Deserialize)]
pub struct ServerConfig {
    pub listen_addr: std::net::SocketAddr,
    pub listen_addr_tls: std::net::SocketAddr,
    pub workers: usize,
    pub max_connections: usize,
    pub connection_idle_timeout: u64,
}

impl From<ServerConfig> for pingora::server::configuration::ServerConf {
    fn from(config: ServerConfig) -> Self {
        let mut server_conf = Self::default();
        server_conf.num_workers = config.workers;
        server_conf.connection_idle_timeout = config.connection_idle_timeout;
        server_conf.max_active_connections = config.max_connections;
        server_conf
    }
}

#[derive(Debug, Clone, Deserialize)]
pub struct HealthConfig {
    pub listen_addr: std::net::SocketAddr,
}

#[derive(Debug, Clone, Deserialize)]
pub struct TlsConfig {
    pub enabled: bool,
    pub cert_path: String,
    pub key_path: String,
}

#[derive(Debug, Clone, Deserialize)]
pub struct Http3Config {
    pub enabled: bool,
    pub listen_addr: std::net::SocketAddr,
}

#[derive(Debug, Clone, Deserialize)]
pub struct AuthConfig {
    pub mode: String, // "none", "jwt", "session", "mixed"
    pub jwt_secret: String,
    pub jwt_algorithm: String,
    pub session_redis_url: String,
    pub cookie_name: String,
    pub cookie_secure: bool,
    pub cookie_http_only: bool,
    pub token_expiry_seconds: u64,
}

#[derive(Debug, Clone, Deserialize)]
pub struct SessionConfig {
    pub enabled: bool,
    pub store_type: String, // "redis", "memory"
    pub redis_url: String,
    pub session_ttl_seconds: u64,
}

#[derive(Debug, Clone, Deserialize)]
pub struct SecurityConfig {
    pub cors_allowed_origins: Vec<String>,
    pub cors_allowed_methods: Vec<String>,
    pub cors_allowed_headers: Vec<String>,
    pub cors_expose_headers: Vec<String>,
    pub xss_protection: bool,
    pub content_security_policy: String,
    pub frame_options: String,
}

#[derive(Debug, Clone, Deserialize)]
pub struct RateLimitConfig {
    pub enabled: bool,
    pub requests_per_minute: u32,
    pub burst_size: u32,
    pub backend: String, // "memory", "redis"
    pub redis_url: String,
}

#[derive(Debug, Clone, Deserialize)]
pub struct LoggingConfig {
    pub level: String,
    pub format: String, // "json", "text"
    pub otlp_endpoint: String,
    pub log_requests: bool,
    pub log_responses: bool,
    pub sample_rate: f32,
}

#[derive(Debug, Clone, Deserialize)]
pub struct MetricsConfig {
    pub prometheus_endpoint: std::net::SocketAddr,
    pub metrics_prefix: String,
    pub export_interval_secs: u64,
}

#[derive(Debug, Clone, Deserialize)]
pub struct GrpcConfig {
    pub proto_file_dir: String,
    pub services: Vec<GrpcServiceConfig>,
}

#[derive(Debug, Clone, Deserialize)]
pub struct GrpcServiceConfig {
    pub name: String,
    pub upstream_addr: String,
    pub max_message_size: usize,
    pub timeout_seconds: u64,
    pub retry_count: u32,
}

#[derive(Debug, Clone, Deserialize)]
pub struct MessagingConfig {
    pub provider: String, // "kafka", "nats"
    pub kafka_brokers: Vec<String>,
    pub nats_url: String,
    pub topics: std::collections::HashMap<String, String>,
}

#[derive(Debug, Clone, Deserialize)]
pub struct UpstreamConfig {
    pub servers: Vec<String>,
    pub protocol: String, // "http", "https", "grpc"
    pub health_check_interval_secs: u64,
    pub max_retries: u32,
    pub connect_timeout_ms: u64,
}

pub async fn load_config<P: AsRef<Path>>(path: P) -> Result<AppConfig> {
    let content = fs::read_to_string(path).await?;
    let config: AppConfig = toml::from_str(&content)?;
    Ok(config)
}

// src/logging.rs
use anyhow::Result;
use opentelemetry::{
    sdk::{trace as sdktrace, Resource},
    KeyValue,
};
use opentelemetry_otlp::WithExportConfig;
use tracing_subscriber::{
    layer::SubscriberExt,
    util::SubscriberInitExt,
    EnvFilter, fmt,
};
use crate::config::LoggingConfig;

pub fn setup_logging(config: &LoggingConfig) -> Result<()> {
    // åˆ›å»ºOTLPå¯¼å‡ºå™¨
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint(&config.otlp_endpoint)
        )
        .with_trace_config(
            sdktrace::config()
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", "pingora-gateway"),
                    KeyValue::new("service.version", env!("CARGO_PKG_VERSION")),
                ]))
                .with_sampler(sdktrace::Sampler::TraceIdRatioBased(config.sample_rate))
        )
        .install_batch(opentelemetry::runtime::Tokio)?;

    // åˆ›å»ºæ—¥å¿—æ ¼å¼åŒ–å™¨
    let fmt_layer = match config.format.as_str() {
        "json" => fmt::layer().json().boxed(),
        _ => fmt::layer().compact().boxed(),
    };

    // è®¾ç½®è¿‡æ»¤å™¨çº§åˆ«
    let filter_layer = EnvFilter::try_from_default_env()
        .or_else(|_| EnvFilter::try_new(&config.level))
        .unwrap_or_else(|_| EnvFilter::new("info"));

    // ç»„è£…å’Œå®‰è£…è®¢é˜…è€…
    tracing_subscriber::registry()
        .with(filter_layer)
        .with(fmt_layer)
        .with(tracing_opentelemetry::layer().with_tracer(tracer))
        .init();

    tracing::info!("Logging system initialized");
    Ok(())
}

// src/metrics.rs
use anyhow::Result;
use metrics_exporter_prometheus::{Matcher, PrometheusBuilder, PrometheusHandle};
use std::time::Duration;
use tokio::task;
use crate::config::MetricsConfig;

pub fn setup_metrics(config: &MetricsConfig) -> Result<PrometheusHandle> {
    // é…ç½®PrometheusæŒ‡æ ‡å¯¼å‡ºå™¨
    let builder = PrometheusBuilder::new()
        .with_http_listener(config.prometheus_endpoint)
        .add_global_label("service", "pingora-gateway")
        .set_buckets_for_metric(
            Matcher::Full("http_request_duration_seconds".to_string()),
            vec![0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
        )?;

    // æ„å»ºå¹¶å¯åŠ¨æœåŠ¡å™¨
    let handle = builder.install()?;

    // å®šæœŸå‘å¸ƒæŒ‡æ ‡
    let interval = Duration::from_secs(config.export_interval_secs);
    let prefix = config.metrics_prefix.clone();

    task::spawn(async move {
        let mut interval = tokio::time::interval(interval);
        loop {
            interval.tick().await;

            // æ”¶é›†å’Œå‘å¸ƒç³»ç»ŸæŒ‡æ ‡
            let mem_info = sys_info::mem_info().unwrap_or_default();
            metrics::gauge!(
                format!("{}_memory_used_bytes", prefix),
                mem_info.total as f64 - mem_info.free as f64
            );

            let load = sys_info::loadavg().unwrap_or_default();
            metrics::gauge!(format!("{}_load_1m", prefix), load.one);
            metrics::gauge!(format!("{}_load_5m", prefix), load.five);
            metrics::gauge!(format!("{}_load_15m", prefix), load.fifteen);

            // æ”¶é›†è¿›ç¨‹æŒ‡æ ‡
            if let Ok(proc_info) = sys_info::proc_stat() {
                metrics::counter!(
                    format!("{}_cpu_total", prefix),
                    proc_info.running as u64
                );
            }
        }
    });

    tracing::info!("Metrics server started on {}", config.prometheus_endpoint);
    Ok(handle)
}
```

## 4 ä¸‰ã€è®¤è¯å’Œæˆæƒä¸­é—´ä»¶

```rust
// src/auth.rs
use anyhow::Result;
use jsonwebtoken::{decode, encode, Algorithm, DecodingKey, EncodingKey, Header, Validation};
use serde::{Deserialize, Serialize};
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use thiserror::Error;
use http::{Request, Response};
use hyper::Body;
use uuid::Uuid;
use crate::{
    config::AuthConfig,
    middleware::SessionStore
};

// è®¤è¯æ¨¡å¼
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum AuthMode {
    None,
    Jwt,
    Session,
    Mixed,
}

// è®¤è¯é”™è¯¯
#[derive(Debug, Error)]
pub enum AuthError {
    #[error("Unauthorized")]
    Unauthorized,

    #[error("Forbidden")]
    Forbidden,

    #[error("JWT is invalid: {0}")]
    InvalidJwt(String),

    #[error("Session is invalid: {0}")]
    InvalidSession(String),

    #[error("Backend error: {0}")]
    BackendError(String),
}

// JWTè½½è·
#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String,  // ç”¨æˆ·ID
    pub exp: u64,     // è¿‡æœŸæ—¶é—´
    pub iat: u64,     // ç­¾å‘æ—¶é—´
    pub roles: Vec<String>, // è§’è‰²åˆ—è¡¨
}

// è®¤è¯ç”¨æˆ·
#[derive(Debug, Clone)]
pub struct AuthenticatedUser {
    pub user_id: String,
    pub roles: Vec<String>,
    pub authenticated_at: SystemTime,
}

// è®¤è¯ç®¡ç†å™¨
pub struct AuthManager {
    config: AuthConfig,
    mode: AuthMode,
    jwt_encoding_key: Option<EncodingKey>,
    jwt_decoding_key: Option<DecodingKey>,
    session_store: Option<Box<dyn SessionStore>>,
}

impl AuthManager {
    pub async fn new(config: &AuthConfig) -> Result<Self> {
        // è§£æè®¤è¯æ¨¡å¼
        let mode = match config.mode.as_str() {
            "none" => AuthMode::None,
            "jwt" => AuthMode::Jwt,
            "session" => AuthMode::Session,
            "mixed" => AuthMode::Mixed,
            _ => AuthMode::None,
        };

        // åˆå§‹åŒ–JWTå¯†é’¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        let (jwt_encoding_key, jwt_decoding_key) = if mode == AuthMode::Jwt || mode == AuthMode::Mixed {
            let encoding_key = EncodingKey::from_secret(config.jwt_secret.as_bytes());
            let decoding_key = DecodingKey::from_secret(config.jwt_secret.as_bytes());
            (Some(encoding_key), Some(decoding_key))
        } else {
            (None, None)
        };

        // åˆå§‹åŒ–ä¼šè¯å­˜å‚¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        let session_store = if mode == AuthMode::Session || mode == AuthMode::Mixed {
            // è¿™é‡Œä¼šå®ä¾‹åŒ–é€‚å½“çš„ä¼šè¯å­˜å‚¨
            Some(crate::middleware::create_session_store(&config.session_redis_url)?)
        } else {
            None
        };

        Ok(Self {
            config: config.clone(),
            mode,
            jwt_encoding_key,
            jwt_decoding_key,
            session_store,
        })
    }

    // è®¤è¯è¯·æ±‚
    pub async fn authenticate<T>(&self, req: &Request<T>) -> Result<AuthenticatedUser, AuthError> {
        match self.mode {
            AuthMode::None => {
                // æ— è®¤è¯æ¨¡å¼ï¼Œåˆ›å»ºåŒ¿åç”¨æˆ·
                Ok(AuthenticatedUser {
                    user_id: "anonymous".to_string(),
                    roles: vec!["guest".to_string()],
                    authenticated_at: SystemTime::now(),
                })
            },
            AuthMode::Jwt => self.authenticate_jwt(req).await,
            AuthMode::Session => self.authenticate_session(req).await,
            AuthMode::Mixed => {
                // å°è¯•JWTè®¤è¯ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•ä¼šè¯è®¤è¯
                match self.authenticate_jwt(req).await {
                    Ok(user) => Ok(user),
                    Err(_) => self.authenticate_session(req).await,
                }
            }
        }
    }

    // JWTè®¤è¯
    async fn authenticate_jwt<T>(&self, req: &Request<T>) -> Result<AuthenticatedUser, AuthError> {
        // ä»Authorizationå¤´è·å–ä»¤ç‰Œ
        let auth_header = req.headers()
            .get(http::header::AUTHORIZATION)
            .ok_or(AuthError::Unauthorized)?
            .to_str()
            .map_err(|_| AuthError::InvalidJwt("Invalid Authorization header".to_string()))?;

        if !auth_header.starts_with("Bearer ") {
            return Err(AuthError::InvalidJwt("Invalid token format".to_string()));
        }

        let token = &auth_header[7..]; // ç§»é™¤"Bearer "å‰ç¼€

        // éªŒè¯JWT
        let decoding_key = self.jwt_decoding_key.as_ref()
            .ok_or_else(|| AuthError::BackendError("JWT keys not configured".to_string()))?;

        let algorithm = match self.config.jwt_algorithm.as_str() {
            "HS256" => Algorithm::HS256,
            "HS384" => Algorithm::HS384,
            "HS512" => Algorithm::HS512,
            _ => Algorithm::HS256,
        };

        let mut validation = Validation::new(algorithm);
        validation.validate_exp = true;

        let claims = decode::<Claims>(
            token,
            decoding_key,
            &validation
        ).map_err(|e| AuthError::InvalidJwt(e.to_string()))?;

        Ok(AuthenticatedUser {
            user_id: claims.claims.sub,
            roles: claims.claims.roles,
            authenticated_at: SystemTime::now(),
        })
    }

    // ä¼šè¯è®¤è¯
    async fn authenticate_session<T>(&self, req: &Request<T>) -> Result<AuthenticatedUser, AuthError> {
        // è·å–ä¼šè¯cookie
        let cookie_header = req.headers()
            .get(http::header::COOKIE)
            .ok_or(AuthError::Unauthorized)?
            .to_str()
            .map_err(|_| AuthError::InvalidSession("Invalid cookie".to_string()))?;

        // è§£æcookieè·å–ä¼šè¯ID
        let cookies: Vec<&str> = cookie_header.split(';').collect();
        let session_cookie = cookies.iter()
            .find(|c| c.trim().starts_with(&format!("{}=", self.config.cookie_name)))
            .ok_or(AuthError::Unauthorized)?;

        let session_id = session_cookie
            .trim()
            .strip_prefix(&format!("{}=", self.config.cookie_name))
            .ok_or(AuthError::InvalidSession("Invalid session cookie format".to_string()))?;

        // ä»ä¼šè¯å­˜å‚¨è·å–ç”¨æˆ·ä¿¡æ¯
        let session_store = self.session_store.as_ref()
            .ok_or_else(|| AuthError::BackendError("Session store not configured".to_string()))?;

        let session_data = session_store.get(session_id).await
            .map_err(|e| AuthError::BackendError(format!("Session store error: {}", e)))?
            .ok_or(AuthError::Unauthorized)?;

        // è§£æä¼šè¯æ•°æ®
        let user_data: AuthenticatedUser = serde_json::from_
/*
Looks like the code was cut off in the middle of the session authentication method in the auth.rs file.
Let me continue from where it left off, starting with completing the session authentication method, then finish the auth module, and move on to the middleware and gRPC modules.
*/

        // è§£æä¼šè¯æ•°æ®
        let user_data: AuthenticatedUser = serde_json::from_str(&session_data)
            .map_err(|e| AuthError::InvalidSession(format!("Invalid session data: {}", e)))?;

        Ok(user_data)
    }

    // ç”ŸæˆJWTä»¤ç‰Œ
    pub fn generate_jwt(&self, user_id: &str, roles: Vec<String>) -> Result<String, AuthError> {
        let encoding_key = self.jwt_encoding_key.as_ref()
            .ok_or_else(|| AuthError::BackendError("JWT keys not configured".to_string()))?;

        let now = SystemTime::now().duration_since(UNIX_EPOCH)
            .map_err(|e| AuthError::BackendError(format!("System time error: {}", e)))?
            .as_secs();

        let expiry = now + self.config.token_expiry_seconds;

        let claims = Claims {
            sub: user_id.to_string(),
            exp: expiry,
            iat: now,
            roles,
        };

        encode(
            &Header::new(Algorithm::HS256),
            &claims,
            encoding_key,
        ).map_err(|e| AuthError::BackendError(format!("JWT encoding error: {}", e)))
    }

    // åˆ›å»ºæ–°ä¼šè¯
    pub async fn create_session(&self, user: &AuthenticatedUser) -> Result<String, AuthError> {
        let session_store = self.session_store.as_ref()
            .ok_or_else(|| AuthError::BackendError("Session store not configured".to_string()))?;

        // ç”Ÿæˆä¼šè¯ID
        let session_id = Uuid::new_v4().to_string();

        // åºåˆ—åŒ–ç”¨æˆ·æ•°æ®
        let user_data = serde_json::to_string(user)
            .map_err(|e| AuthError::BackendError(format!("User serialization error: {}", e)))?;

        // å­˜å‚¨ä¼šè¯
        session_store.set(&session_id, &user_data, self.config.token_expiry_seconds).await
            .map_err(|e| AuthError::BackendError(format!("Session store error: {}", e)))?;

        Ok(session_id)
    }

    // æ’¤é”€ä¼šè¯
    pub async fn revoke_session(&self, session_id: &str) -> Result<(), AuthError> {
        let session_store = self.session_store.as_ref()
            .ok_or_else(|| AuthError::BackendError("Session store not configured".to_string()))?;

        session_store.delete(session_id).await
            .map_err(|e| AuthError::BackendError(format!("Session store error: {}", e)))
    }
}
```

## 5 å››ã€ä¸­é—´ä»¶å®ç°

```rust
// src/middleware.rs
use anyhow::Result;
use async_trait::async_trait;
use http::{Request, Response, StatusCode};
use hyper::Body;
use pingora::http::{HttpContext};
use std::{
    sync::Arc,
    collections::HashMap,
    time::{Duration, Instant},
};
use tokio::sync::{Mutex, RwLock};
use tracing::{info, warn, error, instrument, Span};
use metrics::{counter, gauge, histogram};
use crate::config::{
    SecurityConfig, RateLimitConfig, SessionConfig,
};

// ä¼šè¯å­˜å‚¨æ¥å£
#[async_trait]
pub trait SessionStore: Send + Sync {
    async fn get(&self, key: &str) -> Result<Option<String>>;
    async fn set(&self, key: &str, value: &str, ttl_seconds: u64) -> Result<()>;
    async fn delete(&self, key: &str) -> Result<()>;
}

// Redisä¼šè¯å­˜å‚¨
pub struct RedisSessionStore {
    client: redis::Client,
    pool: deadpool_redis::Pool,
}

#[async_trait]
impl SessionStore for RedisSessionStore {
    async fn get(&self, key: &str) -> Result<Option<String>> {
        let mut conn = self.pool.get().await?;
        let result: Option<String> = redis::cmd("GET")
            .arg(key)
            .query_async(&mut conn)
            .await?;
        Ok(result)
    }

    async fn set(&self, key: &str, value: &str, ttl_seconds: u64) -> Result<()> {
        let mut conn = self.pool.get().await?;
        redis::cmd("SETEX")
            .arg(key)
            .arg(ttl_seconds)
            .arg(value)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }

    async fn delete(&self, key: &str) -> Result<()> {
        let mut conn = self.pool.get().await?;
        redis::cmd("DEL")
            .arg(key)
            .query_async(&mut conn)
            .await?;
        Ok(())
    }
}

// å†…å­˜ä¼šè¯å­˜å‚¨
pub struct MemorySessionStore {
    store: Arc<RwLock<HashMap<String, (String, Instant)>>>,
    ttl: Duration,
}

#[async_trait]
impl SessionStore for MemorySessionStore {
    async fn get(&self, key: &str) -> Result<Option<String>> {
        let store = self.store.read().await;
        if let Some((value, expiry)) = store.get(key) {
            if expiry > &Instant::now() {
                return Ok(Some(value.clone()));
            }
        }
        Ok(None)
    }

    async fn set(&self, key: &str, value: &str, ttl_seconds: u64) -> Result<()> {
        let mut store = self.store.write().await;
        let expiry = Instant::now() + Duration::from_secs(ttl_seconds);
        store.insert(key.to_string(), (value.to_string(), expiry));
        Ok(())
    }

    async fn delete(&self, key: &str) -> Result<()> {
        let mut store = self.store.write().await;
        store.remove(key);
        Ok(())
    }
}

// åˆ›å»ºä¼šè¯å­˜å‚¨çš„å·¥å‚å‡½æ•°
pub fn create_session_store(redis_url: &str) -> Result<Box<dyn SessionStore>> {
    if redis_url.is_empty() || redis_url == "memory" {
        // ä½¿ç”¨å†…å­˜å­˜å‚¨
        Ok(Box::new(MemorySessionStore {
            store: Arc::new(RwLock::new(HashMap::new())),
            ttl: Duration::from_secs(3600), // é»˜è®¤1å°æ—¶
        }))
    } else {
        // ä½¿ç”¨Rediså­˜å‚¨
        let client = redis::Client::open(redis_url)?;
        let pool = deadpool_redis::Config {
            url: Some(redis_url.to_string()),
            connection: None,
            pool: None,
        }
        .create_pool(Some(deadpool_redis::Runtime::Tokio1))?;

        Ok(Box::new(RedisSessionStore { client, pool }))
    }
}

// è¯·æ±‚ä¸­é—´ä»¶æ¥å£
#[async_trait]
pub trait RequestMiddleware: Send + Sync {
    async fn process_request(
        &self,
        req: &mut Request<Body>,
        ctx: &mut HttpContext,
    ) -> Result<Option<Response<Body>>>;
}

// å“åº”ä¸­é—´ä»¶æ¥å£
#[async_trait]
pub trait ResponseMiddleware: Send + Sync {
    async fn process_response(
        &self,
        res: &mut Response<Body>,
        ctx: &HttpContext,
    ) -> Result<()>;
}

// ä¸­é—´ä»¶é“¾
pub struct MiddlewareChain {
    middleware: Vec<Box<dyn RequestMiddleware>>,
    response_middleware: Vec<Box<dyn ResponseMiddleware>>,
}

impl MiddlewareChain {
    pub fn new(middleware: Vec<Box<dyn RequestMiddleware>>) -> Self {
        Self {
            middleware,
            response_middleware: Vec::new(),
        }
    }

    pub fn add_response_middleware(&mut self, middleware: Box<dyn ResponseMiddleware>) {
        self.response_middleware.push(middleware);
    }

    pub async fn process_request(
        &self,
        req: &mut Request<Body>,
        ctx: &mut HttpContext,
    ) -> Result<Option<Response<Body>>> {
        // è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
        let start = Instant::now();
        let method = req.method().to_string();
        let uri = req.uri().to_string();

        // å¢åŠ è¯·æ±‚è®¡æ•°
        counter!("http_requests_total", 1, "method" => method.clone(), "path" => uri.clone());

        // æ‰§è¡Œæ‰€æœ‰è¯·æ±‚ä¸­é—´ä»¶
        for mw in &self.middleware {
            if let Some(response) = mw.process_request(req, ctx).await? {
                // ä¸­é—´ä»¶æå‰è¿”å›å“åº”
                let duration = start.elapsed().as_secs_f64();
                histogram!("http_request_duration_seconds", duration,
                           "method" => method, "path" => uri,
                           "status" => response.status().as_u16().to_string());

                return Ok(Some(response));
            }
        }

        // æ ‡è®°è¯·æ±‚å·²ç»é€šè¿‡æ‰€æœ‰ä¸­é—´ä»¶å¤„ç†
        ctx.extensions_mut().insert("request_start_time", start);

        Ok(None)
    }

    pub async fn process_response(
        &self,
        res: &mut Response<Body>,
        ctx: &HttpContext,
    ) -> Result<()> {
        // è®¡ç®—è¯·æ±‚å¤„ç†æ—¶é—´
        if let Some(start) = ctx.extensions().get::<Instant>("request_start_time") {
            let duration = start.elapsed().as_secs_f64();
            let method = ctx.extensions().get::<String>("request_method")
                .map_or("UNKNOWN", |s| s.as_str());
            let uri = ctx.extensions().get::<String>("request_uri")
                .map_or("UNKNOWN", |s| s.as_str());

            // è®°å½•è¯·æ±‚å¤„ç†æ—¶é—´
            histogram!("http_request_duration_seconds", duration,
                       "method" => method, "path" => uri,
                       "status" => res.status().as_u16().to_string());
        }

        // æ‰§è¡Œæ‰€æœ‰å“åº”ä¸­é—´ä»¶
        for mw in &self.response_middleware {
            mw.process_response(res, ctx).await?;
        }

        Ok(())
    }
}

// å®‰å…¨ä¸­é—´ä»¶
pub struct SecurityMiddleware {
    config: SecurityConfig,
}

impl SecurityMiddleware {
    pub fn new(config: &SecurityConfig) -> Self {
        Self {
            config: config.clone(),
        }
    }
}

#[async_trait]
impl RequestMiddleware for SecurityMiddleware {
    async fn process_request(
        &self,
        req: &mut Request<Body>,
        ctx: &mut HttpContext,
    ) -> Result<Option<Response<Body>>> {
        // å¤„ç†CORS
        if req.method() == http::Method::OPTIONS {
            let origin = req.headers().get("origin")
                .and_then(|v| v.to_str().ok())
                .unwrap_or("");

            // æ£€æŸ¥æ˜¯å¦æ˜¯å…è®¸çš„æº
            let allowed = self.config.cors_allowed_origins.iter()
                .any(|allowed| allowed == "*" || allowed == origin);

            if allowed {
                let mut response = Response::builder()
                    .status(StatusCode::NO_CONTENT)
                    .body(Body::empty())?;

                // æ·»åŠ CORSå¤´
                let headers = response.headers_mut();
                headers.insert("Access-Control-Allow-Origin",
                    origin.parse().unwrap_or_else(|_| "*".parse().unwrap()));

                headers.insert("Access-Control-Allow-Methods",
                    self.config.cors_allowed_methods.join(", ").parse()?);

                headers.insert("Access-Control-Allow-Headers",
                    self.config.cors_allowed_headers.join(", ").parse()?);

                headers.insert("Access-Control-Max-Age",
                    "86400".parse()?);

                return Ok(Some(response));
            }
        }

        // æ·»åŠ å®‰å…¨å¤´éƒ¨
        ctx.extensions_mut().insert("security_headers", true);

        Ok(None)
    }
}

#[async_trait]
impl ResponseMiddleware for SecurityMiddleware {
    async fn process_response(
        &self,
        res: &mut Response<Body>,
        ctx: &HttpContext,
    ) -> Result<()> {
        if ctx.extensions().get::<bool>("security_headers").is_some() {
            let headers = res.headers_mut();

            // æ·»åŠ å®‰å…¨å¤´éƒ¨
            if self.config.xss_protection {
                headers.insert("X-XSS-Protection", "1; mode=block".parse()?);
            }

            if !self.config.content_security_policy.is_empty() {
                headers.insert("Content-Security-Policy",
                    self.config.content_security_policy.parse()?);
            }

            if !self.config.frame_options.is_empty() {
                headers.insert("X-Frame-Options",
                    self.config.frame_options.parse()?);
            }

            // æ·»åŠ CORSå¤´åˆ°å“åº”ä¸­
            if let Some(origin) = ctx.request_headers().get("origin") {
                // æ£€æŸ¥æ˜¯å¦æ˜¯å…è®¸çš„æº
                let origin_str = origin.to_str().unwrap_or("");
                let allowed = self.config.cors_allowed_origins.iter()
                    .any(|allowed| allowed == "*" || allowed == origin_str);

                if allowed {
                    headers.insert("Access-Control-Allow-Origin", origin.clone());

                    if !self.config.cors_expose_headers.is_empty() {
                        headers.insert("Access-Control-Expose-Headers",
                            self.config.cors_expose_headers.join(", ").parse()?);
                    }
                }
            }
        }

        Ok(())
    }
}

// é™æµä¸­é—´ä»¶
pub struct RateLimiter {
    config: RateLimitConfig,
    limiter: Option<governor::RateLimiter<governor::state::NotKeyed, governor::state::InMemoryState, governor::clock::DefaultClock>>,
}

impl RateLimiter {
    pub fn new(config: &RateLimitConfig) -> Self {
        let limiter = if config.enabled {
            // åˆ›å»ºé™æµå™¨
            let limit = governor::Quota::per_minute(nonzero_ext::nonzero!(config.requests_per_minute u32));
            let burst = config.burst_size;
            Some(
                governor::RateLimiter::direct(
                    governor::state::NotKeyed::default(),
                    limit,
                    governor::clock::DefaultClock::default(),
                )
            )
        } else {
            None
        };

        Self {
            config: config.clone(),
            limiter,
        }
    }
}

#[async_trait]
impl RequestMiddleware for RateLimiter {
    async fn process_request(
        &self,
        req: &mut Request<Body>,
        ctx: &mut HttpContext,
    ) -> Result<Option<Response<Body>>> {
        if let Some(limiter) = &self.limiter {
            match limiter.check() {
                Ok(_) => {
                    // å…è®¸è¯·æ±‚
                    Ok(None)
                },
                Err(negative) => {
                    // æ‹’ç»è¯·æ±‚ - è¶…å‡ºé™æµ
                    let wait_time = negative.wait_time_from(std::time::Instant::now());

                    // å¢åŠ é™æµè®¡æ•°
                    counter!("rate_limiter_blocked_requests_total", 1);

                    // ç”Ÿæˆæ‹’ç»å“åº”
                    let response = Response::builder()
                        .status(StatusCode::TOO_MANY_REQUESTS)
                        .header("Retry-After", wait_time.as_secs().to_string())
                        .body(Body::from("Too many requests, please try again later"))?;

                    Ok(Some(response))
                }
            }
        } else {
            // é™æµæœªå¯ç”¨
            Ok(None)
        }
    }
}

// ä¼šè¯ç®¡ç†ä¸­é—´ä»¶
pub struct SessionManager {
    config: SessionConfig,
    store: Option<Box<dyn SessionStore>>,
}

impl SessionManager {
    pub fn new(config: &SessionConfig) -> Self {
        let store = if config.enabled {
            match create_session_store(&config.redis_url) {
                Ok(store) => Some(store),
                Err(e) => {
                    error!("Failed to create session store: {}", e);
                    None
                }
            }
        } else {
            None
        };

        Self {
            config: config.clone(),
            store,
        }
    }
}

#[async_trait]
impl RequestMiddleware for SessionManager {
    async fn process_request(
        &self,
        req: &mut Request<Body>,
        ctx: &mut HttpContext,
    ) -> Result<Option<Response<Body>>> {
        if !self.config.enabled || self.store.is_none() {
            return Ok(None);
        }

        // ä»cookieä¸­æå–ä¼šè¯ID
        if let Some(cookie) = req.headers().get(http::header::COOKIE) {
            if let Ok(cookie_str) = cookie.to_str() {
                for cookie_part in cookie_str.split(';') {
                    let cookie_kv: Vec<&str> = cookie_part.trim().splitn(2, '=').collect();
                    if cookie_kv.len() == 2 && cookie_kv[0] == "session_id" {
                        let session_id = cookie_kv[1];

                        // åŠ è½½ä¼šè¯
                        if let Some(store) = &self.store {
                            if let Ok(Some(session_data)) = store.get(session_id).await {
                                // ä¼šè¯æœ‰æ•ˆï¼Œå­˜å‚¨åˆ°ä¸Šä¸‹æ–‡ä¸­
                                ctx.extensions_mut().insert("session_id", session_id.to_string());
                                ctx.extensions_mut().insert("session_data", session_data);
                            }
                        }
                        break;
                    }
                }
            }
        }

        Ok(None)
    }
}
```

## 6 äº”ã€gRPCæœåŠ¡é›†æˆ

```rust
// src/grpc.rs
use anyhow::Result;
use bytes::Bytes;
use futures::stream::StreamExt;
use http::{Request, Response, StatusCode};
use hyper::Body;
use pingora::http::HttpContext;
use prost::Message;
use std::{collections::HashMap, sync::Arc};
use tokio::sync::{mpsc, RwLock};
use tonic::{
    transport::{Channel, Endpoint},
    Request as TonicRequest,
    Response as TonicResponse,
    Status,
};
use tower::service_fn;
use tracing::{error, debug, info, warn, instrument};

use crate::config::GrpcConfig;

// gRPCé”™è¯¯ç±»å‹
#[derive(Debug, thiserror::Error)]
pub enum GrpcError {
    #[error("Service not found: {0}")]
    ServiceNotFound(String),

    #[error("Method not found: {0}")]
    MethodNotFound(String),

    #[error("Connection error: {0}")]
    ConnectionError(#[from] tonic::transport::Error),

    #[error("Decode error: {0}")]
    DecodeError(#[from] prost::DecodeError),

    #[error("Encode error: {0}")]
    EncodeError(#[from] prost::EncodeError),

    #[error("gRPC status error: {0}")]
    StatusError(#[from] tonic::Status),

    #[error("Hyper error: {0}")]
    HyperError(#[from] hyper::Error),

    #[error("Other error: {0}")]
    Other(String),
}

// gRPCæœåŠ¡æè¿°
#[derive(Debug, Clone)]
pub struct GrpcServiceDescriptor {
    name: String,
    methods: HashMap<String, GrpcMethodDescriptor>,
    endpoint: String,
    timeout_ms: u64,
    retries: u32,
}

// gRPCæ–¹æ³•æè¿°
#[derive(Debug, Clone)]
pub struct GrpcMethodDescriptor {
    name: String,
    streaming: bool,
    input_type: String,
    output_type: String,
}

// gRPCæœåŠ¡ç®¡ç†å™¨
pub struct GrpcServiceManager {
    services: Arc<RwLock<HashMap<String, GrpcServiceDescriptor>>>,
    channels: Arc<RwLock<HashMap<String, Channel>>>,
    config: GrpcConfig,
}

impl GrpcServiceManager {
    pub async fn new(config: &GrpcConfig) -> Result<Self> {
        let services = Arc::new(RwLock::new(HashMap::new()));
        let channels = Arc::new(RwLock::new(HashMap::new()));

        let manager = Self {
            services,
            channels,
            config: config.clone(),
        };

        // ä»é…ç½®åŠ è½½æœåŠ¡æè¿°
        for (service_name, service_config) in &config.services {
            manager.register_service(
                service_name,
                &service_config.endpoint,
                service_config.timeout_ms,
                service_config.retries,
                service_config.methods.clone(),
            ).await?;
        }

        Ok(manager)
    }

    // æ³¨å†ŒgRPCæœåŠ¡
    pub async fn register_service(
        &self,
        name: &str,
        endpoint: &str,
        timeout_ms: u64,
        retries: u32,
        methods: HashMap<String, HashMap<String, String>>,
    ) -> Result<()> {
        // åˆ›å»ºæœåŠ¡æè¿°ç¬¦
        let mut method_descriptors = HashMap::new();

        for (method_name, method_info) in methods {
            let streaming = method_info.get("streaming")
                .map(|s| s == "true")
                .unwrap_or(false);

            let input_type = method_info.get("input_type")
                .cloned()
                .unwrap_or_else(|| "".to_string());

            let output_type = method_info.get("output_type")
                .cloned()
                .unwrap_or_else(|| "".to_string());

            let descriptor = GrpcMethodDescriptor {
                name: method_name.clone(),
                streaming,
                input_type,
                output_type,
            };

            method_descriptors.insert(method_name, descriptor);
        }

        let service_descriptor = GrpcServiceDescriptor {
            name: name.to_string(),
            methods: method_descriptors,
            endpoint: endpoint.to_string(),
            timeout_ms,
            retries,
        };

        // å­˜å‚¨æœåŠ¡æè¿°ç¬¦
        let mut services = self.services.write().await;
        services.insert(name.to_string(), service_descriptor);

        // åˆ›å»ºå’Œå­˜å‚¨Channel
        let channel = self.create_channel(endpoint).await?;

        let mut channels = self.channels.write().await;
        channels.insert(name.to_string(), channel);

        info!("Registered gRPC service: {}", name);
        Ok(())
    }

    // åˆ›å»ºgRPCé€šé“
    async fn create_channel(&self, endpoint: &str) -> Result<Channel> {
        let channel = Endpoint::from_shared(endpoint.to_string())?
            .connect()
            .await?;

        Ok(channel)
    }

    // å¤„ç†gRPCè¯·æ±‚
    pub async fn handle_grpc_request(
        &self,
        request: Request<Body>,
        service_name: &str,
        method_name: &str,
    ) -> Result<Response<Body>, GrpcError> {
        // è·å–æœåŠ¡æè¿°ç¬¦
        let services = self.services.read().await;
        let service = services.get(service_name)
            .ok_or_else(|| GrpcError::ServiceNotFound(service_name.to_string()))?;

        // è·å–æ–¹æ³•æè¿°ç¬¦
        let method = service.methods.get(method_name)
            .ok_or_else(|| GrpcError::MethodNotFound(method_name.to_string()))?;

        // è·å–é€šé“
        let channels = self.channels.read().await;
        let channel = channels.get(service_name)
            .ok_or_else(|| GrpcError::ConnectionError(tonic::transport::Error::new_other("No channel found")))?;

        // ä»HTTPè¯·æ±‚ä½“è¯»å–äºŒè¿›åˆ¶æ•°æ®
        let body_bytes = hyper::body::to_bytes(request.into_body()).await?;

        if method.streaming {
            // å¤„ç†æµå¼è¯·æ±‚
            self.handle_streaming_request(
                service,
                method,
                channel.clone(),
                body_bytes,
            ).await
        } else {
            // å¤„ç†ä¸€å…ƒè¯·æ±‚
            self.handle_unary_request(
                service,
                method,
                channel.clone(),
                body_bytes,
            ).await
        }
    }

    // å¤„ç†ä¸€å…ƒgRPCè¯·æ±‚
    async fn handle_unary_request(
        &self,
        service: &GrpcServiceDescriptor,
        method: &GrpcMethodDescriptor,
        channel: Channel,
        request_bytes: Bytes,
    ) -> Result<Response<Body>, GrpcError> {
        // æ„å»ºè¦è½¬å‘çš„è¯·æ±‚
        let path = format!("/{}.{}/{}", service.name, service.name, method.name);
        debug!("Forwarding unary gRPC request to path: {}", path);

        // ä½¿ç”¨tonicåˆ›å»ºgRPCè¯·æ±‚
        let mut client = tonic::client::Grpc::new(channel);

        // è®¾ç½®è¶…æ—¶
        let timeout = std::time::Duration::from_millis(service.timeout_ms);

        // è®¾ç½®é‡è¯•ç­–ç•¥
        for retry in 0..=service.retries {
            match client.unary(
                path.clone(),
                request_bytes.clone(),
                tonic::codec::ProstCodec::default(),
                timeout,
            ).await {
                Ok(response) => {
                    // æ„å»ºHTTPå“åº”
                    let http_response = Response::builder()
                        .status(StatusCode::OK)
                        .header("content-type", "application/grpc+proto")
                        .header("grpc-status", "0")
                        .body(Body::from(response.get_ref().clone()))?;

                    return Ok(http_response);
                },
                Err(status) if retry < service.retries => {
                    // å¯é‡è¯•çš„é”™è¯¯
                    warn!(
                        "gRPC request failed, retrying ({}/{}): {} - {}",
                        retry + 1,
                        service.retries,
                        status.code(),
                        status.message()
                    );

                    // æ·»åŠ å»¶è¿Ÿåé‡è¯•
                    tokio::time::sleep(std::time::Duration::from_millis(
                        100 * (2_u64.pow(retry) as u64) // æŒ‡æ•°é€€é¿
                    )).await;
                },
                Err(status) => {
                    // æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥æˆ–ä¸å¯é‡è¯•çš„é”™è¯¯
                    error!(
                        "gRPC request failed after {} retries: {} - {}",
                        retry,
                        status.code(),
                        status.message()
                    );

                    // æ„å»ºé”™è¯¯å“åº”
                    let http_response = Response::builder()
                        .status(StatusCode::INTERNAL_SERVER_ERROR)
                        .header("content-type", "application/grpc+proto")
                        .header("grpc-status", status.code().to_string())
                        .header("grpc-message", status.message())
                        .body(Body::empty())?;

                    return Ok(http_response);
                }
            }
        }

        // ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
        Err(GrpcError::Other("Unexpected error in gRPC request handling".to_string()))
    }

    // å¤„ç†æµå¼gRPCè¯·æ±‚
    async fn handle_streaming_request(
        &self,
        service: &GrpcServiceDescriptor,
        method: &GrpcMethodDescriptor,
        channel: Channel,
        request_bytes: Bytes,
    ) -> Result<Response<Body>, GrpcError> {
        // æ„å»ºè¦è½¬å‘çš„è¯·æ±‚
        let path = format!("/{}.{}/{}", service.name, service.name, method.name);
        debug!("Forwarding streaming gRPC request to path: {}", path);

        // ä½¿ç”¨tonicåˆ›å»ºgRPCå®¢æˆ·ç«¯
        let mut client = tonic::client::Grpc::new(channel);

        // è®¾ç½®è¶…æ—¶
        let timeout = std::time::Duration::from_millis(service.timeout_ms);

        // åˆ›å»ºç®¡é“ç”¨äºæµå¼å“åº”
        let (tx, rx) = mpsc::channel(128);

        // å¼‚æ­¥å¤„ç†æµå¼è¯·æ±‚
        tokio::spawn(async move {
            // å°è¯•æµå¼è¯·æ±‚
            match client.streaming(
                path,
                request_bytes,
                tonic::codec::ProstCodec::default(),
                timeout,
            ).await {
                Ok(mut response_stream) => {
                    // å¤„ç†å“åº”æµ
                    while let Some(result) = response_stream.next().await {
                        match result {
                            Ok(data) => {
                                if tx.send(Ok(data)).await.is_err() {
                                    // å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
                                    break;
                                }
                            },
                            Err(status) => {
                                let _ = tx.send(Err(status)).await;
                                break;
                            }
                        }
                    }
                },
                Err(status) => {
                    let _ = tx.send(Err(status)).await;
                }
            }
        });

        // åˆ›å»ºå¯¹åº”çš„HTTPæµå“åº”
        let stream_body = Body::wrap_stream(tokio_stream::wrappers::ReceiverStream::new(rx)
            .map(|result| {
                match result {
                    Ok(data) => Ok(data),
                    Err(status) => {
                        let error_bytes = format!(
                            "gRPC error: {} - {}",
                            status.code(),
                            status.message()
                        ).into_bytes();
                        Ok(Bytes::from(error_bytes))
                    }
                }
            }));

        // æ„å»ºHTTPå“åº”
        let http_response = Response::builder()
            .status(StatusCode::OK)
            .header("content-type", "application/grpc+proto")
            .body(stream_body)?;

        Ok(http_response)
    }
}
```

## 7 å…­ã€æ¶ˆæ¯é˜Ÿåˆ—é›†æˆå’Œç”¨æˆ·è¡Œä¸ºè·Ÿè¸ª

```rust
// src/mq.rs
use anyhow::Result;
use async_trait::async_trait;
use rdkafka::{
    config::ClientConfig,
    producer::{FutureProducer, FutureRecord},
    message::OwnedHeaders,
    consumer::{Consumer, StreamConsumer, CommitMode},
    util::Timeout,
    error::KafkaError,
};
use async_nats::{Client as NatsClient, Connection, Options as NatsOptions};
use serde::{Serialize, Deserialize};
use std::{sync::Arc, time::Duration};
use tokio::{
    sync::RwLock,
    task::JoinHandle,
};
use tracing::{info, error, warn, instrument, Span};
use uuid::Uuid;
use crate::config::MessagingConfig;

// äº‹ä»¶ç±»å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Event {
    pub id: String,
    pub event_type: String,
    pub timestamp: String,
    pub source: String,
    pub data: serde_json::Value,
}

impl Event {
    pub fn new(event_type: &str, data: serde_json::Value) -> Self {
        Self {
            id: Uuid::new_v4().to_string(),
            event_type: event_type.to_string(),
            timestamp: chrono::Utc::now().to_rfc3339(),
            source: "pingora-gateway".to_string(),
            data,
        }
    }
}

// äº‹ä»¶å‘å¸ƒå™¨
pub struct EventPublisher {
    config: MessagingConfig,
    kafka_producer: Option<FutureProducer>,
    nats_client: Option<NatsClient>,
}

impl EventPublisher {
    pub async fn new(config: &MessagingConfig) -> Result<Self> {
        let kafka_producer = if config.use_kafka {
            // åˆ›å»ºKafkaç”Ÿäº§è€…
            let producer: FutureProducer = ClientConfig::new()
                .set("bootstrap.servers", &config.kafka_bootstrap_servers)
                .set("message.timeout.ms", "5000")
                .set("compression.type", "snappy")
                .set("request.required.acks", "1")
                .create()?;

            Some(producer)
        } else {
            None
        };

        let nats_client = if config.use_nats {
            // åˆ›å»ºNATSå®¢æˆ·ç«¯
            let options = NatsOptions::new()
                .with_connection_name("pingora-gateway".to_string());

            let client = async_nats::connect(&config.nats_server_url).await?;
            Some(client)
        } else {
            None
        };

        Ok(Self {
            config: config.clone(),
            kafka_producer,
            nats_client,
        })
    }

    // å‘å¸ƒäº‹ä»¶åˆ°Kafka
    pub async fn publish_to_kafka(
        &self,
        topic: &str,
        event: &Event,
    ) -> Result<(), KafkaError> {
        if let Some(producer) = &self.kafka_producer {
            let payload = serde_json::to_string(event)
                .map_err(|e| KafkaError::MessageProduction(
                    rdkafka::error::RDKafkaErrorCode::UnknownError,
                    format!("JSON serialization error: {}", e)
                ))?;

            let headers = OwnedHeaders::new()
                .add("event_type", &event.event_type)
                .add("source", "pingora-gateway");

            producer.send(
                FutureRecord::to(topic)
                    .payload(&payload)
                    .key(&event.id)
                    .headers(headers),
                Duration::from_secs(5),
            ).await.map(|_| ())?;

            Ok(())
        } else {
            Err(KafkaError::MessageProduction(
                rdkafka::error::RDKafkaErrorCode::UnknownError,
                "Kafka producer not configured".to_string()
            ))
        }
    }

    // å‘å¸ƒäº‹ä»¶åˆ°NATS
    pub async fn publish_to_nats(
        &self,
        subject: &str,
        event: &Event,
    ) -> Result<(), async_nats::Error> {
        if let Some(client) = &self.nats_client {
            let payload = serde_json::to_string(event)
                .map_err(|e| async_nats::Error::Other(format!("JSON serialization error: {}", e)))?;

            client.publish(subject, payload.into()).await?;
            Ok(())
        } else {
            Err(async_nats::Error::Other("NATS client not configured".to_string()))
        }
    }

    // ç»¼åˆå‘å¸ƒäº‹ä»¶
    pub async fn publish_event(&self, event_type: &str, data: serde_json::Value) -> Result<()> {
        let event = Event::new(event_type, data);

        // ç¡®å®šè¦å‘å¸ƒåˆ°çš„ç›®æ ‡
        let topic = match event_type {
            "user.click" | "user.view" | "user.search" => "user_behaviors",
            "auth.login" | "auth.logout" | "auth.failure" => "auth_events",
            "api.request" | "api.response" | "api.error" => "api_events",
            _ => "misc_events",
        };

        // å°è¯•å‘å¸ƒåˆ°Kafka
        if let Some(_) = &self.kafka_producer {
            match self.publish_to_kafka(topic, &event).await {
                Ok(_) => info!("Published event to Kafka: {}", event_type),
                Err(e) => error!("Failed to publish event to Kafka: {}", e),
            }
        }

        // å°è¯•å‘å¸ƒåˆ°NATS
        if let Some(_) = &self.nats_client {
            let subject = format!("events.{}", event_type.replace(".", "."));
            match self.publish_to_nats(&subject, &event).await {
/*
æˆ‘å°†ç»§ç»­å®ŒæˆMQäº‹ä»¶å‘å¸ƒå™¨çš„ä»£ç ï¼Œ
ç„¶åå®ç°ç”¨æˆ·è¡Œä¸ºè·Ÿè¸ªå™¨ï¼Œæœ€åè¿›è¡Œä¸»åº”ç”¨çš„ç»„è£…ä»¥å®Œæˆæ•´ä¸ªç³»ç»Ÿã€‚
*/
                Ok(_) => info!("Published event to NATS: {}", event_type),
                Err(e) => error!("Failed to publish event to NATS: {}", e),
            }
        }

        Ok(())
    }
}

// äº‹ä»¶è®¢é˜…å™¨trait
#[async_trait]
pub trait EventSubscriber: Send + Sync {
    async fn start(&self) -> Result<()>;
    async fn stop(&self) -> Result<()>;
}

// Kafkaäº‹ä»¶è®¢é˜…å™¨
pub struct KafkaEventSubscriber {
    config: MessagingConfig,
    topic: String,
    group_id: String,
    consumer: Arc<StreamConsumer>,
    handler: Box<dyn Fn(Event) -> Result<()> + Send + Sync>,
    task_handle: Arc<RwLock<Option<JoinHandle<()>>>>,
}

impl KafkaEventSubscriber {
    pub fn new<F>(
        config: &MessagingConfig,
        topic: &str,
        group_id: &str,
        handler: F,
    ) -> Result<Self>
    where
        F: Fn(Event) -> Result<()> + Send + Sync + 'static,
    {
        // åˆ›å»ºKafkaæ¶ˆè´¹è€…
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", &config.kafka_bootstrap_servers)
            .set("group.id", group_id)
            .set("enable.auto.commit", "true")
            .set("auto.offset.reset", "earliest")
            .set("session.timeout.ms", "6000")
            .create()?;

        Ok(Self {
            config: config.clone(),
            topic: topic.to_string(),
            group_id: group_id.to_string(),
            consumer: Arc::new(consumer),
            handler: Box::new(handler),
            task_handle: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl EventSubscriber for KafkaEventSubscriber {
    async fn start(&self) -> Result<()> {
        let consumer = self.consumer.clone();
        let topic = self.topic.clone();
        let handler = self.handler.clone();

        // è®¢é˜…ä¸»é¢˜
        consumer.subscribe(&[&topic])?;

        // å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡
        let task_handle = tokio::spawn(async move {
            info!("Started Kafka consumer for topic: {}", topic);

            loop {
                match consumer.recv().await {
                    Ok(msg) => {
                        let payload = match msg.payload_view::<str>() {
                            Some(Ok(s)) => s,
                            Some(Err(_)) => {
                                error!("Failed to decode Kafka message as UTF-8");
                                continue;
                            }
                            None => {
                                warn!("Empty Kafka message received");
                                continue;
                            }
                        };

                        // è§£æäº‹ä»¶
                        match serde_json::from_str::<Event>(payload) {
                            Ok(event) => {
                                if let Err(e) = handler(event) {
                                    error!("Error processing event: {}", e);
                                }
                            }
                            Err(e) => {
                                error!("Failed to parse event: {}", e);
                            }
                        }

                        // æäº¤æ¶ˆæ¯
                        consumer.commit_message(&msg, CommitMode::Async).unwrap_or_else(|e| {
                            error!("Failed to commit Kafka message: {}", e);
                        });
                    }
                    Err(e) => {
                        error!("Kafka consumer error: {}", e);
                        tokio::time::sleep(Duration::from_secs(1)).await;
                    }
                }
            }
        });

        // å­˜å‚¨ä»»åŠ¡å¥æŸ„
        let mut handle = self.task_handle.write().await;
        *handle = Some(task_handle);

        Ok(())
    }

    async fn stop(&self) -> Result<()> {
        let mut handle = self.task_handle.write().await;
        if let Some(task) = handle.take() {
            task.abort();
            info!("Stopped Kafka consumer for topic: {}", self.topic);
        }

        Ok(())
    }
}

// NATSäº‹ä»¶è®¢é˜…å™¨
pub struct NatsEventSubscriber {
    config: MessagingConfig,
    subject: String,
    client: Arc<NatsClient>,
    connection: Arc<Connection>,
    handler: Box<dyn Fn(Event) -> Result<()> + Send + Sync>,
    task_handle: Arc<RwLock<Option<JoinHandle<()>>>>,
}

impl NatsEventSubscriber {
    pub async fn new<F>(
        config: &MessagingConfig,
        subject: &str,
        handler: F,
    ) -> Result<Self>
    where
        F: Fn(Event) -> Result<()> + Send + Sync + 'static,
    {
        // åˆ›å»ºNATSå®¢æˆ·ç«¯
        let options = NatsOptions::new()
            .with_connection_name("pingora-subscriber".to_string());

        let connection = async_nats::connect(&config.nats_server_url).await?;
        let client = connection.clone();

        Ok(Self {
            config: config.clone(),
            subject: subject.to_string(),
            client: Arc::new(client),
            connection: Arc::new(connection),
            handler: Box::new(handler),
            task_handle: Arc::new(RwLock::new(None)),
        })
    }
}

#[async_trait]
impl EventSubscriber for NatsEventSubscriber {
    async fn start(&self) -> Result<()> {
        let client = self.client.clone();
        let subject = self.subject.clone();
        let handler = self.handler.clone();

        // è®¢é˜…ä¸»é¢˜
        let mut subscriber = client.subscribe(subject.clone()).await?;

        // å¯åŠ¨æ¶ˆè´¹ä»»åŠ¡
        let task_handle = tokio::spawn(async move {
            info!("Started NATS subscriber for subject: {}", subject);

            while let Some(msg) = subscriber.next().await {
                let payload = String::from_utf8_lossy(&msg.payload);

                // è§£æäº‹ä»¶
                match serde_json::from_str::<Event>(&payload) {
                    Ok(event) => {
                        if let Err(e) = handler(event) {
                            error!("Error processing NATS event: {}", e);
                        }
                    }
                    Err(e) => {
                        error!("Failed to parse NATS event: {}", e);
                    }
                }
            }

            info!("NATS subscriber for subject {} stopped", subject);
        });

        // å­˜å‚¨ä»»åŠ¡å¥æŸ„
        let mut handle = self.task_handle.write().await;
        *handle = Some(task_handle);

        Ok(())
    }

    async fn stop(&self) -> Result<()> {
        let mut handle = self.task_handle.write().await;
        if let Some(task) = handle.take() {
            task.abort();
            info!("Stopped NATS subscriber for subject: {}", self.subject);
        }

        Ok(())
    }
}

// ç”¨æˆ·è¡Œä¸ºè·Ÿè¸ªå™¨
pub struct UserBehaviorTracker {
    event_publisher: Arc<EventPublisher>,
}

impl UserBehaviorTracker {
    pub fn new(event_publisher: Arc<EventPublisher>) -> Self {
        Self { event_publisher }
    }

    pub async fn track_request(
        &self,
        req: &http::Request<hyper::Body>,
        ctx: &HttpContext,
    ) -> Result<()> {
        // æå–ç”¨æˆ·ID (å¦‚æœæœ‰)
        let user_id = ctx.extensions()
            .get::<String>("user_id")
            .map(|id| id.clone())
            .unwrap_or_else(|| "anonymous".to_string());

        // æå–è¯·æ±‚ä¿¡æ¯
        let method = req.method().to_string();
        let path = req.uri().path().to_string();
        let query = req.uri().query().unwrap_or("").to_string();

        // åˆ›å»ºç”¨æˆ·è¡Œä¸ºæ•°æ®
        let data = serde_json::json!({
            "user_id": user_id,
            "method": method,
            "path": path,
            "query": query,
            "user_agent": req.headers().get(http::header::USER_AGENT)
                .and_then(|h| h.to_str().ok())
                .unwrap_or(""),
            "referer": req.headers().get(http::header::REFERER)
                .and_then(|h| h.to_str().ok())
                .unwrap_or(""),
            "ip": ctx.extensions()
                .get::<String>("client_ip")
                .map(|ip| ip.clone())
                .unwrap_or_else(|| "unknown".to_string()),
        });

        // å‘å¸ƒäº‹ä»¶
        self.event_publisher.publish_event("api.request", data).await?;

        Ok(())
    }

    pub async fn track_response(
        &self,
        req: &http::Request<hyper::Body>,
        res: &http::Response<hyper::Body>,
        ctx: &HttpContext,
        duration_ms: u64,
    ) -> Result<()> {
        // æå–ç”¨æˆ·ID (å¦‚æœæœ‰)
        let user_id = ctx.extensions()
            .get::<String>("user_id")
            .map(|id| id.clone())
            .unwrap_or_else(|| "anonymous".to_string());

        // æå–è¯·æ±‚å’Œå“åº”ä¿¡æ¯
        let method = req.method().to_string();
        let path = req.uri().path().to_string();
        let status = res.status().as_u16();

        // åˆ›å»ºå“åº”æ•°æ®
        let data = serde_json::json!({
            "user_id": user_id,
            "method": method,
            "path": path,
            "status": status,
            "duration_ms": duration_ms,
            "content_type": res.headers().get(http::header::CONTENT_TYPE)
                .and_then(|h| h.to_str().ok())
                .unwrap_or(""),
            "content_length": res.headers().get(http::header::CONTENT_LENGTH)
                .and_then(|h| h.to_str().ok())
                .unwrap_or("0"),
        });

        // å‘å¸ƒäº‹ä»¶
        let event_type = if status >= 400 {
            "api.error"
        } else {
            "api.response"
        };

        self.event_publisher.publish_event(event_type, data).await?;

        Ok(())
    }

    pub async fn track_user_action(
        &self,
        action: &str,
        user_id: &str,
        data: serde_json::Value,
    ) -> Result<()> {
        // åˆå¹¶æ•°æ®
        let mut event_data = data;
        if let serde_json::Value::Object(ref mut map) = event_data {
            map.insert("user_id".to_string(), serde_json::Value::String(user_id.to_string()));
            map.insert("timestamp".to_string(), serde_json::Value::String(
                chrono::Utc::now().to_rfc3339()
            ));
        }

        // å‘å¸ƒäº‹ä»¶
        self.event_publisher.publish_event(&format!("user.{}", action), event_data).await?;

        Ok(())
    }
}
```

## 8 ä¸ƒã€é…ç½®æ–‡ä»¶ç¤ºä¾‹

```toml
# config/default.toml
[server]
listen_addr = "0.0.0.0:8080"
listen_addr_tls = "0.0.0.0:8443"
workers = 8
max_connections = 10000
connection_idle_timeout = 60  # ç§’

[health]
listen_addr = "0.0.0.0:9090"

[tls]
enabled = true
cert_path = "certs/server.crt"
key_path = "certs/server.key"

[http3]
enabled = true
listen_addr = "0.0.0.0:8443"

[auth]
mode = "jwt"  # none, jwt, session, mixed
jwt_secret = "your-secret-key-here-make-it-long-and-secure"
jwt_algorithm = "HS256"
session_redis_url = "redis://127.0.0.1:6379/0"
cookie_name = "session_id"
cookie_secure = true
cookie_http_only = true
token_expiry_seconds = 3600

[session]
enabled = true
store_type = "redis"  # redis, memory
redis_url = "redis://127.0.0.1:6379/0"
session_ttl_seconds = 3600

[security]
enable_cors = true
cors_allow_origins = ["https://example.com", "https://api.example.com"]
cors_allow_methods = ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
cors_allow_headers = ["Authorization", "Content-Type"]
cors_expose_headers = ["X-Request-ID"]
cors_max_age = 86400
cors_allow_credentials = true

enable_csp = true
csp = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline';"

xss_protection = "1; mode=block"
content_type_options = "nosniff"
frame_options = "DENY"
hsts_enabled = true
hsts_max_age = 31536000  # 1å¹´

[rate_limit]
enabled = true
requests_per_second = 10
burst = 50

[logging]
level = "info"
format = "json"  # json, text
otlp_endpoint = "http://127.0.0.1:4317"
sample_rate = 0.1

[metrics]
prometheus_endpoint = "0.0.0.0:9091"
metrics_prefix = "pingora_http_gateway"
export_interval_secs = 15

[grpc]
services = {
  "auth" = {
    endpoint = "http://localhost:50051",
    timeout_ms = 5000,
    retries = 3,
    methods = [
      { name = "Login", streaming = false, input_type = "LoginRequest", output_type = "LoginResponse" },
      { name = "Logout", streaming = false, input_type = "LogoutRequest", output_type = "LogoutResponse" },
      { name = "ValidateToken", streaming = false, input_type = "ValidateTokenRequest", output_type = "ValidateTokenResponse" }
    ]
  },
  "user" = {
    endpoint = "http://localhost:50052",
    timeout_ms = 3000,
    retries = 2,
    methods = [
      { name = "GetUser", streaming = false, input_type = "GetUserRequest", output_type = "GetUserResponse" },
      { name = "UpdateUser", streaming = false, input_type = "UpdateUserRequest", output_type = "UpdateUserResponse" },
      { name = "StreamUserEvents", streaming = true, input_type = "StreamUserEventsRequest", output_type = "UserEvent" }
    ]
  }
}

[messaging]
use_kafka = true
kafka_bootstrap_servers = "localhost:9092"
use_nats = true
nats_server_url = "nats://localhost:4222"

# ä¸Šæ¸¸æœåŠ¡é…ç½®
[upstreams.api]
servers = ["127.0.0.1:3000", "127.0.0.1:3001"]
health_check_interval_secs = 5

[upstreams.auth]
servers = ["127.0.0.1:3010"]
health_check_interval_secs = 5

[upstreams.admin]
servers = ["127.0.0.1:3020", "127.0.0.1:3021"]
health_check_interval_secs = 10
```

## 9 å…«ã€éƒ¨ç½²ä¸Dockeré…ç½®

```dockerfile
# Dockerfile
FROM rust:1.75 as builder

WORKDIR /app

# å®‰è£…æ„å»ºä¾èµ–
RUN apt-get update && \
    apt-get install -y \
    protobuf-compiler \
    cmake \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY Cargo.toml Cargo.lock ./
COPY src ./src/
COPY proto ./proto/
COPY build.rs ./

# æ„å»ºåº”ç”¨
RUN cargo build --release

# åˆ›å»ºæœ€ç»ˆé•œåƒ
FROM debian:bookworm-slim

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && \
    apt-get install -y \
    ca-certificates \
    libssl3 \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r pingora && useradd -r -g pingora pingora

WORKDIR /app

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶äºŒè¿›åˆ¶æ–‡ä»¶
COPY --from=builder /app/target/release/pingora-gateway /app/pingora-gateway

# å¤åˆ¶é…ç½®æ–‡ä»¶
COPY config /app/config/
COPY certs /app/certs/

# è®¾ç½®æƒé™
RUN chown -R pingora:pingora /app && \
    chmod -R 755 /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER pingora

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV RUST_LOG=info

# æš´éœ²ç«¯å£
EXPOSE 8080 8443 9090 9091

# å¯åŠ¨åº”ç”¨
CMD ["/app/pingora-gateway", "--config", "/app/config/default.toml"]
```

ä½¿ç”¨Docker Composeè¿›è¡Œéƒ¨ç½²ï¼š

```yaml
# docker-compose.yml
version: '3.8'

services:
  pingora-gateway:
    build: .
    ports:
      - "8080:8080"
      - "8443:8443"
      - "9090:9090"  # å¥åº·æ£€æŸ¥ç«¯å£
      - "9091:9091"  # æŒ‡æ ‡ç«¯å£
    volumes:
      - ./config:/app/config
      - ./certs:/app/certs
    environment:
      - RUST_LOG=info
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
    depends_on:
      - redis
      - kafka
      - nats
      - otel-collector
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Redis for session storage
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped

  # Kafka for messaging
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    restart: unless-stopped

  # NATS for messaging
  nats:
    image: nats:2.10-alpine
    ports:
      - "4222:4222"
      - "8222:8222"
    restart: unless-stopped

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # metrics
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    command: ["--config=/etc/otel-collector-config.yaml"]
    restart: unless-stopped

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:v2.48.0
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    restart: unless-stopped

  # Jaeger for tracing visualization
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"   # UI
      - "14250:14250"   # gRPC for tracing
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    restart: unless-stopped

volumes:
  redis-data:
  zookeeper-data:
  kafka-data:
  prometheus-data:
```

## 10 ä¹ã€æ€»ç»“ä¸åŠŸèƒ½æ¦‚è§ˆ

ä½¿ç”¨Rust 2025ç‰ˆPingoraå®ç°çš„åˆ†å¸ƒå¼HTTPé›†ç¾¤æœåŠ¡å…·æœ‰ä»¥ä¸‹æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§ï¼š

1. **å®Œæ•´çš„HTTPåè®®æ”¯æŒ**
   - HTTP/1.x, HTTP/2, HTTP/3 (QUIC)æ”¯æŒ
   - æ”¯æŒTLSå’ŒéTLSè¿æ¥
   - å®Œæ•´çš„HTTPè¯­ä¹‰å¤„ç†ï¼ŒåŒ…æ‹¬é‡å®šå‘ç­‰

2. **å…¨é¢çš„è§‚æµ‹æ€§**
   - åˆ†å¸ƒå¼è·Ÿè¸ª (OpenTelemetry)
   - ç»“æ„åŒ–æ—¥å¿—è®°å½•
   - PrometheusæŒ‡æ ‡é›†æˆ
   - å®Œæ•´çš„è¯·æ±‚-å“åº”ç›‘æ§

3. **å®‰å…¨è®¤è¯ä¸æˆæƒ**
   - JWTä»¤ç‰Œè®¤è¯
   - ä¼šè¯ç®¡ç† (Redis/å†…å­˜)
   - çµæ´»çš„è®¤è¯ç­–ç•¥ (JWT/ä¼šè¯/æ··åˆ)
   - CORSå’Œå…¶ä»–å®‰å…¨å¤´å¤„ç†

4. **gRPCé›†æˆ**
   - è‡ªåŠ¨åè®®è½¬æ¢(HTTP -> gRPC)
   - åŒå‘æµæ”¯æŒ
   - è‡ªåŠ¨é‡è¯•å’Œå¹‚ç­‰æ”¯æŒ
   - æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡

5. **æ¶ˆæ¯é˜Ÿåˆ—é›†æˆ**
   - Kafkaå’ŒNATSæ”¯æŒ
   - ç”¨æˆ·è¡Œä¸ºäº‹ä»¶è·Ÿè¸ª
   - å¼‚æ­¥äº‹ä»¶å¤„ç†
   - äº‹ä»¶å‘å¸ƒ-è®¢é˜…æ¨¡å¼

6. **é«˜çº§è¯·æ±‚å¤„ç†**
   - è¯·æ±‚è·¯ç”±å’Œè§„åˆ™
   - é™æµå’Œç†”æ–­
   - è¯·æ±‚/å“åº”è½¬æ¢
   - ä¸­é—´ä»¶å¤„ç†é“¾

æ­¤æ¶æ„ä¸ºç°ä»£APIç½‘å…³æä¾›äº†æ‰€æœ‰å¿…è¦çš„åŠŸèƒ½ï¼Œå…·å¤‡é«˜æ€§èƒ½ã€é«˜å¯é æ€§å’Œå¯è§‚æµ‹æ€§ï¼Œéå¸¸é€‚åˆæ„å»ºå¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿã€‚
Rustçš„å®‰å…¨æ€§å’Œæ€§èƒ½ä¼˜åŠ¿ï¼ŒåŠ ä¸ŠPingoraçš„ä¸“ä¸šç½‘å…³åŠŸèƒ½ï¼Œä½¿è¿™ä¸ªæ–¹æ¡ˆæˆä¸ºé«˜æµé‡ã€å®‰å…¨æ•æ„Ÿç¯å¢ƒçš„ç†æƒ³é€‰æ‹©ã€‚
