# ç»¼åˆåº”ç”¨

## ğŸ“‹ ç›®å½•

- [ç»¼åˆåº”ç”¨](#ç»¼åˆåº”ç”¨)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1 åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡](#1-åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡)
    - [1.1 ç»¼åˆåº”ç”¨01-ç®€å•çš„ç¤ºä¾‹åº”ç”¨ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰€æœ‰è¿™äº›ç»„ä»¶](#11-ç»¼åˆåº”ç”¨01-ç®€å•çš„ç¤ºä¾‹åº”ç”¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰€æœ‰è¿™äº›ç»„ä»¶)
    - [1.2 ç»¼åˆåº”ç”¨02-åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒå™¨](#12-ç»¼åˆåº”ç”¨02-åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒå™¨)
    - [1.3 ç»¼åˆåº”ç”¨03-Rediså®ç°çš„å»¶è¿Ÿä»»åŠ¡å­˜å‚¨](#13-ç»¼åˆåº”ç”¨03-rediså®ç°çš„å»¶è¿Ÿä»»åŠ¡å­˜å‚¨)
    - [1.4 ç»¼åˆåº”ç”¨04-åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ](#14-ç»¼åˆåº”ç”¨04-åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ)
    - [1.5 ç»¼åˆåº”ç”¨05-åˆ†å¸ƒå¼æ—¥å¿—ç³»ç»Ÿ](#15-ç»¼åˆåº”ç”¨05-åˆ†å¸ƒå¼æ—¥å¿—ç³»ç»Ÿ)
    - [1.6 ç»¼åˆåº”ç”¨06-åˆ†å¸ƒå¼æœç´¢å¼•æ“](#16-ç»¼åˆåº”ç”¨06-åˆ†å¸ƒå¼æœç´¢å¼•æ“)
    - [1.7 ç»¼åˆåº”ç”¨07-åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“](#17-ç»¼åˆåº”ç”¨07-åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“)
    - [1.8 ç»¼åˆåº”ç”¨08-åˆ†å¸ƒå¼å›¾æ•°æ®åº“](#18-ç»¼åˆåº”ç”¨08-åˆ†å¸ƒå¼å›¾æ•°æ®åº“)
    - [1.9 ç»¼åˆåº”ç”¨09-åˆ†å¸ƒå¼é”æœåŠ¡](#19-ç»¼åˆåº”ç”¨09-åˆ†å¸ƒå¼é”æœåŠ¡)
    - [1.10 ç»¼åˆåº”ç”¨10-åˆ†å¸ƒå¼é…ç½®æœåŠ¡](#110-ç»¼åˆåº”ç”¨10-åˆ†å¸ƒå¼é…ç½®æœåŠ¡)
    - [1.11 ç»¼åˆåº”ç”¨11-åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—](#111-ç»¼åˆåº”ç”¨11-åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—)
    - [1.12 ç»¼åˆåº”ç”¨12-åˆ†å¸ƒå¼å…±è¯†å¼•æ“](#112-ç»¼åˆåº”ç”¨12-åˆ†å¸ƒå¼å…±è¯†å¼•æ“)
    - [1.13 ç»¼åˆåº”ç”¨13-åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ](#113-ç»¼åˆåº”ç”¨13-åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ)
    - [1.14 ç»¼åˆåº”ç”¨14-åˆ†å¸ƒå¼æœç´¢å¼•æ“](#114-ç»¼åˆåº”ç”¨14-åˆ†å¸ƒå¼æœç´¢å¼•æ“)
    - [1.15 ç»¼åˆåº”ç”¨15-åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ](#115-ç»¼åˆåº”ç”¨15-åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ)
    - [1.16 ç»¼åˆåº”ç”¨16-åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ](#116-ç»¼åˆåº”ç”¨16-åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ)
    - [1.17 ç»¼åˆåº”ç”¨17-åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ](#117-ç»¼åˆåº”ç”¨17-åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ)

---

## 1 åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡

### 1.1 ç»¼åˆåº”ç”¨01-ç®€å•çš„ç¤ºä¾‹åº”ç”¨ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰€æœ‰è¿™äº›ç»„ä»¶

```rust
// ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹åº”ç”¨ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰€æœ‰è¿™äº›ç»„ä»¶
struct UserService {
    framework: Arc<DistributedSystemFramework>,
    user_repository: Arc<UserRepository>,
    event_publisher: EventPublisher,
    cache_client: CacheClient,
}

struct User {
    id: String,
    username: String,
    email: String,
    created_at: SystemTime,
    updated_at: SystemTime,
}

struct UserRepository {
    db_connection: Arc<DatabaseConnection>,
    metrics_registry: Arc<MetricsRegistry>,
}

struct DatabaseConnection {
    connection_string: String,
    connection_pool: Arc<ConnectionPool>,
}

struct ConnectionPool {
    // ç®€åŒ–å®ç°
    connections: Mutex<Vec<Connection>>,
    max_connections: usize,
}

struct Connection {
    id: String,
    created_at: Instant,
    last_used: Mutex<Instant>,
}

impl UserRepository {
    fn new(db_connection: Arc<DatabaseConnection>, metrics_registry: Arc<MetricsRegistry>) -> Self {
        UserRepository {
            db_connection,
            metrics_registry,
        }
    }

    fn find_by_id(&self, id: &str) -> Result<Option<User>, String> {
        // åˆ›å»ºåº¦é‡
        let timer = self.metrics_registry.timer(
            "repository.user.find_by_id",
            "æŸ¥æ‰¾ç”¨æˆ·é€šè¿‡IDçš„æ—¶é—´",
        );

        // å¼€å§‹è®¡æ—¶
        let timer_id = timer.start();

        // æ¨¡æ‹Ÿæ•°æ®åº“è°ƒç”¨
        let result = self.db_connection.execute_query(&format!("SELECT * FROM users WHERE id = '{}'", id))
            .map(|rows| {
                if rows.is_empty() {
                    None
                } else {
                    // å‡è®¾rows[0]æ˜¯ç¬¬ä¸€è¡Œæ•°æ®
                    let row = &rows[0];
                    Some(User {
                        id: row.get("id").unwrap().to_string(),
                        username: row.get("username").unwrap().to_string(),
                        email: row.get("email").unwrap().to_string(),
                        created_at: SystemTime::now(), // æ¨¡æ‹Ÿ
                        updated_at: SystemTime::now(), // æ¨¡æ‹Ÿ
                    })
                }
            });

        // åœæ­¢è®¡æ—¶
        timer.stop(&timer_id);

        result
    }

    fn save(&self, user: &User) -> Result<(), String> {
        // åˆ›å»ºåº¦é‡
        let timer = self.metrics_registry.timer(
            "repository.user.save",
            "ä¿å­˜ç”¨æˆ·çš„æ—¶é—´",
        );

        // å¼€å§‹è®¡æ—¶
        let timer_id = timer.start();

        // æ¨¡æ‹Ÿä¿å­˜ç”¨æˆ·
        let query = format!(
            "INSERT INTO users (id, username, email, created_at, updated_at) VALUES ('{}', '{}', '{}', NOW(), NOW()) ON DUPLICATE KEY UPDATE username = '{}', email = '{}', updated_at = NOW()",
            user.id, user.username, user.email, user.username, user.email
        );

        let result = self.db_connection.execute_update(&query);

        // åœæ­¢è®¡æ—¶
        timer.stop(&timer_id);

        result
    }
}

impl DatabaseConnection {
    fn new(connection_string: &str, max_connections: usize) -> Self {
        DatabaseConnection {
            connection_string: connection_string.to_string(),
            connection_pool: Arc::new(ConnectionPool {
                connections: Mutex::new(Vec::with_capacity(max_connections)),
                max_connections,
            }),
        }
    }

    fn get_connection(&self) -> Result<Arc<Connection>, String> {
        let mut connections = self.connection_pool.connections.lock().unwrap();

        // æŸ¥æ‰¾å¯ç”¨è¿æ¥
        if let Some(conn) = connections.pop() {
            let connection = Arc::new(conn);
            *connection.last_used.lock().unwrap() = Instant::now();
            return Ok(connection);
        }

        // å¦‚æœæ²¡æœ‰å¯ç”¨è¿æ¥ä¸”æœªè¾¾åˆ°æœ€å¤§è¿æ¥æ•°ï¼Œåˆ›å»ºæ–°è¿æ¥
        if connections.len() < self.connection_pool.max_connections {
            let connection = Arc::new(Connection {
                id: uuid::Uuid::new_v4().to_string(),
                created_at: Instant::now(),
                last_used: Mutex::new(Instant::now()),
            });

            return Ok(connection);
        }

        Err("æ— æ³•è·å–æ•°æ®åº“è¿æ¥ï¼šè¿æ¥æ± å·²æ»¡".to_string())
    }

    fn execute_query(&self, query: &str) -> Result<Vec<HashMap<String, String>>, String> {
        // è·å–è¿æ¥
        let connection = self.get_connection()?;

        // æ¨¡æ‹Ÿæ‰§è¡ŒæŸ¥è¯¢
        println!("æ‰§è¡ŒæŸ¥è¯¢: {}", query);

        // æ¨¡æ‹Ÿè¿”å›ç»“æœ
        let mut result = Vec::new();
        if query.contains("WHERE id = '1'") {
            let mut row = HashMap::new();
            row.insert("id".to_string(), "1".to_string());
            row.insert("username".to_string(), "admin".to_string());
            row.insert("email".to_string(), "admin@example.com".to_string());
            result.push(row);
        }

        Ok(result)
    }

    fn execute_update(&self, query: &str) -> Result<(), String> {
        // è·å–è¿æ¥
        let connection = self.get_connection()?;

        // æ¨¡æ‹Ÿæ‰§è¡Œæ›´æ–°
        println!("æ‰§è¡Œæ›´æ–°: {}", query);

        Ok(())
    }
}

impl UserService {
    fn new(
        framework: Arc<DistributedSystemFramework>,
        user_repository: Arc<UserRepository>,
    ) -> Self {
        UserService {
            event_publisher: framework.create_event_publisher("user-service"),
            cache_client: framework.create_cache_client("user-cache"),
            framework,
            user_repository,
        }
    }

    fn get_user(&self, id: &str) -> Result<Option<User>, String> {
        // åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªè·¨åº¦
        let tracer = &self.framework.tracer;
        let span = tracer.start_span("UserService.get_user");
        tracer.set_tag(&span, "user_id", id);

        // ä»ç¼“å­˜è·å–
        let cache_key = format!("user:{}", id);
        let cache_result = self.cache_client.get::<User>(&cache_key);

        let result = match cache_result {
            Ok(Some(user)) => {
                // ç¼“å­˜å‘½ä¸­
                tracer.set_tag(&span, "cache_hit", "true");
                Ok(Some(user))
            },
            Ok(None) => {
                // ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“è·å–
                tracer.set_tag(&span, "cache_hit", "false");

                match self.user_repository.find_by_id(id) {
                    Ok(Some(user)) => {
                        // æ‰¾åˆ°ç”¨æˆ·ï¼Œç¼“å­˜ç»“æœ
                        let _ = self.cache_client.set(&cache_key, &user, Some(Duration::from_secs(300)));
                        Ok(Some(user))
                    },
                    Ok(None) => Ok(None),
                    Err(err) => {
                        tracer.set_tag(&span, "error", "true");
                        tracer.log_kv(&span, {
                            let mut fields = HashMap::new();
                            fields.insert("error.kind".to_string(), "database_error".to_string());
                            fields.insert("error.message".to_string(), err.clone());
                            fields
                        });
                        Err(err)
                    }
                }
            },
            Err(err) => {
                // ç¼“å­˜é”™è¯¯ï¼Œç›´æ¥ä»æ•°æ®åº“è·å–
                tracer.set_tag(&span, "cache_error", "true");

                match self.user_repository.find_by_id(id) {
                    Ok(Some(user)) => Ok(Some(user)),
                    Ok(None) => Ok(None),
                    Err(db_err) => Err(format!("ç¼“å­˜é”™è¯¯: {}, æ•°æ®åº“é”™è¯¯: {}", err, db_err)),
                }
            }
        };

        // å®Œæˆè·Ÿè¸ªè·¨åº¦
        tracer.finish_span(&span);

        result
    }

    fn create_user(&self, username: &str, email: &str) -> Result<User, String> {
        // åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªè·¨åº¦
        let tracer = &self.framework.tracer;
        let span = tracer.start_span("UserService.create_user");

        // åˆ›å»ºç”¨æˆ·
        let user = User {
            id: uuid::Uuid::new_v4().to_string(),
            username: username.to_string(),
            email: email.to_string(),
            created_at: SystemTime::now(),
            updated_at: SystemTime::now(),
        };

        // ä¿å­˜ç”¨æˆ·
        match self.user_repository.save(&user) {
            Ok(_) => {
                // ç¼“å­˜ç”¨æˆ·
                let cache_key = format!("user:{}", user.id);
                let _ = self.cache_client.set(&cache_key, &user, Some(Duration::from_secs(300)));

                // å‘å¸ƒç”¨æˆ·åˆ›å»ºäº‹ä»¶
                let mut payload = serde_json::Map::new();
                payload.insert("user_id".to_string(), serde_json::Value::String(user.id.clone()));
                payload.insert("username".to_string(), serde_json::Value::String(user.username.clone()));
                payload.insert("email".to_string(), serde_json::Value::String(user.email.clone()));

                let _ = self.event_publisher.publish("user.created", serde_json::Value::Object(payload));

                Ok(user)
            },
            Err(err) => {
                tracer.set_tag(&span, "error", "true");
                tracer.log_kv(&span, {
                    let mut fields = HashMap::new();
                    fields.insert("error.kind".to_string(), "database_error".to_string());
                    fields.insert("error.message".to_string(), err.clone());
                    fields
                });

                Err(err)
            }
        }
    }

    fn update_user(&self, id: &str, username: &str, email: &str) -> Result<User, String> {
        // åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªè·¨åº¦
        let tracer = &self.framework.tracer;
        let span = tracer.start_span("UserService.update_user");
        tracer.set_tag(&span, "user_id", id);

        // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        match self.get_user(id) {
            Ok(Some(mut user)) => {
                // æ›´æ–°ç”¨æˆ·ä¿¡æ¯
                user.username = username.to_string();
                user.email = email.to_string();
                user.updated_at = SystemTime::now();

                // ä¿å­˜ç”¨æˆ·
                match self.user_repository.save(&user) {
                    Ok(_) => {
                        // æ›´æ–°ç¼“å­˜
                        let cache_key = format!("user:{}", user.id);
                        let _ = self.cache_client.set(&cache_key, &user, Some(Duration::from_secs(300)));

                        // å‘å¸ƒç”¨æˆ·æ›´æ–°äº‹ä»¶
                        let mut payload = serde_json::Map::new();
                        payload.insert("user_id".to_string(), serde_json::Value::String(user.id.clone()));
                        payload.insert("username".to_string(), serde_json::Value::String(user.username.clone()));
                        payload.insert("email".to_string(), serde_json::Value::String(user.email.clone()));

                        let _ = self.event_publisher.publish("user.updated", serde_json::Value::Object(payload));

                        Ok(user)
                    },
                    Err(err) => {
                        tracer.set_tag(&span, "error", "true");
                        tracer.log_kv(&span, {
                            let mut fields = HashMap::new();
                            fields.insert("error.kind".to_string(), "database_error".to_string());
                            fields.insert("error.message".to_string(), err.clone());
                            fields
                        });

                        Err(err)
                    }
                }
            },
            Ok(None) => {
                tracer.set_tag(&span, "error", "true");
                tracer.log_kv(&span, {
                    let mut fields = HashMap::new();
                    fields.insert("error.kind".to_string(), "not_found".to_string());
                    fields.insert("error.message".to_string(), format!("ç”¨æˆ· {} ä¸å­˜åœ¨", id));
                    fields
                });

                Err(format!("ç”¨æˆ· {} ä¸å­˜åœ¨", id))
            },
            Err(err) => {
                tracer.set_tag(&span, "error", "true");
                tracer.log_kv(&span, {
                    let mut fields = HashMap::new();
                    fields.insert("error.kind".to_string(), "error".to_string());
                    fields.insert("error.message".to_string(), err.clone());
                    fields
                });

                Err(err)
            }
        }
    }

    fn delete_user(&self, id: &str) -> Result<(), String> {
        // åˆ›å»ºä¸€ä¸ªè·Ÿè¸ªè·¨åº¦
        let tracer = &self.framework.tracer;
        let span = tracer.start_span("UserService.delete_user");
        tracer.set_tag(&span, "user_id", id);

        // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        match self.get_user(id) {
            Ok(Some(_)) => {
                // æ¨¡æ‹Ÿåˆ é™¤ç”¨æˆ·
                // åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥è°ƒç”¨repository.deleteæ–¹æ³•
                println!("åˆ é™¤ç”¨æˆ·: {}", id);

                // åˆ é™¤ç¼“å­˜
                let cache_key = format!("user:{}", id);
                let _ = self.cache_client.delete(&cache_key);

                // å‘å¸ƒç”¨æˆ·åˆ é™¤äº‹ä»¶
                let mut payload = serde_json::Map::new();
                payload.insert("user_id".to_string(), serde_json::Value::String(id.to_string()));

                let _ = self.event_publisher.publish("user.deleted", serde_json::Value::Object(payload));

                Ok(())
            },
            Ok(None) => {
                tracer.set_tag(&span, "error", "true");
                tracer.log_kv(&span, {
                    let mut fields = HashMap::new();
                    fields.insert("error.kind".to_string(), "not_found".to_string());
                    fields.insert("error.message".to_string(), format!("ç”¨æˆ· {} ä¸å­˜åœ¨", id));
                    fields
                });

                Err(format!("ç”¨æˆ· {} ä¸å­˜åœ¨", id))
            },
            Err(err) => {
                tracer.set_tag(&span, "error", "true");
                tracer.log_kv(&span, {
                    let mut fields = HashMap::new();
                    fields.insert("error.kind".to_string(), "error".to_string());
                    fields.insert("error.message".to_string(), err.clone());
                    fields
                });

                Err(err)
            }
        }
    }
}

// éƒ¨ç½²å’Œç¼–æ’å·¥å…·
struct DeploymentManager {
    service_registry: Arc<ServiceRegistry>,
    health_check_registry: Arc<HealthCheckRegistry>,
    config_system: Arc<DistributedConfigSystem>,
}

struct ServiceDeployment {
    service_id: String,
    service_name: String,
    version: String,
    instances: Vec<ServiceDeploymentInstance>,
    desired_instances: usize,
    deployment_strategy: DeploymentStrategy,
    health_check_path: String,
    environment_variables: HashMap<String, String>,
    resources: ResourceRequirements,
    created_at: SystemTime,
    updated_at: SystemTime,
}

struct ServiceDeploymentInstance {
    instance_id: String,
    host: String,
    port: u16,
    status: ServiceInstanceStatus,
    health_status: HealthStatus,
    started_at: SystemTime,
    last_checked: SystemTime,
}

enum ServiceInstanceStatus {
    Pending,
    Starting,
    Running,
    Stopping,
    Stopped,
    Failed,
}

enum DeploymentStrategy {
    RollingUpdate {
        max_surge: usize,
        max_unavailable: usize,
    },
    BlueGreen,
    Canary {
        percentage: u8,
    },
}

struct ResourceRequirements {
    cpu: String,    // ä¾‹å¦‚ "0.5" è¡¨ç¤º0.5ä¸ªCPUæ ¸å¿ƒ
    memory: String, // ä¾‹å¦‚ "512Mi" è¡¨ç¤º512MBå†…å­˜
    disk: String,   // ä¾‹å¦‚ "1Gi" è¡¨ç¤º1GBç£ç›˜
}

impl DeploymentManager {
    fn new(
        service_registry: Arc<ServiceRegistry>,
        health_check_registry: Arc<HealthCheckRegistry>,
        config_system: Arc<DistributedConfigSystem>,
    ) -> Self {
        DeploymentManager {
            service_registry,
            health_check_registry,
            config_system,
        }
    }

    fn deploy_service(&self, deployment: ServiceDeployment) -> Result<(), String> {
        // åˆ›å»ºæœåŠ¡æ³¨å†Œ
        let service_registration = ServiceRegistration {
            id: deployment.service_id.clone(),
            name: deployment.service_name.clone(),
            address: "", // å°†åœ¨å®ä¾‹ä¸­è®¾ç½®
            port: 0,     // å°†åœ¨å®ä¾‹ä¸­è®¾ç½®
            tags: vec![
                format!("version={}", deployment.version),
                format!("deployment=true"),
            ],
            metadata: {
                let mut metadata = HashMap::new();
                metadata.insert("deployment_strategy".to_string(), format!("{:?}", deployment.deployment_strategy));
                metadata.insert("desired_instances".to_string(), deployment.desired_instances.to_string());
                metadata
            },
            health_check: Some(HealthCheck {
                http_path: Some(deployment.health_check_path.clone()),
                interval: Duration::from_secs(10),
                timeout: Duration::from_secs(2),
                healthy_threshold: 2,
                unhealthy_threshold: 3,
            }),
            ttl: Some(Duration::from_secs(30)),
        };

        // å­˜å‚¨é…ç½®
        for (key, value) in &deployment.environment_variables {
            let config_key = format!("services.{}.{}", deployment.service_name, key);
            if let Err(err) = self.config_system.set(&config_key, value, None) {
                return Err(format!("æ— æ³•è®¾ç½®é…ç½® {}: {}", config_key, err.message));
            }
        }

        // éƒ¨ç½²æœåŠ¡å®ä¾‹
        match deployment.deployment_strategy {
            DeploymentStrategy::RollingUpdate { max_surge, max_unavailable } => {
                self.deploy_rolling_update(&deployment, max_surge, max_unavailable)
            },
            DeploymentStrategy::BlueGreen => {
                self.deploy_blue_green(&deployment)
            },
            DeploymentStrategy::Canary { percentage } => {
                self.deploy_canary(&deployment, percentage)
            }
        }
    }

    fn deploy_rolling_update(&self, deployment: &ServiceDeployment, max_surge: usize, max_unavailable: usize) -> Result<(), String> {
        println!("ä½¿ç”¨æ»šåŠ¨æ›´æ–°ç­–ç•¥éƒ¨ç½²æœåŠ¡ {}", deployment.service_name);
        println!("æœ€å¤§è¶…é‡: {}, æœ€å¤§ä¸å¯ç”¨: {}", max_surge, max_unavailable);
        println!("æœŸæœ›å®ä¾‹æ•°: {}", deployment.desired_instances);

        // æ¨¡æ‹Ÿéƒ¨ç½²
        for i in 0..deployment.desired_instances {
            let instance = ServiceDeploymentInstance {
                instance_id: uuid::Uuid::new_v4().to_string(),
                host: format!("host-{}", i),
                port: 8080 + i as u16,
                status: ServiceInstanceStatus::Running,
                health_status: HealthStatus::Up,
                started_at: SystemTime::now(),
                last_checked: SystemTime::now(),
            };

            println!("éƒ¨ç½²å®ä¾‹ {} åˆ° {}:{}", instance.instance_id, instance.host, instance.port);

            // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è¿›è¡Œå®é™…çš„å®¹å™¨éƒ¨ç½²å’ŒæœåŠ¡æ³¨å†Œ
        }

        Ok(())
    }

    fn deploy_blue_green(&self, deployment: &ServiceDeployment) -> Result<(), String> {
        println!("ä½¿ç”¨è“ç»¿éƒ¨ç½²ç­–ç•¥éƒ¨ç½²æœåŠ¡ {}", deployment.service_name);
        println!("æœŸæœ›å®ä¾‹æ•°: {}", deployment.desired_instances);

        // æ¨¡æ‹Ÿè“ç»¿éƒ¨ç½²
        println!("éƒ¨ç½²ç»¿è‰²ç¯å¢ƒ");
        for i in 0..deployment.desired_instances {
            let instance = ServiceDeploymentInstance {
                instance_id: uuid::Uuid::new_v4().to_string(),
                host: format!("green-host-{}", i),
                port: 8080 + i as u16,
                status: ServiceInstanceStatus::Running,
                health_status: HealthStatus::Up,
                started_at: SystemTime::now(),
                last_checked: SystemTime::now(),
            };

            println!("éƒ¨ç½²å®ä¾‹ {} åˆ° {}:{}", instance.instance_id, instance.host, instance.port);
        }

        println!("ç»¿è‰²ç¯å¢ƒéƒ¨ç½²å®Œæˆï¼Œè¿›è¡Œå¥åº·æ£€æŸ¥");
        println!("å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œåˆ‡æ¢æµé‡åˆ°ç»¿è‰²ç¯å¢ƒ");
        println!("ç­‰å¾…ç¡®è®¤ï¼Œç»¿è‰²ç¯å¢ƒç¨³å®šè¿è¡Œ");
        println!("ç§»é™¤è“è‰²ç¯å¢ƒ");

        Ok(())
    }

    fn deploy_canary(&self, deployment: &ServiceDeployment, percentage: u8) -> Result<(), String> {
        println!("ä½¿ç”¨é‡‘ä¸é›€éƒ¨ç½²ç­–ç•¥éƒ¨ç½²æœåŠ¡ {}", deployment.service_name);
        println!("æœŸæœ›å®ä¾‹æ•°: {}, é‡‘ä¸é›€ç™¾åˆ†æ¯”: {}%", deployment.desired_instances, percentage);

        // è®¡ç®—é‡‘ä¸é›€å®ä¾‹æ•°å’Œç¨³å®šå®ä¾‹æ•°
        let canary_instances = (deployment.desired_instances as f32 * (percentage as f32 / 100.0)).ceil() as usize;
        let stable_instances = deployment.desired_instances - canary_instances;

        println!("éƒ¨ç½² {} ä¸ªé‡‘ä¸é›€å®ä¾‹å’Œ {} ä¸ªç¨³å®šå®ä¾‹", canary_instances, stable_instances);

        // æ¨¡æ‹Ÿéƒ¨ç½²ç¨³å®šå®ä¾‹
        for i in 0..stable_instances {
            let instance = ServiceDeploymentInstance {
                instance_id: uuid::Uuid::new_v4().to_string(),
                host: format!("stable-host-{}", i),
                port: 8080 + i as u16,
                status: ServiceInstanceStatus::Running,
                health_status: HealthStatus::Up,
                started_at: SystemTime::now(),
                last_checked: SystemTime::now(),
            };

            println!("éƒ¨ç½²ç¨³å®šå®ä¾‹ {} åˆ° {}:{}", instance.instance_id, instance.host, instance.port);
        }

        // æ¨¡æ‹Ÿéƒ¨ç½²é‡‘ä¸é›€å®ä¾‹
        for i in 0..canary_instances {
            let instance = ServiceDeploymentInstance {
                instance_id: uuid::Uuid::new_v4().to_string(),
                host: format!("canary-host-{}", i),
                port: 9080 + i as u16,
                status: ServiceInstanceStatus::Running,
                health_status: HealthStatus::Up,
                started_at: SystemTime::now(),
                last_checked: SystemTime::now(),
            };

            println!("éƒ¨ç½²é‡‘ä¸é›€å®ä¾‹ {} åˆ° {}:{}", instance.instance_id, instance.host, instance.port);
        }

        println!("é‡‘ä¸é›€éƒ¨ç½²å®Œæˆï¼Œç›‘æ§é‡‘ä¸é›€å®ä¾‹çš„æ€§èƒ½å’Œé”™è¯¯ç‡");

        Ok(())
    }

    fn undeploy_service(&self, service_name: &str) -> Result<(), String> {
        println!("å¸è½½æœåŠ¡ {}", service_name);

        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è¿›è¡Œå®é™…çš„æœåŠ¡å¸è½½å’Œæ³¨å†Œä¸­å¿ƒæ¸…ç†

        Ok(())
    }

    fn scale_service(&self, service_name: &str, instances: usize) -> Result<(), String> {
        println!("ç¼©æ”¾æœåŠ¡ {} åˆ° {} ä¸ªå®ä¾‹", service_name, instances);

        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è¿›è¡Œå®é™…çš„æœåŠ¡ç¼©æ”¾

        Ok(())
    }

    fn restart_service(&self, service_name: &str) -> Result<(), String> {
        println!("é‡å¯æœåŠ¡ {}", service_name);

        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è¿›è¡Œå®é™…çš„æœåŠ¡é‡å¯

        Ok(())
    }

    fn get_service_status(&self, service_name: &str) -> Result<Vec<ServiceDeploymentInstance>, String> {
        println!("è·å–æœåŠ¡ {} çš„çŠ¶æ€", service_name);

        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥ä»æ³¨å†Œä¸­å¿ƒå’Œå¥åº·æ£€æŸ¥ç³»ç»Ÿè·å–æœåŠ¡çŠ¶æ€
        let instances = Vec::new();

        Ok(instances)
    }
}

// ä½¿ç”¨ç¤ºä¾‹
fn main() {
    println!("åˆ›å»ºåˆ†å¸ƒå¼ç³»ç»Ÿæ¡†æ¶");

    // åˆ›å»ºå„ç§ç»„ä»¶ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›åº”è¯¥ä½¿ç”¨å®é™…çš„å®ç°è€Œä¸æ˜¯ç©ºå®ç°ï¼‰
    // è¿™é‡Œä»…å±•ç¤ºæ¡†æ¶çš„ä½¿ç”¨æ–¹å¼

    // åˆ›å»ºæ¡†æ¶
    let framework = Arc::new(DistributedSystemFramework::new(
        Arc::new(ServiceRegistry::new()),
        Arc::new(MetricsRegistry::new()),
        Arc::new(HealthCheckRegistry::new()),
        Arc::new(EventDrivenSystem::new(
            Arc::new(DistributedMessageBus::new("memory")),
            |event| Ok(vec![]), // ç®€åŒ–çš„åºåˆ—åŒ–å™¨
            |data| Err("ä¸æ”¯æŒ".to_string()), // ç®€åŒ–çš„ååºåˆ—åŒ–å™¨
        )),
        Arc::new(DistributedCacheSystem::new("memory")),
        Arc::new(DistributedQueueSystem::new("memory")),
        Arc::new(ResilienceManager::new()),
        Arc::new(DistributedTracer::new(
            "my-service",
            Box::new(ConstSampler { decision: true }),
            Box::new(NoopPropagator {}),
            Box::new(NoopReporter {}),
        )),
    ));

    // åˆ›å»ºæ•°æ®åº“è¿æ¥
    let db_connection = Arc::new(DatabaseConnection::new("jdbc:mysql://localhost:3306/mydb", 10));

    // åˆ›å»ºç”¨æˆ·ä»“åº“
    let user_repository = Arc::new(UserRepository::new(
        db_connection,
        framework.metrics_registry.clone(),
    ));

    // åˆ›å»ºç”¨æˆ·æœåŠ¡
    let user_service = UserService::new(
        framework.clone(),
        user_repository,
    );

    // ä½¿ç”¨æœåŠ¡
    match user_service.create_user("alice", "alice@example.com") {
        Ok(user) => {
            println!("åˆ›å»ºç”¨æˆ·: {}", user.id);

            // è·å–ç”¨æˆ·
            match user_service.get_user(&user.id) {
                Ok(Some(found_user)) => {
                    println!("æ‰¾åˆ°ç”¨æˆ·: {}, {}", found_user.username, found_user.email);

                    // æ›´æ–°ç”¨æˆ·
                    match user_service.update_user(&user.id, "alice2", "alice2@example.com") {
                        Ok(updated_user) => {
                            println!("æ›´æ–°ç”¨æˆ·: {}, {}", updated_user.username, updated_user.email);

                            // åˆ é™¤ç”¨æˆ·
                            match user_service.delete_user(&user.id) {
                                Ok(_) => println!("åˆ é™¤ç”¨æˆ·æˆåŠŸ"),
                                Err(err) => println!("åˆ é™¤ç”¨æˆ·å¤±è´¥: {}", err),
                            }
                        },
                        Err(err) => println!("æ›´æ–°ç”¨æˆ·å¤±è´¥: {}", err),
                    }
                },
                Ok(None) => println!("æ‰¾ä¸åˆ°ç”¨æˆ·"),
                Err(err) => println!("è·å–ç”¨æˆ·å¤±è´¥: {}", err),
            }
        },
        Err(err) => println!("åˆ›å»ºç”¨æˆ·å¤±è´¥: {}", err),
    }

    // éƒ¨ç½²æœåŠ¡
    let deployment_manager = DeploymentManager::new(
        framework.service_registry.clone(),
        framework.health_check_registry.clone(),
        Arc::new(DistributedConfigSystem::new("memory")),
    );

    let service_deployment = ServiceDeployment {
        service_id: uuid::Uuid::new_v4().to_string(),
        service_name: "user-service".to_string(),
        version: "1.0.0".to_string(),
        instances: Vec::new(),
        desired_instances: 3,
        deployment_strategy: DeploymentStrategy::RollingUpdate {
            max_surge: 1,
            max_unavailable: 1,
        },
        health_check_path: "/health".to_string(),
        environment_variables: {
            let mut env = HashMap::new();
            env.insert("DB_URL".to_string(), "jdbc:mysql://localhost:3306/mydb".to_string());
            env.insert("LOG_LEVEL".to_string(), "INFO".to_string());
            env
        },
        resources: ResourceRequirements {
            cpu: "0.5".to_string(),
            memory: "512Mi".to_string(),
            disk: "1Gi".to_string(),
        },
        created_at: SystemTime::now(),
        updated_at: SystemTime::now(),
    };

    match deployment_manager.deploy_service(service_deployment) {
        Ok(_) => println!("æœåŠ¡éƒ¨ç½²æˆåŠŸ"),
        Err(err) => println!("æœåŠ¡éƒ¨ç½²å¤±è´¥: {}", err),
    }
}

// ç©ºå®ç°ï¼Œç”¨äºç¤ºä¾‹
struct NoopPropagator;

impl Propagator for NoopPropagator {
    fn inject(&self, _span_context: &SpanContext, _carrier: &mut dyn PropagationCarrier) {}
    fn extract(&self, _carrier: &dyn PropagationCarrier) -> Option<SpanContext> {
        None
    }
}

struct NoopReporter;

impl Reporter for NoopReporter {
    fn report(&self, _span: &Span) {}
    fn close(&self) {}
}

// åˆ†å¸ƒå¼å…±è¯†ç®—æ³•å®ç°

// Raftåè®®å®ç°
struct RaftNode {
    id: String,
    state: RaftState,
    current_term: u64,
    voted_for: Option<String>,
    log: Vec<LogEntry>,
    commit_index: u64,
    last_applied: u64,
    next_index: HashMap<String, u64>,
    match_index: HashMap<String, u64>,
    peers: Vec<String>,
    election_timeout: Duration,
    heartbeat_interval: Duration,
    last_heartbeat: Instant,
    random_election_timeout: Duration,
    storage: Box<dyn RaftStorage>,
    state_machine: Box<dyn StateMachine>,
    rpc_client: Box<dyn RaftRpcClient>,
}

enum RaftState {
    Follower,
    Candidate,
    Leader,
}

struct LogEntry {
    term: u64,
    index: u64,
    command: Vec<u8>,
}

trait RaftStorage: Send + Sync {
    fn save_state(&self, current_term: u64, voted_for: Option<String>) -> Result<(), String>;
    fn load_state(&self) -> Result<(u64, Option<String>), String>;
    fn append_log_entries(&self, entries: &[LogEntry]) -> Result<(), String>;
    fn get_log_entries(&self, start_index: u64, end_index: u64) -> Result<Vec<LogEntry>, String>;
    fn delete_logs_from(&self, start_index: u64) -> Result<(), String>;
}

trait StateMachine: Send + Sync {
    fn apply(&mut self, command: &[u8]) -> Result<Vec<u8>, String>;
    fn snapshot(&self) -> Result<Vec<u8>, String>;
    fn restore_from_snapshot(&mut self, snapshot_data: &[u8]) -> Result<(), String>;
}

trait RaftRpcClient: Send + Sync {
    fn request_vote(&self, target: &str, term: u64, candidate_id: &str, last_log_index: u64, last_log_term: u64) -> Result<(u64, bool), String>;
    fn append_entries(&self, target: &str, term: u64, leader_id: &str, prev_log_index: u64, prev_log_term: u64, entries: &[LogEntry], leader_commit: u64) -> Result<(u64, bool), String>;
    fn install_snapshot(&self, target: &str, term: u64, leader_id: &str, last_included_index: u64, last_included_term: u64, snapshot_data: &[u8]) -> Result<(u64, bool), String>;
}

impl RaftNode {
    fn new(
        id: &str,
        peers: Vec<String>,
        storage: Box<dyn RaftStorage>,
        state_machine: Box<dyn StateMachine>,
        rpc_client: Box<dyn RaftRpcClient>,
    ) -> Result<Self, String> {
        let (current_term, voted_for) = storage.load_state()?;

        let mut node = RaftNode {
            id: id.to_string(),
            state: RaftState::Follower,
            current_term,
            voted_for,
            log: Vec::new(),
            commit_index: 0,
            last_applied: 0,
            next_index: HashMap::new(),
            match_index: HashMap::new(),
            peers,
            election_timeout: Duration::from_millis(300),
            heartbeat_interval: Duration::from_millis(100),
            last_heartbeat: Instant::now(),
            random_election_timeout: Duration::from_millis(300 + rand::random::<u64>() % 300),
            storage,
            state_machine,
            rpc_client,
        };

        // åŠ è½½æ—¥å¿—
        node.log = node.storage.get_log_entries(0, u64::MAX)?;

        Ok(node)
    }

    fn start(&mut self) {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™ä¼šå¯åŠ¨å¤šä¸ªçº¿ç¨‹æ¥å¤„ç†é€‰ä¸¾ã€æ—¥å¿—å¤åˆ¶ç­‰
        println!("å¯åŠ¨RaftèŠ‚ç‚¹: {}", self.id);

        // å¯åŠ¨ä¸»å¾ªç¯
        self.main_loop();
    }

    fn main_loop(&mut self) {
        loop {
            match self.state {
                RaftState::Follower => self.follower_tick(),
                RaftState::Candidate => self.candidate_tick(),
                RaftState::Leader => self.leader_tick(),
            }

            // åº”ç”¨æäº¤çš„æ—¥å¿—åˆ°çŠ¶æ€æœº
            self.apply_logs();

            // çŸ­æš‚ä¼‘çœ ä»¥é¿å…CPUå ç”¨è¿‡é«˜
            std::thread::sleep(Duration::from_millis(10));
        }
    }

    fn follower_tick(&mut self) {
        // æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if self.last_heartbeat.elapsed() > self.random_election_timeout {
            println!("è·Ÿéšè€…è¶…æ—¶ï¼Œå¼€å§‹é€‰ä¸¾: {}", self.id);
            self.become_candidate();
        }
    }

    fn candidate_tick(&mut self) {
        // æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if self.last_heartbeat.elapsed() > self.random_election_timeout {
            println!("å€™é€‰è€…è¶…æ—¶ï¼Œé‡æ–°å¼€å§‹é€‰ä¸¾: {}", self.id);
            self.start_election();
        }
    }

    fn leader_tick(&mut self) {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€å¿ƒè·³
        if self.last_heartbeat.elapsed() > self.heartbeat_interval {
            println!("é¢†å¯¼è€…å‘é€å¿ƒè·³: {}", self.id);
            self.send_heartbeats();
            self.last_heartbeat = Instant::now();
        }
    }

    fn become_candidate(&mut self) {
        self.state = RaftState::Candidate;
        self.current_term += 1;
        self.voted_for = Some(self.id.clone());
        self.last_heartbeat = Instant::now();
        self.random_election_timeout = Duration::from_millis(300 + rand::random::<u64>() % 300);

        // ä¿å­˜çŠ¶æ€
        if let Err(err) = self.storage.save_state(self.current_term, self.voted_for.clone()) {
            println!("ä¿å­˜çŠ¶æ€å¤±è´¥: {}", err);
        }

        // å¼€å§‹é€‰ä¸¾
        self.start_election();
    }

    fn start_election(&mut self) {
        println!("å¼€å§‹é€‰ä¸¾ï¼Œä»»æœŸ: {}", self.current_term);

        let mut votes_received = 1; // ç»™è‡ªå·±æŠ•ç¥¨

        let last_log_index = if self.log.is_empty() { 0 } else { self.log.last().unwrap().index };
        let last_log_term = if self.log.is_empty() { 0 } else { self.log.last().unwrap().term };

        for peer in &self.peers {
            if peer == &self.id {
                continue; // è·³è¿‡è‡ªå·±
            }

            // å‘é€è¯·æ±‚æŠ•ç¥¨RPC
            match self.rpc_client.request_vote(
                peer,
                self.current_term,
                &self.id,
                last_log_index,
                last_log_term,
            ) {
                Ok((term, vote_granted)) => {
                    if term > self.current_term {
                        // å‘ç°æ›´é«˜ä»»æœŸï¼Œè½¬ä¸ºè·Ÿéšè€…
                        self.become_follower(term);
                        return;
                    }

                    if vote_granted {
                        votes_received += 1;

                        // æ£€æŸ¥æ˜¯å¦è·å¾—å¤šæ•°ç¥¨
                        if votes_received * 2 > self.peers.len() + 1 {
                            println!("è·å¾—å¤šæ•°ç¥¨ï¼Œæˆä¸ºé¢†å¯¼è€…: {}", self.id);
                            self.become_leader();
                            return;
                        }
                    }
                },
                Err(err) => {
                    println!("è¯·æ±‚æŠ•ç¥¨å¤±è´¥: {}", err);
                }
            }
        }
    }

    fn become_follower(&mut self, term: u64) {
        println!("æˆä¸ºè·Ÿéšè€…ï¼Œä»»æœŸ: {}", term);
        self.state = RaftState::Follower;
        self.current_term = term;
        self.voted_for = None;
        self.last_heartbeat = Instant::now();
        self.random_election_timeout = Duration::from_millis(300 + rand::random::<u64>() % 300);

        // ä¿å­˜çŠ¶æ€
        if let Err(err) = self.storage.save_state(self.current_term, self.voted_for.clone()) {
            println!("ä¿å­˜çŠ¶æ€å¤±è´¥: {}", err);
        }
    }

    fn become_leader(&mut self) {
        println!("æˆä¸ºé¢†å¯¼è€…ï¼Œä»»æœŸ: {}", self.current_term);
        self.state = RaftState::Leader;

        // åˆå§‹åŒ–é¢†å¯¼è€…æ•°æ®
        let last_log_index = if self.log.is_empty() { 0 } else { self.log.last().unwrap().index };

        for peer in &self.peers {
            self.next_index.insert(peer.clone(), last_log_index + 1);
            self.match_index.insert(peer.clone(), 0);
        }

        // å‘é€å¿ƒè·³
        self.send_heartbeats();
        self.last_heartbeat = Instant::now();
    }

    fn send_heartbeats(&mut self) {
        for peer in &self.peers {
            if peer == &self.id {
                continue; // è·³è¿‡è‡ªå·±
            }

            let next_idx = *self.next_index.get(peer).unwrap_or(&1);
            let prev_log_index = next_idx - 1;
            let prev_log_term = if prev_log_index == 0 {
                0
            } else if let Some(entry) = self.log.iter().find(|e| e.index == prev_log_index) {
                entry.term
            } else {
                // æ‰¾ä¸åˆ°å‰ä¸€ä¸ªæ—¥å¿—ï¼Œå¯èƒ½éœ€è¦å‘é€å¿«ç…§
                println!("æ‰¾ä¸åˆ°å‰ä¸€ä¸ªæ—¥å¿—ï¼Œå¯èƒ½éœ€è¦å‘é€å¿«ç…§");
                continue;
            };

            // è·å–è¦å‘é€çš„æ—¥å¿—æ¡ç›®
            let entries: Vec<LogEntry> = self.log.iter()
                .filter(|e| e.index >= next_idx)
                .cloned()
                .collect();

            // å‘é€é™„åŠ æ—¥å¿—RPCï¼ˆæˆ–å¿ƒè·³ï¼‰
            match self.rpc_client.append_entries(
                peer,
                self.current_term,
                &self.id,
                prev_log_index,
                prev_log_term,
                &entries,
                self.commit_index,
            ) {
                Ok((term, success)) => {
                    if term > self.current_term {
                        // å‘ç°æ›´é«˜ä»»æœŸï¼Œè½¬ä¸ºè·Ÿéšè€…
                        self.become_follower(term);
                        return;
                    }

                    if success {
                        // æ›´æ–°å¤åˆ¶è¿›åº¦
                        if !entries.is_empty() {
                            let last_entry = entries.last().unwrap();
                            self.next_index.insert(peer.clone(), last_entry.index + 1);
                            self.match_index.insert(peer.clone(), last_entry.index);

                            // æ›´æ–°æäº¤ç´¢å¼•
                            self.update_commit_index();
                        }
                    } else {
                        // æ—¥å¿—ä¸ä¸€è‡´ï¼Œå‡å°‘next_index
                        let next_idx = self.next_index.get_mut(peer).unwrap();
                        *next_idx = (*next_idx).saturating_sub(1);
                    }
                },
                Err(err) => {
                    println!("å‘é€å¿ƒè·³å¤±è´¥: {}", err);
                }
            }
        }
    }

    fn update_commit_index(&mut self) {
        // æ‰¾åˆ°å·²ç»å¤åˆ¶åˆ°å¤šæ•°èŠ‚ç‚¹çš„æœ€å¤§æ—¥å¿—ç´¢å¼•
        let mut sorted_indices: Vec<u64> = self.match_index.values().cloned().collect();
        sorted_indices.push(self.log.last().map_or(0, |e| e.index)); // åŒ…æ‹¬è‡ªå·±
        sorted_indices.sort_unstable();

        let majority_index = sorted_indices[sorted_indices.len() / 2];

        // ç¡®ä¿æ—¥å¿—æ¡ç›®å±äºå½“å‰ä»»æœŸ
        if majority_index > self.commit_index {
            if let Some(entry) = self.log.iter().find(|e| e.index == majority_index) {
                if entry.term == self.current_term {
                    println!("æ›´æ–°æäº¤ç´¢å¼•: {} -> {}", self.commit_index, majority_index);
                    self.commit_index = majority_index;
                }
            }
        }
    }

    fn apply_logs(&mut self) {
        while self.last_applied < self.commit_index {
            self.last_applied += 1;

            if let Some(entry) = self.log.iter().find(|e| e.index == self.last_applied) {
                match self.state_machine.apply(&entry.command) {
                    Ok(_) => {
                        println!("åº”ç”¨æ—¥å¿—ï¼Œç´¢å¼•: {}", entry.index);
                    },
                    Err(err) => {
                        println!("åº”ç”¨æ—¥å¿—å¤±è´¥: {}", err);
                    }
                }
            } else {
                println!("æ‰¾ä¸åˆ°è¦åº”ç”¨çš„æ—¥å¿—ï¼Œç´¢å¼•: {}", self.last_applied);
                self.last_applied = self.commit_index; // ä¿®æ­£çŠ¶æ€
                break;
            }
        }
    }

    fn propose_command(&mut self, command: &[u8]) -> Result<(), String> {
        if !matches!(self.state, RaftState::Leader) {
            return Err(format!("èŠ‚ç‚¹ {} ä¸æ˜¯é¢†å¯¼è€…", self.id));
        }

        let index = if self.log.is_empty() { 1 } else { self.log.last().unwrap().index + 1 };

        let entry = LogEntry {
            term: self.current_term,
            index,
            command: command.to_vec(),
        };

        // æ·»åŠ åˆ°æœ¬åœ°æ—¥å¿—
        self.log.push(entry.clone());

        // æŒä¹…åŒ–æ—¥å¿—
        if let Err(err) = self.storage.append_log_entries(&[entry]) {
            return Err(format!("æŒä¹…åŒ–æ—¥å¿—å¤±è´¥: {}", err));
        }

        // å°è¯•å¤åˆ¶æ—¥å¿—
        self.send_heartbeats();

        Ok(())
    }

    // RPCå¤„ç†æ–¹æ³•

    fn handle_request_vote(&mut self, term: u64, candidate_id: &str, last_log_index: u64, last_log_term: u64) -> (u64, bool) {
        println!("æ”¶åˆ°è¯·æ±‚æŠ•ç¥¨ï¼Œæ¥è‡ªï¼š{}ï¼Œä»»æœŸï¼š{}", candidate_id, term);

        if term < self.current_term {
            return (self.current_term, false);
        }

        if term > self.current_term {
            self.become_follower(term);
        }

        let vote_granted = (self.voted_for.is_none() || self.voted_for.as_ref() == Some(candidate_id)) &&
                           self.is_candidate_log_up_to_date(last_log_index, last_log_term);

        if vote_granted {
            self.voted_for = Some(candidate_id.to_string());
            self.last_heartbeat = Instant::now(); // é‡ç½®é€‰ä¸¾è¶…æ—¶

            // ä¿å­˜çŠ¶æ€
            if let Err(err) = self.storage.save_state(self.current_term, self.voted_for.clone()) {
                println!("ä¿å­˜çŠ¶æ€å¤±è´¥: {}", err);
            }
        }

        (self.current_term, vote_granted)
    }

    fn is_candidate_log_up_to_date(&self, last_log_index: u64, last_log_term: u64) -> bool {
        let own_last_term = if self.log.is_empty() { 0 } else { self.log.last().unwrap().term };
        let own_last_index = if self.log.is_empty() { 0 } else { self.log.last().unwrap().index };

        // å…ˆæ¯”è¾ƒä»»æœŸï¼Œä»»æœŸå¤§çš„æ—¥å¿—æ›´æ–°ï¼›ä»»æœŸç›¸åŒåˆ™ç´¢å¼•å¤§çš„æ›´æ–°
        last_log_term > own_last_term || (last_log_term == own_last_term && last_log_index >= own_last_index)
    }

    fn handle_append_entries(&mut self, term: u64, leader_id: &str, prev_log_index: u64, prev_log_term: u64, entries: &[LogEntry], leader_commit: u64) -> (u64, bool) {
        println!("æ”¶åˆ°é™„åŠ æ—¥å¿—ï¼Œæ¥è‡ªï¼š{}ï¼Œä»»æœŸï¼š{}", leader_id, term);

        if term < self.current_term {
            return (self.current_term, false);
        }

        // æ”¶åˆ°å¿ƒè·³æˆ–æ—¥å¿—ï¼Œé‡ç½®é€‰ä¸¾è¶…æ—¶
        self.last_heartbeat = Instant::now();

        if term > self.current_term || !matches!(self.state, RaftState::Follower) {
            self.become_follower(term);
        }

        // æ£€æŸ¥å‰ä¸€ä¸ªæ—¥å¿—æ¡ç›®
        let log_ok = prev_log_index == 0 ||
                     self.log.iter().any(|e| e.index == prev_log_index && e.term == prev_log_term);

        if !log_ok {
            return (self.current_term, false);
        }

        // å¤„ç†æ—¥å¿—æ¡ç›®
        if !entries.is_empty() {
            // æŸ¥æ‰¾å†²çªå¹¶åˆ é™¤
            let conflict_idx = self.log.iter()
                .position(|e| e.index == entries[0].index && e.term != entries[0].term);

            if let Some(idx) = conflict_idx {
                // åˆ é™¤å†²çªåŠå…¶åçš„æ‰€æœ‰æ—¥å¿—
                self.log.truncate(idx);

                // ä»æŒä¹…åŒ–å­˜å‚¨ä¸­åˆ é™¤
                if let Err(err) = self.storage.delete_logs_from(entries[0].index) {
                    println!("åˆ é™¤å†²çªæ—¥å¿—å¤±è´¥: {}", err);
                    return (self.current_term, false);
                }
            }

            // è¿½åŠ æ–°æ—¥å¿—
            let new_entries: Vec<LogEntry> = entries.iter()
                .filter(|&e| !self.log.iter().any(|existing| existing.index == e.index))
                .cloned()
                .collect();

            if !new_entries.is_empty() {
                // è¿½åŠ åˆ°å†…å­˜æ—¥å¿—
                self.log.extend(new_entries.clone());

                // æŒä¹…åŒ–æ—¥å¿—
                if let Err(err) = self.storage.append_log_entries(&new_entries) {
                    println!("æŒä¹…åŒ–æ—¥å¿—å¤±è´¥: {}", err);
                    return (self.current_term, false);
                }
            }
        }

        // æ›´æ–°æäº¤ç´¢å¼•
        if leader_commit > self.commit_index {
            let last_new_index = if self.log.is_empty() { 0 } else { self.log.last().unwrap().index };
            self.commit_index = leader_commit.min(last_new_index);
        }

        (self.current_term, true)
    }
}

// æ•°æ®åˆ†åŒºå’Œåˆ†ç‰‡
struct ShardingManager {
    shard_count: usize,
    hash_algorithm: Box<dyn Fn(&[u8]) -> u64>,
    shard_info: Vec<ShardInfo>,
}

struct ShardInfo {
    shard_id: usize,
    primary_node: String,
    replica_nodes: Vec<String>,
    status: ShardStatus,
}

enum ShardStatus {
    Available,
    Rebalancing,
    Degraded,
    Unavailable,
}

impl ShardingManager {
    fn new(shard_count: usize, hash_algorithm: Box<dyn Fn(&[u8]) -> u64>) -> Self {
        ShardingManager {
            shard_count,
            hash_algorithm,
            shard_info: Vec::with_capacity(shard_count),
        }
    }

    fn initialize_shards(&mut self, nodes: &[String]) {
        if nodes.is_empty() {
            return;
        }

        // ä¸ºæ¯ä¸ªåˆ†ç‰‡åˆ†é…èŠ‚ç‚¹
        for shard_id in 0..self.shard_count {
            let primary_idx = shard_id % nodes.len();
            let primary_node = nodes[primary_idx].clone();

            // é€‰æ‹©å‰¯æœ¬èŠ‚ç‚¹ï¼ˆç®€å•ç­–ç•¥ï¼šé€‰æ‹©ä¸»èŠ‚ç‚¹åçš„å‡ ä¸ªèŠ‚ç‚¹ï¼‰
            let mut replica_nodes = Vec::new();
            for i in 1..3 { // å‡è®¾æˆ‘ä»¬æƒ³è¦2ä¸ªå‰¯æœ¬
                let replica_idx = (primary_idx + i) % nodes.len();
                replica_nodes.push(nodes[replica_idx].clone());
            }

            self.shard_info.push(ShardInfo {
                shard_id,
                primary_node,
                replica_nodes,
                status: ShardStatus::Available,
            });
        }
    }

    fn get_shard_for_key(&self, key: &[u8]) -> usize {
        let hash = (self.hash_algorithm)(key);
        (hash as usize) % self.shard_count
    }

    fn get_node_for_key(&self, key: &[u8]) -> Option<String> {
        let shard_id = self.get_shard_for_key(key);
        self.shard_info.get(shard_id).map(|info| info.primary_node.clone())
    }

    fn get_replicas_for_key(&self, key: &[u8]) -> Option<Vec<String>> {
        let shard_id = self.get_shard_for_key(key);
        self.shard_info.get(shard_id).map(|info| info.replica_nodes.clone())
    }

    fn rebalance_shards(&mut self, nodes: &[String]) -> Result<(), String> {
        if nodes.is_empty() {
            return Err("èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©º".to_string());
        }

        println!("å¼€å§‹é‡æ–°å¹³è¡¡åˆ†ç‰‡");

        // è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹åº”è¯¥æŒæœ‰çš„åˆ†ç‰‡æ•°
        let shards_per_node = (self.shard_count as f64 / nodes.len() as f64).ceil() as usize;

        // è·Ÿè¸ªæ¯ä¸ªèŠ‚ç‚¹æ‹¥æœ‰çš„åˆ†ç‰‡æ•°
        let mut node_shard_counts = HashMap::new();
        for node in nodes {
            node_shard_counts.insert(node.clone(), 0);
        }

        // æ ‡è®°æ‰€æœ‰åˆ†ç‰‡ä¸ºé‡æ–°å¹³è¡¡çŠ¶æ€
        for shard in &mut self.shard_info {
            shard.status = ShardStatus::Rebalancing;
        }

        // é‡æ–°åˆ†é…åˆ†ç‰‡
        for shard in &mut self.shard_info {
            // æ‰¾åˆ°æ‹¥æœ‰æœ€å°‘åˆ†ç‰‡çš„èŠ‚ç‚¹
            let min_node = node_shard_counts.iter()
                .min_by_key(|(_, &count)| count)
                .map(|(node, _)| node.clone())
                .unwrap();

            // æ›´æ–°åˆ†ç‰‡ä¿¡æ¯
            shard.primary_node = min_node.clone();

            // æ›´æ–°è®¡æ•°
            *node_shard_counts.get_mut(&min_node).unwrap() += 1;

            // é€‰æ‹©å‰¯æœ¬èŠ‚ç‚¹
            shard.replica_nodes.clear();
            let mut sorted_nodes: Vec<(String, usize)> = node_shard_counts.iter()
                .map(|(node, &count)| (node.clone(), count))
                .collect();

            sorted_nodes.sort_by_key(|(_, count)| *count);

            for i in 0..2.min(sorted_nodes.len() - 1) {
                let (node, _) = &sorted_nodes[i];
                if node != &shard.primary_node {
                    shard.replica_nodes.push(node.clone());
                    *node_shard_counts.get_mut(node).unwrap() += 1;
                }
            }

            // æ¢å¤åˆ†ç‰‡çŠ¶æ€
            shard.status = ShardStatus::Available;
        }

        println!("åˆ†ç‰‡é‡æ–°å¹³è¡¡å®Œæˆ");

        Ok(())
    }
}

// ä¸€è‡´æ€§å“ˆå¸Œå®ç°
struct ConsistentHash {
    ring: BTreeMap<u64, String>,
    virtual_nodes: usize,
    hash_algorithm: Box<dyn Fn(&str) -> u64>,
}

impl ConsistentHash {
    fn new(virtual_nodes: usize, hash_algorithm: Box<dyn Fn(&str) -> u64>) -> Self {
        ConsistentHash {
            ring: BTreeMap::new(),
            virtual_nodes,
            hash_algorithm,
        }
    }

    fn add_node(&mut self, node: &str) {
        for i in 0..self.virtual_nodes {
            let key = format!("{}:{}", node, i);
            let hash = (self.hash_algorithm)(&key);
            self.ring.insert(hash, node.to_string());
        }
    }

    fn remove_node(&mut self, node: &str) {
        for i in 0..self.virtual_nodes {
            let key = format!("{}:{}", node, i);
            let hash = (self.hash_algorithm)(&key);
            self.ring.remove(&hash);
        }
    }

    fn get_node(&self, key: &str) -> Option<String> {
        if self.ring.is_empty() {
            return None;
        }

        let hash = (self.hash_algorithm)(key);

        let entry = self.ring.range(hash..).next()
            .or_else(|| self.ring.iter().next())
            .map(|(_, node)| node.clone());

        entry
    }

    fn get_nodes(&self, key: &str, count: usize) -> Vec<String> {
        if self.ring.is_empty() {
            return Vec::new();
        }

        let mut result = Vec::new();
        let mut seen = HashSet::new();

        let hash = (self.hash_algorithm)(key);

        // ä»å“ˆå¸Œå¼€å§‹éå†ç¯
        let mut iter = self.ring.range(hash..);
        while result.len() < count {
            match iter.next() {
                Some((_, node)) => {
                    if !seen.contains(node) {
                        seen.insert(node.clone());
                        result.push(node.clone());
                    }
                },
                None => {
                    // åˆ°è¾¾ç¯çš„æœ«å°¾ï¼Œä»å¼€å§‹ç»§ç»­
                    iter = self.ring.iter();

                    // é¿å…æ— é™å¾ªç¯
                    if seen.len() == self.ring.values().collect::<HashSet<_>>().len() {
                        break;
                    }
                }
            }
        }

        result
    }
}

// å¤šåŒºåŸŸéƒ¨ç½²å’Œæ•°æ®å¤åˆ¶
struct MultiRegionDeployment {
    regions: Vec<Region>,
    services: HashMap<String, Vec<RegionalService>>,
    data_strategy: DataReplicationStrategy,
    routing_strategy: RoutingStrategy,
}

struct Region {
    id: String,
    name: String,
    location: String,
    is_active: bool,
    priority: u8,
}

struct RegionalService {
    region_id: String,
    service_name: String,
    instances: Vec<ServiceInstance>,
    status: RegionalServiceStatus,
}

enum RegionalServiceStatus {
    Active,
    Passive,
    Degraded,
    Offline,
}

enum DataReplicationStrategy {
    ActiveActive {
        conflict_resolution: ConflictResolutionStrategy,
    },
    ActivePassive {
        primary_region: String,
        failover_regions: Vec<String>,
    },
    ReadLocalWriteGlobal {
        write_region: String,
    },
}

enum ConflictResolutionStrategy {
    LastWriterWins,
    Vector {
        clock_skew_tolerance: Duration,
    },
    Custom {
        resolver: Box<dyn Fn(&[u8], &[u8]) -> Vec<u8>>,
    },
}

enum RoutingStrategy {
    GeoProximity,
    RegionPriority,
    LatencyBased,
    LoadBased,
}

impl MultiRegionDeployment {
    fn new(regions: Vec<Region>, data_strategy: DataReplicationStrategy, routing_strategy: RoutingStrategy) -> Self {
        MultiRegionDeployment {
            regions,
            services: HashMap::new(),
            data_strategy,
            routing_strategy,
        }
    }

    fn register_service(&mut self, service_name: &str, regional_services: Vec<RegionalService>) -> Result<(), String> {
        if self.services.contains_key(service_name) {
            return Err(format!("æœåŠ¡ {} å·²å­˜åœ¨", service_name));
        }

        // éªŒè¯æ‰€æœ‰åŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
        for service in &regional_services {
            if !self.regions.iter().any(|r| r.id == service.region_id) {
                return Err(format!("åŒºåŸŸ {} ä¸å­˜åœ¨", service.region_id));
            }
        }

        self.services.insert(service_name.to_string(), regional_services);
        Ok(())
    }

    fn route_request(&self, service_name: &str, client_location: &str, request_type: RequestType) -> Result<ServiceInstance, String> {
        let regional_services = self.services.get(service_name)
            .ok_or_else(|| format!("æœåŠ¡ {} ä¸å­˜åœ¨", service_name))?;

        // æ ¹æ®è·¯ç”±ç­–ç•¥é€‰æ‹©åŒºåŸŸ
        let selected_region = match self.routing_strategy {
            RoutingStrategy::GeoProximity => self.select_by_geo_proximity(regional_services, client_location),
            RoutingStrategy::RegionPriority => self.select_by_region_priority(regional_services),
            RoutingStrategy::LatencyBased => self.select_by_latency(regional_services, client_location),
            RoutingStrategy::LoadBased => self.select_by_load(regional_services),
        }?;

        // å¯¹äºå†™è¯·æ±‚ï¼Œå¯èƒ½éœ€è¦æ ¹æ®æ•°æ®ç­–ç•¥è¿›ä¸€æ­¥è·¯ç”±
        if request_type == RequestType::Write {
            match &self.data_strategy {
                DataReplicationStrategy::ActivePassive { primary_region, .. } => {
                    if selected_region.region_id != *primary_region {
                        // æ‰¾åˆ°ä¸»åŒºåŸŸçš„æœåŠ¡
                        return self.find_service_in_region(regional_services, primary_region);
                    }
                },
                DataReplicationStrategy::ReadLocalWriteGlobal { write_region } => {
                    // æ‰€æœ‰å†™è¯·æ±‚éƒ½è·¯ç”±åˆ°å†™åŒºåŸŸ
                    return self.find_service_in_region(regional_services, write_region);
                },
                _ => {} // ActiveActiveå…è®¸åœ¨ä»»ä½•åŒºåŸŸå†™å…¥
            }
        }

        // ä»é€‰å®šåŒºåŸŸä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå®ä¾‹
        if selected_region.instances.is_empty() {
            return Err(format!("åŒºåŸŸ {} ä¸­æ²¡æœ‰å¯ç”¨çš„æœåŠ¡å®ä¾‹", selected_region.region_id));
        }

        let instance_index = rand::random::<usize>() % selected_region.instances.len();
        Ok(selected_region.instances[instance_index].clone())
    }

    fn select_by_geo_proximity(&self, services: &[RegionalService], client_location: &str) -> Result<&RegionalService, String> {
        // ç®€åŒ–å®ç°ï¼Œå®é™…ä¸Šéœ€è¦åœ°ç†ä½ç½®è®¡ç®—
        // è¿™é‡Œå‡è®¾client_locationæ˜¯ä¸€ä¸ªåŒºåŸŸID
        services.iter()
            .filter(|s| s.status == RegionalServiceStatus::Active)
            .min_by_key(|s| {
                if s.region_id == client_location {
                    0
                } else {
                    // ç®€å•æ¨¡æ‹Ÿè·ç¦»
                    let region = self.regions.iter().find(|r| r.id == s.region_id).unwrap();
                    let client_region = self.regions.iter().find(|r| r.id == client_location);

                    if let Some(client_region) = client_region {
                        (region.location.len() as i32 - client_region.location.len() as i32).abs() as u32
                    } else {
                        100 // è¿œè·ç¦»
                    }
                }
            })
            .ok_or_else(|| "æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„åŒºåŸŸæœåŠ¡".to_string())
    }

    fn select_by_region_priority(&self, services: &[RegionalService]) -> Result<&RegionalService, String> {
        services.iter()
            .filter(|s| s.status == RegionalServiceStatus::Active)
            .max_by_key(|s| {
                self.regions.iter()
                    .find(|r| r.id == s.region_id)
                    .map(|r| r.priority)
                    .unwrap_or(0)
            })
            .ok_or_else(|| "æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„åŒºåŸŸæœåŠ¡".to_string())
    }

    fn select_by_latency(&self, services: &[RegionalService], client_location: &str) -> Result<&RegionalService, String> {
        // ç®€åŒ–å®ç°ï¼Œå®é™…ä¸Šéœ€è¦çœŸå®å»¶è¿Ÿæµ‹é‡
        // è¿™é‡Œå‡è®¾ä¸geo_proximityç›¸åŒçš„é€»è¾‘
        self.select_by_geo_proximity(services, client_location)
    }

    fn select_by_load(&self, services: &[RegionalService]) -> Result<&RegionalService, String> {
        // ç®€åŒ–å®ç°ï¼Œé€‰æ‹©å®ä¾‹æ•°æœ€å¤šçš„åŒºåŸŸ
        services.iter()
            .filter(|s| s.status == RegionalServiceStatus::Active)
            .max_by_key(|s| s.instances.len())
            .ok_or_else(|| "æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„åŒºåŸŸæœåŠ¡".to_string())
    }

    fn find_service_in_region(&self, services: &[RegionalService], region_id: &str) -> Result<ServiceInstance, String> {
        let service = services.iter()
            .find(|s| s.region_id == *region_id && s.status == RegionalServiceStatus::Active)
            .ok_or_else(|| format!("åœ¨åŒºåŸŸ {} ä¸­æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒçš„æœåŠ¡", region_id))?;

        if service.instances.is_empty() {
            return Err(format!("åŒºåŸŸ {} ä¸­æ²¡æœ‰å¯ç”¨çš„æœåŠ¡å®ä¾‹", region_id));
        }

        let instance_index = rand::random::<usize>() % service.instances.len();
        Ok(service.instances[instance_index].clone())
    }

    fn handle_region_failure(&mut self, region_id: &str) -> Result<(), String> {
        // æ›´æ–°åŒºåŸŸçŠ¶æ€
        let region = self.regions.iter_mut()
            .find(|r| r.id == *region_id)
            .ok_or_else(|| format!("åŒºåŸŸ {} ä¸å­˜åœ¨", region_id))?;

        println!("åŒºåŸŸ {} å‘ç”Ÿæ•…éšœ", region_id);
        region.is_active = false;

        // æ›´æ–°æ‰€æœ‰å—å½±å“çš„æœåŠ¡
        for (_, regional_services) in &mut self.services {
            for service in regional_services {
                if service.region_id == *region_id {
                    service.status = RegionalServiceStatus::Offline;
                }
            }
        }

        // å¦‚æœä½¿ç”¨ActivePassiveç­–ç•¥ä¸”ä¸»åŒºåŸŸæ•…éšœï¼Œæ‰§è¡Œæ•…éšœè½¬ç§»
        if let DataReplicationStrategy::ActivePassive { primary_region, failover_regions } = &mut self.data_strategy {
            if *primary_region == *region_id {
                println!("ä¸»åŒºåŸŸ {} æ•…éšœï¼Œæ‰§è¡Œæ•…éšœè½¬ç§»", region_id);

                // é€‰æ‹©æ–°çš„ä¸»åŒºåŸŸ
                for failover in failover_regions {
                    if self.regions.iter().any(|r| r.id == *failover && r.is_active) {
                        println!("åˆ‡æ¢ä¸»åŒºåŸŸï¼š{} -> {}", primary_region, failover);
                        *primary_region = fail

```rust
                        *primary_region = failover.clone();
                        break;
                    }
                }
            }
        }

        Ok(())
    }

    fn handle_region_recovery(&mut self, region_id: &str) -> Result<(), String> {
        // æ›´æ–°åŒºåŸŸçŠ¶æ€
        let region = self.regions.iter_mut()
            .find(|r| r.id == *region_id)
            .ok_or_else(|| format!("åŒºåŸŸ {} ä¸å­˜åœ¨", region_id))?;

        println!("åŒºåŸŸ {} æ¢å¤", region_id);
        region.is_active = true;

        // æ›´æ–°æ‰€æœ‰å—å½±å“çš„æœåŠ¡ï¼Œä½†ä¸è‡ªåŠ¨å°†å…¶è®¾ä¸ºActive
        for (_, regional_services) in &mut self.services {
            for service in regional_services {
                if service.region_id == *region_id {
                    service.status = RegionalServiceStatus::Passive;
                }
            }
        }

        // å¦‚æœä½¿ç”¨ActivePassiveç­–ç•¥ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å†³å®šæ˜¯å¦æ•…éšœæ¢å¤
        if let DataReplicationStrategy::ActivePassive { primary_region, .. } = &self.data_strategy {
            println!("å½“å‰ä¸»åŒºåŸŸ: {}", primary_region);
            println!("åŒºåŸŸ {} å·²æ¢å¤ï¼Œä½†éœ€è¦æ‰‹åŠ¨å†³å®šæ˜¯å¦å°†å…¶æ¢å¤ä¸ºä¸»åŒºåŸŸ", region_id);
        }

        Ok(())
    }

    fn promote_region(&mut self, region_id: &str) -> Result<(), String> {
        // éªŒè¯åŒºåŸŸæ˜¯å¦å­˜åœ¨ä¸”æ´»è·ƒ
        let region_active = self.regions.iter()
            .any(|r| r.id == *region_id && r.is_active);

        if !region_active {
            return Err(format!("åŒºåŸŸ {} ä¸å­˜åœ¨æˆ–ä¸æ´»è·ƒ", region_id));
        }

        println!("æå‡åŒºåŸŸ {} çŠ¶æ€", region_id);

        // æ›´æ–°æ‰€æœ‰åœ¨è¯¥åŒºåŸŸçš„æœåŠ¡ä¸ºActive
        for (_, regional_services) in &mut self.services {
            for service in regional_services {
                if service.region_id == *region_id {
                    service.status = RegionalServiceStatus::Active;
                }
            }
        }

        // å¦‚æœä½¿ç”¨ActivePassiveç­–ç•¥ï¼Œæ›´æ–°ä¸»åŒºåŸŸ
        if let DataReplicationStrategy::ActivePassive { primary_region, .. } = &mut self.data_strategy {
            println!("æ›´æ–°ä¸»åŒºåŸŸï¼š{} -> {}", primary_region, region_id);
            *primary_region = region_id.to_string();
        }

        Ok(())
    }
}

enum RequestType {
    Read,
    Write,
}

// CRDT: å†²çªè§£å†³æ•°æ®ç±»å‹
trait CRDT {
    type Value;

    // åˆå¹¶ä¸¤ä¸ªCRDTå®ä¾‹
    fn merge(&mut self, other: &Self);

    // è·å–å½“å‰å€¼
    fn value(&self) -> Self::Value;
}

// å¢é•¿è®¡æ•°å™¨CRDT
struct GCounter {
    counts: HashMap<String, u64>,
    node_id: String,
}

impl GCounter {
    fn new(node_id: &str) -> Self {
        let mut counts = HashMap::new();
        counts.insert(node_id.to_string(), 0);

        GCounter {
            counts,
            node_id: node_id.to_string(),
        }
    }

    fn increment(&mut self, amount: u64) {
        let count = self.counts.entry(self.node_id.clone()).or_insert(0);
        *count += amount;
    }
}

impl CRDT for GCounter {
    type Value = u64;

    fn merge(&mut self, other: &Self) {
        for (node, &count) in &other.counts {
            let entry = self.counts.entry(node.clone()).or_insert(0);
            *entry = (*entry).max(count);
        }
    }

    fn value(&self) -> Self::Value {
        self.counts.values().sum()
    }
}

// å¢å‡è®¡æ•°å™¨CRDT
struct PNCounter {
    increments: GCounter,
    decrements: GCounter,
}

impl PNCounter {
    fn new(node_id: &str) -> Self {
        PNCounter {
            increments: GCounter::new(node_id),
            decrements: GCounter::new(node_id),
        }
    }

    fn increment(&mut self, amount: u64) {
        self.increments.increment(amount);
    }

    fn decrement(&mut self, amount: u64) {
        self.decrements.increment(amount);
    }
}

impl CRDT for PNCounter {
    type Value = i64;

    fn merge(&mut self, other: &Self) {
        self.increments.merge(&other.increments);
        self.decrements.merge(&other.decrements);
    }

    fn value(&self) -> Self::Value {
        self.increments.value() as i64 - self.decrements.value() as i64
    }
}

// æœ€ç»ˆä¸€è‡´é›†åˆCRDT
struct GSet<T: Clone + Eq + Hash> {
    elements: HashSet<T>,
}

impl<T: Clone + Eq + Hash> GSet<T> {
    fn new() -> Self {
        GSet {
            elements: HashSet::new(),
        }
    }

    fn add(&mut self, element: T) {
        self.elements.insert(element);
    }

    fn contains(&self, element: &T) -> bool {
        self.elements.contains(element)
    }
}

impl<T: Clone + Eq + Hash> CRDT for GSet<T> {
    type Value = HashSet<T>;

    fn merge(&mut self, other: &Self) {
        for element in &other.elements {
            self.elements.insert(element.clone());
        }
    }

    fn value(&self) -> Self::Value {
        self.elements.clone()
    }
}

// å¯å¢å‡é›†åˆCRDT
struct TwoPhaseSet<T: Clone + Eq + Hash> {
    additions: GSet<T>,
    removals: GSet<T>,
}

impl<T: Clone + Eq + Hash> TwoPhaseSet<T> {
    fn new() -> Self {
        TwoPhaseSet {
            additions: GSet::new(),
            removals: GSet::new(),
        }
    }

    fn add(&mut self, element: T) {
        self.additions.add(element);
    }

    fn remove(&mut self, element: T) {
        if self.additions.contains(&element) {
            self.removals.add(element);
        }
    }

    fn contains(&self, element: &T) -> bool {
        self.additions.contains(element) && !self.removals.contains(element)
    }
}

impl<T: Clone + Eq + Hash> CRDT for TwoPhaseSet<T> {
    type Value = HashSet<T>;

    fn merge(&mut self, other: &Self) {
        self.additions.merge(&other.additions);
        self.removals.merge(&other.removals);
    }

    fn value(&self) -> Self::Value {
        let mut result = HashSet::new();

        for element in &self.additions.elements {
            if !self.removals.contains(element) {
                result.insert(element.clone());
            }
        }

        result
    }
}

// å‘é‡æ—¶é’Ÿ
struct VectorClock {
    timestamps: HashMap<String, u64>,
    node_id: String,
}

impl VectorClock {
    fn new(node_id: &str) -> Self {
        let mut timestamps = HashMap::new();
        timestamps.insert(node_id.to_string(), 0);

        VectorClock {
            timestamps,
            node_id: node_id.to_string(),
        }
    }

    fn increment(&mut self) {
        let timestamp = self.timestamps.entry(self.node_id.clone()).or_insert(0);
        *timestamp += 1;
    }

    fn merge(&mut self, other: &Self) {
        for (node, &timestamp) in &other.timestamps {
            let entry = self.timestamps.entry(node.clone()).or_insert(0);
            *entry = (*entry).max(timestamp);
        }
    }

    fn compare(&self, other: &Self) -> VectorClockOrder {
        let mut less = false;
        let mut greater = false;

        // æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹
        let all_nodes: HashSet<String> = self.timestamps.keys().chain(other.timestamps.keys())
            .cloned()
            .collect();

        for node in all_nodes {
            let self_timestamp = *self.timestamps.get(&node).unwrap_or(&0);
            let other_timestamp = *other.timestamps.get(&node).unwrap_or(&0);

            if self_timestamp < other_timestamp {
                less = true;
            } else if self_timestamp > other_timestamp {
                greater = true;
            }
        }

        if less && !greater {
            VectorClockOrder::Less
        } else if greater && !less {
            VectorClockOrder::Greater
        } else if !less && !greater {
            VectorClockOrder::Equal
        } else {
            VectorClockOrder::Concurrent
        }
    }
}

enum VectorClockOrder {
    Less,       // ä¸¥æ ¼å°äº
    Greater,    // ä¸¥æ ¼å¤§äº
    Equal,      // ç›¸ç­‰
    Concurrent, // å¹¶å‘ï¼Œä¸å¯æ¯”è¾ƒ
}

// æœ€åå†™å…¥è·èƒœå¯„å­˜å™¨
struct LWWRegister<T: Clone> {
    value: T,
    timestamp: u64,
    node_id: String,
}

impl<T: Clone> LWWRegister<T> {
    fn new(value: T, node_id: &str) -> Self {
        LWWRegister {
            value,
            timestamp: 0,
            node_id: node_id.to_string(),
        }
    }

    fn write(&mut self, value: T, timestamp: u64) {
        if timestamp > self.timestamp || (timestamp == self.timestamp && self.node_id < value.to_string()) {
            self.value = value;
            self.timestamp = timestamp;
        }
    }

    fn read(&self) -> T {
        self.value.clone()
    }
}

impl<T: Clone> CRDT for LWWRegister<T> {
    type Value = T;

    fn merge(&mut self, other: &Self) {
        if other.timestamp > self.timestamp ||
           (other.timestamp == self.timestamp && other.node_id < self.node_id) {
            self.value = other.value.clone();
            self.timestamp = other.timestamp;
            self.node_id = other.node_id.clone();
        }
    }

    fn value(&self) -> Self::Value {
        self.value.clone()
    }
}

// åŸºäºå‘é‡æ—¶é’Ÿçš„å¤šå€¼å¯„å­˜å™¨
struct MVRegister<T: Clone> {
    values: HashMap<VectorClock, T>,
    context: VectorClock,
    node_id: String,
}

impl<T: Clone> MVRegister<T> {
    fn new(node_id: &str) -> Self {
        MVRegister {
            values: HashMap::new(),
            context: VectorClock::new(node_id),
            node_id: node_id.to_string(),
        }
    }

    fn write(&mut self, value: T) {
        // å¢åŠ æœ¬åœ°å‘é‡æ—¶é’Ÿ
        let mut new_clock = self.context.clone();
        new_clock.increment();

        // æ¸…é™¤è¢«æ–°æ—¶é’Ÿè¦†ç›–çš„å€¼
        self.values.retain(|clock, _| {
            clock.compare(&new_clock) == VectorClockOrder::Concurrent
        });

        // æ·»åŠ æ–°å€¼
        self.values.insert(new_clock.clone(), value);

        // æ›´æ–°ä¸Šä¸‹æ–‡
        self.context = new_clock;
    }

    fn read(&self) -> Vec<T> {
        self.values.values().cloned().collect()
    }
}

impl<T: Clone> CRDT for MVRegister<T> {
    type Value = Vec<T>;

    fn merge(&mut self, other: &Self) {
        // åˆå¹¶ä¸Šä¸‹æ–‡
        self.context.merge(&other.context);

        // åˆå¹¶å€¼ï¼Œä¿ç•™æ‰€æœ‰å¹¶å‘å†™å…¥
        for (clock, value) in &other.values {
            let mut dominated = false;

            // æ£€æŸ¥æ˜¯å¦è¢«ç°æœ‰æ—¶é’Ÿè¦†ç›–
            for existing_clock in self.values.keys() {
                if existing_clock.compare(clock) == VectorClockOrder::Greater {
                    dominated = true;
                    break;
                }
            }

            if !dominated {
                // ç§»é™¤è¢«è¿™ä¸ªæ—¶é’Ÿè¦†ç›–çš„å€¼
                self.values.retain(|existing_clock, _| {
                    existing_clock.compare(clock) != VectorClockOrder::Less
                });

                // æ·»åŠ æ–°å€¼
                self.values.insert(clock.clone(), value.clone());
            }
        }
    }

    fn value(&self) -> Self::Value {
        self.values.values().cloned().collect()
    }
}

// åˆ†å¸ƒå¼æ•°æ®åŒæ­¥æ¡†æ¶
struct DataSyncFramework {
    node_id: String,
    storage: Box<dyn DataStorage>,
    sync_strategy: SyncStrategy,
    conflict_resolver: Box<dyn ConflictResolver>,
    peers: Vec<String>,
}

trait DataStorage: Send + Sync {
    fn get(&self, key: &str) -> Result<Option<Vec<u8>>, String>;
    fn put(&self, key: &str, value: &[u8], metadata: Option<&[u8]>) -> Result<(), String>;
    fn delete(&self, key: &str) -> Result<bool, String>;
    fn list_keys(&self, prefix: &str) -> Result<Vec<String>, String>;
    fn get_with_metadata(&self, key: &str) -> Result<Option<(Vec<u8>, Option<Vec<u8>>)>, String>;
}

trait ConflictResolver: Send + Sync {
    fn resolve(&self, key: &str, local: Option<&[u8]>, remote: Option<&[u8]>, local_metadata: Option<&[u8]>, remote_metadata: Option<&[u8]>) -> Result<Option<Vec<u8>>, String>;
}

enum SyncStrategy {
    PullBased {
        interval: Duration,
    },
    PushBased {
        change_buffer_size: usize,
    },
    Hybrid {
        pull_interval: Duration,
        push_threshold: usize,
    },
}

impl DataSyncFramework {
    fn new(
        node_id: &str,
        storage: Box<dyn DataStorage>,
        sync_strategy: SyncStrategy,
        conflict_resolver: Box<dyn ConflictResolver>,
        peers: Vec<String>,
    ) -> Self {
        DataSyncFramework {
            node_id: node_id.to_string(),
            storage,
            sync_strategy,
            conflict_resolver,
            peers,
        }
    }

    fn start_sync(&self) {
        match &self.sync_strategy {
            SyncStrategy::PullBased { interval } => {
                println!("å¯åŠ¨åŸºäºæ‹‰å–çš„åŒæ­¥ï¼Œé—´éš”ï¼š{:?}", interval);
                // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªå®šæ—¶å™¨
            },
            SyncStrategy::PushBased { change_buffer_size } => {
                println!("å¯åŠ¨åŸºäºæ¨é€çš„åŒæ­¥ï¼Œç¼“å†²åŒºå¤§å°ï¼š{}", change_buffer_size);
                // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè®¾ç½®ä¸€ä¸ªå˜æ›´ç›‘å¬å™¨
            },
            SyncStrategy::Hybrid { pull_interval, push_threshold } => {
                println!("å¯åŠ¨æ··åˆåŒæ­¥ç­–ç•¥ï¼Œæ‹‰å–é—´éš”ï¼š{:?}ï¼Œæ¨é€é˜ˆå€¼ï¼š{}", pull_interval, push_threshold);
                // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåŒæ—¶è®¾ç½®å®šæ—¶å™¨å’Œå˜æ›´ç›‘å¬å™¨
            },
        }
    }

    fn sync_with_peer(&mut self, peer: &str) -> Result<SyncStats, String> {
        println!("ä¸èŠ‚ç‚¹ {} åŒæ­¥", peer);

        let mut stats = SyncStats {
            keys_synced: 0,
            bytes_transferred: 0,
            conflicts_resolved: 0,
            failed_keys: Vec::new(),
        };

        // è·å–è¿œç¨‹å¯†é’¥åˆ—è¡¨
        let remote_keys = self.get_keys_from_peer(peer)?;

        // è·å–æœ¬åœ°å¯†é’¥åˆ—è¡¨
        let local_keys = self.storage.list_keys("")?;

        // æ‰¾å‡ºæ‰€æœ‰å”¯ä¸€å¯†é’¥
        let all_keys: HashSet<String> = local_keys.iter().chain(remote_keys.iter())
            .cloned()
            .collect();

        // åŒæ­¥æ¯ä¸ªå¯†é’¥
        for key in all_keys {
            match self.sync_key(&key, peer) {
                Ok((bytes, had_conflict)) => {
                    stats.keys_synced += 1;
                    stats.bytes_transferred += bytes;

                    if had_conflict {
                        stats.conflicts_resolved += 1;
                    }
                },
                Err(err) => {
                    println!("åŒæ­¥å¯†é’¥ {} å¤±è´¥: {}", key, err);
                    stats.failed_keys.push(key);
                }
            }
        }

        Ok(stats)
    }

    fn sync_key(&mut self, key: &str, peer: &str) -> Result<(usize, bool), String> {
        // è·å–æœ¬åœ°å€¼å’Œå…ƒæ•°æ®
        let local_data = self.storage.get_with_metadata(key)?;

        // è·å–è¿œç¨‹å€¼å’Œå…ƒæ•°æ®
        let remote_data = self.get_from_peer(peer, key)?;

        let (local_value, local_metadata) = match local_data {
            Some((value, metadata)) => (Some(value), metadata),
            None => (None, None),
        };

        let (remote_value, remote_metadata) = match remote_data {
            Some((value, metadata)) => (Some(value), metadata),
            None => (None, None),
        };

        // æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†²çª
        let has_conflict = match (&local_value, &remote_value) {
            (Some(_), Some(_)) => true, // ç®€åŒ–å¤„ç†ï¼Œå®é™…ä¸Šåº”è¯¥åŸºäºå…ƒæ•°æ®åˆ¤æ–­
            _ => false,
        };

        // å¦‚æœæœ‰å†²çªï¼Œä½¿ç”¨å†²çªè§£å†³å™¨
        if has_conflict {
            let resolved = self.conflict_resolver.resolve(
                key,
                local_value.as_deref(),
                remote_value.as_deref(),
                local_metadata.as_deref(),
                remote_metadata.as_deref(),
            )?;

            // å­˜å‚¨è§£å†³åçš„å€¼
            if let Some(value) = resolved {
                // ç”Ÿæˆæ–°çš„å…ƒæ•°æ®
                let new_metadata = self.generate_metadata(key)?;

                // ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨
                self.storage.put(key, &value, Some(&new_metadata))?;

                // ä¹Ÿå°†è§£å†³åçš„å€¼æ¨é€åˆ°å¯¹æ–¹
                self.put_to_peer(peer, key, &value, Some(&new_metadata))?;

                return Ok((value.len() * 2, true)); // åŒå‘ä¼ è¾“
            }

            return Ok((0, true)); // å†²çªè§£å†³ä¸ºåˆ é™¤
        }

        // æ²¡æœ‰å†²çªï¼Œä½¿ç”¨è¾ƒæ–°çš„å€¼
        match (&local_value, &remote_value) {
            (Some(local), None) => {
                // æœ¬åœ°æœ‰ï¼Œè¿œç¨‹æ²¡æœ‰ï¼Œæ¨é€åˆ°è¿œç¨‹
                self.put_to_peer(peer, key, local, local_metadata.as_deref())?;
                Ok((local.len(), false))
            },
            (None, Some(remote)) => {
                // æœ¬åœ°æ²¡æœ‰ï¼Œè¿œç¨‹æœ‰ï¼Œä»è¿œç¨‹æ‹‰å–
                self.storage.put(key, remote, remote_metadata.as_deref())?;
                Ok((remote.len(), false))
            },
            (Some(_), Some(_)) => {
                // ä¸¤è¾¹éƒ½æœ‰ä½†æ²¡æœ‰å†²çªï¼ˆå·²åœ¨å‰é¢å¤„ç†ï¼‰
                Ok((0, false))
            },
            (None, None) => {
                // ä¸¤è¾¹éƒ½æ²¡æœ‰ï¼Œä¸åº”è¯¥å‘ç”Ÿ
                Ok((0, false))
            }
        }
    }

    fn generate_metadata(&self, key: &str) -> Result<Vec<u8>, String> {
        // ç®€åŒ–å®ç°ï¼Œä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºå…ƒæ•°æ®
        let timestamp = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .map_err(|e| format!("è·å–ç³»ç»Ÿæ—¶é—´å¤±è´¥: {}", e))?
            .as_secs();

        let metadata = format!("{}:{}:{}", self.node_id, key, timestamp);
        Ok(metadata.into_bytes())
    }

    // ä»¥ä¸‹æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦é€šè¿‡ç½‘ç»œè°ƒç”¨è¿œç¨‹èŠ‚ç‚¹

    fn get_keys_from_peer(&self, peer: &str) -> Result<Vec<String>, String> {
        // æ¨¡æ‹Ÿä»å¯¹ç­‰èŠ‚ç‚¹è·å–é”®åˆ—è¡¨
        println!("ä»èŠ‚ç‚¹ {} è·å–é”®åˆ—è¡¨", peer);
        Ok(Vec::new()) // ç®€åŒ–å®ç°
    }

    fn get_from_peer(&self, peer: &str, key: &str) -> Result<Option<(Vec<u8>, Option<Vec<u8>>)>, String> {
        // æ¨¡æ‹Ÿä»å¯¹ç­‰èŠ‚ç‚¹è·å–å€¼å’Œå…ƒæ•°æ®
        println!("ä»èŠ‚ç‚¹ {} è·å–é”® {}", peer, key);
        Ok(None) // ç®€åŒ–å®ç°
    }

    fn put_to_peer(&self, peer: &str, key: &str, value: &[u8], metadata: Option<&[u8]>) -> Result<(), String> {
        // æ¨¡æ‹Ÿå‘å¯¹ç­‰èŠ‚ç‚¹å‘é€å€¼å’Œå…ƒæ•°æ®
        println!("å‘èŠ‚ç‚¹ {} å‘é€é”® {}", peer, key);
        Ok(()) // ç®€åŒ–å®ç°
    }
}

struct SyncStats {
    keys_synced: usize,
    bytes_transferred: usize,
    conflicts_resolved: usize,
    failed_keys: Vec<String>,
}

// æœ€åå†™å…¥è·èƒœå†²çªè§£å†³å™¨
struct LastWriteWinsResolver;

impl ConflictResolver for LastWriteWinsResolver {
    fn resolve(&self, _key: &str, local: Option<&[u8]>, remote: Option<&[u8]>, local_metadata: Option<&[u8]>, remote_metadata: Option<&[u8]>) -> Result<Option<Vec<u8>>, String> {
        // åŸºäºæ—¶é—´æˆ³æ¯”è¾ƒï¼Œå‡è®¾å…ƒæ•°æ®æ ¼å¼ä¸º "node_id:key:timestamp"
        let local_ts = local_metadata.and_then(|md| {
            let s = String::from_utf8_lossy(md);
            s.split(':').last().and_then(|ts| ts.parse::<u64>().ok())
        }).unwrap_or(0);

        let remote_ts = remote_metadata.and_then(|md| {
            let s = String::from_utf8_lossy(md);
            s.split(':').last().and_then(|ts| ts.parse::<u64>().ok())
        }).unwrap_or(0);

        if remote_ts > local_ts {
            // è¿œç¨‹æ›´æ–°ï¼Œä½¿ç”¨è¿œç¨‹å€¼
            Ok(remote.map(|v| v.to_vec()))
        } else {
            // æœ¬åœ°æ›´æ–°æˆ–ç›¸åŒï¼Œä½¿ç”¨æœ¬åœ°å€¼
            Ok(local.map(|v| v.to_vec()))
        }
    }
}

// å‘é‡æ—¶é’Ÿå†²çªè§£å†³å™¨
struct VectorClockResolver;

impl ConflictResolver for VectorClockResolver {
    fn resolve(&self, _key: &str, local: Option<&[u8]>, remote: Option<&[u8]>, local_metadata: Option<&[u8]>, remote_metadata: Option<&[u8]>) -> Result<Option<Vec<u8>>, String> {
        // è§£æå…ƒæ•°æ®ä¸­çš„å‘é‡æ—¶é’Ÿ
        // ç®€åŒ–å®ç°ï¼Œå‡è®¾å…ƒæ•°æ®æ˜¯åºåˆ—åŒ–çš„å‘é‡æ—¶é’Ÿ

        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œéœ€è¦è§£æå‘é‡æ—¶é’Ÿå¹¶æ¯”è¾ƒå®ƒä»¬
        // ç°åœ¨åªæ˜¯åŸºäºé•¿åº¦åšä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿ

        let local_len = local_metadata.map_or(0, |m| m.len());
        let remote_len = remote_metadata.map_or(0, |m| m.len());

        if remote_len > local_len {
            // å‡è®¾è¿œç¨‹æ›´æ–°
            Ok(remote.map(|v| v.to_vec()))
        } else if local_len > remote_len {
            // å‡è®¾æœ¬åœ°æ›´æ–°
            Ok(local.map(|v| v.to_vec()))
        } else {
            // æ— æ³•åˆ¤æ–­ï¼Œä¿ç•™ä¸¤è€…å¹¶æ ‡è®°å†²çª
            if let (Some(l), Some(r)) = (local, remote) {
                let mut merged = l.to_vec();
                merged.extend_from_slice(b"---CONFLICT---");
                merged.extend_from_slice(r);
                Ok(Some(merged))
            } else {
                Ok(local.map(|v| v.to_vec()).or_else(|| remote.map(|v| v.to_vec())))
            }
        }
    }
}

// åˆå¹¶å‡½æ•°å†²çªè§£å†³å™¨
struct MergeFunctionResolver<F>
where
    F: Fn(Option<&[u8]>, Option<&[u8]>) -> Option<Vec<u8>> + Send + Sync,
{
    merge_fn: F,
}

impl<F> ConflictResolver for MergeFunctionResolver<F>
where
    F: Fn(Option<&[u8]>, Option<&[u8]>) -> Option<Vec<u8>> + Send + Sync,
{
    fn resolve(&self, _key: &str, local: Option<&[u8]>, remote: Option<&[u8]>, _local_metadata: Option<&[u8]>, _remote_metadata: Option<&[u8]>) -> Result<Option<Vec<u8>>, String> {
        Ok((self.merge_fn)(local, remote))
    }
}

// è¿™äº›ä»£ç å±•ç¤ºäº†åˆ†å¸ƒå¼ç³»ç»Ÿä¸­çš„ä¸€äº›å…³é”®ç»„ä»¶ï¼š
// 1. Raftå…±è¯†ç®—æ³•å®ç°ï¼Œç”¨äºåˆ†å¸ƒå¼ä¸€è‡´æ€§
// 2. åˆ†ç‰‡ç®¡ç†å™¨ï¼Œç”¨äºæ•°æ®åˆ†åŒº
// 3. ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•ï¼Œç”¨äºèŠ‚ç‚¹åˆ†é…
// 4. å¤šåŒºåŸŸéƒ¨ç½²æ¡†æ¶ï¼Œç”¨äºåœ°ç†åˆ†å¸ƒå¼ç³»ç»Ÿ
// 5. CRDTå®ç°ï¼Œç”¨äºæ— å†²çªæ•°æ®å¤åˆ¶
// 6. å‘é‡æ—¶é’Ÿï¼Œç”¨äºé¡ºåºè·Ÿè¸ª
// 7. æ•°æ®åŒæ­¥æ¡†æ¶ï¼Œç”¨äºèŠ‚ç‚¹é—´æ•°æ®å¤åˆ¶
// 8. å†²çªè§£å†³ç­–ç•¥ï¼Œç”¨äºè§£å†³æ•°æ®å†²çª

// é€šè¿‡ç»„åˆè¿™äº›ç»„ä»¶ï¼Œå¯ä»¥æ„å»ºå…·æœ‰é«˜å¯ç”¨æ€§ã€å®¹é”™æ€§å’Œæ‰©å±•æ€§çš„åˆ†å¸ƒå¼ç³»ç»Ÿã€‚
```

### 1.2 ç»¼åˆåº”ç”¨02-åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒå™¨

```rust
// åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒå™¨
struct TransactionCoordinator {
    coordinator_id: String,
    storage: Box<dyn TransactionStorage>,
    participants: HashMap<String, Box<dyn TransactionParticipant>>,
    active_transactions: RwLock<HashMap<String, TransactionState>>,
    retry_policy: RetryPolicy,
    recovery_manager: RecoveryManager,
}

enum TransactionState {
    Created {
        created_at: SystemTime,
    },
    Preparing {
        prepared_participants: HashSet<String>,
        started_at: SystemTime,
    },
    Prepared {
        prepared_at: SystemTime,
    },
    Committing {
        committed_participants: HashSet<String>,
        started_at: SystemTime,
    },
    Committed {
        committed_at: SystemTime,
    },
    Aborting {
        aborted_participants: HashSet<String>,
        started_at: SystemTime,
    },
    Aborted {
        aborted_at: SystemTime,
        reason: String,
    },
    Unknown,
}

trait TransactionStorage: Send + Sync {
    fn create_transaction(&self, tx_id: &str) -> Result<(), String>;
    fn update_transaction_state(&self, tx_id: &str, state: &TransactionState) -> Result<(), String>;
    fn get_transaction_state(&self, tx_id: &str) -> Result<Option<TransactionState>, String>;
    fn list_active_transactions(&self) -> Result<Vec<(String, TransactionState)>, String>;
    fn mark_transaction_completed(&self, tx_id: &str) -> Result<(), String>;
}

trait TransactionParticipant: Send + Sync {
    fn prepare(&self, tx_id: &str, actions: &[TransactionAction]) -> Result<bool, String>;
    fn commit(&self, tx_id: &str) -> Result<bool, String>;
    fn abort(&self, tx_id: &str) -> Result<bool, String>;
    fn get_state(&self, tx_id: &str) -> Result<ParticipantState, String>;
}

enum ParticipantState {
    Unknown,
    Prepared,
    Committed,
    Aborted,
}

struct TransactionAction {
    participant_id: String,
    operation_type: String,
    resource_id: String,
    data: serde_json::Value,
}

struct RecoveryManager {
    storage: Arc<dyn TransactionStorage>,
    participants: Arc<HashMap<String, Box<dyn TransactionParticipant>>>,
    recovery_interval: Duration,
    max_recovery_attempts: u32,
}

impl TransactionCoordinator {
    fn new(
        coordinator_id: &str,
        storage: Box<dyn TransactionStorage>,
        participants: HashMap<String, Box<dyn TransactionParticipant>>,
        retry_policy: RetryPolicy,
        recovery_interval: Duration,
        max_recovery_attempts: u32,
    ) -> Self {
        let shared_storage = Arc::new(storage);
        let shared_participants = Arc::new(participants);

        let recovery_manager = RecoveryManager {
            storage: shared_storage.clone(),
            participants: shared_participants.clone(),
            recovery_interval,
            max_recovery_attempts,
        };

        TransactionCoordinator {
            coordinator_id: coordinator_id.to_string(),
            storage: shared_storage as Box<dyn TransactionStorage>,
            participants: Arc::try_unwrap(shared_participants).unwrap_or_else(|arc| (*arc).clone()),
            active_transactions: RwLock::new(HashMap::new()),
            retry_policy,
            recovery_manager,
        }
    }

    fn start_transaction(&self) -> Result<String, String> {
        let tx_id = format!("tx-{}-{}", self.coordinator_id, uuid::Uuid::new_v4());

        // åˆ›å»ºäº‹åŠ¡çŠ¶æ€
        let state = TransactionState::Created {
            created_at: SystemTime::now(),
        };

        // å­˜å‚¨äº‹åŠ¡
        self.storage.create_transaction(&tx_id)?;
        self.storage.update_transaction_state(&tx_id, &state)?;

        // æ·»åŠ åˆ°æ´»åŠ¨äº‹åŠ¡
        let mut active_transactions = self.active_transactions.write().unwrap();
        active_transactions.insert(tx_id.clone(), state);

        println!("åˆ›å»ºäº‹åŠ¡: {}", tx_id);

        Ok(tx_id)
    }

    fn prepare(&self, tx_id: &str, actions: &[TransactionAction]) -> Result<bool, String> {
        // ç¡®ä¿äº‹åŠ¡å­˜åœ¨
        if !self.transaction_exists(tx_id)? {
            return Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id));
        }

        println!("å‡†å¤‡äº‹åŠ¡: {}", tx_id);

        // å°†çŠ¶æ€æ›´æ–°ä¸º"å‡†å¤‡ä¸­"
        let mut prepared_participants = HashSet::new();
        let preparing_state = TransactionState::Preparing {
            prepared_participants: prepared_participants.clone(),
            started_at: SystemTime::now(),
        };
        self.update_transaction_state(tx_id, &preparing_state)?;

        // æŒ‰å‚ä¸è€…åˆ†ç»„æ“ä½œ
        let mut actions_by_participant: HashMap<String, Vec<TransactionAction>> = HashMap::new();
        for action in actions {
            actions_by_participant
                .entry(action.participant_id.clone())
                .or_insert_with(Vec::new)
                .push(action.clone());
        }

        // è¯·æ±‚æ¯ä¸ªå‚ä¸è€…å‡†å¤‡
        let mut all_prepared = true;
        for (participant_id, participant_actions) in &actions_by_participant {
            let participant = self.participants.get(participant_id)
                .ok_or_else(|| format!("å‚ä¸è€… {} ä¸å­˜åœ¨", participant_id))?;

            // ä½¿ç”¨é‡è¯•ç­–ç•¥
            let prepare_result = self.retry_policy.execute_with_check(
                || participant.prepare(tx_id, participant_actions),
                |err| {
                    // åªæœ‰ä¸´æ—¶é”™è¯¯æ‰é‡è¯•
                    !err.contains("æ°¸ä¹…å¤±è´¥")
                },
            );

            match prepare_result {
                Ok(prepared) => {
                    if prepared {
                        prepared_participants.insert(participant_id.clone());

                        // æ›´æ–°å‡†å¤‡å¥½çš„å‚ä¸è€…åˆ—è¡¨
                        let updating_state = TransactionState::Preparing {
                            prepared_participants: prepared_participants.clone(),
                            started_at: SystemTime::now(), // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ä¿ç•™åŸå§‹å¼€å§‹æ—¶é—´
                        };
                        self.update_transaction_state(tx_id, &updating_state)?;
                    } else {
                        all_prepared = false;
                        break;
                    }
                },
                Err(err) => {
                    println!("å‚ä¸è€… {} å‡†å¤‡å¤±è´¥: {}", participant_id, err);
                    all_prepared = false;
                    break;
                }
            }
        }

        // å¦‚æœæ‰€æœ‰å‚ä¸è€…éƒ½å‡†å¤‡å¥½äº†ï¼Œæ›´æ–°çŠ¶æ€ä¸º"å·²å‡†å¤‡"
        if all_prepared {
            let prepared_state = TransactionState::Prepared {
                prepared_at: SystemTime::now(),
            };
            self.update_transaction_state(tx_id, &prepared_state)?;
            Ok(true)
        } else {
            // å¦‚æœæœ‰å‚ä¸è€…å‡†å¤‡å¤±è´¥ï¼Œå›æ»šäº‹åŠ¡
            self.abort_transaction(tx_id, "å‡†å¤‡é˜¶æ®µå¤±è´¥")?;
            Ok(false)
        }
    }

    fn commit_transaction(&self, tx_id: &str) -> Result<bool, String> {
        // ç¡®ä¿äº‹åŠ¡å­˜åœ¨ä¸”å¤„äºæ­£ç¡®çŠ¶æ€
        let current_state = self.get_transaction_state(tx_id)?;
        match current_state {
            TransactionState::Prepared { .. } => {
                // ç»§ç»­æäº¤
            },
            _ => {
                return Err(format!("äº‹åŠ¡ {} ä¸åœ¨å¯æäº¤çŠ¶æ€", tx_id));
            }
        }

        println!("æäº¤äº‹åŠ¡: {}", tx_id);

        // æ›´æ–°çŠ¶æ€ä¸º"æäº¤ä¸­"
        let committing_state = TransactionState::Committing {
            committed_participants: HashSet::new(),
            started_at: SystemTime::now(),
        };
        self.update_transaction_state(tx_id, &committing_state)?;

        // è·å–å‡†å¤‡å¥½çš„å‚ä¸è€…
        let prepared_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Preparing { prepared_participants, .. }) => prepared_participants,
            _ => HashSet::new(),
        };

        // æäº¤æ¯ä¸ªå‚ä¸è€…
        let mut committed_participants = HashSet::new();
        let mut all_committed = true;

        for participant_id in prepared_participants {
            if let Some(participant) = self.participants.get(&participant_id) {
                // ä½¿ç”¨é‡è¯•ç­–ç•¥
                match self.retry_policy.execute(|| participant.commit(tx_id)) {
                    Ok(true) => {
                        committed_participants.insert(participant_id.clone());

                        // æ›´æ–°å·²æäº¤çš„å‚ä¸è€…åˆ—è¡¨
                        let updating_state = TransactionState::Committing {
                            committed_participants: committed_participants.clone(),
                            started_at: SystemTime::now(), // ç®€åŒ–å¤„ç†
                        };
                        self.update_transaction_state(tx_id, &updating_state)?;
                    },
                    Ok(false) => {
                        println!("å‚ä¸è€… {} æ‹’ç»æäº¤", participant_id);
                        all_committed = false;
                    },
                    Err(err) => {
                        println!("å‚ä¸è€… {} æäº¤å¤±è´¥: {}", participant_id, err);
                        all_committed = false;
                    }
                }
            }
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        if all_committed {
            let committed_state = TransactionState::Committed {
                committed_at: SystemTime::now(),
            };
            self.update_transaction_state(tx_id, &committed_state)?;
            self.storage.mark_transaction_completed(tx_id)?;

            // ä»æ´»åŠ¨äº‹åŠ¡ä¸­ç§»é™¤
            let mut active_transactions = self.active_transactions.write().unwrap();
            active_transactions.remove(tx_id);

            Ok(true)
        } else {
            // ç†æƒ³æƒ…å†µä¸‹ï¼Œæäº¤ä¸åº”è¯¥å¤±è´¥ï¼Œä½†å¦‚æœå¤±è´¥ï¼Œè®°å½•ä¸ºæœªçŸ¥çŠ¶æ€
            let unknown_state = TransactionState::Unknown;
            self.update_transaction_state(tx_id, &unknown_state)?;

            Err("éƒ¨åˆ†å‚ä¸è€…æäº¤å¤±è´¥ï¼Œäº‹åŠ¡å¤„äºæœªçŸ¥çŠ¶æ€".to_string())
        }
    }

    fn abort_transaction(&self, tx_id: &str, reason: &str) -> Result<bool, String> {
        // ç¡®ä¿äº‹åŠ¡å­˜åœ¨
        if !self.transaction_exists(tx_id)? {
            return Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id));
        }

        println!("ä¸­æ­¢äº‹åŠ¡: {}, åŸå› : {}", tx_id, reason);

        // æ›´æ–°çŠ¶æ€ä¸º"ä¸­æ­¢ä¸­"
        let aborting_state = TransactionState::Aborting {
            aborted_participants: HashSet::new(),
            started_at: SystemTime::now(),
        };
        self.update_transaction_state(tx_id, &aborting_state)?;

        // è·å–å·²å‡†å¤‡çš„å‚ä¸è€…
        let prepared_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Preparing { prepared_participants, .. }) => prepared_participants,
            _ => HashSet::new(),
        };

        // ä¸­æ­¢æ¯ä¸ªå‚ä¸è€…
        let mut aborted_participants = HashSet::new();

        for participant_id in prepared_participants {
            if let Some(participant) = self.participants.get(&participant_id) {
                // ä½¿ç”¨é‡è¯•ç­–ç•¥
                match self.retry_policy.execute(|| participant.abort(tx_id)) {
                    Ok(true) => {
                        aborted_participants.insert(participant_id.clone());

                        // æ›´æ–°å·²ä¸­æ­¢çš„å‚ä¸è€…åˆ—è¡¨
                        let updating_state = TransactionState::Aborting {
                            aborted_participants: aborted_participants.clone(),
                            started_at: SystemTime::now(), // ç®€åŒ–å¤„ç†
                        };
                        self.update_transaction_state(tx_id, &updating_state)?;
                    },
                    Ok(false) => {
                        println!("å‚ä¸è€… {} æ‹’ç»ä¸­æ­¢", participant_id);
                    },
                    Err(err) => {
                        println!("å‚ä¸è€… {} ä¸­æ­¢å¤±è´¥: {}", participant_id, err);
                    }
                }
            }
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        let aborted_state = TransactionState::Aborted {
            aborted_at: SystemTime::now(),
            reason: reason.to_string(),
        };
        self.update_transaction_state(tx_id, &aborted_state)?;
        self.storage.mark_transaction_completed(tx_id)?;

        // ä»æ´»åŠ¨äº‹åŠ¡ä¸­ç§»é™¤
        let mut active_transactions = self.active_transactions.write().unwrap();
        active_transactions.remove(tx_id);

        Ok(true)
    }

    fn transaction_exists(&self, tx_id: &str) -> Result<bool, String> {
        // é¦–å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„æ´»åŠ¨äº‹åŠ¡
        {
            let active_transactions = self.active_transactions.read().unwrap();
            if active_transactions.contains_key(tx_id) {
                return Ok(true);
            }
        }

        // å¦‚æœä¸åœ¨å†…å­˜ä¸­ï¼Œæ£€æŸ¥å­˜å‚¨
        match self.storage.get_transaction_state(tx_id)? {
            Some(_) => Ok(true),
            None => Ok(false),
        }
    }

    fn get_transaction_state(&self, tx_id: &str) -> Result<TransactionState, String> {
        // é¦–å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„æ´»åŠ¨äº‹åŠ¡
        {
            let active_transactions = self.active_transactions.read().unwrap();
            if let Some(state) = active_transactions.get(tx_id) {
                return Ok(state.clone());
            }
        }

        // å¦‚æœä¸åœ¨å†…å­˜ä¸­ï¼Œä»å­˜å‚¨è·å–
        match self.storage.get_transaction_state(tx_id)? {
            Some(state) => Ok(state),
            None => Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id)),
        }
    }

    fn update_transaction_state(&self, tx_id: &str, state: &TransactionState) -> Result<(), String> {
        // æ›´æ–°å­˜å‚¨
        self.storage.update_transaction_state(tx_id, state)?;

        // æ›´æ–°å†…å­˜çŠ¶æ€
        let mut active_transactions = self.active_transactions.write().unwrap();
        active_transactions.insert(tx_id.to_string(), state.clone());

        Ok(())
    }

    fn start_recovery(&self) {
        println!("å¯åŠ¨äº‹åŠ¡æ¢å¤è¿›ç¨‹");

        // å¯åŠ¨æ¢å¤ç®¡ç†å™¨
        let coordinator_id = self.coordinator_id.clone();
        std::thread::spawn(move || {
            println!("äº‹åŠ¡æ¢å¤çº¿ç¨‹å·²å¯åŠ¨: {}", coordinator_id);
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘¨æœŸæ€§æ£€æŸ¥æœªå®Œæˆçš„äº‹åŠ¡å¹¶å°è¯•æ¢å¤
        });
    }
}

impl RecoveryManager {
    fn recover_transactions(&self) -> Result<(), String> {
        println!("å¼€å§‹æ¢å¤æœªå®Œæˆäº‹åŠ¡");

        // è·å–æ‰€æœ‰æ´»åŠ¨äº‹åŠ¡
        let active_transactions = self.storage.list_active_transactions()?;

        for (tx_id, state) in active_transactions {
            match state {
                TransactionState::Created { created_at } => {
                    // å¦‚æœäº‹åŠ¡åˆ›å»ºåé•¿æ—¶é—´æœªæœ‰è¿›å±•ï¼Œä¸­æ­¢å®ƒ
                    if created_at.elapsed().unwrap() > Duration::from_secs(3600) {
                        self.abort_transaction(&tx_id, "äº‹åŠ¡åˆ›å»ºåè¶…æ—¶")?;
                    }
                },
                TransactionState::Preparing { started_at, .. } => {
                    // å¦‚æœå‡†å¤‡ä¸­çš„äº‹åŠ¡è¶…æ—¶ï¼Œä¸­æ­¢å®ƒ
                    if started_at.elapsed().unwrap() > Duration::from_secs(3600) {
                        self.abort_transaction(&tx_id, "å‡†å¤‡é˜¶æ®µè¶…æ—¶")?;
                    }
                },
                TransactionState::Prepared { prepared_at } => {
                    // å¦‚æœäº‹åŠ¡å·²å‡†å¤‡å¥½ä½†æœªæäº¤ï¼Œå°è¯•æäº¤
                    if prepared_at.elapsed().unwrap() > Duration::from_secs(60) {
                        self.commit_transaction(&tx_id)?;
                    }
                },
                TransactionState::Committing { started_at, .. } => {
                    // å¦‚æœæäº¤ä¸­çš„äº‹åŠ¡è¶…æ—¶ï¼Œé‡è¯•æäº¤
                    if started_at.elapsed().unwrap() > Duration::from_secs(3600) {
                        self.retry_commit(&tx_id)?;
                    }
                },
                TransactionState::Aborting { started_at, .. } => {
                    // å¦‚æœä¸­æ­¢ä¸­çš„äº‹åŠ¡è¶…æ—¶ï¼Œé‡è¯•ä¸­æ­¢
                    if started_at.elapsed().unwrap() > Duration::from_secs(3600) {
                        self.retry_abort(&tx_id)?;
                    }
                },
                _ => {
                    // å…¶ä»–çŠ¶æ€ä¸éœ€è¦æ¢å¤
                }
            }
        }

        Ok(())
    }

    fn abort_transaction(&self, tx_id: &str, reason: &str) -> Result<(), String> {
        println!("æ¢å¤: ä¸­æ­¢äº‹åŠ¡ {}, åŸå› : {}", tx_id, reason);

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€ä¸ºä¸­æ­¢ä¸­
        let aborting_state = TransactionState::Aborting {
            aborted_participants: HashSet::new(),
            started_at: SystemTime::now(),
        };
        self.storage.update_transaction_state(tx_id, &aborting_state)?;

        // è·å–å·²å‡†å¤‡çš„å‚ä¸è€…
        let prepared_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Preparing { prepared_participants, .. }) => prepared_participants,
            _ => HashSet::new(),
        };

        // ä¸­æ­¢æ¯ä¸ªå‚ä¸è€…
        for participant_id in prepared_participants {
            if let Some(participant) = self.participants.get(&participant_id) {
                // å°è¯•ä¸­æ­¢ï¼Œä½†ä¸é‡è¯•ï¼ˆæ¢å¤è¿‡ç¨‹ä¸­ä¼šé‡è¯•æ•´ä¸ªè¿‡ç¨‹ï¼‰
                if let Err(err) = participant.abort(tx_id) {
                    println!("æ¢å¤: å‚ä¸è€… {} ä¸­æ­¢å¤±è´¥: {}", participant_id, err);
                }
            }
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        let aborted_state = TransactionState::Aborted {
            aborted_at: SystemTime::now(),
            reason: reason.to_string(),
        };
        self.storage.update_transaction_state(tx_id, &aborted_state)?;
        self.storage.mark_transaction_completed(tx_id)?;

        Ok(())
    }

    fn commit_transaction(&self, tx_id: &str) -> Result<(), String> {
        println!("æ¢å¤: æäº¤äº‹åŠ¡ {}", tx_id);

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€ä¸ºæäº¤ä¸­
        let committing_state = TransactionState::Committing {
            committed_participants: HashSet::new(),
            started_at: SystemTime::now(),
        };
        self.storage.update_transaction_state(tx_id, &committing_state)?;

        // è·å–å·²å‡†å¤‡çš„å‚ä¸è€…
        let prepared_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Preparing { prepared_participants, .. }) => prepared_participants,
            _ => HashSet::new(),
        };

        // æäº¤æ¯ä¸ªå‚ä¸è€…
        for participant_id in prepared_participants {
            if let Some(participant) = self.participants.get(&participant_id) {
                // å°è¯•æäº¤ï¼Œä½†ä¸é‡è¯•ï¼ˆæ¢å¤è¿‡ç¨‹ä¸­ä¼šé‡è¯•æ•´ä¸ªè¿‡ç¨‹ï¼‰
                if let Err(err) = participant.commit(tx_id) {
                    println!("æ¢å¤: å‚ä¸è€… {} æäº¤å¤±è´¥: {}", participant_id, err);
                }
            }
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        let committed_state = TransactionState::Committed {
            committed_at: SystemTime::now(),
        };
        self.storage.update_transaction_state(tx_id, &committed_state)?;
        self.storage.mark_transaction_completed(tx_id)?;

        Ok(())
    }

    fn retry_commit(&self, tx_id: &str) -> Result<(), String> {
        println!("æ¢å¤: é‡è¯•æäº¤äº‹åŠ¡ {}", tx_id);

        // è·å–å·²æäº¤çš„å‚ä¸è€…
        let committed_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Committing { committed_participants, .. }) => committed_participants,
            _ => HashSet::new(),
        };

        // è·å–å·²å‡†å¤‡çš„å‚ä¸è€…
        let prepared_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Preparing { prepared_participants, .. }) => prepared_participants,
            _ => HashSet::new(),
        };

        // æ‰¾å‡ºå°šæœªæäº¤çš„å‚ä¸è€…
        let uncommitted = prepared_participants.difference(&committed_participants);

        // å°è¯•æäº¤æ¯ä¸ªæœªæäº¤çš„å‚ä¸è€…
        for participant_id in uncommitted {
            if let Some(participant) = self.participants.get(participant_id) {
                // å°è¯•æäº¤ï¼Œä½†ä¸é‡è¯•ï¼ˆæ¢å¤è¿‡ç¨‹ä¸­ä¼šé‡è¯•æ•´ä¸ªè¿‡ç¨‹ï¼‰
                if let Err(err) = participant.commit(tx_id) {
                    println!("æ¢å¤: å‚ä¸è€… {} æäº¤å¤±è´¥: {}", participant_id, err);
                }
            }
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        let committed_state = TransactionState::Committed {
            committed_at: SystemTime::now(),
        };
        self.storage.update_transaction_state(tx_id, &committed_state)?;
        self.storage.mark_transaction_completed(tx_id)?;

        Ok(())
    }

    fn retry_abort(&self, tx_id: &str) -> Result<(), String> {
        println!("æ¢å¤: é‡è¯•ä¸­æ­¢äº‹åŠ¡ {}", tx_id);

        // è·å–å·²ä¸­æ­¢çš„å‚ä¸è€…
        let aborted_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Aborting { aborted_participants, .. }) => aborted_participants,
            _ => HashSet::new(),
        };

        // è·å–å·²å‡†å¤‡çš„å‚ä¸è€…
        let prepared_participants = match self.storage.get_transaction_state(tx_id)? {
            Some(TransactionState::Preparing { prepared_participants, .. }) => prepared_participants,
            _ => HashSet::new(),
        };

        // æ‰¾å‡ºå°šæœªä¸­æ­¢çš„å‚ä¸è€…
        let unaborted = prepared_participants.difference(&aborted_participants);

        // å°è¯•ä¸­æ­¢æ¯ä¸ªæœªä¸­æ­¢çš„å‚ä¸è€…
        for participant_id in unaborted {
            if let Some(participant) = self.participants.get(participant_id) {
                // å°è¯•ä¸­æ­¢ï¼Œä½†ä¸é‡è¯•ï¼ˆæ¢å¤è¿‡ç¨‹ä¸­ä¼šé‡è¯•æ•´ä¸ªè¿‡ç¨‹ï¼‰
                if let Err(err) = participant.abort(tx_id) {
                    println!("æ¢å¤: å‚ä¸è€… {} ä¸­æ­¢å¤±è´¥: {}", participant_id, err);
                }
            }
        }

        // æ›´æ–°æœ€ç»ˆçŠ¶æ€
        let aborted_state = TransactionState::Aborted {
            aborted_at: SystemTime::now(),
            reason: "æ¢å¤è¿‡ç¨‹ä¸­ä¸­æ­¢".to_string(),
        };
        self.storage.update_transaction_state(tx_id, &aborted_state)?;
        self.storage.mark_transaction_completed(tx_id)?;

        Ok(())
    }
}

// åˆ†å¸ƒå¼é”å®ç°ï¼ˆRedlockç®—æ³•ï¼‰
struct RedLock {
    nodes: Vec<Box<dyn LockNode>>,
    lock_timeout: Duration,
    retry_count: u32,
    retry_delay: Duration,
    clock_drift_factor: f64,
}

trait LockNode: Send + Sync {
    fn acquire(&self, resource: &str, lock_id: &str, ttl: Duration) -> Result<bool, String>;
    fn release(&self, resource: &str, lock_id: &str) -> Result<bool, String>;
}

impl RedLock {
    fn new(nodes: Vec<Box<dyn LockNode>>, lock_timeout: Duration, retry_count: u32, retry_delay: Duration) -> Self {
        RedLock {
            nodes,
            lock_timeout,
            retry_count,
            retry_delay,
            clock_drift_factor: 0.01, // é»˜è®¤æ—¶é’Ÿæ¼‚ç§»å› å­
        }
    }

    fn acquire_lock(&self, resource: &str) -> Result<Option<Lock>, String> {
        if self.nodes.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨çš„é”èŠ‚ç‚¹".to_string());
        }

        let lock_id = uuid::Uuid::new_v4().to_string();

        for _ in 0..self.retry_count {
            let start_time = Instant::now();

            // è®°å½•æˆåŠŸè·å–é”çš„èŠ‚ç‚¹æ•°
            let mut acquired_nodes = 0;

            // å°è¯•åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè·å–é”
            for node in &self.nodes {
                match node.acquire(resource, &lock_id, self.lock_timeout) {
                    Ok(true) => {
                        acquired_nodes += 1;
                    },
                    Ok(false) => {
                        // èŠ‚ç‚¹å·²è¢«é”å®š
                    },
                    Err(err) => {
                        println!("è·å–èŠ‚ç‚¹é”å¤±è´¥: {}", err);
                    }
                }
            }

            // è®¡ç®—è·å–é”æ‰€èŠ±è´¹çš„æ—¶é—´
            let elapsed = start_time.elapsed();

            // è®¡ç®—é”æœ‰æ•ˆæ—¶é—´
            let drift = (self.lock_timeout.as_millis() as f64 * self.clock_drift_factor) as u64;
            let valid_time = self.lock_timeout.checked_sub(Duration::from_millis(drift))
                                            .and_then(|t| t.checked_sub(elapsed));

            // æ£€æŸ¥æ˜¯å¦è·å¾—å¤šæ•°èŠ‚ç‚¹é”
            let quorum = self.nodes.len() / 2 + 1;
            if acquired_nodes >= quorum {
                if let Some(valid_time) = valid_time {
                    if valid_time > Duration::from_millis(0) {
                        return Ok(Some(Lock {
                            resource: resource.to_string(),
                            lock_id: lock_id.clone(),
                            valid_time,
                            redlock: self,
                        }));
                    }
                }
            }

            // é”è·å–å¤±è´¥ï¼Œé‡Šæ”¾æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„é”
            for node in &self.nodes {
                let _ = node.release(resource, &lock_id);
            }

            // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            std::thread::sleep(self.retry_delay);
        }

        Ok(None)
    }

    fn release_lock(&self, resource: &str, lock_id: &str) -> Result<(), String> {
        let mut release_errors = Vec::new();

        // å°è¯•é‡Šæ”¾æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„é”
        for node in &self.nodes {
            match node.release(resource, lock_id) {
                Ok(true) => {
                    // æˆåŠŸé‡Šæ”¾
                },
                Ok(false) => {
                    // é”ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ
                },
                Err(err) => {
                    release_errors.push(err);
                }
            }
        }

        if !release_errors.is_empty() {
            return Err(format!("é‡Šæ”¾é”æ—¶å‘ç”Ÿé”™è¯¯: {:?}", release_errors));
        }

        Ok(())
    }
}

struct Lock<'a> {
    resource: String,
    lock_id: String,
    valid_time: Duration,
    redlock: &'a RedLock,
}

impl<'a> Drop for Lock<'a> {
    fn drop(&mut self) {
        // åœ¨é”å¯¹è±¡è¢«é”€æ¯æ—¶è‡ªåŠ¨é‡Šæ”¾é”
        if let Err(err) = self.redlock.release_lock(&self.resource, &self.lock_id) {
            println!("é‡Šæ”¾é”å¤±è´¥: {}", err);
        }
    }
}

// æ¨¡æ‹ŸRedisé”èŠ‚ç‚¹å®ç°
struct RedisLockNode {
    client: redis::Client,
    name: String,
}

impl RedisLockNode {
    fn new(redis_url: &str, name: &str) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;

        Ok(RedisLockNode {
            client,
            name: name.to_string(),
        })
    }
}

impl LockNode for RedisLockNode {
    fn acquire(&self, resource: &str, lock_id: &str, ttl: Duration) -> Result<bool, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        // ä½¿ç”¨SET NXå‘½ä»¤å°è¯•è·å–é”
        let lock_key = format!("lock:{}", resource);
        let ttl_millis = ttl.as_millis() as usize;

        let result: bool = redis::cmd("SET")
            .arg(&lock_key)
            .arg(lock_id)
            .arg("NX")
            .arg("PX")
            .arg(ttl_millis)
            .query(&mut conn)
            .map_err(|err| format!("Redis SETæ“ä½œå¤±è´¥: {}", err))?;

        Ok(result)
    }

    fn release(&self, resource: &str, lock_id: &str) -> Result<bool, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let lock_key = format!("lock:{}", resource);

        // ä½¿ç”¨Luaè„šæœ¬ç¡®ä¿åªåˆ é™¤è‡ªå·±çš„é”
        let script = r"
            if redis.call('get', KEYS[1]) == ARGV[1] then
                return redis.call('del', KEYS[1])
            else
                return 0
            end
        ";

        let result: i32 = redis::Script::new(script)
            .key(&lock_key)
            .arg(lock_id)
            .invoke(&mut conn)
            .map_err(|err| format!("Redisè„šæœ¬æ‰§è¡Œå¤±è´¥: {}", err))?;

        Ok(result == 1)
    }
}

// æ—¶é—´åŒæ­¥æœåŠ¡ï¼ˆNTPé£æ ¼ï¼‰
struct TimeServer {
    server_id: String,
    reference_clock: Arc<AtomicU64>,
    stratum: u8,
    precision: i8, // ç²¾åº¦ï¼Œä»¥2çš„å¹‚è¡¨ç¤ºï¼Œä¾‹å¦‚-20è¡¨ç¤ºå¾®ç§’çº§
    clients: RwLock<HashMap<String, TimeClient>>,
}

struct TimeClient {
    client_id: String,
    last_sync: SystemTime,
    offset: AtomicI64,     // å®¢æˆ·ç«¯ç›¸å¯¹äºæœåŠ¡å™¨çš„æ—¶é—´åç§»ï¼ˆçº³ç§’ï¼‰
    round_trip_delay: AtomicU64, // å¾€è¿”å»¶è¿Ÿï¼ˆçº³ç§’ï¼‰
    dispersion: AtomicU64, // æ—¶é—´åˆ†æ•£åº¦ï¼ˆçº³ç§’ï¼‰
}

struct TimeQuery {
    client_id: String,
    t1: u64, // å®¢æˆ·ç«¯å‘é€æ—¶é—´
    t2: u64, // æœåŠ¡å™¨æ¥æ”¶æ—¶é—´
    t3: u64, // æœåŠ¡å™¨å‘é€æ—¶é—´
    t4: u64, // å®¢æˆ·ç«¯æ¥æ”¶æ—¶é—´
}

impl TimeServer {
    fn new(server_id: &str, stratum: u8, precision: i8) -> Self {
        // åˆå§‹åŒ–å‚è€ƒæ—¶é’Ÿ
        let now = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_nanos() as u64;

        TimeServer {
            server_id: server_id.to_string(),
            reference_clock: Arc::new(AtomicU64::new(now)),
            stratum,
            precision,
            clients: RwLock::new(HashMap::new()),
        }
    }

    fn start(&self) {
        // å¯åŠ¨æ—¶é’Ÿæ›´æ–°çº¿ç¨‹
        let reference_clock = self.reference_clock.clone();
        std::thread::spawn(move || {
            loop {
                // æ›´æ–°å‚è€ƒæ—¶é’Ÿ
                let now = SystemTime::now()
                    .duration_since(SystemTime::UNIX_EPOCH)
                    .unwrap()
                    .as_nanos() as u64;

                reference_clock.store(now, Ordering::SeqCst);

                // æ¯100æ¯«ç§’æ›´æ–°ä¸€æ¬¡
                std::thread::sleep(Duration::from_millis(100));
            }
        });
    }

    fn register_client(&self, client_id: &str) -> Result<(), String> {
        let mut clients = self.clients.write().unwrap();

        if clients.contains_key(client_id) {
            return Err(format!("å®¢æˆ·ç«¯ {} å·²å­˜åœ¨", client_id));
        }

        clients.insert(client_id.to_string(), TimeClient {
            client_id: client_id.to_string(),
            last_sync: SystemTime::now(),
            offset: AtomicI64::new(0),
            round_trip_delay: AtomicU64::new(0),
            dispersion: AtomicU64::new(0),
        });

        Ok(())
    }

    fn handle_query(&self, query: TimeQuery) -> Result<(), String> {
        // æ›´æ–°å®¢æˆ·ç«¯çŠ¶æ€
        let clients = self.clients.read().unwrap();

        let client = clients.get(&query.client_id)
            .ok_or_else(|| format!("å®¢æˆ·ç«¯ {} ä¸å­˜åœ¨", query.client_id))?;

        // è®¡ç®—å¾€è¿”å»¶è¿Ÿå’Œåç§»
        let rtt = (query.t4 - query.t1) - (query.t3 - query.t2);
        let offset = ((query.t2 - query.t1) + (query.t3 - query.t4)) / 2;

        // æ›´æ–°å®¢æˆ·ç«¯çŠ¶æ€
        client.round_trip_delay.store(rtt, Ordering::SeqCst);
        client.offset.store(offset as i64, Ordering::SeqCst);

        // è®¡ç®—åˆ†æ•£åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        let dispersion = rtt / 2;
        client.dispersion.store(dispersion, Ordering::SeqCst);

        Ok(())
    }

    fn get_current_time(&self) -> u64 {
        self.reference_clock.load(Ordering::SeqCst)
    }

    fn get_client_stats(&self, client_id: &str) -> Result<(i64, u64, u64), String> {
        let clients = self.clients.read().unwrap();

        let client = clients.get(client_id)
            .ok_or_else(|| format!("å®¢æˆ·ç«¯ {} ä¸å­˜åœ¨", client_id))?;

        Ok((
            client.offset.load(Ordering::SeqCst),
            client.round_trip_delay.load(Ordering::SeqCst),
            client.dispersion.load(Ordering::SeqCst),
        ))
    }
}

// åˆ†å¸ƒå¼ç‡é™åˆ¶å™¨ï¼ˆåŸºäºä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰
struct TokenBucketRateLimiter {


```rust
struct TokenBucketRateLimiter {
    storage: Box<dyn RateLimiterStorage>,
    lock_manager: Arc<RedLock>,
    rate: u32,             // ä»¤ç‰Œç”Ÿæˆé€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
    bucket_size: u32,      // æ¡¶å®¹é‡
    lock_timeout: Duration, // é”è¶…æ—¶æ—¶é—´
}

trait RateLimiterStorage: Send + Sync {
    fn get_token_count(&self, key: &str) -> Result<Option<(u32, u64)>, String>;
    fn update_token_count(&self, key: &str, count: u32, timestamp: u64) -> Result<(), String>;
}

impl TokenBucketRateLimiter {
    fn new(
        storage: Box<dyn RateLimiterStorage>,
        lock_manager: Arc<RedLock>,
        rate: u32,
        bucket_size: u32,
        lock_timeout: Duration,
    ) -> Self {
        TokenBucketRateLimiter {
            storage,
            lock_manager,
            rate,
            bucket_size,
            lock_timeout,
        }
    }

    fn try_acquire(&self, key: &str, tokens: u32) -> Result<bool, String> {
        // è·å–åˆ†å¸ƒå¼é”é˜²æ­¢å¹¶å‘é—®é¢˜
        let lock_resource = format!("rate_limiter:{}", key);
        let lock = match self.lock_manager.acquire_lock(&lock_resource)? {
            Some(lock) => lock,
            None => return Err("æ— æ³•è·å–é€Ÿç‡é™åˆ¶å™¨é”".to_string()),
        };

        // è·å–å½“å‰ä»¤ç‰Œæ•°
        let current_time = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();

        let (token_count, last_refill_time) = match self.storage.get_token_count(key)? {
            Some((count, time)) => (count, time),
            None => (self.bucket_size, current_time), // åˆå§‹çŠ¶æ€ï¼Œæ¡¶æ˜¯æ»¡çš„
        };

        // è®¡ç®—æ–°çš„ä»¤ç‰Œæ•°
        let time_elapsed = current_time.saturating_sub(last_refill_time);
        let new_tokens = (time_elapsed as f64 * self.rate as f64 / 1.0).floor() as u32;
        let current_tokens = (token_count + new_tokens).min(self.bucket_size);

        // å°è¯•æ¶ˆè´¹ä»¤ç‰Œ
        if current_tokens >= tokens {
            // æ›´æ–°ä»¤ç‰Œæ•°
            let new_count = current_tokens - tokens;
            self.storage.update_token_count(key, new_count, current_time)?;
            Ok(true)
        } else {
            // ä»¤ç‰Œä¸è¶³
            self.storage.update_token_count(key, current_tokens, current_time)?;
            Ok(false)
        }

        // é”ä¼šåœ¨ä½œç”¨åŸŸç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
    }

    fn get_token_count(&self, key: &str) -> Result<u32, String> {
        // è·å–å½“å‰ä»¤ç‰Œæ•°
        let current_time = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();

        match self.storage.get_token_count(key)? {
            Some((count, time)) => {
                // è®¡ç®—æ–°çš„ä»¤ç‰Œæ•°
                let time_elapsed = current_time.saturating_sub(time);
                let new_tokens = (time_elapsed as f64 * self.rate as f64 / 1.0).floor() as u32;
                Ok((count + new_tokens).min(self.bucket_size))
            },
            None => Ok(self.bucket_size), // åˆå§‹çŠ¶æ€ï¼Œæ¡¶æ˜¯æ»¡çš„
        }
    }
}

// Rediså®ç°çš„é€Ÿç‡é™åˆ¶å™¨å­˜å‚¨
struct RedisRateLimiterStorage {
    client: redis::Client,
    key_prefix: String,
    ttl: Duration, // é”®è¿‡æœŸæ—¶é—´
}

impl RedisRateLimiterStorage {
    fn new(redis_url: &str, key_prefix: &str, ttl: Duration) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;

        Ok(RedisRateLimiterStorage {
            client,
            key_prefix: key_prefix.to_string(),
            ttl,
        })
    }

    fn get_storage_key(&self, key: &str) -> String {
        format!("{}:{}", self.key_prefix, key)
    }
}

impl RateLimiterStorage for RedisRateLimiterStorage {
    fn get_token_count(&self, key: &str) -> Result<Option<(u32, u64)>, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // è·å–ä»¤ç‰Œæ•°å’Œæ—¶é—´æˆ³
        let result: Option<(String, String)> = redis::cmd("HMGET")
            .arg(&storage_key)
            .arg("count")
            .arg("timestamp")
            .query(&mut conn)
            .map_err(|err| format!("Redis HMGETæ“ä½œå¤±è´¥: {}", err))?;

        match result {
            Some((count_str, timestamp_str)) => {
                let count = count_str.parse::<u32>()
                    .map_err(|_| format!("æ— æ•ˆçš„ä»¤ç‰Œæ•°: {}", count_str))?;
                let timestamp = timestamp_str.parse::<u64>()
                    .map_err(|_| format!("æ— æ•ˆçš„æ—¶é—´æˆ³: {}", timestamp_str))?;

                Ok(Some((count, timestamp)))
            },
            None => Ok(None),
        }
    }

    fn update_token_count(&self, key: &str, count: u32, timestamp: u64) -> Result<(), String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // è®¾ç½®ä»¤ç‰Œæ•°å’Œæ—¶é—´æˆ³
        let _: () = redis::cmd("HMSET")
            .arg(&storage_key)
            .arg("count")
            .arg(count.to_string())
            .arg("timestamp")
            .arg(timestamp.to_string())
            .query(&mut conn)
            .map_err(|err| format!("Redis HMSETæ“ä½œå¤±è´¥: {}", err))?;

        // è®¾ç½®è¿‡æœŸæ—¶é—´
        let _: () = redis::cmd("EXPIRE")
            .arg(&storage_key)
            .arg(self.ttl.as_secs() as usize)
            .query(&mut conn)
            .map_err(|err| format!("Redis EXPIREæ“ä½œå¤±è´¥: {}", err))?;

        Ok(())
    }
}

// åˆ†å¸ƒå¼è®¡æ•°å™¨ï¼ˆHyperLogLogå®ç°ï¼‰
struct HLLCounter {
    storage: Box<dyn HLLStorage>,
}

trait HLLStorage: Send + Sync {
    fn add(&self, key: &str, value: &str) -> Result<bool, String>;
    fn count(&self, key: &str) -> Result<u64, String>;
    fn merge(&self, destination: &str, sources: &[String]) -> Result<(), String>;
}

impl HLLCounter {
    fn new(storage: Box<dyn HLLStorage>) -> Self {
        HLLCounter {
            storage,
        }
    }

    fn add(&self, counter_name: &str, value: &str) -> Result<bool, String> {
        self.storage.add(counter_name, value)
    }

    fn count(&self, counter_name: &str) -> Result<u64, String> {
        self.storage.count(counter_name)
    }

    fn merge(&self, destination: &str, sources: &[String]) -> Result<(), String> {
        self.storage.merge(destination, sources)
    }
}

// Rediså®ç°çš„HyperLogLogå­˜å‚¨
struct RedisHLLStorage {
    client: redis::Client,
    key_prefix: String,
}

impl RedisHLLStorage {
    fn new(redis_url: &str, key_prefix: &str) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;

        Ok(RedisHLLStorage {
            client,
            key_prefix: key_prefix.to_string(),
        })
    }

    fn get_storage_key(&self, key: &str) -> String {
        format!("{}:{}", self.key_prefix, key)
    }
}

impl HLLStorage for RedisHLLStorage {
    fn add(&self, key: &str, value: &str) -> Result<bool, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // ä½¿ç”¨PFADDå‘½ä»¤æ·»åŠ å…ƒç´ 
        let result: i32 = redis::cmd("PFADD")
            .arg(&storage_key)
            .arg(value)
            .query(&mut conn)
            .map_err(|err| format!("Redis PFADDæ“ä½œå¤±è´¥: {}", err))?;

        Ok(result == 1)
    }

    fn count(&self, key: &str) -> Result<u64, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // ä½¿ç”¨PFCOUNTå‘½ä»¤è·å–è®¡æ•°
        let count: u64 = redis::cmd("PFCOUNT")
            .arg(&storage_key)
            .query(&mut conn)
            .map_err(|err| format!("Redis PFCOUNTæ“ä½œå¤±è´¥: {}", err))?;

        Ok(count)
    }

    fn merge(&self, destination: &str, sources: &[String]) -> Result<(), String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let dest_key = self.get_storage_key(destination);

        // è½¬æ¢æºé”®
        let source_keys: Vec<String> = sources.iter()
            .map(|s| self.get_storage_key(s))
            .collect();

        // ä½¿ç”¨PFMERGEå‘½ä»¤åˆå¹¶HyperLogLog
        let _: () = redis::cmd("PFMERGE")
            .arg(&dest_key)
            .arg(source_keys)
            .query(&mut conn)
            .map_err(|err| format!("Redis PFMERGEæ“ä½œå¤±è´¥: {}", err))?;

        Ok(())
    }
}

// å¸ƒéš†è¿‡æ»¤å™¨
struct BloomFilter {
    storage: Box<dyn BloomFilterStorage>,
    hash_functions: Vec<Box<dyn Fn(&str) -> u64 + Send + Sync>>,
    bits_per_item: f64,
    size: usize,
    items_count: usize,
}

trait BloomFilterStorage: Send + Sync {
    fn set_bit(&self, key: &str, bit: usize) -> Result<bool, String>;
    fn get_bit(&self, key: &str, bit: usize) -> Result<bool, String>;
    fn get_many_bits(&self, key: &str, bits: &[usize]) -> Result<Vec<bool>, String>;
}

impl BloomFilter {
    fn new(
        storage: Box<dyn BloomFilterStorage>,
        expected_items: usize,
        false_positive_rate: f64,
        hash_functions: Vec<Box<dyn Fn(&str) -> u64 + Send + Sync>>,
    ) -> Self {
        // è®¡ç®—å¸ƒéš†è¿‡æ»¤å™¨å‚æ•°
        let bits_per_item = -1.0 * (false_positive_rate.ln() / (2.0_f64.ln().powi(2)));
        let size = (bits_per_item * expected_items as f64).ceil() as usize;

        BloomFilter {
            storage,
            hash_functions,
            bits_per_item,
            size,
            items_count: 0,
        }
    }

    fn add(&mut self, item: &str, key: &str) -> Result<(), String> {
        // è®¡ç®—æ‰€æœ‰å“ˆå¸Œå€¼
        let bit_positions = self.calculate_bit_positions(item);

        // è®¾ç½®æ‰€æœ‰ä½
        for bit in &bit_positions {
            self.storage.set_bit(key, *bit)?;
        }

        self.items_count += 1;

        Ok(())
    }

    fn contains(&self, item: &str, key: &str) -> Result<bool, String> {
        // è®¡ç®—æ‰€æœ‰å“ˆå¸Œå€¼
        let bit_positions = self.calculate_bit_positions(item);

        // æ£€æŸ¥æ‰€æœ‰ä½
        let bits = self.storage.get_many_bits(key, &bit_positions)?;

        // å¦‚æœæœ‰ä»»ä½•ä¸€ä½ä¸º0ï¼Œåˆ™é¡¹ä¸åœ¨é›†åˆä¸­
        Ok(bits.iter().all(|&bit| bit))
    }

    fn calculate_bit_positions(&self, item: &str) -> Vec<usize> {
        self.hash_functions.iter()
            .map(|hash_fn| (hash_fn(item) % (self.size as u64)) as usize)
            .collect()
    }

    fn estimate_items_count(&self) -> usize {
        self.items_count
    }

    fn estimate_current_false_positive_rate(&self) -> f64 {
        let m = self.size as f64;
        let k = self.hash_functions.len() as f64;
        let n = self.items_count as f64;

        // è®¡ç®—å‡é˜³æ€§ç‡ï¼š(1 - e^(-k*n/m))^k
        (1.0 - (-k * n / m).exp()).powf(k)
    }
}

// Rediså®ç°çš„å¸ƒéš†è¿‡æ»¤å™¨å­˜å‚¨
struct RedisBloomFilterStorage {
    client: redis::Client,
    key_prefix: String,
}

impl RedisBloomFilterStorage {
    fn new(redis_url: &str, key_prefix: &str) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;

        Ok(RedisBloomFilterStorage {
            client,
            key_prefix: key_prefix.to_string(),
        })
    }

    fn get_storage_key(&self, key: &str) -> String {
        format!("{}:{}", self.key_prefix, key)
    }
}

impl BloomFilterStorage for RedisBloomFilterStorage {
    fn set_bit(&self, key: &str, bit: usize) -> Result<bool, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // ä½¿ç”¨SETBITå‘½ä»¤è®¾ç½®ä½
        let result: i32 = redis::cmd("SETBIT")
            .arg(&storage_key)
            .arg(bit)
            .arg(1)
            .query(&mut conn)
            .map_err(|err| format!("Redis SETBITæ“ä½œå¤±è´¥: {}", err))?;

        Ok(result == 1)
    }

    fn get_bit(&self, key: &str, bit: usize) -> Result<bool, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // ä½¿ç”¨GETBITå‘½ä»¤è·å–ä½
        let result: i32 = redis::cmd("GETBIT")
            .arg(&storage_key)
            .arg(bit)
            .query(&mut conn)
            .map_err(|err| format!("Redis GETBITæ“ä½œå¤±è´¥: {}", err))?;

        Ok(result == 1)
    }

    fn get_many_bits(&self, key: &str, bits: &[usize]) -> Result<Vec<bool>, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        let storage_key = self.get_storage_key(key);

        // ä½¿ç”¨ç®¡é“ä¸€æ¬¡æ€§è·å–å¤šä¸ªä½
        let mut pipe = redis::pipe();
        for &bit in bits {
            pipe.cmd("GETBIT").arg(&storage_key).arg(bit);
        }

        let results: Vec<i32> = pipe.query(&mut conn)
            .map_err(|err| format!("Redis pipelineæ“ä½œå¤±è´¥: {}", err))?;

        Ok(results.iter().map(|&r| r == 1).collect())
    }
}

// åˆ†å¸ƒå¼å»¶è¿Ÿä»»åŠ¡é˜Ÿåˆ—
struct DelayedTaskQueue {
    storage: Box<dyn DelayedTaskStorage>,
    worker_pool: ThreadPool,
    task_handlers: Arc<RwLock<HashMap<String, Box<dyn Fn(&Task) -> Result<(), String> + Send + Sync>>>>,
    polling_interval: Duration,
}

struct Task {
    id: String,
    task_type: String,
    payload: Vec<u8>,
    created_at: SystemTime,
    execute_at: SystemTime,
    retries: u32,
    max_retries: u32,
    retry_delay: Duration,
    last_error: Option<String>,
}

trait DelayedTaskStorage: Send + Sync {
    fn add_task(&self, task: &Task) -> Result<(), String>;
    fn get_ready_tasks(&self, limit: usize) -> Result<Vec<Task>, String>;
    fn mark_task_complete(&self, task_id: &str) -> Result<(), String>;
    fn mark_task_failed(&self, task_id: &str, error: &str) -> Result<(), String>;
    fn reschedule_task(&self, task_id: &str, execute_at: SystemTime) -> Result<(), String>;
}

impl DelayedTaskQueue {
    fn new(
        storage: Box<dyn DelayedTaskStorage>,
        num_workers: usize,
        polling_interval: Duration,
    ) -> Self {
        DelayedTaskQueue {
            storage,
            worker_pool: ThreadPool::new(num_workers),
            task_handlers: Arc::new(RwLock::new(HashMap::new())),
            polling_interval,
        }
    }

    fn register_handler<F>(&self, task_type: &str, handler: F) -> Result<(), String>
    where
        F: Fn(&Task) -> Result<(), String> + Send + Sync + 'static,
    {
        let mut handlers = self.task_handlers.write().unwrap();

        if handlers.contains_key(task_type) {
            return Err(format!("å¤„ç†å™¨å·²å­˜åœ¨: {}", task_type));
        }

        handlers.insert(task_type.to_string(), Box::new(handler));

        Ok(())
    }

    fn schedule_task(&self, task_type: &str, payload: &[u8], execute_at: SystemTime, max_retries: u32, retry_delay: Duration) -> Result<String, String> {
        // éªŒè¯å¤„ç†å™¨æ˜¯å¦å­˜åœ¨
        {
            let handlers = self.task_handlers.read().unwrap();
            if !handlers.contains_key(task_type) {
                return Err(format!("æœªæ³¨å†Œçš„ä»»åŠ¡ç±»å‹: {}", task_type));
            }
        }

        // åˆ›å»ºä»»åŠ¡
        let task_id = uuid::Uuid::new_v4().to_string();
        let task = Task {
            id: task_id.clone(),
            task_type: task_type.to_string(),
            payload: payload.to_vec(),
            created_at: SystemTime::now(),
            execute_at,
            retries: 0,
            max_retries,
            retry_delay,
            last_error: None,
        };

        // æ·»åŠ åˆ°å­˜å‚¨
        self.storage.add_task(&task)?;

        Ok(task_id)
    }

    fn start(&self) {
        // å¯åŠ¨ä»»åŠ¡è½®è¯¢çº¿ç¨‹
        let storage = Arc::new(self.storage.clone());
        let task_handlers = self.task_handlers.clone();
        let worker_pool = self.worker_pool.clone();
        let polling_interval = self.polling_interval;

        std::thread::spawn(move || {
            loop {
                // è·å–å‡†å¤‡å¥½çš„ä»»åŠ¡
                match storage.get_ready_tasks(10) {
                    Ok(tasks) => {
                        for task in tasks {
                            // è·å–å¤„ç†å™¨
                            let handlers = task_handlers.read().unwrap();
                            if let Some(handler) = handlers.get(&task.task_type) {
                                // å…‹éš†å¤„ç†å™¨å’Œå­˜å‚¨ä»¥åœ¨çº¿ç¨‹ä¸­ä½¿ç”¨
                                let handler_clone = handler.clone();
                                let storage_clone = storage.clone();
                                let task_clone = task.clone();

                                // æäº¤ä»»åŠ¡åˆ°å·¥ä½œæ± 
                                worker_pool.execute(move || {
                                    let task_id = task_clone.id.clone();

                                    // æ‰§è¡Œä»»åŠ¡
                                    match handler_clone(&task_clone) {
                                        Ok(_) => {
                                            // æ ‡è®°ä»»åŠ¡å®Œæˆ
                                            if let Err(err) = storage_clone.mark_task_complete(&task_id) {
                                                println!("æ ‡è®°ä»»åŠ¡å®Œæˆå¤±è´¥: {}", err);
                                            }
                                        },
                                        Err(err) => {
                                            println!("ä»»åŠ¡ {} æ‰§è¡Œå¤±è´¥: {}", task_id, err);

                                            // å¤„ç†é‡è¯•
                                            if task_clone.retries < task_clone.max_retries {
                                                // è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
                                                let retry_delay = task_clone.retry_delay * (2_u32.pow(task_clone.retries)) as u32;
                                                let next_execute_at = SystemTime::now() + retry_delay;

                                                // é‡æ–°å®‰æ’ä»»åŠ¡
                                                if let Err(reschedule_err) = storage_clone.reschedule_task(&task_id, next_execute_at) {
                                                    println!("é‡æ–°å®‰æ’ä»»åŠ¡å¤±è´¥: {}", reschedule_err);
                                                }
                                            } else {
                                                // ä»»åŠ¡å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                                                if let Err(mark_err) = storage_clone.mark_task_failed(&task_id, &err) {
                                                    println!("æ ‡è®°ä»»åŠ¡å¤±è´¥å¤±è´¥: {}", mark_err);
                                                }
                                            }
                                        }
                                    }
                                });
                            } else {
                                println!("æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹çš„å¤„ç†å™¨: {}", task.task_type);
                            }
                        }
                    },
                    Err(err) => {
                        println!("è·å–å‡†å¤‡å¥½çš„ä»»åŠ¡å¤±è´¥: {}", err);
                    }
                }

                // ç­‰å¾…ä¸‹ä¸€ä¸ªè½®è¯¢é—´éš”
                std::thread::sleep(polling_interval);
            }
        });
    }
}

// ä»¥ä¸Šä»£ç å®ç°äº†ä¸€ç³»åˆ—åˆ†å¸ƒå¼ç³»ç»Ÿå¸¸ç”¨ç»„ä»¶ï¼š
// 1. åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒå™¨ï¼ˆäºŒé˜¶æ®µæäº¤ï¼‰
// 2. åˆ†å¸ƒå¼é”ï¼ˆRedlockç®—æ³•ï¼‰
// 3. æ—¶é—´åŒæ­¥æœåŠ¡ï¼ˆç±»NTPï¼‰
// 4. åˆ†å¸ƒå¼é€Ÿç‡é™åˆ¶å™¨ï¼ˆä»¤ç‰Œæ¡¶ï¼‰
// 5. åˆ†å¸ƒå¼è®¡æ•°å™¨ï¼ˆHyperLogLogï¼‰
// 6. å¸ƒéš†è¿‡æ»¤å™¨
// 7. åˆ†å¸ƒå¼å»¶è¿Ÿä»»åŠ¡é˜Ÿåˆ—

// è¿™äº›ç»„ä»¶å…±åŒæ„æˆäº†åˆ†å¸ƒå¼ç³»ç»Ÿçš„åŸºç¡€è®¾æ–½å±‚ï¼Œå¯ä»¥å¸®åŠ©æ„å»ºé«˜å¯ç”¨ã€é«˜æ€§èƒ½çš„åˆ†å¸ƒå¼åº”ç”¨ã€‚
```

### 1.3 ç»¼åˆåº”ç”¨03-Rediså®ç°çš„å»¶è¿Ÿä»»åŠ¡å­˜å‚¨

```rust
// Rediså®ç°çš„å»¶è¿Ÿä»»åŠ¡å­˜å‚¨
struct RedisDelayedTaskStorage {
    client: redis::Client,
    tasks_key: String,
    scheduled_set_key: String,
}

impl RedisDelayedTaskStorage {
    fn new(redis_url: &str, queue_name: &str) -> Result<Self, redis::RedisError> {
        let client = redis::Client::open(redis_url)?;

        Ok(RedisDelayedTaskStorage {
            client,
            tasks_key: format!("delayed_tasks:{}:tasks", queue_name),
            scheduled_set_key: format!("delayed_tasks:{}:schedule", queue_name),
        })
    }

    fn task_to_json(&self, task: &Task) -> Result<String, String> {
        let json = serde_json::json!({
            "id": task.id,
            "task_type": task.task_type,
            "payload": base64::encode(&task.payload),
            "created_at": task.created_at.duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs(),
            "execute_at": task.execute_at.duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs(),
            "retries": task.retries,
            "max_retries": task.max_retries,
            "retry_delay": task.retry_delay.as_secs(),
            "last_error": task.last_error,
        });

        serde_json::to_string(&json)
            .map_err(|err| format!("åºåˆ—åŒ–ä»»åŠ¡å¤±è´¥: {}", err))
    }

    fn json_to_task(&self, json: &str) -> Result<Task, String> {
        let value: serde_json::Value = serde_json::from_str(json)
            .map_err(|err| format!("è§£æä»»åŠ¡JSONå¤±è´¥: {}", err))?;

        let payload = base64::decode(value["payload"].as_str().unwrap_or(""))
            .map_err(|err| format!("è§£ç ä»»åŠ¡è½½è·å¤±è´¥: {}", err))?;

        let created_at_secs = value["created_at"].as_u64().unwrap_or(0);
        let execute_at_secs = value["execute_at"].as_u64().unwrap_or(0);
        let retry_delay_secs = value["retry_delay"].as_u64().unwrap_or(0);

        Ok(Task {
            id: value["id"].as_str().unwrap_or("").to_string(),
            task_type: value["task_type"].as_str().unwrap_or("").to_string(),
            payload,
            created_at: SystemTime::UNIX_EPOCH + Duration::from_secs(created_at_secs),
            execute_at: SystemTime::UNIX_EPOCH + Duration::from_secs(execute_at_secs),
            retries: value["retries"].as_u64().unwrap_or(0) as u32,
            max_retries: value["max_retries"].as_u64().unwrap_or(0) as u32,
            retry_delay: Duration::from_secs(retry_delay_secs),
            last_error: value["last_error"].as_str().map(|s| s.to_string()),
        })
    }
}

impl DelayedTaskStorage for RedisDelayedTaskStorage {
    fn add_task(&self, task: &Task) -> Result<(), String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        // åºåˆ—åŒ–ä»»åŠ¡
        let task_json = self.task_to_json(task)?;

        // è®¡ç®—æ‰§è¡Œæ—¶é—´çš„åˆ†æ•°
        let score = task.execute_at
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap_or(Duration::from_secs(0))
            .as_secs() as f64;

        // ä½¿ç”¨ç®¡é“æ‰§è¡Œäº‹åŠ¡
        let mut pipe = redis::pipe();
        pipe.atomic();

        // å­˜å‚¨ä»»åŠ¡å¯¹è±¡
        pipe.cmd("HSET")
            .arg(&self.tasks_key)
            .arg(&task.id)
            .arg(&task_json);

        // å°†ä»»åŠ¡æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆæœ‰åºé›†åˆï¼‰
        pipe.cmd("ZADD")
            .arg(&self.scheduled_set_key)
            .arg(score)
            .arg(&task.id);

        let _: () = pipe.query(&mut conn)
            .map_err(|err| format!("Redisç®¡é“æ“ä½œå¤±è´¥: {}", err))?;

        Ok(())
    }

    fn get_ready_tasks(&self, limit: usize) -> Result<Vec<Task>, String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        // è®¡ç®—å½“å‰æ—¶é—´ä½œä¸ºåˆ†æ•°
        let now = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap_or(Duration::from_secs(0))
            .as_secs() as f64;

        // è·å–å‡†å¤‡å¥½çš„ä»»åŠ¡IDï¼ˆåˆ†æ•°å°äºç­‰äºå½“å‰æ—¶é—´ï¼‰
        let task_ids: Vec<String> = redis::cmd("ZRANGEBYSCORE")
            .arg(&self.scheduled_set_key)
            .arg(0)
            .arg(now)
            .arg("LIMIT")
            .arg(0)
            .arg(limit)
            .query(&mut conn)
            .map_err(|err| format!("Redis ZRANGEBYSCOREæ“ä½œå¤±è´¥: {}", err))?;

        if task_ids.is_empty() {
            return Ok(Vec::new());
        }

        // ä»é›†åˆä¸­ç§»é™¤è¿™äº›ä»»åŠ¡
        let _: () = redis::cmd("ZREM")
            .arg(&self.scheduled_set_key)
            .arg(&task_ids)
            .query(&mut conn)
            .map_err(|err| format!("Redis ZREMæ“ä½œå¤±è´¥: {}", err))?;

        // è·å–ä»»åŠ¡JSON
        let mut pipe = redis::pipe();
        for task_id in &task_ids {
            pipe.cmd("HGET").arg(&self.tasks_key).arg(task_id);
        }

        let task_jsons: Vec<Option<String>> = pipe.query(&mut conn)
            .map_err(|err| format!("Redisç®¡é“æ“ä½œå¤±è´¥: {}", err))?;

        // è§£æä»»åŠ¡
        let mut tasks = Vec::new();
        for json_opt in task_jsons {
            if let Some(json) = json_opt {
                match self.json_to_task(&json) {
                    Ok(task) => tasks.push(task),
                    Err(err) => println!("è§£æä»»åŠ¡å¤±è´¥: {}", err),
                }
            }
        }

        Ok(tasks)
    }

    fn mark_task_complete(&self, task_id: &str) -> Result<(), String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        // ä»å“ˆå¸Œè¡¨ä¸­åˆ é™¤ä»»åŠ¡
        let _: () = redis::cmd("HDEL")
            .arg(&self.tasks_key)
            .arg(task_id)
            .query(&mut conn)
            .map_err(|err| format!("Redis HDELæ“ä½œå¤±è´¥: {}", err))?;

        Ok(())
    }

    fn mark_task_failed(&self, task_id: &str, error: &str) -> Result<(), String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        // è·å–ä»»åŠ¡JSON
        let task_json: Option<String> = redis::cmd("HGET")
            .arg(&self.tasks_key)
            .arg(task_id)
            .query(&mut conn)
            .map_err(|err| format!("Redis HGETæ“ä½œå¤±è´¥: {}", err))?;

        if let Some(json) = task_json {
            // è§£æä»»åŠ¡
            let mut task = self.json_to_task(&json)?;

            // æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.last_error = Some(error.to_string());
            task.retries = task.max_retries; // ç¡®ä¿ä¸ä¼šé‡è¯•

            // åºåˆ—åŒ–ä»»åŠ¡
            let updated_json = self.task_to_json(&task)?;

            // æ›´æ–°ä»»åŠ¡
            let _: () = redis::cmd("HSET")
                .arg(&self.tasks_key)
                .arg(task_id)
                .arg(&updated_json)
                .query(&mut conn)
                .map_err(|err| format!("Redis HSETæ“ä½œå¤±è´¥: {}", err))?;
        }

        Ok(())
    }

    fn reschedule_task(&self, task_id: &str, execute_at: SystemTime) -> Result<(), String> {
        let mut conn = match self.client.get_connection() {
            Ok(conn) => conn,
            Err(err) => {
                return Err(format!("æ— æ³•è¿æ¥åˆ°Redis: {}", err));
            }
        };

        // è·å–ä»»åŠ¡JSON
        let task_json: Option<String> = redis::cmd("HGET")
            .arg(&self.tasks_key)
            .arg(task_id)
            .query(&mut conn)
            .map_err(|err| format!("Redis HGETæ“ä½œå¤±è´¥: {}", err))?;

        if let Some(json) = task_json {
            // è§£æä»»åŠ¡
            let mut task = self.json_to_task(&json)?;

            // æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.execute_at = execute_at;
            task.retries += 1;

            // åºåˆ—åŒ–ä»»åŠ¡
            let updated_json = self.task_to_json(&task)?;

            // è®¡ç®—æ‰§è¡Œæ—¶é—´çš„åˆ†æ•°
            let score = task.execute_at
                .duration_since(SystemTime::UNIX_EPOCH)
                .unwrap_or(Duration::from_secs(0))
                .as_secs() as f64;

            // ä½¿ç”¨ç®¡é“æ‰§è¡Œäº‹åŠ¡
            let mut pipe = redis::pipe();
            pipe.atomic();

            // æ›´æ–°ä»»åŠ¡
            pipe.cmd("HSET")
                .arg(&self.tasks_key)
                .arg(task_id)
                .arg(&updated_json);

            // å°†ä»»åŠ¡æ·»åŠ åˆ°ä¼˜å…ˆçº§é˜Ÿåˆ—
            pipe.cmd("ZADD")
                .arg(&self.scheduled_set_key)
                .arg(score)
                .arg(task_id);

            let _: () = pipe.query(&mut conn)
                .map_err(|err| format!("Redisç®¡é“æ“ä½œå¤±è´¥: {}", err))?;
        } else {
            return Err(format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id));
        }

        Ok(())
    }
}

// ç‚¹å¯¹ç‚¹ç½‘ç»œå±‚
struct P2PNetwork {
    node_id: String,
    listen_addr: SocketAddr,
    peers: RwLock<HashMap<String, PeerInfo>>,
    message_handlers: RwLock<HashMap<String, Box<dyn Fn(&NetworkMessage) -> Result<Vec<u8>, String> + Send + Sync>>>,
}

struct PeerInfo {
    id: String,
    addr: SocketAddr,
    last_seen: Instant,
    status: PeerStatus,
}

enum PeerStatus {
    Connected,
    Disconnected,
    Failed,
}

struct NetworkMessage {
    id: String,
    sender: String,
    receiver: String,
    message_type: String,
    content: Vec<u8>,
    created_at: SystemTime,
    ttl: u32,
    hops: u32,
}

impl P2PNetwork {
    fn new(node_id: &str, listen_addr: SocketAddr) -> Self {
        P2PNetwork {
            node_id: node_id.to_string(),
            listen_addr,
            peers: RwLock::new(HashMap::new()),
            message_handlers: RwLock::new(HashMap::new()),
        }
    }

    fn start(&self) -> Result<(), String> {
        // å¯åŠ¨ç›‘å¬çº¿ç¨‹
        let node_id = self.node_id.clone();
        let listen_addr = self.listen_addr;
        let peers = self.peers.clone();
        let message_handlers = self.message_handlers.clone();

        std::thread::spawn(move || {
            // åˆ›å»ºç›‘å¬å¥—æ¥å­—
            let listener = match TcpListener::bind(listen_addr) {
                Ok(listener) => listener,
                Err(err) => {
                    println!("ç»‘å®šåœ°å€å¤±è´¥: {}", err);
                    return;
                }
            };

            println!("èŠ‚ç‚¹ {} æ­£åœ¨ç›‘å¬ {}", node_id, listen_addr);

            for stream in listener.incoming() {
                match stream {
                    Ok(stream) => {
                        // å¤„ç†æ–°è¿æ¥
                        let node_id = node_id.clone();
                        let peers = peers.clone();
                        let message_handlers = message_handlers.clone();

                        std::thread::spawn(move || {
                            if let Err(err) = handle_connection(stream, &node_id, &peers, &message_handlers) {
                                println!("å¤„ç†è¿æ¥å¤±è´¥: {}", err);
                            }
                        });
                    },
                    Err(err) => {
                        println!("æ¥å—è¿æ¥å¤±è´¥: {}", err);
                    }
                }
            }
        });

        Ok(())
    }

    fn connect_to_peer(&self, peer_id: &str, addr: SocketAddr) -> Result<(), String> {
        // æ£€æŸ¥æ˜¯å¦å·²è¿æ¥
        {
            let peers = self.peers.read().unwrap();
            if let Some(peer) = peers.get(peer_id) {
                if matches!(peer.status, PeerStatus::Connected) {
                    return Ok(()); // å·²è¿æ¥
                }
            }
        }

        // è¿æ¥åˆ°å¯¹ç­‰èŠ‚ç‚¹
        let stream = TcpStream::connect(addr)
            .map_err(|err| format!("è¿æ¥åˆ°å¯¹ç­‰èŠ‚ç‚¹å¤±è´¥: {}", err))?;

        // æ³¨å†Œå¯¹ç­‰èŠ‚ç‚¹
        let peer_info = PeerInfo {
            id: peer_id.to_string(),
            addr,
            last_seen: Instant::now(),
            status: PeerStatus::Connected,
        };

        let mut peers = self.peers.write().unwrap();
        peers.insert(peer_id.to_string(), peer_info);

        // å¯åŠ¨å‘é€çº¿ç¨‹
        let node_id = self.node_id.clone();
        let peer_id = peer_id.to_string();

        std::thread::spawn(move || {
            if let Err(err) = handle_outbound_connection(stream, &node_id, &peer_id) {
                println!("å¤„ç†å‡ºç«™è¿æ¥å¤±è´¥: {}", err);
            }
        });

        Ok(())
    }

    fn register_message_handler<F>(&self, message_type: &str, handler: F) -> Result<(), String>
    where
        F: Fn(&NetworkMessage) -> Result<Vec<u8>, String> + Send + Sync + 'static,
    {
        let mut handlers = self.message_handlers.write().unwrap();

        if handlers.contains_key(message_type) {
            return Err(format!("æ¶ˆæ¯å¤„ç†å™¨å·²å­˜åœ¨: {}", message_type));
        }

        handlers.insert(message_type.to_string(), Box::new(handler));

        Ok(())
    }

    fn send_message(&self, receiver: &str, message_type: &str, content: &[u8]) -> Result<String, String> {
        // åˆ›å»ºæ¶ˆæ¯
        let message_id = uuid::Uuid::new_v4().to_string();
        let message = NetworkMessage {
            id: message_id.clone(),
            sender: self.node_id.clone(),
            receiver: receiver.to_string(),
            message_type: message_type.to_string(),
            content: content.to_vec(),
            created_at: SystemTime::now(),
            ttl: 10, // é»˜è®¤TTL
            hops: 0,
        };

        // åºåˆ—åŒ–æ¶ˆæ¯
        let message_data = serde_json::to_vec(&message)
            .map_err(|err| format!("åºåˆ—åŒ–æ¶ˆæ¯å¤±è´¥: {}", err))?;

        // æŸ¥æ‰¾æ¥æ”¶èŠ‚ç‚¹
        let peers = self.peers.read().unwrap();
        if let Some(peer) = peers.get(receiver) {
            if matches!(peer.status, PeerStatus::Connected) {
                // è¿æ¥åˆ°å¯¹ç­‰èŠ‚ç‚¹å¹¶å‘é€æ¶ˆæ¯
                let mut stream = TcpStream::connect(peer.addr)
                    .map_err(|err| format!("è¿æ¥åˆ°å¯¹ç­‰èŠ‚ç‚¹å¤±è´¥: {}", err))?;

                // å‘é€æ¶ˆæ¯é•¿åº¦
                let length = message_data.len() as u32;
                let length_bytes = length.to_be_bytes();
                stream.write_all(&length_bytes)
                    .map_err(|err| format!("å‘é€æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err))?;

                // å‘é€æ¶ˆæ¯æ•°æ®
                stream.write_all(&message_data)
                    .map_err(|err| format!("å‘é€æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err))?;

                return Ok(message_id);
            }
        }

        // å¦‚æœç›´æ¥è¿æ¥ä¸å¯ç”¨ï¼Œå°è¯•æ´ªæ³›
        self.flood_message(&message)?;

        Ok(message_id)
    }

    fn flood_message(&self, message: &NetworkMessage) -> Result<(), String> {
        // æ£€æŸ¥TTL
        if message.ttl == 0 {
            return Ok(());
        }

        // åˆ›å»ºæ–°æ¶ˆæ¯ï¼ˆé€’å‡TTLå¹¶å¢åŠ è·³æ•°ï¼‰
        let mut new_message = message.clone();
        new_message.ttl -= 1;
        new_message.hops += 1;

        // åºåˆ—åŒ–æ¶ˆæ¯
        let message_data = serde_json::to_vec(&new_message)
            .map_err(|err| format!("åºåˆ—åŒ–æ¶ˆæ¯å¤±è´¥: {}", err))?;

        // å‘æ‰€æœ‰è¿æ¥çš„å¯¹ç­‰èŠ‚ç‚¹å¹¿æ’­æ¶ˆæ¯
        let peers = self.peers.read().unwrap();
        for (_, peer) in peers.iter() {
            if matches!(peer.status, PeerStatus::Connected) && peer.id != message.sender {
                // å°è¯•è¿æ¥å¹¶å‘é€æ¶ˆæ¯
                if let Ok(mut stream) = TcpStream::connect(peer.addr) {
                    // å‘é€æ¶ˆæ¯é•¿åº¦
                    let length = message_data.len() as u32;
                    let length_bytes = length.to_be_bytes();
                    if let Err(err) = stream.write_all(&length_bytes) {
                        println!("å‘é€æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err);
                        continue;
                    }

                    // å‘é€æ¶ˆæ¯æ•°æ®
                    if let Err(err) = stream.write_all(&message_data) {
                        println!("å‘é€æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err);
                        continue;
                    }
                }
            }
        }

        Ok(())
    }

    fn discover_peers(&self, bootstrap_nodes: &[SocketAddr]) -> Result<usize, String> {
        let mut discovered = 0;

        // è¿æ¥åˆ°å¼•å¯¼èŠ‚ç‚¹å¹¶è¯·æ±‚å¯¹ç­‰èŠ‚ç‚¹åˆ—è¡¨
        for &addr in bootstrap_nodes {
            match TcpStream::connect(addr) {
                Ok(mut stream) => {
                    // åˆ›å»ºå‘ç°æ¶ˆæ¯
                    let message = NetworkMessage {
                        id: uuid::Uuid::new_v4().to_string(),
                        sender: self.node_id.clone(),
                        receiver: "any".to_string(),
                        message_type: "DISCOVER".to_string(),
                        content: Vec::new(),
                        created_at: SystemTime::now(),
                        ttl: 1,
                        hops: 0,
                    };

                    // åºåˆ—åŒ–æ¶ˆæ¯
                    let message_data = match serde_json::to_vec(&message) {
                        Ok(data) => data,
                        Err(err) => {
                            println!("åºåˆ—åŒ–å‘ç°æ¶ˆæ¯å¤±è´¥: {}", err);
                            continue;
                        }
                    };

                    // å‘é€æ¶ˆæ¯é•¿åº¦
                    let length = message_data.len() as u32;
                    let length_bytes = length.to_be_bytes();
                    if let Err(err) = stream.write_all(&length_bytes) {
                        println!("å‘é€æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err);
                        continue;
                    }

                    // å‘é€æ¶ˆæ¯æ•°æ®
                    if let Err(err) = stream.write_all(&message_data) {
                        println!("å‘é€æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err);
                        continue;
                    }

                    // è¯»å–å“åº”é•¿åº¦
                    let mut length_bytes = [0u8; 4];
                    if let Err(err) = stream.read_exact(&mut length_bytes) {
                        println!("è¯»å–å“åº”é•¿åº¦å¤±è´¥: {}", err);
                        continue;
                    }

                    let length = u32::from_be_bytes(length_bytes) as usize;

                    // è¯»å–å“åº”æ•°æ®
                    let mut response_data = vec![0u8; length];
                    if let Err(err) = stream.read_exact(&mut response_data) {
                        println!("è¯»å–å“åº”æ•°æ®å¤±è´¥: {}", err);
                        continue;
                    }

                    // è§£æå“åº”
                    let response: NetworkMessage = match serde_json::from_slice(&response_data) {
                        Ok(resp) => resp,
                        Err(err) => {
                            println!("è§£æå“åº”å¤±è´¥: {}", err);
                            continue;
                        }
                    };

                    // å¤„ç†å¯¹ç­‰èŠ‚ç‚¹åˆ—è¡¨
                    if response.message_type == "DISCOVER_RESPONSE" {
                        let peer_list: Vec<(String, SocketAddr)> = match serde_json::from_slice(&response.content) {
                            Ok(list) => list,
                            Err(err) => {
                                println!("è§£æå¯¹ç­‰èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: {}", err);
                                continue;
                            }
                        };

                        // è¿æ¥åˆ°æ–°å‘ç°çš„å¯¹ç­‰èŠ‚ç‚¹
                        for (peer_id, peer_addr) in peer_list {
                            if peer_id != self.node_id {
                                if let Err(err) = self.connect_to_peer(&peer_id, peer_addr) {
                                    println!("è¿æ¥åˆ°å¯¹ç­‰èŠ‚ç‚¹å¤±è´¥: {}", err);
                                } else {
                                    discovered += 1;
                                }
                            }
                        }
                    }
                },
                Err(err) => {
                    println!("è¿æ¥åˆ°å¼•å¯¼èŠ‚ç‚¹ {} å¤±è´¥: {}", addr, err);
                }
            }
        }

        Ok(discovered)
    }
}

fn handle_connection(
    stream: TcpStream,
    node_id: &str,
    peers: &RwLock<HashMap<String, PeerInfo>>,
    message_handlers: &RwLock<HashMap<String, Box<dyn Fn(&NetworkMessage) -> Result<Vec<u8>, String> + Send + Sync>>>,
) -> Result<(), String> {
    // è®¾ç½®éé˜»å¡æ¨¡å¼
    stream.set_nonblocking(true)
        .map_err(|err| format!("è®¾ç½®éé˜»å¡æ¨¡å¼å¤±è´¥: {}", err))?;

    let mut reader = BufReader::new(stream.try_clone().unwrap());
    let mut writer = BufWriter::new(stream);

    loop {
        // è¯»å–æ¶ˆæ¯é•¿åº¦
        let mut length_bytes = [0u8; 4];
        match reader.read_exact(&mut length_bytes) {
            Ok(_) => {
                let length = u32::from_be_bytes(length_bytes) as usize;

                // è¯»å–æ¶ˆæ¯æ•°æ®
                let mut message_data = vec![0u8; length];
                if let Err(err) = reader.read_exact(&mut message_data) {
                    return Err(format!("è¯»å–æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err));
                }

                // è§£ææ¶ˆæ¯
                let message: NetworkMessage = match serde_json::from_slice(&message_data) {
                    Ok(msg) => msg,
                    Err(err) => {
                        return Err(format!("è§£ææ¶ˆæ¯å¤±è´¥: {}", err));
                    }
                };

                // æ›´æ–°å¯¹ç­‰èŠ‚ç‚¹ä¿¡æ¯
                {
                    let mut peers_guard = peers.write().unwrap();
                    peers_guard.insert(message.sender.clone(), PeerInfo {
                        id: message.sender.clone(),
                        addr: writer.get_ref().peer_addr().unwrap(),
                        last_seen: Instant::now(),
                        status: PeerStatus::Connected,
                    });
                }

                // å¤„ç†æ¶ˆæ¯
                if message.receiver == node_id || message.receiver == "any" {
                    let handlers = message_handlers.read().unwrap();
                    if let Some(handler) = handlers.get(&message.message_type) {
                        match handler(&message) {
                            Ok(response) => {
                                // åˆ›å»ºå“åº”æ¶ˆæ¯
                                let response_message = NetworkMessage {
                                    id: uuid::Uuid::new_v4().to_string(),
                                    sender: node_id.to_string(),
                                    receiver: message.sender.clone(),
                                    message_type: format!("{}_RESPONSE", message.message_type),
                                    content: response,
                                    created_at: SystemTime::now(),
                                    ttl: 1,
                                    hops: 0,
                                };

                                // åºåˆ—åŒ–å“åº”
                                let response_data = match serde_json::to_vec(&response_message) {
                                    Ok(data) => data,
                                    Err(err) => {
                                        return Err(format!("åºåˆ—åŒ–å“åº”å¤±è´¥: {}", err));
                                    }
                                };

                                // å‘é€å“åº”é•¿åº¦
                                let length = response_data.len() as u32;
                                let length_bytes = length.to_be_bytes();
                                writer.write_all(&length_bytes)
                                    .map_err(|err| format!("å‘é€å“åº”é•¿åº¦å¤±è´¥: {}", err))?;

                                // å‘é€å“åº”æ•°æ®
                                writer.write_all(&response_data)
                                    .map_err(|err| format!("å‘é€å“åº”æ•°æ®å¤±è´¥: {}", err))?;

                                writer.flush()
                                    .map_err(|err| format!("åˆ·æ–°å“åº”å¤±è´¥: {}", err))?;
                            },
                            Err(err) => {
                                println!("å¤„ç†æ¶ˆæ¯å¤±è´¥: {}", err);
                            }
                        }
                    }
                } else {
                    // æ¶ˆæ¯ä¸æ˜¯å‘ç»™å½“å‰èŠ‚ç‚¹çš„ï¼Œå°è¯•è½¬å‘
                    let peers_guard = peers.read().unwrap();
                    if let Some(peer) = peers_guard.get(&message.receiver) {
                        if matches!(peer.status, PeerStatus::Connected) {
                            // è½¬å‘æ¶ˆæ¯
                            let mut forward_stream = match TcpStream::connect(peer.addr) {
                                Ok(stream) => stream,
                                Err(err) => {
                                    println!("è¿æ¥åˆ°è½¬å‘ç›®æ ‡å¤±è´¥: {}", err);
                                    continue;
                                }
                            };

                            // å‘é€æ¶ˆæ¯é•¿åº¦
                            let length = message_data.len() as u32;
                            let length_bytes = length.to_be_bytes();
                            if let Err(err) = forward_stream.write_all(&length_bytes) {
                                println!("å‘é€è½¬å‘æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err);
                                continue;
                            }

                            // å‘é€æ¶ˆæ¯æ•°æ®
                            if let Err(err) = forward_stream.write_all(&message_data) {
                                println!("å‘é€è½¬å‘æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err);
                                continue;
                            }
                        }
                    }
                }
            },
            Err(ref err) if err.kind() == ErrorKind::WouldBlock => {
                // æ²¡æœ‰æ•°æ®å¯è¯»ï¼Œä¼‘çœ ä¸€æ®µæ—¶é—´
                std::thread::sleep(Duration::from_millis(10));
            },
            Err(err) => {
                return Err(format!("è¯»å–æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err));
            }
        }
    }
}

fn handle_outbound_connection(
    stream: TcpStream,
    node_id: &str,
    peer_id: &str,
) -> Result<(), String> {
    // è®¾ç½®éé˜»å¡æ¨¡å¼
    stream.set_nonblocking(true)
        .map_err(|err| format!("è®¾ç½®éé˜»å¡æ¨¡å¼å¤±è´¥: {}", err))?;

    // å‘é€èŠ‚ç‚¹ä¿¡æ¯
    let mut writer = BufWriter::new(stream);

    // åˆ›å»ºæ¡æ‰‹æ¶ˆæ¯
    let handshake_message = NetworkMessage {
        id: uuid::Uuid::new_v4().to_string(),
        sender: node_id.to_string(),
        receiver: peer_id.to_string(),
        message_type: "HANDSHAKE".to_string(),
        content: Vec::new(),
        created_at: SystemTime::now(),
        ttl: 1,
        hops: 0,
    };

    // åºåˆ—åŒ–æ¡æ‰‹æ¶ˆæ¯
    let handshake_data = serde_json::to_vec(&handshake_message)
        .map_err(|err| format!("åºåˆ—åŒ–æ¡æ‰‹æ¶ˆæ¯å¤±è´¥: {}", err))?;

    // å‘é€æ¡æ‰‹æ¶ˆæ¯é•¿åº¦
    let length = handshake_data.len() as u32;
    let length_bytes = length.to_be_bytes();
    writer.write_all(&length_bytes)
        .map_err(|err| format!("å‘é€æ¡æ‰‹æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err))?;

    // å‘é€æ¡æ‰‹æ¶ˆæ¯æ•°æ®
    writer.write_all(&handshake_data)
        .map_err(|err| format!("å‘é€æ¡æ‰‹æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err))?;

    writer.flush()
        .map_err(|err| format!("åˆ·æ–°æ¡æ‰‹æ¶ˆæ¯å¤±è´¥: {}", err))?;

    // ä¿æŒè¿æ¥æ´»è·ƒï¼ˆå¯ä»¥å®ç°å¿ƒè·³æœºåˆ¶ï¼‰
    loop {
        // å‘é€å¿ƒè·³
        std::thread::sleep(Duration::from_secs(30));

        // åˆ›å»ºå¿ƒè·³æ¶ˆæ¯
        let heartbeat_message = NetworkMessage {
            id: uuid::Uuid::new_v4().to_string(),
            sender: node_id.to_string(),
            receiver: peer_id.to_string(),
            message_type: "HEARTBEAT".to_string(),
            content: Vec::new(),
            created_at: SystemTime::now(),
            ttl: 1,
            hops: 0,
        };

        // åºåˆ—åŒ–å¿ƒè·³æ¶ˆæ¯
        let heartbeat_data = match serde_json::to_vec(&heartbeat_message) {
            Ok(data) => data,
            Err(err) => {
                return Err(format!("åºåˆ—åŒ–å¿ƒè·³æ¶ˆæ¯å¤±è´¥: {}", err));
            }
        };

        // å‘é€å¿ƒè·³æ¶ˆæ¯é•¿åº¦
        let length = heartbeat_data.len() as u32;
        let length_bytes = length.to_be_bytes();
        if let Err(err) = writer.write_all(&length_bytes) {
            return Err(format!("å‘é€å¿ƒè·³æ¶ˆæ¯é•¿åº¦å¤±è´¥: {}", err));
        }

        // å‘é€å¿ƒè·³æ¶ˆæ¯æ•°æ®
        if let Err(err) = writer.write_all(&heartbeat_data) {
            return Err(format!("å‘é€å¿ƒè·³æ¶ˆæ¯æ•°æ®å¤±è´¥: {}", err));
        }

        if let Err(err) = writer.flush() {
            return Err(format!("åˆ·æ–°å¿ƒè·³æ¶ˆæ¯å¤±è´¥: {}", err));
        }
    }
}

// ä»¥ä¸Šå®ç°äº†ä¸€ä¸ªåŸºæœ¬çš„ç‚¹å¯¹ç‚¹ç½‘ç»œå±‚ï¼ŒåŒ…æ‹¬ï¼š
// 1. å¯¹ç­‰èŠ‚ç‚¹è¿æ¥ç®¡ç†
// 2. æ¶ˆæ¯ä¼ é€’å’Œè·¯ç”±
// 3. å¯¹ç­‰èŠ‚ç‚¹å‘ç°
// 4. æ¶ˆæ¯å¤„ç†æ³¨å†Œ
```

### 1.4 ç»¼åˆåº”ç”¨04-åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ

```rust
// åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ
struct DistributedDatabase {
    node_id: String,
    storage_engine: Box<dyn StorageEngine>,
    replication_manager: ReplicationManager,
    partition_manager: PartitionManager,
    query_processor: QueryProcessor,
    transaction_manager: TransactionManager,
}

trait StorageEngine: Send + Sync {
    fn put(&self, key: &[u8], value: &[u8]) -> Result<(), String>;
    fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>, String>;
    fn delete(&self, key: &[u8]) -> Result<bool, String>;
    fn scan(&self, start_key: &[u8], end_key: &[u8], limit: usize) -> Result<Vec<(Vec<u8>, Vec<u8>)>, String>;
    fn flush(&self) -> Result<(), String>;
}

struct ReplicationManager {
    node_id: String,
    replication_factor: usize,
    replication_strategy: ReplicationStrategy,
    peers: RwLock<HashMap<String, ReplicationPeer>>,
}

enum ReplicationStrategy {
    Synchronous,
    Asynchronous,
    QuorumBased { write_quorum: usize, read_quorum: usize },
}

struct ReplicationPeer {
    node_id: String,
    address: SocketAddr,
    status: PeerStatus,
    last_heartbeat: Instant,
    replication_lag: Duration,
}

struct PartitionManager {
    node_id: String,
    partitioning_strategy: PartitioningStrategy,
    partitions: RwLock<HashMap<String, PartitionInfo>>,
    ring: Arc<ConsistentHash>,
}

enum PartitioningStrategy {
    HashBased,
    RangeBased,
    ConsistentHashing,
}

struct PartitionInfo {
    id: String,
    key_range: (Vec<u8>, Vec<u8>),
    nodes: Vec<String>,
    primary_node: String,
    status: PartitionStatus,
}

enum PartitionStatus {
    Normal,
    Rebalancing,
    Degraded,
    Unavailable,
}

struct QueryProcessor {
    node_id: String,
    query_handlers: RwLock<HashMap<QueryType, Box<dyn Fn(&Query) -> Result<QueryResult, String> + Send + Sync>>>,
    query_optimizer: QueryOptimizer,
    partition_manager: Arc<PartitionManager>,
}

enum QueryType {
    Get,
    Put,
    Delete,
    Scan,
    CreateTable,
    DropTable,
    Select,
    Insert,
    Update,
    Delete,
}

struct Query {
    id: String,
    query_type: QueryType,
    parameters: Vec<u8>,
    timeout: Duration,
    consistency_level: ConsistencyLevel,
}

enum ConsistencyLevel {
    One,
    Quorum,
    All,
    LocalQuorum,
    EachQuorum,
}

struct QueryResult {
    success: bool,
    result_data: Vec<u8>,
    execution_time: Duration,
    affected_rows: usize,
}

struct QueryOptimizer {
    statistics: Arc<Statistics>,
    cost_models: HashMap<QueryType, Box<dyn Fn(&Query, &Statistics) -> f64 + Send + Sync>>,
}

struct Statistics {
    table_stats: RwLock<HashMap<String, TableStatistics>>,
}

struct TableStatistics {
    row_count: u64,
    avg_row_size: usize,
    column_stats: HashMap<String, ColumnStatistics>,
}

struct ColumnStatistics {
    distinct_values: u64,
    min_value: Vec<u8>,
    max_value: Vec<u8>,
    null_count: u64,
}

struct TransactionManager {
    node_id: String,
    active_transactions: RwLock<HashMap<String, TransactionContext>>,
    coordinator: Arc<TransactionCoordinator>,
}

struct TransactionContext {
    id: String,
    isolation_level: IsolationLevel,
    started_at: SystemTime,
    timeout: Duration,
    status: TransactionStatus,
    locks: Vec<LockInfo>,
    operations: Vec<Operation>,
}

enum IsolationLevel {
    ReadUncommitted,
    ReadCommitted,
    RepeatableRead,
    Serializable,
    Snapshot,
}

enum TransactionStatus {
    Active,
    Prepared,
    Committed,
    Aborted,
}

struct LockInfo {
    resource: Vec<u8>,
    lock_type: LockType,
    acquired_at: SystemTime,
}

enum LockType {
    Shared,
    Exclusive,
}

struct Operation {
    operation_type: OperationType,
    resource: Vec<u8>,
    data: Vec<u8>,
    timestamp: SystemTime,
}

enum OperationType {
    Read,
    Write,
    Delete,
}

impl DistributedDatabase {
    fn new(
        node_id: &str,
        storage_engine: Box<dyn StorageEngine>,
        replication_factor: usize,
        replication_strategy: ReplicationStrategy,
        partitioning_strategy: PartitioningStrategy,
    ) -> Self {
        let ring = Arc::new(ConsistentHash::new(
            100, // è™šæ‹ŸèŠ‚ç‚¹æ•°
            Box::new(|s| {
                let mut hasher = DefaultHasher::new();
                s.hash(&mut hasher);
                hasher.finish()
            }),
        ));

        let partition_manager = PartitionManager {
            node_id: node_id.to_string(),
            partitioning_strategy,
            partitions: RwLock::new(HashMap::new()),
            ring: ring.clone(),
        };

        let partition_manager_arc = Arc::new(partition_manager);

        let statistics = Arc::new(Statistics {
            table_stats: RwLock::new(HashMap::new()),
        });

        let query_optimizer = QueryOptimizer {
            statistics: statistics.clone(),
            cost_models: HashMap::new(),
        };

        let query_processor = QueryProcessor {
            node_id: node_id.to_string(),
            query_handlers: RwLock::new(HashMap::new()),
            query_optimizer,
            partition_manager: partition_manager_arc.clone(),
        };

        let replication_manager = ReplicationManager {
            node_id: node_id.to_string(),
            replication_factor,
            replication_strategy,
            peers: RwLock::new(HashMap::new()),
        };

        let transaction_coordinator = Arc::new(TransactionCoordinator::new(
            node_id,
            Box::new(InMemoryTransactionStorage::new()),
            HashMap::new(),
            RetryPolicy::new(3, Duration::from_millis(100)),
            Duration::from_secs(10),
            3,
        ));

        let transaction_manager = TransactionManager {
            node_id: node_id.to_string(),
            active_transactions: RwLock::new(HashMap::new()),
            coordinator: transaction_coordinator,
        };

        DistributedDatabase {
            node_id: node_id.to_string(),
            storage_engine,
            replication_manager,
            partition_manager: *partition_manager_arc,
            query_processor,
            transaction_manager,
        }
    }

    fn start(&self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æ•°æ®åº“èŠ‚ç‚¹: {}", self.node_id);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨å„ä¸ªç»„ä»¶

        Ok(())
    }

    fn execute_query(&self, query: &Query) -> Result<QueryResult, String> {
        let start_time = Instant::now();

        // è·å–æŸ¥è¯¢å¤„ç†å™¨
        let handlers = self.query_processor.query_handlers.read().unwrap();

        let handler = handlers.get(&query.query_type)
            .ok_or_else(|| format!("ä¸æ”¯æŒçš„æŸ¥è¯¢ç±»å‹: {:?}", query.query_type))?;

        // æ‰§è¡ŒæŸ¥è¯¢
        let result = handler(query)?;

        // è®¡ç®—æ‰§è¡Œæ—¶é—´
        let execution_time = start_time.elapsed();

        Ok(QueryResult {
            success: result.success,
            result_data: result.result_data,
            execution_time,
            affected_rows: result.affected_rows,
        })
    }

    fn add_node(&self, node_id: &str, address: SocketAddr) -> Result<(), String> {
        // å‘å¤åˆ¶ç®¡ç†å™¨æ·»åŠ å¯¹ç­‰èŠ‚ç‚¹
        let mut peers = self.replication_manager.peers.write().unwrap();

        if peers.contains_key(node_id) {
            return Err(format!("èŠ‚ç‚¹ {} å·²å­˜åœ¨", node_id));
        }

        peers.insert(node_id.to_string(), ReplicationPeer {
            node_id: node_id.to_string(),
            address,
            status: PeerStatus::Connected,
            last_heartbeat: Instant::now(),
            replication_lag: Duration::from_secs(0),
        });

        // å‘ä¸€è‡´æ€§å“ˆå¸Œç¯æ·»åŠ èŠ‚ç‚¹
        self.partition_manager.ring.add_node(node_id);

        // é‡æ–°å¹³è¡¡åˆ†åŒº
        self.rebalance_partitions()?;

        Ok(())
    }

    fn remove_node(&self, node_id: &str) -> Result<(), String> {
        // ä»å¤åˆ¶ç®¡ç†å™¨ç§»é™¤å¯¹ç­‰èŠ‚ç‚¹
        let mut peers = self.replication_manager.peers.write().unwrap();

        if !peers.contains_key(node_id) {
            return Err(format!("èŠ‚ç‚¹ {} ä¸å­˜åœ¨", node_id));
        }

        peers.remove(node_id);

        // ä»ä¸€è‡´æ€§å“ˆå¸Œç¯ç§»é™¤èŠ‚ç‚¹
        self.partition_manager.ring.remove_node(node_id);

        // é‡æ–°å¹³è¡¡åˆ†åŒº
        self.rebalance_partitions()?;

        Ok(())
    }

    fn rebalance_partitions(&self) -> Result<(), String> {
        println!("å¼€å§‹é‡æ–°å¹³è¡¡åˆ†åŒº");

        // è·å–æ‰€æœ‰åˆ†åŒº
        let mut partitions = self.partition_manager.partitions.write().unwrap();

        // è·å–æ‰€æœ‰èŠ‚ç‚¹
        let peers = self.replication_manager.peers.read().unwrap();
        let nodes: Vec<String> = peers.keys().cloned().collect();

        if nodes.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹è¿›è¡Œåˆ†åŒºé‡æ–°å¹³è¡¡".to_string());
        }

        // é‡æ–°åˆ†é…åˆ†åŒº
        for (_, partition) in partitions.iter_mut() {
            // è®¾ç½®åˆ†åŒºçŠ¶æ€ä¸ºé‡æ–°å¹³è¡¡
            partition.status = PartitionStatus::Rebalancing;

            // è®¡ç®—åˆ†åŒºçš„å“ˆå¸Œå€¼
            let partition_key = format!("partition:{}", partition.id);
            let assigned_node = self.partition_manager.ring.get_node(&partition_key)
                .unwrap_or_else(|| nodes[0].clone());

            // æ›´æ–°ä¸»èŠ‚ç‚¹
            partition.primary_node = assigned_node.clone();

            // é€‰æ‹©å¤åˆ¶èŠ‚ç‚¹
            partition.nodes.clear();
            partition.nodes.push(assigned_node.clone());

            let replica_count = self.replication_manager.replication_factor - 1;
            if replica_count > 0 {
                let replica_nodes = self.partition_manager.ring.get_nodes(&partition_key, replica_count + 1);

                for node in replica_nodes {
                    if node != assigned_node && !partition.nodes.contains(&node) {
                        partition.nodes.push(node);

                        if partition.nodes.len() >= self.replication_manager.replication_factor {
                            break;
                        }
                    }
                }
            }

            // æ¢å¤åˆ†åŒºçŠ¶æ€
            partition.status = PartitionStatus::Normal;
        }

        println!("åˆ†åŒºé‡æ–°å¹³è¡¡å®Œæˆ");

        Ok(())
    }

    fn begin_transaction(&self, isolation_level: IsolationLevel) -> Result<String, String> {
        let tx_id = self.transaction_manager.coordinator.start_transaction()?;

        // åˆ›å»ºäº‹åŠ¡ä¸Šä¸‹æ–‡
        let tx_context = TransactionContext {
            id: tx_id.clone(),
            isolation_level,
            started_at: SystemTime::now(),
            timeout: Duration::from_secs(30), // é»˜è®¤è¶…æ—¶æ—¶é—´
            status: TransactionStatus::Active,
            locks: Vec::new(),
            operations: Vec::new(),
        };

        // æ·»åŠ åˆ°æ´»åŠ¨äº‹åŠ¡
        let mut active_transactions = self.transaction_manager.active_transactions.write().unwrap();
        active_transactions.insert(tx_id.clone(), tx_context);

        Ok(tx_id)
    }

    fn commit_transaction(&self, tx_id: &str) -> Result<bool, String> {
        // è·å–äº‹åŠ¡ä¸Šä¸‹æ–‡
        let tx_context = {
            let mut active_transactions = self.transaction_manager.active_transactions.write().unwrap();

            if !active_transactions.contains_key(tx_id) {
                return Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id));
            }

            let mut tx_context = active_transactions.remove(tx_id).unwrap();
            tx_context.status = TransactionStatus::Prepared;
            tx_context
        };

        // å‡†å¤‡äº‹åŠ¡æ“ä½œ
        let actions: Vec<TransactionAction> = tx_context.operations.iter().map(|op| {
            TransactionAction {
                participant_id: self.node_id.clone(),
                operation_type: match op.operation_type {
                    OperationType::Read => "READ".to_string(),
                    OperationType::Write => "WRITE".to_string(),
                    OperationType::Delete => "DELETE".to_string(),
                },
                resource_id: hex::encode(&op.resource),
                data: serde_json::json!({
                    "resource": hex::encode(&op.resource),
                    "data": base64::encode(&op.data),
                    "timestamp": op.timestamp.duration_since(SystemTime::UNIX_EPOCH).unwrap().as_secs(),
                }),
            }
        }).collect();

        // å‡†å¤‡é˜¶æ®µ
        if !self.transaction_manager.coordinator.prepare(tx_id, &actions)? {
            // å‡†å¤‡å¤±è´¥ï¼Œå›æ»šäº‹åŠ¡
            self.transaction_manager.coordinator.abort_transaction(tx_id, "å‡†å¤‡é˜¶æ®µå¤±è´¥")?;
            return Ok(false);
        }

        // æäº¤é˜¶æ®µ
        self.transaction_manager.coordinator.commit_transaction(tx_id)?;

        Ok(true)
    }

    fn abort_transaction(&self, tx_id: &str) -> Result<bool, String> {
        // è·å–äº‹åŠ¡ä¸Šä¸‹æ–‡
        {
            let mut active_transactions = self.transaction_manager.active_transactions.write().unwrap();

            if !active_transactions.contains_key(tx_id) {
                return Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id));
            }

            active_transactions.remove(tx_id);
        }

        // ä¸­æ­¢äº‹åŠ¡
        self.transaction_manager.coordinator.abort_transaction(tx_id, "ç”¨æˆ·è¯·æ±‚ä¸­æ­¢")?;

        Ok(true)
    }
}

// å†…å­˜å­˜å‚¨å¼•æ“å®ç°
struct InMemoryStorageEngine {
    data: RwLock<BTreeMap<Vec<u8>, Vec<u8>>>,
}

impl InMemoryStorageEngine {
    fn new() -> Self {
        InMemoryStorageEngine {
            data: RwLock::new(BTreeMap::new()),
        }
    }
}

impl StorageEngine for InMemoryStorageEngine {
    fn put(&self, key: &[u8], value: &[u8]) -> Result<(), String> {
        let mut data = self.data.write().unwrap();
        data.insert(key.to_vec(), value.to_vec());
        Ok(())
    }

    fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>, String> {
        let data = self.data.read().unwrap();
        Ok(data.get(key).cloned())
    }

    fn delete(&self, key: &[u8]) -> Result<bool, String> {
        let mut data = self.data.write().unwrap();
        Ok(data.remove(key).is_some())
    }

    fn scan(&self, start_key: &[u8], end_key: &[u8], limit: usize) -> Result<Vec<(Vec<u8>, Vec<u8>)>, String> {
        let data = self.data.read().unwrap();

        let mut result = Vec::new();

        for (key, value) in data.range(start_key.to_vec()..end_key.to_vec()) {
            if result.len() >= limit {
                break;
            }

            result.push((key.clone(), value.clone()));
        }

        Ok(result)
    }

    fn flush(&self) -> Result<(), String> {
        // å†…å­˜å­˜å‚¨å¼•æ“ä¸éœ€è¦åˆ·æ–°
        Ok(())
    }
}

// å†…å­˜äº‹åŠ¡å­˜å‚¨å®ç°
struct InMemoryTransactionStorage {
    transactions: RwLock<HashMap<String, TransactionState>>,
}

impl InMemoryTransactionStorage {
    fn new() -> Self {
        InMemoryTransactionStorage {
            transactions: RwLock::new(HashMap::new()),
        }
    }
}

impl TransactionStorage for InMemoryTransactionStorage {
    fn create_transaction(&self, tx_id: &str) -> Result<(), String> {
        let mut transactions = self.transactions.write().unwrap();

        if transactions.contains_key(tx_id) {
            return Err(format!("äº‹åŠ¡ {} å·²å­˜åœ¨", tx_id));
        }

        transactions.insert(tx_id.to_string(), TransactionState::Created {
            created_at: SystemTime::now(),
        });

        Ok(())
    }

    fn update_transaction_state(&self, tx_id: &str, state: &TransactionState) -> Result<(), String> {
        let mut transactions = self.transactions.write().unwrap();

        if !transactions.contains_key(tx_id) {
            return Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id));
        }

        transactions.insert(tx_id.to_string(), state.clone());

        Ok(())
    }

    fn get_transaction_state(&self, tx_id: &str) -> Result<Option<TransactionState>, String> {
        let transactions = self.transactions.read().unwrap();
        Ok(transactions.get(tx_id).cloned())
    }

    fn list_active_transactions(&self) -> Result<Vec<(String, TransactionState)>, String> {
        let transactions = self.transactions.read().unwrap();

        Ok(transactions.iter()
            .filter(|(_, state)| {
                match state {
                    TransactionState::Committed { .. } => false,
                    TransactionState::Aborted { .. } => false,
                    _ => true,
                }
            })
            .map(|(id, state)| (id.clone(), state.clone()))
            .collect())
    }

    fn mark_transaction_completed(&self, tx_id: &str) -> Result<(), String> {
        let mut transactions = self.transactions.write().unwrap();

        if !transactions.contains_key(tx_id) {
            return Err(format!("äº‹åŠ¡ {} ä¸å­˜åœ¨", tx_id));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œå¯èƒ½ä¼šå°†äº‹åŠ¡ç§»åŠ¨åˆ°å·²å®Œæˆåˆ—è¡¨æˆ–ä»æ´»åŠ¨åˆ—è¡¨ä¸­åˆ é™¤

        Ok(())
    }
}

// åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
struct DistributedFileSystem {
    node_id: String,
    storage_manager: StorageManager,
    metadata_manager: MetadataManager,
    replication_manager: ReplicationManager,
    chunk_manager: ChunkManager,
}

struct StorageManager {
    node_id: String,
    base_path: PathBuf,
    available_space: Arc<AtomicU64>,
    total_space: u64,
}

struct MetadataManager {
    node_id: String,
    metadata_store: Box<dyn MetadataStore>,
}

trait MetadataStore: Send + Sync {
    fn create_file(&self, path: &str, size: u64, owner: &str, permissions: u32) -> Result<FileMetadata, String>;
    fn get_file(&self, path: &str) -> Result<Option<FileMetadata>, String>;
    fn update_file(&self, path: &str, metadata: &FileMetadata) -> Result<(), String>;
    fn delete_file(&self, path: &str) -> Result<bool, String>;
    fn list_directory(&self, path: &str) -> Result<Vec<FileMetadata>, String>;
}

struct FileMetadata {
    path: String,
    size: u64,
    chunks: Vec<ChunkMetadata>,
    created_at: SystemTime,
    modified_at: SystemTime,
    accessed_at: SystemTime,
    owner: String,
    permissions: u32,
}

struct ChunkMetadata {
    id: String,
    size: u64,
    offset: u64,
    checksum: Vec<u8>,
    locations: Vec<String>,
}

struct ChunkManager {
    node_id: String,
    chunk_size: usize,
    storage_manager: Arc<StorageManager>,
}

impl DistributedFileSystem {
    fn new(
        node_id: &str,
        base_path: PathBuf,
        total_space: u64,
        metadata_store: Box<dyn MetadataStore>,
        replication_factor: usize,
        chunk_size: usize,
    ) -> Self {
        let storage_manager = StorageManager {
            node_id: node_id.to_string(),
            base_path,
            available_space: Arc::new(AtomicU64::new(total_space)),
            total_space,
        };

        let storage_manager_arc = Arc::new(storage_manager);

        let metadata_manager = MetadataManager {
            node_id: node_id.to_string(),
            metadata_store,
        };

        let replication_manager = ReplicationManager {
            node_id: node_id.to_string(),
            replication_factor,
            replication_strategy: ReplicationStrategy::Asynchronous,
            peers: RwLock::new(HashMap::new()),
        };

        let chunk_manager = ChunkManager {
            node_id: node_id.to_string(),
            chunk_size,
            storage_manager: storage_manager_arc.clone(),
        };

        DistributedFileSystem {
            node_id: node_id.to_string(),
            storage_manager: *storage_manager_arc,
            metadata_manager,
            replication_manager,
            chunk_manager,
        }
    }

    fn create_file(&self, path: &str, size: u64, owner: &str, permissions: u32) -> Result<FileMetadata, String> {
        // æ£€æŸ¥å¯ç”¨ç©ºé—´
        if size > self.storage_manager.available_space.load(Ordering::SeqCst) {
            return Err(format!("ç©ºé—´ä¸è¶³ï¼šéœ€è¦ {} å­—èŠ‚ï¼Œä½†åªæœ‰ {} å­—èŠ‚å¯ç”¨",
                size, self.storage_manager.available_space.load(Ordering::SeqCst)));
        }

        // åˆ›å»ºæ–‡ä»¶å…ƒæ•°æ®
        let metadata = self.metadata_manager.metadata_store.create_file(path, size, owner, permissions)?;

        // æ›´æ–°å¯ç”¨ç©ºé—´
        self.storage_manager.available_space.fetch_sub(size, Ordering::SeqCst);

        Ok(metadata)
    }

    fn read_file(&self, path: &str, offset: u64, length: usize) -> Result<Vec<u8>, String> {
        // è·å–æ–‡ä»¶å…ƒæ•°æ®
        let metadata = match self.metadata_manager.metadata_store.get_file(path)? {
            Some(metadata) => metadata,
            None => return Err(format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", path)),
        };

        // è®¡ç®—éœ€è¦è¯»å–çš„å—
        let start_chunk_index = (offset / self.chunk_manager.chunk_size as u64) as usize;
        let end_chunk_index = ((offset + length as u64) / self.chunk_manager.chunk_size as u64) as usize;

        if start_chunk_index >= metadata.chunks.len() || end_chunk_index >= metadata.chunks.len() {
            return Err(format!("è¯»å–èŒƒå›´è¶…å‡ºæ–‡ä»¶è¾¹ç•Œ"));
        }

        let mut result = Vec::with_capacity(length);

        for chunk_index in start_chunk_index..=end_chunk_index {
            let chunk = &metadata.chunks[chunk_index];

            // è®¡ç®—åœ¨å—å†…çš„åç§»å’Œé•¿åº¦
            let chunk_offset = if chunk_index == start_chunk_index {
                offset - (chunk_index as u64 * self.chunk_manager.chunk_size as u64)
            } else {
                0
            };

            let chunk_length = if chunk_index == end_chunk_index {
                ((offset + length as u64) % self.chunk_manager.chunk_size as u64) as usize
            } else {
                self.chunk_manager.chunk_size - chunk_offset as usize
            };

            // è¯»å–å—æ•°æ®
            let chunk_data = self.read_chunk(chunk, chunk_offset, chunk_length)?;
            result.extend_from_slice(&chunk_data);
        }

        // æ›´æ–°è®¿é—®æ—¶é—´
        let mut updated_metadata = metadata.clone();
        updated_metadata.accessed_at = SystemTime::now();
        self.metadata_manager.metadata_store.update_file(path, &updated_metadata)?;

        Ok(result)
    }

    fn write_file(&self, path: &str, offset: u64, data: &[u8]) -> Result<u64, String> {
        // è·å–æ–‡ä»¶å…ƒæ•°æ®
        let mut metadata = match self.metadata_manager.metadata_store.get_file(path)? {
            Some(metadata) => metadata,
            None => return Err(format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", path)),
        };

        // è®¡ç®—æ–°æ–‡ä»¶å¤§å°
        let new_size = (offset + data.len() as u64).max(metadata.size);
        let size_increase = new_size - metadata.size;

        // æ£€æŸ¥ç©ºé—´
        if size_increase > 0 && size_increase > self.storage_manager.available_space.load(Ordering::SeqCst) {
            return Err(format!("ç©ºé—´ä¸è¶³ï¼šéœ€è¦é¢å¤– {} å­—èŠ‚ï¼Œä½†åªæœ‰ {} å­—èŠ‚å¯ç”¨",
                size_increase, self.storage_manager.available_space.load(Ordering::SeqCst)));
        }

        // è®¡ç®—éœ€è¦å†™å…¥çš„å—
        let start_chunk_index = (offset / self.chunk_manager.chunk_size as u64) as usize;
        let end_chunk_index = ((offset + data.len() as u64 - 1) / self.chunk_manager.chunk_size as u64) as usize;

        // å¦‚æœéœ€è¦ï¼Œåˆ›å»ºæ–°çš„å—
        while metadata.chunks.len() <= end_chunk_index {
            let chunk_id = uuid::Uuid::new_v4().to_string();
            metadata.chunks.push(ChunkMetadata {
                id: chunk_id,
                size: 0,
                offset: metadata.chunks.len() as u64 * self.chunk_manager.chunk_size as u64,
                checksum: Vec::new(),
                locations: vec![self.node_id.clone()],
            });
        }

        let mut bytes_written = 0;
        let mut data_offset = 0;

        for chunk_index in start_chunk_index..=end_chunk_index {
            let chunk = &mut metadata.chunks[chunk_index];

            // è®¡ç®—åœ¨å—å†…çš„åç§»å’Œé•¿åº¦
            let chunk_offset = if chunk_index == start_chunk_index {
                offset - (chunk_index as u64 * self.chunk_manager.chunk_size as u64)
            } else {
                0
            };

            let chunk_length = if chunk_index == end_chunk_index {
                data.len() - data_offset
            } else {
                self.chunk_manager.chunk_size - chunk_offset as usize
            };

            // å†™å…¥å—æ•°æ®
            let written = self.write_chunk(chunk, chunk_offset, &data[data_offset..(data_offset + chunk_length)])?;
            bytes_written += written;
            data_offset += chunk_length;

            // æ›´æ–°å—å…ƒæ•°æ®
            chunk.size = (chunk_offset + written as u64).max(chunk.size);
            // åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦è®¡ç®—æ–°çš„æ ¡éªŒå’Œ
        }

        // æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®
        metadata.size = new_size;
        metadata.modified_at = SystemTime::now();
        self.metadata_manager.metadata_store.update_file(path, &metadata)?;

        // æ›´æ–°å¯ç”¨ç©ºé—´
        if size_increase > 0 {
            self.storage_manager.available_space.fetch_sub(size_increase, Ordering::SeqCst);
        }

        Ok(bytes_written as u64)
    }

    fn delete_file(&self, path: &str) -> Result<bool, String> {
        // è·å–æ–‡ä»¶å…ƒæ•°æ®
        let metadata = match self.metadata_manager.metadata_store.get_file(path)? {
            Some(metadata) => metadata,
            None => return Err(format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", path)),
        };

        // åˆ é™¤æ–‡ä»¶å…ƒæ•°æ®
        let deleted = self.metadata_manager.metadata_store.delete_file(path)?;

        if deleted {
            // åˆ é™¤å—æ•°æ®
            for chunk in &metadata.chunks {
                if chunk.locations.contains(&self.node_id) {
                    self.delete_chunk(chunk)?;
                }
            }

            // æ›´æ–°å¯ç”¨ç©ºé—´
            self.storage_manager.available_space.fetch_add(metadata.size, Ordering::SeqCst);
        }

        Ok(deleted)
    }

    fn list_directory(&self, path: &str) -> Result<Vec<FileMetadata>, String> {
        self.metadata_manager.metadata_store.list_directory(path)
    }

    fn read_chunk(&self, chunk: &ChunkMetadata, offset: u64, length: usize) -> Result<Vec<u8>, String> {
        // æ£€æŸ¥å—ä½ç½®
        if !chunk.locations.contains(&self.node_id) {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦ä»å…¶ä»–èŠ‚ç‚¹è·å–å—æ•°æ®
            return Err(format!("å— {} ä¸åœ¨æœ¬åœ°", chunk.id));
        }

        // æ„é€ å—æ–‡ä»¶è·¯å¾„
        let chunk_path = self.storage_manager.base_path.join(&chunk.id);

        // æ‰“å¼€æ–‡ä»¶
        let mut file = match File::open(&chunk_path) {
            Ok(file) => file,
            Err(err) => return Err(format!("æ‰“å¼€å—æ–‡ä»¶å¤±è´¥: {}", err)),
        };

        // å®šä½åˆ°åç§»ä½ç½®
        if let Err(err) = file.seek(SeekFrom::Start(offset)) {
            return Err(format!("æ–‡ä»¶å®šä½å¤±è´¥: {}", err));
        }

        // è¯»å–æ•°æ®
        let mut buffer = vec![0u8; length];
        match file.read_exact(&mut buffer) {
            Ok(_) => Ok(buffer),
            Err(err) => Err(format!("è¯»å–å—æ•°æ®å¤±è´¥: {}", err)),
        }
    }

    fn write_chunk(&self, chunk: &ChunkMetadata, offset: u64, data: &[u8]) -> Result<usize, String> {
        // æ„é€ å—æ–‡ä»¶è·¯å¾„
        let chunk_path = self.storage_manager.base_path.join(&chunk.id);

        // æ‰“å¼€æˆ–åˆ›å»ºæ–‡ä»¶
        let mut file = match OpenOptions::new()
            .write(true)
            .create(true)
            .open(&chunk_path) {
            Ok(file) => file,
            Err(err) => return Err(format!("æ‰“å¼€å—æ–‡ä»¶å¤±è´¥: {}", err)),
        };

        // å®šä½åˆ°åç§»ä½ç½®
        if let Err(err) = file.seek(SeekFrom::Start(offset)) {
            return Err(format!("æ–‡ä»¶å®šä½å¤±è´¥: {}", err));
        }

        // å†™å…¥æ•°æ®
        match file.write(data) {
            Ok(written) => Ok(written),
            Err(err) => Err(format!("å†™å…¥å—æ•°æ®å¤±è´¥: {}", err)),
        }
    }

    fn delete_chunk(&self, chunk: &ChunkMetadata) -> Result<(), String> {
        // æ„é€ å—æ–‡ä»¶è·¯å¾„
        let chunk_path = self.storage_manager.base_path.join(&chunk.id);

        // åˆ é™¤æ–‡ä»¶
        match fs::remove_file(&chunk_path) {
            Ok(_) => Ok(()),
            Err(err) => Err(format!("åˆ é™¤å—æ–‡ä»¶å¤±è´¥: {}", err)),
        }
    }
}

// å†…å­˜å…ƒæ•°æ®å­˜å‚¨å®ç°
struct InMemoryMetadataStore {
    files: RwLock<HashMap<String, FileMetadata>>,
}

impl InMemoryMetadataStore {
    fn new() -> Self {
        InMemoryMetadataStore {
            files: RwLock::new(HashMap::new()),
        }
    }

    fn get_parent_path(path: &str) -> String {
        let path_obj = Path::new(path);
        path_obj.parent().unwrap_or(Path::new("")).to_string_lossy().to_string()
    }
}

impl MetadataStore for InMemoryMetadataStore {
    fn create_file(&self, path: &str, size: u64, owner: &str, permissions: u32) -> Result<FileMetadata, String> {
        let mut files = self.files.write().unwrap();

        if files.contains_key(path) {
            return Err(format!("æ–‡ä»¶å·²å­˜åœ¨: {}", path));
        }

        // æ£€æŸ¥çˆ¶ç›®å½•æ˜¯å¦å­˜åœ¨
        let parent_path = Self::get_parent_path(path);
        if parent_path != "" && !files.contains_key(&parent_path) {
            return Err(format!("çˆ¶ç›®å½•ä¸å­˜åœ¨: {}", parent_path));
        }

        let now = SystemTime::now();

        let metadata = FileMetadata {
            path: path.to_string(),
            size,
            chunks: Vec::new(),
            created_at: now,
            modified_at: now,
            accessed_at: now,
            owner: owner.to_string(),
            permissions,
        };

        files.insert(path.to_string(), metadata.clone());

        Ok(metadata)
    }

    fn get_file(&self, path: &str) -> Result<Option<FileMetadata>, String> {
        let files = self.files.read().unwrap();
        Ok(files.get(path).cloned())
    }

    fn update_file(&self, path: &str, metadata: &FileMetadata) -> Result<(), String> {
        let mut files = self.files.write().unwrap();

        if !files.contains_key(path) {
            return Err(format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", path));
        }

        files.insert(path.to_string(), metadata.clone());

        Ok(())
    }

    fn delete_file(&self, path: &str) -> Result<bool, String> {
        let mut files = self.files.write().unwrap();
        Ok(files.remove(path).is_some())
    }

    fn list_directory(&self, path: &str) -> Result<Vec<FileMetadata>, String> {
        let files = self.files.read().unwrap();

        // æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if !path.ends_with('/') && !files.contains_key(path) {
            return Err(format!("ç›®å½•ä¸å­˜åœ¨: {}", path));
        }

        // æŸ¥æ‰¾ä»¥æŒ‡å®šè·¯å¾„å¼€å¤´çš„æ‰€æœ‰æ–‡ä»¶
        let prefix = if path.ends_with('/') { path.to_string() } else { format!("{}/", path) };

        let mut result = Vec::new();

        for (file_path, metadata) in files.iter() {
            if file_path.starts_with(&prefix) {
                // åªåŒ…å«ç›´æ¥å­é¡¹
                let remaining = &file_path[prefix.len()..];
                if !remaining.contains('/') {
                    result.push(metadata.clone());
                }
            }
        }

        Ok(result)
    }
}

// è¿™äº›ä»£ç ç¤ºä¾‹å±•ç¤ºäº†åˆ†å¸ƒ

```rust
// åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
struct DistributedComputeFramework {
    node_id: String,
    task_scheduler: TaskScheduler,
    worker_manager: WorkerManager,
    resource_manager: ResourceManager,
    data_manager: DataManager,
}

struct TaskScheduler {
    node_id: String,
    task_queue: RwLock<Vec<Task>>,
    active_tasks: RwLock<HashMap<String, TaskStatus>>,
    completed_tasks: RwLock<HashMap<String, TaskResult>>,
    scheduling_policy: SchedulingPolicy,
}

enum SchedulingPolicy {
    FIFO,
    Priority,
    FairShare,
    ResourceAware,
}

struct Task {
    id: String,
    name: String,
    priority: u32,
    resource_requirements: ResourceRequirements,
    dependencies: Vec<String>,
    code: Vec<u8>,
    input_data: Vec<DataReference>,
    output_data: Vec<DataReference>,
    created_at: SystemTime,
    timeout: Duration,
}

struct ResourceRequirements {
    cpu_cores: u32,
    memory_mb: u64,
    gpu_count: u32,
    disk_space_mb: u64,
}

struct TaskStatus {
    task_id: String,
    state: TaskState,
    worker_id: Option<String>,
    started_at: Option<SystemTime>,
    progress: f64,
    error_message: Option<String>,
}

enum TaskState {
    Pending,
    Scheduled,
    Running,
    Completed,
    Failed,
    Cancelled,
    Timeout,
}

struct TaskResult {
    task_id: String,
    success: bool,
    result_data: Option<Vec<u8>>,
    error_message: Option<String>,
    execution_time: Duration,
    resource_usage: ResourceUsage,
}

struct ResourceUsage {
    cpu_usage: f64,
    memory_peak_mb: u64,
    gpu_usage: f64,
    disk_io_read_mb: u64,
    disk_io_write_mb: u64,
    network_io_read_mb: u64,
    network_io_write_mb: u64,
}

struct DataReference {
    id: String,
    location_type: DataLocationType,
    location: String,
    size: u64,
    format: String,
    partitions: Option<Vec<DataPartition>>,
}

enum DataLocationType {
    LocalFile,
    DistributedFS,
    Database,
    RemoteURL,
    InMemory,
}

struct DataPartition {
    id: String,
    offset: u64,
    size: u64,
    location: String,
}

struct WorkerManager {
    node_id: String,
    workers: RwLock<HashMap<String, WorkerInfo>>,
    worker_capacity: u32,
}

struct WorkerInfo {
    id: String,
    address: SocketAddr,
    status: WorkerStatus,
    capabilities: WorkerCapabilities,
    current_tasks: Vec<String>,
    resource_usage: ResourceUsage,
    last_heartbeat: SystemTime,
}

enum WorkerStatus {
    Online,
    Busy,
    Offline,
    Failed,
}

struct WorkerCapabilities {
    cpu_cores: u32,
    memory_mb: u64,
    gpu_models: Vec<String>,
    gpu_count: u32,
    disk_space_mb: u64,
    supported_frameworks: Vec<String>,
}

struct ResourceManager {
    node_id: String,
    total_resources: ResourceCapacity,
    allocated_resources: RwLock<ResourceCapacity>,
    resource_reservation_policy: ResourceReservationPolicy,
}

struct ResourceCapacity {
    cpu_cores: u32,
    memory_mb: u64,
    gpu_count: u32,
    disk_space_mb: u64,
}

enum ResourceReservationPolicy {
    Strict,
    BestEffort,
    Preemptive,
}

struct DataManager {
    node_id: String,
    data_cache: RwLock<HashMap<String, CachedData>>,
    cache_size_limit_mb: u64,
    cache_policy: CachePolicy,
}

struct CachedData {
    id: String,
    data: Vec<u8>,
    size: u64,
    last_accessed: SystemTime,
    access_count: u64,
}

enum CachePolicy {
    LRU,
    LFU,
    FIFO,
}

impl DistributedComputeFramework {
    fn new(
        node_id: &str,
        worker_capacity: u32,
        total_resources: ResourceCapacity,
        cache_size_limit_mb: u64,
    ) -> Self {
        let task_scheduler = TaskScheduler {
            node_id: node_id.to_string(),
            task_queue: RwLock::new(Vec::new()),
            active_tasks: RwLock::new(HashMap::new()),
            completed_tasks: RwLock::new(HashMap::new()),
            scheduling_policy: SchedulingPolicy::ResourceAware,
        };

        let worker_manager = WorkerManager {
            node_id: node_id.to_string(),
            workers: RwLock::new(HashMap::new()),
            worker_capacity,
        };

        let resource_manager = ResourceManager {
            node_id: node_id.to_string(),
            total_resources,
            allocated_resources: RwLock::new(ResourceCapacity {
                cpu_cores: 0,
                memory_mb: 0,
                gpu_count: 0,
                disk_space_mb: 0,
            }),
            resource_reservation_policy: ResourceReservationPolicy::BestEffort,
        };

        let data_manager = DataManager {
            node_id: node_id.to_string(),
            data_cache: RwLock::new(HashMap::new()),
            cache_size_limit_mb,
            cache_policy: CachePolicy::LRU,
        };

        DistributedComputeFramework {
            node_id: node_id.to_string(),
            task_scheduler,
            worker_manager,
            resource_manager,
            data_manager,
        }
    }

    fn submit_task(&self, task: Task) -> Result<String, String> {
        // éªŒè¯ä»»åŠ¡
        if task.resource_requirements.cpu_cores > self.resource_manager.total_resources.cpu_cores ||
           task.resource_requirements.memory_mb > self.resource_manager.total_resources.memory_mb ||
           task.resource_requirements.gpu_count > self.resource_manager.total_resources.gpu_count ||
           task.resource_requirements.disk_space_mb > self.resource_manager.total_resources.disk_space_mb {
            return Err("ä»»åŠ¡èµ„æºéœ€æ±‚è¶…è¿‡ç³»ç»Ÿæ€»èµ„æº".to_string());
        }

        // æ£€æŸ¥ä¾èµ–ä»»åŠ¡æ˜¯å¦å®Œæˆ
        let completed_tasks = self.task_scheduler.completed_tasks.read().unwrap();
        for dep_id in &task.dependencies {
            if !completed_tasks.contains_key(dep_id) {
                return Err(format!("ä¾èµ–ä»»åŠ¡æœªå®Œæˆ: {}", dep_id));
            }
        }

        // æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
        let mut task_queue = self.task_scheduler.task_queue.write().unwrap();
        task_queue.push(task.clone());

        // åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        let mut active_tasks = self.task_scheduler.active_tasks.write().unwrap();
        active_tasks.insert(task.id.clone(), TaskStatus {
            task_id: task.id.clone(),
            state: TaskState::Pending,
            worker_id: None,
            started_at: None,
            progress: 0.0,
            error_message: None,
        });

        println!("æäº¤ä»»åŠ¡: {}", task.id);

        // å°è¯•è°ƒåº¦ä»»åŠ¡
        drop(task_queue);
        drop(active_tasks);
        self.schedule_tasks()?;

        Ok(task.id)
    }

    fn schedule_tasks(&self) -> Result<usize, String> {
        println!("å¼€å§‹è°ƒåº¦ä»»åŠ¡...");

        let mut scheduled_count = 0;

        // è·å–ä»»åŠ¡é˜Ÿåˆ—
        let mut task_queue = self.task_scheduler.task_queue.write().unwrap();
        if task_queue.is_empty() {
            println!("æ²¡æœ‰ä»»åŠ¡éœ€è¦è°ƒåº¦");
            return Ok(0);
        }

        // æ ¹æ®è°ƒåº¦ç­–ç•¥å¯¹ä»»åŠ¡æ’åº
        match self.task_scheduler.scheduling_policy {
            SchedulingPolicy::FIFO => {
                // å·²ç»æ˜¯FIFOé¡ºåºï¼Œä¸éœ€è¦æ’åº
            },
            SchedulingPolicy::Priority => {
                task_queue.sort_by(|a, b| b.priority.cmp(&a.priority));
            },
            SchedulingPolicy::FairShare => {
                // ç®€åŒ–å®ç°ï¼šæŒ‰ç…§åˆ›å»ºæ—¶é—´æ’åº
                task_queue.sort_by(|a, b| a.created_at.cmp(&b.created_at));
            },
            SchedulingPolicy::ResourceAware => {
                // ç®€åŒ–å®ç°ï¼šä¼˜å…ˆè°ƒåº¦èµ„æºéœ€æ±‚å°çš„ä»»åŠ¡
                task_queue.sort_by(|a, b| {
                    let a_total = a.resource_requirements.cpu_cores as u64 +
                                 a.resource_requirements.memory_mb / 1024 +
                                 a.resource_requirements.gpu_count as u64 * 10;
                    let b_total = b.resource_requirements.cpu_cores as u64 +
                                 b.resource_requirements.memory_mb / 1024 +
                                 b.resource_requirements.gpu_count as u64 * 10;
                    a_total.cmp(&b_total)
                });
            },
        }

        // è·å–å¯ç”¨å·¥ä½œèŠ‚ç‚¹
        let workers = self.worker_manager.workers.read().unwrap();
        if workers.is_empty() {
            println!("æ²¡æœ‰å¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹");
            return Ok(0);
        }

        // è·å–å·²åˆ†é…çš„èµ„æº
        let mut allocated_resources = self.resource_manager.allocated_resources.write().unwrap();

        // è·å–æ´»åŠ¨ä»»åŠ¡çŠ¶æ€
        let mut active_tasks = self.task_scheduler.active_tasks.write().unwrap();

        // å°è¯•ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…èµ„æºå’Œå·¥ä½œèŠ‚ç‚¹
        let mut i = 0;
        while i < task_queue.len() {
            let task = &task_queue[i];

            // æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„èµ„æº
            let has_resources = allocated_resources.cpu_cores + task.resource_requirements.cpu_cores <= self.resource_manager.total_resources.cpu_cores &&
                              allocated_resources.memory_mb + task.resource_requirements.memory_mb <= self.resource_manager.total_resources.memory_mb &&
                              allocated_resources.gpu_count + task.resource_requirements.gpu_count <= self.resource_manager.total_resources.gpu_count &&
                              allocated_resources.disk_space_mb + task.resource_requirements.disk_space_mb <= self.resource_manager.total_resources.disk_space_mb;

            if !has_resources {
                println!("èµ„æºä¸è¶³ï¼Œæ— æ³•è°ƒåº¦ä»»åŠ¡: {}", task.id);
                i += 1;
                continue;
            }

            // æŸ¥æ‰¾åˆé€‚çš„å·¥ä½œèŠ‚ç‚¹
            let mut selected_worker = None;

            for (worker_id, worker_info) in workers.iter() {
                if worker_info.status != WorkerStatus::Online {
                    continue;
                }

                // æ£€æŸ¥å·¥ä½œèŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿèµ„æº
                let worker_has_resources = worker_info.capabilities.cpu_cores >= task.resource_requirements.cpu_cores &&
                                         worker_info.capabilities.memory_mb >= task.resource_requirements.memory_mb &&
                                         worker_info.capabilities.gpu_count >= task.resource_requirements.gpu_count &&
                                         worker_info.capabilities.disk_space_mb >= task.resource_requirements.disk_space_mb;

                if worker_has_resources {
                    selected_worker = Some(worker_id.clone());
                    break;
                }
            }

            if let Some(worker_id) = selected_worker {
                println!("ä¸ºä»»åŠ¡ {} åˆ†é…å·¥ä½œèŠ‚ç‚¹: {}", task.id, worker_id);

                // æ›´æ–°ä»»åŠ¡çŠ¶æ€
                let task_id = task.id.clone();
                active_tasks.insert(task_id.clone(), TaskStatus {
                    task_id: task_id.clone(),
                    state: TaskState::Scheduled,
                    worker_id: Some(worker_id.clone()),
                    started_at: Some(SystemTime::now()),
                    progress: 0.0,
                    error_message: None,
                });

                // æ›´æ–°å·²åˆ†é…èµ„æº
                allocated_resources.cpu_cores += task.resource_requirements.cpu_cores;
                allocated_resources.memory_mb += task.resource_requirements.memory_mb;
                allocated_resources.gpu_count += task.resource_requirements.gpu_count;
                allocated_resources.disk_space_mb += task.resource_requirements.disk_space_mb;

                // ä»é˜Ÿåˆ—ä¸­ç§»é™¤ä»»åŠ¡
                task_queue.remove(i);

                // å°†ä»»åŠ¡åˆ†æ´¾ç»™å·¥ä½œèŠ‚ç‚¹ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ¶‰åŠç½‘ç»œé€šä¿¡ï¼‰
                // self.dispatch_task_to_worker(&task, &worker_id);

                scheduled_count += 1;
            } else {
                println!("æ²¡æœ‰åˆé€‚çš„å·¥ä½œèŠ‚ç‚¹ç”¨äºä»»åŠ¡: {}", task.id);
                i += 1;
            }
        }

        println!("æˆåŠŸè°ƒåº¦ {} ä¸ªä»»åŠ¡", scheduled_count);

        Ok(scheduled_count)
    }

    fn register_worker(&self, worker_info: WorkerInfo) -> Result<(), String> {
        println!("æ³¨å†Œå·¥ä½œèŠ‚ç‚¹: {}", worker_info.id);

        let mut workers = self.worker_manager.workers.write().unwrap();

        if workers.len() >= self.worker_manager.worker_capacity as usize {
            return Err("å·²è¾¾åˆ°å·¥ä½œèŠ‚ç‚¹å®¹é‡ä¸Šé™".to_string());
        }

        workers.insert(worker_info.id.clone(), worker_info);

        Ok(())
    }

    fn update_task_status(&self, task_id: &str, status: TaskStatus) -> Result<(), String> {
        println!("æ›´æ–°ä»»åŠ¡çŠ¶æ€: {}, çŠ¶æ€: {:?}", task_id, status.state);

        let mut active_tasks = self.task_scheduler.active_tasks.write().unwrap();

        if !active_tasks.contains_key(task_id) {
            return Err(format!("ä»»åŠ¡ {} ä¸å­˜åœ¨æˆ–å·²å®Œæˆ", task_id));
        }

        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
        active_tasks.insert(task_id.to_string(), status.clone());

        // å¦‚æœä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼Œé‡Šæ”¾èµ„æº
        if let TaskState::Completed | TaskState::Failed | TaskState::Cancelled | TaskState::Timeout = status.state {
            // è·å–ä»»åŠ¡ä¿¡æ¯
            let task_queue = self.task_scheduler.task_queue.read().unwrap();
            let task = task_queue.iter().find(|t| t.id == task_id);

            if let Some(task) = task {
                // é‡Šæ”¾èµ„æº
                let mut allocated_resources = self.resource_manager.allocated_resources.write().unwrap();
                allocated_resources.cpu_cores -= task.resource_requirements.cpu_cores;
                allocated_resources.memory_mb -= task.resource_requirements.memory_mb;
                allocated_resources.gpu_count -= task.resource_requirements.gpu_count;
                allocated_resources.disk_space_mb -= task.resource_requirements.disk_space_mb;
            }

            // ä»æ´»åŠ¨ä»»åŠ¡ä¸­ç§»é™¤
            active_tasks.remove(task_id);

            // å¦‚æœä»»åŠ¡å®Œæˆï¼Œæ·»åŠ åˆ°å®Œæˆä»»åŠ¡åˆ—è¡¨
            if let TaskState::Completed = status.state {
                let mut completed_tasks = self.task_scheduler.completed_tasks.write().unwrap();
                completed_tasks.insert(task_id.to_string(), TaskResult {
                    task_id: task_id.to_string(),
                    success: true,
                    result_data: None, // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŒ…å«ä»»åŠ¡ç»“æœ
                    error_message: None,
                    execution_time: status.started_at.unwrap().elapsed().unwrap_or(Duration::from_secs(0)),
                    resource_usage: ResourceUsage {
                        cpu_usage: 0.0,
                        memory_peak_mb: 0,
                        gpu_usage: 0.0,
                        disk_io_read_mb: 0,
                        disk_io_write_mb: 0,
                        network_io_read_mb: 0,
                        network_io_write_mb: 0,
                    },
                });
            }
        }

        Ok(())
    }

    fn get_task_status(&self, task_id: &str) -> Result<Option<TaskStatus>, String> {
        // é¦–å…ˆæ£€æŸ¥æ´»åŠ¨ä»»åŠ¡
        let active_tasks = self.task_scheduler.active_tasks.read().unwrap();
        if let Some(status) = active_tasks.get(task_id) {
            return Ok(Some(status.clone()));
        }

        // ç„¶åæ£€æŸ¥å·²å®Œæˆä»»åŠ¡
        let completed_tasks = self.task_scheduler.completed_tasks.read().unwrap();
        if let Some(result) = completed_tasks.get(task_id) {
            let state = if result.success {
                TaskState::Completed
            } else {
                TaskState::Failed
            };

            return Ok(Some(TaskStatus {
                task_id: task_id.to_string(),
                state,
                worker_id: None, // å·²å®Œæˆä»»åŠ¡å¯èƒ½æ²¡æœ‰å·¥ä½œèŠ‚ç‚¹ä¿¡æ¯
                started_at: None,
                progress: 1.0,
                error_message: result.error_message.clone(),
            }));
        }

        Ok(None)
    }

    fn cancel_task(&self, task_id: &str) -> Result<bool, String> {
        println!("å–æ¶ˆä»»åŠ¡: {}", task_id);

        // é¦–å…ˆæ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—
        let mut task_queue = self.task_scheduler.task_queue.write().unwrap();
        let queue_index = task_queue.iter().position(|t| t.id == task_id);

        if let Some(index) = queue_index {
            // ä»»åŠ¡è¿˜åœ¨é˜Ÿåˆ—ä¸­ï¼Œç›´æ¥ç§»é™¤
            task_queue.remove(index);
            println!("ä»é˜Ÿåˆ—ä¸­ç§»é™¤ä»»åŠ¡: {}", task_id);
            return Ok(true);
        }

        // æ£€æŸ¥æ´»åŠ¨ä»»åŠ¡
        let mut active_tasks = self.task_scheduler.active_tasks.write().unwrap();
        if let Some(status) = active_tasks.get(task_id) {
            match status.state {
                TaskState::Completed | TaskState::Failed | TaskState::Cancelled | TaskState::Timeout => {
                    return Err(format!("ä»»åŠ¡ {} å·²å¤„äºæœ€ç»ˆçŠ¶æ€: {:?}", task_id, status.state));
                },
                _ => {
                    // æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å–æ¶ˆ
                    let mut new_status = status.clone();
                    new_status.state = TaskState::Cancelled;
                    active_tasks.insert(task_id.to_string(), new_status);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€šçŸ¥å·¥ä½œèŠ‚ç‚¹å–æ¶ˆä»»åŠ¡

                    println!("å–æ¶ˆæ´»åŠ¨ä»»åŠ¡: {}", task_id);
                    return Ok(true);
                }
            }
        }

        Err(format!("ä»»åŠ¡ {} ä¸å­˜åœ¨", task_id))
    }

    fn cache_data(&self, data_id: &str, data: &[u8]) -> Result<(), String> {
        let mut data_cache = self.data_manager.data_cache.write().unwrap();

        // æ£€æŸ¥ç¼“å­˜å¤§å°
        let data_size = data.len() as u64 / (1024 * 1024); // è½¬æ¢ä¸ºMB
        let current_cache_size: u64 = data_cache.values().map(|d| d.size).sum();

        if current_cache_size + data_size > self.data_manager.cache_size_limit_mb {
            // ç¼“å­˜ç©ºé—´ä¸è¶³ï¼Œéœ€è¦æ¸…ç†
            match self.data_manager.cache_policy {
                CachePolicy::LRU => {
                    // æŒ‰æœ€è¿‘è®¿é—®æ—¶é—´æ’åº
                    let mut entries: Vec<_> = data_cache.iter().collect();
                    entries.sort_by(|a, b| a.1.last_accessed.cmp(&b.1.last_accessed));

                    // æ¸…ç†ç¼“å­˜ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
                    let mut freed_space = 0;
                    for (key, entry) in entries {
                        data_cache.remove(key);
                        freed_space += entry.size;
                        if current_cache_size + data_size - freed_space <= self.data_manager.cache_size_limit_mb {
                            break;
                        }
                    }
                },
                CachePolicy::LFU => {
                    // æŒ‰è®¿é—®æ¬¡æ•°æ’åº
                    let mut entries: Vec<_> = data_cache.iter().collect();
                    entries.sort_by(|a, b| a.1.access_count.cmp(&b.1.access_count));

                    // æ¸…ç†ç¼“å­˜ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
                    let mut freed_space = 0;
                    for (key, entry) in entries {
                        data_cache.remove(key);
                        freed_space += entry.size;
                        if current_cache_size + data_size - freed_space <= self.data_manager.cache_size_limit_mb {
                            break;
                        }
                    }
                },
                CachePolicy::FIFO => {
                    // æŒ‰æ·»åŠ é¡ºåºæ’åºï¼ˆç®€åŒ–ï¼Œä½¿ç”¨æœ€åè®¿é—®æ—¶é—´ä½œä¸ºä»£ç†ï¼‰
                    let mut entries: Vec<_> = data_cache.iter().collect();
                    entries.sort_by(|a, b| a.1.last_accessed.cmp(&b.1.last_accessed));

                    // æ¸…ç†ç¼“å­˜ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
                    let mut freed_space = 0;
                    for (key, entry) in entries {
                        data_cache.remove(key);
                        freed_space += entry.size;
                        if current_cache_size + data_size - freed_space <= self.data_manager.cache_size_limit_mb {
                            break;
                        }
                    }
                },
            }
        }

        // æ·»åŠ åˆ°ç¼“å­˜
        data_cache.insert(data_id.to_string(), CachedData {
            id: data_id.to_string(),
            data: data.to_vec(),
            size: data_size,
            last_accessed: SystemTime::now(),
            access_count: 1,
        });

        println!("æ•°æ®å·²ç¼“å­˜: {}, å¤§å°: {} MB", data_id, data_size);

        Ok(())
    }

    fn get_cached_data(&self, data_id: &str) -> Result<Option<Vec<u8>>, String> {
        let mut data_cache = self.data_manager.data_cache.write().unwrap();

        if let Some(entry) = data_cache.get_mut(data_id) {
            // æ›´æ–°è®¿é—®ä¿¡æ¯
            entry.last_accessed = SystemTime::now();
            entry.access_count += 1;

            return Ok(Some(entry.data.clone()));
        }

        Ok(None)
    }
}

// å¾®æœåŠ¡æ³¨å†Œä¸å‘ç°ç³»ç»Ÿ
struct MicroserviceRegistry {
    node_id: String,
    services: RwLock<HashMap<String, ServiceInfo>>,
    instances: RwLock<HashMap<String, Vec<ServiceInstance>>>,
    health_checker: HealthChecker,
}

struct ServiceInfo {
    id: String,
    name: String,
    version: String,
    description: Option<String>,
    endpoints: Vec<EndpointInfo>,
    metadata: HashMap<String, String>,
    created_at: SystemTime,
    updated_at: SystemTime,
}

struct EndpointInfo {
    name: String,
    method: HttpMethod,
    path: String,
    description: Option<String>,
    request_schema: Option<String>,
    response_schema: Option<String>,
}

enum HttpMethod {
    GET,
    POST,
    PUT,
    DELETE,
    PATCH,
    HEAD,
    OPTIONS,
}

struct ServiceInstance {
    id: String,
    service_id: String,
    host: String,
    port: u16,
    status: InstanceStatus,
    metadata: HashMap<String, String>,
    registered_at: SystemTime,
    last_heartbeat: SystemTime,
    health_check: HealthCheckConfig,
}

enum InstanceStatus {
    UP,
    DOWN,
    STARTING,
    OUT_OF_SERVICE,
    UNKNOWN,
}

struct HealthCheckConfig {
    check_type: HealthCheckType,
    endpoint: Option<String>,
    interval: Duration,
    timeout: Duration,
    retry_count: u32,
}

enum HealthCheckType {
    HTTP,
    TCP,
    SCRIPT,
    NONE,
}

struct HealthChecker {
    check_interval: Duration,
    running: AtomicBool,
    check_thread: Option<JoinHandle<()>>,
}

impl MicroserviceRegistry {
    fn new(node_id: &str) -> Self {
        let health_checker = HealthChecker {
            check_interval: Duration::from_secs(30),
            running: AtomicBool::new(false),
            check_thread: None,
        };

        MicroserviceRegistry {
            node_id: node_id.to_string(),
            services: RwLock::new(HashMap::new()),
            instances: RwLock::new(HashMap::new()),
            health_checker,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¾®æœåŠ¡æ³¨å†Œä¸­å¿ƒ: {}", self.node_id);

        // å¯åŠ¨å¥åº·æ£€æŸ¥çº¿ç¨‹
        let registry_services = Arc::clone(&self.services);
        let registry_instances = Arc::clone(&self.instances);
        let check_interval = self.health_checker.check_interval;

        self.health_checker.running.store(true, Ordering::SeqCst);

        let thread = thread::spawn(move || {
            while running_flag.load(Ordering::SeqCst) {
                // æ‰§è¡Œå¥åº·æ£€æŸ¥
                Self::check_instances_health(&registry_services, &registry_instances);

                // ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æŸ¥
                thread::sleep(check_interval);
            }
        });

        self.health_checker.check_thread = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢å¾®æœåŠ¡æ³¨å†Œä¸­å¿ƒ: {}", self.node_id);

        // åœæ­¢å¥åº·æ£€æŸ¥çº¿ç¨‹
        self.health_checker.running.store(false, Ordering::SeqCst);

        if let Some(thread) = self.health_checker.check_thread.take() {
            if let Err(err) = thread.join() {
                println!("å¥åº·æ£€æŸ¥çº¿ç¨‹å…³é—­å‡ºé”™: {:?}", err);
            }
        }

        Ok(())
    }

    fn register_service(&self, service: ServiceInfo) -> Result<(), String> {
        println!("æ³¨å†ŒæœåŠ¡: {}", service.name);

        let mut services = self.services.write().unwrap();

        if services.contains_key(&service.id) {
            return Err(format!("æœåŠ¡ {} å·²å­˜åœ¨", service.id));
        }

        services.insert(service.id.clone(), service);

        Ok(())
    }

    fn register_instance(&self, instance: ServiceInstance) -> Result<(), String> {
        println!("æ³¨å†ŒæœåŠ¡å®ä¾‹: {}, æœåŠ¡: {}", instance.id, instance.service_id);

        // æ£€æŸ¥æœåŠ¡æ˜¯å¦å­˜åœ¨
        let services = self.services.read().unwrap();
        if !services.contains_key(&instance.service_id) {
            return Err(format!("æœåŠ¡ {} ä¸å­˜åœ¨", instance.service_id));
        }

        let mut instances = self.instances.write().unwrap();

        // è·å–æˆ–åˆ›å»ºæœåŠ¡å®ä¾‹åˆ—è¡¨
        let service_instances = instances.entry(instance.service_id.clone()).or_insert_with(Vec::new);

        // æ£€æŸ¥å®ä¾‹æ˜¯å¦å·²å­˜åœ¨
        if let Some(pos) = service_instances.iter().position(|i| i.id == instance.id) {
            service_instances[pos] = instance;
        } else {
            service_instances.push(instance);
        }

        Ok(())
    }

    fn deregister_instance(&self, service_id: &str, instance_id: &str) -> Result<bool, String> {
        println!("æ³¨é”€æœåŠ¡å®ä¾‹: {}, æœåŠ¡: {}", instance_id, service_id);

        let mut instances = self.instances.write().unwrap();

        if let Some(service_instances) = instances.get_mut(service_id) {
            let len_before = service_instances.len();
            service_instances.retain(|i| i.id != instance_id);
            let removed = len_before > service_instances.len();

            if service_instances.is_empty() {
                instances.remove(service_id);
            }

            return Ok(removed);
        }

        Ok(false)
    }

    fn deregister_service(&self, service_id: &str) -> Result<bool, String> {
        println!("æ³¨é”€æœåŠ¡: {}", service_id);

        let mut services = self.services.write().unwrap();
        let removed_service = services.remove(service_id).is_some();

        // åˆ é™¤æ‰€æœ‰ç›¸å…³çš„å®ä¾‹
        let mut instances = self.instances.write().unwrap();
        instances.remove(service_id);

        Ok(removed_service)
    }

    fn get_service(&self, service_id: &str) -> Result<Option<ServiceInfo>, String> {
        let services = self.services.read().unwrap();
        Ok(services.get(service_id).cloned())
    }

    fn get_service_by_name(&self, name: &str, version: Option<&str>) -> Result<Vec<ServiceInfo>, String> {
        let services = self.services.read().unwrap();

        let matching_services: Vec<ServiceInfo> = services.values()
            .filter(|s| s.name == name && (version.is_none() || Some(s.version.as_str()) == version))
            .cloned()
            .collect();

        Ok(matching_services)
    }

    fn get_instances(&self, service_id: &str) -> Result<Vec<ServiceInstance>, String> {
        let instances = self.instances.read().unwrap();

        if let Some(service_instances) = instances.get(service_id) {
            return Ok(service_instances.clone());
        }

        Ok(Vec::new())
    }

    fn get_instance(&self, service_id: &str, instance_id: &str) -> Result<Option<ServiceInstance>, String> {
        let instances = self.instances.read().unwrap();

        if let Some(service_instances) = instances.get(service_id) {
            return Ok(service_instances.iter()
                .find(|i| i.id == instance_id)
                .cloned());
        }

        Ok(None)
    }

    fn heartbeat(&self, service_id: &str, instance_id: &str) -> Result<bool, String> {
        println!("æ¥æ”¶å¿ƒè·³: å®ä¾‹ {}, æœåŠ¡ {}", instance_id, service_id);

        let mut instances = self.instances.write().unwrap();

        if let Some(service_instances) = instances.get_mut(service_id) {
            if let Some(instance) = service_instances.iter_mut().find(|i| i.id == instance_id) {
                instance.last_heartbeat = SystemTime::now();

                // å¦‚æœå®ä¾‹çŠ¶æ€ä¸ºDOWNï¼Œæ›´æ–°ä¸ºUP
                if instance.status == InstanceStatus::DOWN {
                    instance.status = InstanceStatus::UP;
                }

                return Ok(true);
            }
        }

        Ok(false)
    }

    fn set_instance_status(&self, service_id: &str, instance_id: &str, status: InstanceStatus) -> Result<bool, String> {
        println!("è®¾ç½®å®ä¾‹çŠ¶æ€: å®ä¾‹ {}, æœåŠ¡ {}, çŠ¶æ€: {:?}", instance_id, service_id, status);

        let mut instances = self.instances.write().unwrap();

        if let Some(service_instances) = instances.get_mut(service_id) {
            if let Some(instance) = service_instances.iter_mut().find(|i| i.id == instance_id) {
                instance.status = status;
                return Ok(true);
            }
        }

        Ok(false)
    }

    fn check_instances_health(services: &RwLock<HashMap<String, ServiceInfo>>, instances: &RwLock<HashMap<String, Vec<ServiceInstance>>>) {
        println!("æ‰§è¡ŒæœåŠ¡å®ä¾‹å¥åº·æ£€æŸ¥");

        let mut instances_write = instances.write().unwrap();

        for (service_id, service_instances) in instances_write.iter_mut() {
            for instance in service_instances.iter_mut() {
                // æ£€æŸ¥ä¸Šæ¬¡å¿ƒè·³æ—¶é—´
                if let Ok(duration) = SystemTime::now().duration_since(instance.last_heartbeat) {
                    // å¦‚æœè¶…è¿‡å¿ƒè·³é—´éš”çš„3å€ï¼Œæ ‡è®°ä¸ºDOWN
                    if duration > instance.health_check.interval.mul_f32(3.0) {
                        println!("å®ä¾‹ {} å¿ƒè·³è¶…æ—¶ï¼Œæ ‡è®°ä¸ºDOWN", instance.id);
                        instance.status = InstanceStatus::DOWN;
                        continue;
                    }
                }

                // æ ¹æ®å¥åº·æ£€æŸ¥ç±»å‹æ‰§è¡Œæ£€æŸ¥
                match instance.health_check.check_type {
                    HealthCheckType::HTTP => {
                        if let Some(endpoint) = &instance.health_check.endpoint {
                            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€HTTPè¯·æ±‚æ£€æŸ¥å¥åº·çŠ¶æ€
                            println!("æ‰§è¡ŒHTTPå¥åº·æ£€æŸ¥: {}", endpoint);
                        }
                    },
                    HealthCheckType::TCP => {
                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°è¯•å»ºç«‹TCPè¿æ¥æ£€æŸ¥å¥åº·çŠ¶æ€
                        println!("æ‰§è¡ŒTCPå¥åº·æ£€æŸ¥: {}:{}", instance.host, instance.port);
                    },
                    HealthCheckType::SCRIPT => {
                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡Œè„šæœ¬æ£€æŸ¥å¥åº·çŠ¶æ€
                        println!("æ‰§è¡Œè„šæœ¬å¥åº·æ£€æŸ¥");
                    },
                    HealthCheckType::NONE => {
                        // ä¸æ‰§è¡Œå¥åº·æ£€æŸ¥
                    },
                }
            }
        }

        println!("å¥åº·æ£€æŸ¥å®Œæˆ");
    }
}

// æœåŠ¡ç½‘å…³
struct ServiceGateway {
    node_id: String,
    routes: RwLock<HashMap<String, RouteConfig>>,
    service_registry: Arc<MicroserviceRegistry>,
    load_balancer: Box<dyn LoadBalancer>,
    rate_limiter: Arc<RateLimiter>,
    circuit_breaker: Arc<CircuitBreaker>,
}

struct RouteConfig {
    id: String,
    path: String,
    service_id: String,
    strip_prefix: bool,
    rewrite_path: Option<String>,
    filters: Vec<FilterConfig>,
    rate_limit: Option<RateLimitConfig>,
    circuit_breaker: Option<CircuitBreakerConfig>,
}

struct FilterConfig {
    name: String,
    config: HashMap<String, String>,
}

struct RateLimitConfig {
    requests_per_second: u32,
    burst: u32,
}

struct CircuitBreakerConfig {
    failure_threshold: f64,
    reset_timeout: Duration,
    request_volume_threshold: u32,
}

trait LoadBalancer: Send + Sync {
    fn choose_instance(&self, instances: &[ServiceInstance]) -> Option<&ServiceInstance>;
}

struct RoundRobinLoadBalancer {
    counters: RwLock<HashMap<String, AtomicUsize>>,
}

impl LoadBalancer for RoundRobinLoadBalancer {
    fn choose_instance(&self, instances: &[ServiceInstance]) -> Option<&ServiceInstance> {
        if instances.is_empty() {
            return None;
        }

        // åªé€‰æ‹©çŠ¶æ€ä¸ºUPçš„å®ä¾‹
        let up_instances: Vec<&ServiceInstance> = instances.iter()
            .filter

```rust
            .filter(|i| i.status == InstanceStatus::UP)
            .collect();

        if up_instances.is_empty() {
            return None;
        }

        // è·å–æˆ–åˆ›å»ºè®¡æ•°å™¨
        let service_id = if !up_instances.is_empty() {
            up_instances[0].service_id.clone()
        } else {
            return None;
        };

        let mut counters = self.counters.write().unwrap();
        let counter = counters.entry(service_id).or_insert_with(|| AtomicUsize::new(0));

        // é€’å¢è®¡æ•°å™¨å¹¶å–æ¨¡
        let index = counter.fetch_add(1, Ordering::SeqCst) % up_instances.len();

        Some(up_instances[index])
    }
}

struct RandomLoadBalancer;

impl LoadBalancer for RandomLoadBalancer {
    fn choose_instance(&self, instances: &[ServiceInstance]) -> Option<&ServiceInstance> {
        if instances.is_empty() {
            return None;
        }

        // åªé€‰æ‹©çŠ¶æ€ä¸ºUPçš„å®ä¾‹
        let up_instances: Vec<&ServiceInstance> = instances.iter()
            .filter(|i| i.status == InstanceStatus::UP)
            .collect();

        if up_instances.is_empty() {
            return None;
        }

        // éšæœºé€‰æ‹©ä¸€ä¸ªå®ä¾‹
        let mut rng = rand::thread_rng();
        let index = rng.gen_range(0, up_instances.len());

        Some(up_instances[index])
    }
}

struct RateLimiter {
    rate_limits: RwLock<HashMap<String, TokenBucket>>,
}

struct TokenBucket {
    tokens: AtomicUsize,
    capacity: usize,
    refill_rate: f64,  // æ¯ç§’è¡¥å……çš„ä»¤ç‰Œæ•°
    last_refill: Mutex<Instant>,
}

impl RateLimiter {
    fn new() -> Self {
        RateLimiter {
            rate_limits: RwLock::new(HashMap::new()),
        }
    }

    fn add_rate_limit(&self, key: &str, requests_per_second: u32, burst: u32) {
        let mut rate_limits = self.rate_limits.write().unwrap();

        rate_limits.insert(key.to_string(), TokenBucket {
            tokens: AtomicUsize::new(burst as usize),
            capacity: burst as usize,
            refill_rate: requests_per_second as f64,
            last_refill: Mutex::new(Instant::now()),
        });
    }

    fn try_acquire(&self, key: &str, tokens: usize) -> bool {
        let rate_limits = self.rate_limits.read().unwrap();

        if let Some(bucket) = rate_limits.get(key) {
            // å°è¯•åˆ·æ–°ä»¤ç‰Œ
            self.refill_tokens(bucket);

            // å°è¯•è·å–ä»¤ç‰Œ
            let current = bucket.tokens.load(Ordering::SeqCst);
            if current >= tokens {
                // ä½¿ç”¨CASæ“ä½œå°è¯•å‡å°‘ä»¤ç‰Œ
                let mut current = current;
                while current >= tokens {
                    match bucket.tokens.compare_exchange(
                        current,
                        current - tokens,
                        Ordering::SeqCst,
                        Ordering::SeqCst,
                    ) {
                        Ok(_) => return true,
                        Err(actual) => current = actual,
                    }
                }
            }

            false
        } else {
            // å¦‚æœæ²¡æœ‰é’ˆå¯¹æ­¤é”®çš„é€Ÿç‡é™åˆ¶ï¼Œåˆ™å…è®¸è¯·æ±‚
            true
        }
    }

    fn refill_tokens(&self, bucket: &TokenBucket) {
        let mut last_refill = bucket.last_refill.lock().unwrap();
        let now = Instant::now();
        let elapsed = now.duration_since(*last_refill).as_secs_f64();

        if elapsed > 0.0 {
            let new_tokens = (elapsed * bucket.refill_rate) as usize;
            if new_tokens > 0 {
                // æ·»åŠ æ–°ä»¤ç‰Œï¼Œä½†ä¸è¶…è¿‡å®¹é‡
                let current = bucket.tokens.load(Ordering::SeqCst);
                let new_count = std::cmp::min(current + new_tokens, bucket.capacity);
                bucket.tokens.store(new_count, Ordering::SeqCst);
                *last_refill = now;
            }
        }
    }
}

struct CircuitBreaker {
    circuits: RwLock<HashMap<String, Circuit>>,
}

enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

struct Circuit {
    state: AtomicU8,
    failure_threshold: f64,
    reset_timeout: Duration,
    request_volume_threshold: u32,
    last_state_change: Mutex<Instant>,
    success_count: AtomicUsize,
    failure_count: AtomicUsize,
    total_count: AtomicUsize,
}

impl CircuitBreaker {
    fn new() -> Self {
        CircuitBreaker {
            circuits: RwLock::new(HashMap::new()),
        }
    }

    fn add_circuit(&self, key: &str, config: CircuitBreakerConfig) {
        let mut circuits = self.circuits.write().unwrap();

        circuits.insert(key.to_string(), Circuit {
            state: AtomicU8::new(0), // 0 = Closed, 1 = Open, 2 = HalfOpen
            failure_threshold: config.failure_threshold,
            reset_timeout: config.reset_timeout,
            request_volume_threshold: config.request_volume_threshold,
            last_state_change: Mutex::new(Instant::now()),
            success_count: AtomicUsize::new(0),
            failure_count: AtomicUsize::new(0),
            total_count: AtomicUsize::new(0),
        });
    }

    fn allow_request(&self, key: &str) -> bool {
        let circuits = self.circuits.read().unwrap();

        if let Some(circuit) = circuits.get(key) {
            // è·å–å½“å‰çŠ¶æ€
            let state = match circuit.state.load(Ordering::SeqCst) {
                0 => CircuitState::Closed,
                1 => CircuitState::Open,
                _ => CircuitState::HalfOpen,
            };

            match state {
                CircuitState::Closed => {
                    // åœ¨å…³é—­çŠ¶æ€ä¸‹ï¼Œå§‹ç»ˆå…è®¸è¯·æ±‚
                    true
                },
                CircuitState::Open => {
                    // åœ¨å¼€è·¯çŠ¶æ€ä¸‹ï¼Œæ£€æŸ¥æ˜¯å¦å·²è¿‡é‡ç½®è¶…æ—¶
                    let mut last_state_change = circuit.last_state_change.lock().unwrap();
                    let now = Instant::now();
                    let elapsed = now.duration_since(*last_state_change);

                    if elapsed >= circuit.reset_timeout {
                        // åˆ‡æ¢åˆ°åŠå¼€çŠ¶æ€
                        circuit.state.store(2, Ordering::SeqCst);
                        *last_state_change = now;
                        // å…è®¸ä¸€ä¸ªè¯·æ±‚
                        true
                    } else {
                        // åœ¨å¼€è·¯çŠ¶æ€ä¸‹ä¸å…è®¸è¯·æ±‚
                        false
                    }
                },
                CircuitState::HalfOpen => {
                    // åœ¨åŠå¼€çŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬åªå…è®¸ä¸€ä¸ªè¯·æ±‚é€šè¿‡
                    // ä½¿ç”¨CASæ“ä½œæ¥ç¡®ä¿åªæœ‰ä¸€ä¸ªè¯·æ±‚é€šè¿‡
                    let total = circuit.total_count.load(Ordering::SeqCst);
                    circuit.total_count.compare_exchange(
                        total,
                        total + 1,
                        Ordering::SeqCst,
                        Ordering::SeqCst,
                    ).is_ok()
                }
            }
        } else {
            // å¦‚æœæ²¡æœ‰é’ˆå¯¹æ­¤é”®çš„æ–­è·¯å™¨ï¼Œåˆ™å…è®¸è¯·æ±‚
            true
        }
    }

    fn record_success(&self, key: &str) {
        let circuits = self.circuits.read().unwrap();

        if let Some(circuit) = circuits.get(key) {
            // å¢åŠ æˆåŠŸè®¡æ•°
            circuit.success_count.fetch_add(1, Ordering::SeqCst);
            circuit.total_count.fetch_add(1, Ordering::SeqCst);

            // è·å–å½“å‰çŠ¶æ€
            let state = match circuit.state.load(Ordering::SeqCst) {
                0 => CircuitState::Closed,
                1 => CircuitState::Open,
                _ => CircuitState::HalfOpen,
            };

            if state == CircuitState::HalfOpen {
                // åœ¨åŠå¼€çŠ¶æ€ä¸‹ï¼Œå¦‚æœæˆåŠŸï¼Œæˆ‘ä»¬åˆ‡æ¢å›å…³é—­çŠ¶æ€
                circuit.state.store(0, Ordering::SeqCst);
                let mut last_state_change = circuit.last_state_change.lock().unwrap();
                *last_state_change = Instant::now();

                // é‡ç½®è®¡æ•°å™¨
                circuit.success_count.store(0, Ordering::SeqCst);
                circuit.failure_count.store(0, Ordering::SeqCst);
                circuit.total_count.store(0, Ordering::SeqCst);
            }
        }
    }

    fn record_failure(&self, key: &str) {
        let circuits = self.circuits.read().unwrap();

        if let Some(circuit) = circuits.get(key) {
            // å¢åŠ å¤±è´¥è®¡æ•°
            circuit.failure_count.fetch_add(1, Ordering::SeqCst);
            circuit.total_count.fetch_add(1, Ordering::SeqCst);

            // è·å–å½“å‰çŠ¶æ€
            let state = match circuit.state.load(Ordering::SeqCst) {
                0 => CircuitState::Closed,
                1 => CircuitState::Open,
                _ => CircuitState::HalfOpen,
            };

            match state {
                CircuitState::Closed => {
                    // åœ¨å…³é—­çŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰“å¼€æ–­è·¯å™¨
                    let total = circuit.total_count.load(Ordering::SeqCst);
                    let failures = circuit.failure_count.load(Ordering::SeqCst);

                    if total >= circuit.request_volume_threshold as usize &&
                       (failures as f64 / total as f64) >= circuit.failure_threshold {
                        // åˆ‡æ¢åˆ°å¼€è·¯çŠ¶æ€
                        circuit.state.store(1, Ordering::SeqCst);
                        let mut last_state_change = circuit.last_state_change.lock().unwrap();
                        *last_state_change = Instant::now();
                    }
                },
                CircuitState::HalfOpen => {
                    // åœ¨åŠå¼€çŠ¶æ€ä¸‹ï¼Œå¦‚æœå¤±è´¥ï¼Œæˆ‘ä»¬åˆ‡æ¢å›å¼€è·¯çŠ¶æ€
                    circuit.state.store(1, Ordering::SeqCst);
                    let mut last_state_change = circuit.last_state_change.lock().unwrap();
                    *last_state_change = Instant::now();
                },
                _ => {}
            }
        }
    }
}

impl ServiceGateway {
    fn new(
        node_id: &str,
        service_registry: Arc<MicroserviceRegistry>,
        load_balancer: Box<dyn LoadBalancer>,
    ) -> Self {
        ServiceGateway {
            node_id: node_id.to_string(),
            routes: RwLock::new(HashMap::new()),
            service_registry,
            load_balancer,
            rate_limiter: Arc::new(RateLimiter::new()),
            circuit_breaker: Arc::new(CircuitBreaker::new()),
        }
    }

    fn add_route(&self, route: RouteConfig) -> Result<(), String> {
        println!("æ·»åŠ è·¯ç”±: {}", route.path);

        // éªŒè¯æœåŠ¡æ˜¯å¦å­˜åœ¨
        if self.service_registry.get_service(&route.service_id)?.is_none() {
            return Err(format!("æœåŠ¡ {} ä¸å­˜åœ¨", route.service_id));
        }

        // æ·»åŠ é€Ÿç‡é™åˆ¶ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if let Some(rate_limit) = &route.rate_limit {
            self.rate_limiter.add_rate_limit(
                &route.id,
                rate_limit.requests_per_second,
                rate_limit.burst,
            );
        }

        // æ·»åŠ æ–­è·¯å™¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if let Some(circuit_breaker) = &route.circuit_breaker {
            self.circuit_breaker.add_circuit(
                &route.id,
                circuit_breaker.clone(),
            );
        }

        // æ·»åŠ è·¯ç”±
        let mut routes = self.routes.write().unwrap();
        routes.insert(route.id.clone(), route);

        Ok(())
    }

    fn remove_route(&self, route_id: &str) -> Result<bool, String> {
        println!("ç§»é™¤è·¯ç”±: {}", route_id);

        let mut routes = self.routes.write().unwrap();
        Ok(routes.remove(route_id).is_some())
    }

    fn handle_request(&self, request_path: &str, request_method: HttpMethod, request_body: &[u8]) -> Result<Vec<u8>, String> {
        println!("å¤„ç†è¯·æ±‚: {}, æ–¹æ³•: {:?}", request_path, request_method);

        // æŸ¥æ‰¾åŒ¹é…çš„è·¯ç”±
        let routes = self.routes.read().unwrap();
        let mut matching_route = None;

        for route in routes.values() {
            // ç®€å•çš„è·¯å¾„åŒ¹é…ï¼ˆåœ¨å®é™…å®ç°ä¸­ä¼šä½¿ç”¨æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘ï¼‰
            if request_path.starts_with(&route.path) {
                matching_route = Some(route);
                break;
            }
        }

        let route = matching_route.ok_or_else(|| format!("æ²¡æœ‰åŒ¹é…çš„è·¯ç”±: {}", request_path))?;

        // æ£€æŸ¥é€Ÿç‡é™åˆ¶
        if let Some(rate_limit) = &route.rate_limit {
            if !self.rate_limiter.try_acquire(&route.id, 1) {
                return Err("è¯·æ±‚è¢«é€Ÿç‡é™åˆ¶".to_string());
            }
        }

        // æ£€æŸ¥æ–­è·¯å™¨
        if let Some(_) = &route.circuit_breaker {
            if !self.circuit_breaker.allow_request(&route.id) {
                return Err("æ–­è·¯å™¨å¼€è·¯ï¼Œè¯·æ±‚è¢«æ‹’ç»".to_string());
            }
        }

        // è·å–æœåŠ¡å®ä¾‹
        let instances = self.service_registry.get_instances(&route.service_id)?;
        if instances.is_empty() {
            return Err(format!("æœåŠ¡ {} æ²¡æœ‰å¯ç”¨å®ä¾‹", route.service_id));
        }

        // ä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨é€‰æ‹©å®ä¾‹
        let instance = self.load_balancer.choose_instance(&instances)
            .ok_or_else(|| format!("æ²¡æœ‰å¯ç”¨çš„æœåŠ¡å®ä¾‹: {}", route.service_id))?;

        // æ„å»ºç›®æ ‡URL
        let target_path = if route.strip_prefix {
            // ç§»é™¤å‰ç¼€
            &request_path[route.path.len()..]
        } else {
            request_path
        };

        let target_path = if let Some(rewrite) = &route.rewrite_path {
            // ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè·¯å¾„é‡å†™
            // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªæ˜¯æ›¿æ¢è·¯å¾„
            rewrite.clone()
        } else {
            target_path.to_string()
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€HTTPè¯·æ±‚åˆ°ç›®æ ‡æœåŠ¡
        println!("è½¬å‘è¯·æ±‚åˆ° {}:{}{}", instance.host, instance.port, target_path);

        // æ¨¡æ‹Ÿè°ƒç”¨æœåŠ¡
        let result = self.call_service(instance, &target_path, &request_method, request_body);

        match result {
            Ok(response) => {
                // è®°å½•æˆåŠŸ
                if let Some(_) = &route.circuit_breaker {
                    self.circuit_breaker.record_success(&route.id);
                }
                Ok(response)
            },
            Err(err) => {
                // è®°å½•å¤±è´¥
                if let Some(_) = &route.circuit_breaker {
                    self.circuit_breaker.record_failure(&route.id);
                }
                Err(err)
            }
        }
    }

    fn call_service(&self, instance: &ServiceInstance, path: &str, method: &HttpMethod, body: &[u8]) -> Result<Vec<u8>, String> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€HTTPè¯·æ±‚å¹¶è¿”å›å“åº”
        // è¿™é‡Œåªæ˜¯ç®€å•æ¨¡æ‹Ÿ

        // æ¨¡æ‹ŸæœåŠ¡å¯èƒ½çš„æ•…éšœ
        let failure_rate = 0.1; // 10%çš„æ•…éšœç‡
        let mut rng = rand::thread_rng();
        if rng.gen::<f64>() < failure_rate {
            return Err("æœåŠ¡è°ƒç”¨å¤±è´¥".to_string());
        }

        println!("è°ƒç”¨æœåŠ¡æˆåŠŸ: {}:{}{}", instance.host, instance.port, path);

        // ç”Ÿæˆä¸€ä¸ªç®€å•çš„å“åº”
        Ok(format!("æ¥è‡ª {}:{} çš„å“åº”: æˆåŠŸå¤„ç† {:?} è¯·æ±‚åˆ° {}",
                  instance.host, instance.port, method, path).into_bytes())
    }
}

// Merkleæ ‘æ•°æ®ç»“æ„ï¼Œç”¨äºé«˜æ•ˆçš„æ•°æ®ä¸€è‡´æ€§éªŒè¯
struct MerkleTree {
    root: Option<Box<MerkleNode>>,
    hash_function: Box<dyn Fn(&[u8]) -> Vec<u8> + Send + Sync>,
}

struct MerkleNode {
    hash: Vec<u8>,
    left: Option<Box<MerkleNode>>,
    right: Option<Box<MerkleNode>>,
    data: Option<Vec<u8>>,
}

impl MerkleTree {
    fn new<F>(hash_function: F) -> Self
    where
        F: Fn(&[u8]) -> Vec<u8> + Send + Sync + 'static
    {
        MerkleTree {
            root: None,
            hash_function: Box::new(hash_function),
        }
    }

    fn build(&mut self, data: &[Vec<u8>]) {
        if data.is_empty() {
            self.root = None;
            return;
        }

        // åˆ›å»ºå¶èŠ‚ç‚¹
        let mut nodes: Vec<MerkleNode> = data.iter().map(|item| {
            let hash = (self.hash_function)(item);
            MerkleNode {
                hash,
                left: None,
                right: None,
                data: Some(item.clone()),
            }
        }).collect();

        // å¦‚æœèŠ‚ç‚¹æ•°ä¸ºå¥‡æ•°ï¼Œå¤åˆ¶æœ€åä¸€ä¸ªèŠ‚ç‚¹
        if nodes.len() % 2 == 1 {
            nodes.push(nodes.last().unwrap().clone());
        }

        // è‡ªåº•å‘ä¸Šæ„å»ºæ ‘
        while nodes.len() > 1 {
            let mut new_level = Vec::new();

            for i in (0..nodes.len()).step_by(2) {
                let left = Box::new(nodes[i].clone());
                let right = Box::new(nodes[i + 1].clone());

                // åˆå¹¶ä¸¤ä¸ªå­èŠ‚ç‚¹çš„å“ˆå¸Œ
                let mut combined = left.hash.clone();
                combined.extend_from_slice(&right.hash);
                let parent_hash = (self.hash_function)(&combined);

                new_level.push(MerkleNode {
                    hash: parent_hash,
                    left: Some(left),
                    right: Some(right),
                    data: None,
                });
            }

            nodes = new_level;
        }

        self.root = Some(Box::new(nodes[0].clone()));
    }

    fn get_root_hash(&self) -> Option<Vec<u8>> {
        self.root.as_ref().map(|node| node.hash.clone())
    }

    fn verify(&self, data: &[u8], proof: &[Vec<u8>], index: usize, root_hash: &[u8]) -> bool {
        // è®¡ç®—æ•°æ®çš„å“ˆå¸Œ
        let mut current_hash = (self.hash_function)(data);

        // åº”ç”¨è¯æ˜
        let mut current_index = index;
        for sibling_hash in proof {
            let mut combined = Vec::new();

            if current_index % 2 == 0 {
                // å½“å‰èŠ‚ç‚¹æ˜¯å·¦å­èŠ‚ç‚¹
                combined.extend_from_slice(&current_hash);
                combined.extend_from_slice(sibling_hash);
            } else {
                // å½“å‰èŠ‚ç‚¹æ˜¯å³å­èŠ‚ç‚¹
                combined.extend_from_slice(sibling_hash);
                combined.extend_from_slice(&current_hash);
            }

            current_hash = (self.hash_function)(&combined);
            current_index /= 2;
        }

        // éªŒè¯æ ¹å“ˆå¸Œ
        current_hash == root_hash
    }

    fn generate_proof(&self, index: usize) -> Option<Vec<Vec<u8>>> {
        if self.root.is_none() {
            return None;
        }

        let mut proof = Vec::new();
        let mut path = Vec::new();
        let mut current_index = index;

        // è®¡ç®—ä»æ ¹èŠ‚ç‚¹åˆ°ç›®æ ‡èŠ‚ç‚¹çš„è·¯å¾„
        let mut i = index;
        let mut height = 0;
        while i > 0 {
            path.push(i % 2 == 1);
            i /= 2;
            height += 1;
        }
        path.reverse();

        // ä»æ ¹èŠ‚ç‚¹å¼€å§‹éå†
        let mut current_node = self.root.as_ref().unwrap();

        for &is_right in &path {
            if is_right {
                // å¦‚æœè·¯å¾„æŒ‡å‘å³å­èŠ‚ç‚¹ï¼Œæ·»åŠ å·¦å­èŠ‚ç‚¹çš„å“ˆå¸Œåˆ°è¯æ˜
                if let Some(left) = &current_node.left {
                    proof.push(left.hash.clone());
                    current_node = current_node.right.as_ref().unwrap();
                } else {
                    return None;
                }
            } else {
                // å¦‚æœè·¯å¾„æŒ‡å‘å·¦å­èŠ‚ç‚¹ï¼Œæ·»åŠ å³å­èŠ‚ç‚¹çš„å“ˆå¸Œåˆ°è¯æ˜
                if let Some(right) = &current_node.right {
                    proof.push(right.hash.clone());
                    current_node = current_node.left.as_ref().unwrap();
                } else {
                    return None;
                }
            }
        }

        Some(proof)
    }
}

impl Clone for MerkleNode {
    fn clone(&self) -> Self {
        MerkleNode {
            hash: self.hash.clone(),
            left: self.left.clone(),
            right: self.right.clone(),
            data: self.data.clone(),
        }
    }
}

// åŒºå—é“¾æ•°æ®ç»“æ„ï¼Œç”¨äºä¸å¯å˜çš„åˆ†å¸ƒå¼è´¦æœ¬
struct Blockchain {
    chain: Vec<Block>,
    pending_transactions: Vec<Transaction>,
    difficulty: u32,
    mining_reward: f64,
    hash_function: Box<dyn Fn(&[u8]) -> Vec<u8> + Send + Sync>,
}

struct Block {
    index: u64,
    timestamp: u64,
    transactions: Vec<Transaction>,
    previous_hash: Vec<u8>,
    nonce: u64,
    hash: Vec<u8>,
}

struct Transaction {
    sender: String,
    recipient: String,
    amount: f64,
    timestamp: u64,
    signature: Option<Vec<u8>>,
}

impl Blockchain {
    fn new<F>(mining_reward: f64, difficulty: u32, hash_function: F) -> Self
    where
        F: Fn(&[u8]) -> Vec<u8> + Send + Sync + 'static
    {
        let mut blockchain = Blockchain {
            chain: Vec::new(),
            pending_transactions: Vec::new(),
            difficulty,
            mining_reward,
            hash_function: Box::new(hash_function),
        };

        // åˆ›å»ºåˆ›ä¸–åŒºå—
        blockchain.create_genesis_block();

        blockchain
    }

    fn create_genesis_block(&mut self) {
        println!("åˆ›å»ºåˆ›ä¸–åŒºå—");

        let genesis_block = Block {
            index: 0,
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
            transactions: Vec::new(),
            previous_hash: vec![0; 32],
            nonce: 0,
            hash: vec![0; 32],
        };

        // è®¡ç®—å¹¶è®¾ç½®åˆ›ä¸–åŒºå—çš„å“ˆå¸Œ
        let hash = self.calculate_hash(&genesis_block);
        let mut genesis_block = genesis_block;
        genesis_block.hash = hash;

        self.chain.push(genesis_block);
    }

    fn get_latest_block(&self) -> &Block {
        self.chain.last().unwrap()
    }

    fn mine_pending_transactions(&mut self, miner_address: &str) -> Result<u64, String> {
        println!("æŒ–æ˜æ–°åŒºå—...");

        // åˆ›å»ºæŒ–çŸ¿å¥–åŠ±äº¤æ˜“
        let reward_tx = Transaction {
            sender: "system".to_string(),
            recipient: miner_address.to_string(),
            amount: self.mining_reward,
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
            signature: None,
        };

        // æ·»åŠ å¥–åŠ±äº¤æ˜“åˆ°å¾…å¤„ç†äº¤æ˜“
        let mut block_transactions = self.pending_transactions.clone();
        block_transactions.push(reward_tx);

        // åˆ›å»ºæ–°åŒºå—
        let previous_block = self.get_latest_block();
        let new_block = Block {
            index: previous_block.index + 1,
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
            transactions: block_transactions,
            previous_hash: previous_block.hash.clone(),
            nonce: 0,
            hash: Vec::new(),
        };

        // æŒ–æ˜åŒºå—ï¼ˆå·¥ä½œé‡è¯æ˜ï¼‰
        let mined_block = self.proof_of_work(new_block)?;

        // å°†æ–°åŒºå—æ·»åŠ åˆ°é“¾ä¸­
        self.chain.push(mined_block);

        // æ¸…ç©ºå¾…å¤„ç†äº¤æ˜“
        self.pending_transactions = Vec::new();

        Ok(self.chain.last().unwrap().index)
    }

    fn proof_of_work(&self, mut block: Block) -> Result<Block, String> {
        println!("æ‰§è¡Œå·¥ä½œé‡è¯æ˜...");

        let target = vec![0; (self.difficulty / 8) as usize];
        let target_prefix = target.as_slice();

        let start_time = Instant::now();
        let mut iterations = 0;

        loop {
            // è®¡ç®—åŒºå—å“ˆå¸Œ
            let hash = self.calculate_hash(&block);

            // æ£€æŸ¥å“ˆå¸Œæ˜¯å¦æ»¡è¶³éš¾åº¦è¦æ±‚
            if hash.starts_with(target_prefix) {
                block.hash = hash;
                let elapsed = start_time.elapsed();
                println!("å·¥ä½œé‡è¯æ˜å®Œæˆï¼Œè€—æ—¶: {:?}ï¼Œè¿­ä»£æ¬¡æ•°: {}", elapsed, iterations);
                return Ok(block);
            }

            // å¢åŠ nonceå¹¶é‡è¯•
            block.nonce += 1;
            iterations += 1;

            // è¶…æ—¶æ£€æŸ¥
            if iterations % 100000 == 0 {
                let elapsed = start_time.elapsed();
                if elapsed > Duration::from_secs(300) { // 5åˆ†é’Ÿè¶…æ—¶
                    return Err("å·¥ä½œé‡è¯æ˜è¶…æ—¶".to_string());
                }
            }
        }
    }

    fn calculate_hash(&self, block: &Block) -> Vec<u8> {
        // åºåˆ—åŒ–åŒºå—æ•°æ®
        let mut data = Vec::new();
        data.extend_from_slice(&block.index.to_le_bytes());
        data.extend_from_slice(&block.timestamp.to_le_bytes());
        data.extend_from_slice(&block.nonce.to_le_bytes());
        data.extend_from_slice(&block.previous_hash);

        // æ·»åŠ äº¤æ˜“æ•°æ®
        for tx in &block.transactions {
            data.extend_from_slice(tx.sender.as_bytes());
            data.extend_from_slice(tx.recipient.as_bytes());
            data.extend_from_slice(&tx.amount.to_le_bytes());
            data.extend_from_slice(&tx.timestamp.to_le_bytes());
        }

        // è®¡ç®—å“ˆå¸Œ
        (self.hash_function)(&data)
    }

    fn add_transaction(&mut self, transaction: Transaction) -> Result<(), String> {
        // éªŒè¯äº¤æ˜“
        if transaction.sender.is_empty() || transaction.recipient.is_empty() {
            return Err("æ— æ•ˆçš„å‘é€è€…æˆ–æ¥æ”¶è€…".to_string());
        }

        if transaction.amount <= 0.0 {
            return Err("äº¤æ˜“é‡‘é¢å¿…é¡»ä¸ºæ­£æ•°".to_string());
        }

        // éªŒè¯å‘é€è€…ä½™é¢ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼‰
        // if self.get_balance(&transaction.sender) < transaction.amount {
        //     return Err("ä½™é¢ä¸è¶³".to_string());
        // }

        // éªŒè¯ç­¾åï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼‰
        // if !self.verify_signature(&transaction) {
        //     return Err("æ— æ•ˆçš„äº¤æ˜“ç­¾å".to_string());
        // }

        self.pending_transactions.push(transaction);

        Ok(())
    }

    fn is_chain_valid(&self) -> bool {
        for i in 1..self.chain.len() {
            let current_block = &self.chain[i];
            let previous_block = &self.chain[i - 1];

            // éªŒè¯åŒºå—å“ˆå¸Œ
            if current_block.hash != self.calculate_hash(current_block) {
                println!("åŒºå— {} çš„å“ˆå¸Œæ— æ•ˆ", current_block.index);
                return false;
            }

            // éªŒè¯åŒºå—é“¾æ¥
            if current_block.previous_hash != previous_block.hash {
                println!("åŒºå— {} çš„å‰åºå“ˆå¸Œæ— æ•ˆ", current_block.index);
                return false;
            }
        }

        true
    }

    fn get_balance(&self, address: &str) -> f64 {
        let mut balance = 0.0;

        // è®¡ç®—æ‰€æœ‰åŒºå—ä¸­ä¸æ­¤åœ°å€ç›¸å…³çš„äº¤æ˜“
        for block in &self.chain {
            for tx in &block.transactions {
                if tx.recipient == address {
                    balance += tx.amount;
                }

                if tx.sender == address {
                    balance -= tx.amount;
                }
            }
        }

        balance
    }
}
```

### 1.5 ç»¼åˆåº”ç”¨05-åˆ†å¸ƒå¼æ—¥å¿—ç³»ç»Ÿ

```rust
// åˆ†å¸ƒå¼æ—¥å¿—ç³»ç»Ÿ
struct DistributedLogSystem {
    node_id: String,
    storage: Box<dyn LogStorage>,
    replication_factor: usize,
    partitions: u32,
    retention_policy: RetentionPolicy,
    peers: RwLock<HashMap<String, LogPeer>>,
}

trait LogStorage: Send + Sync {
    fn append(&self, partition: u32, entries: &[LogEntry]) -> Result<u64, String>;
    fn read(&self, partition: u32, start_offset: u64, max_entries: usize) -> Result<Vec<LogEntry>, String>;
    fn get_latest_offset(&self, partition: u32) -> Result<u64, String>;
    fn delete_before(&self, partition: u32, offset: u64) -> Result<u64, String>;
}

struct LogEntry {
    offset: u64,
    timestamp: u64,
    key: Option<Vec<u8>>,
    value: Vec<u8>,
    headers: HashMap<String, String>,
}

struct RetentionPolicy {
    time_based: Option<Duration>,
    size_based: Option<u64>,
    compaction_enabled: bool,
}

struct LogPeer {
    node_id: String,
    address: SocketAddr,
    status: PeerStatus,
    partitions: Vec<u32>,
    last_heartbeat: Instant,
}

impl DistributedLogSystem {
    fn new(node_id: &str, storage: Box<dyn LogStorage>, replication_factor: usize, partitions: u32) -> Self {
        let retention_policy = RetentionPolicy {
            time_based: Some(Duration::from_secs(7 * 24 * 60 * 60)), // 7å¤©
            size_based: Some(1024 * 1024 * 1024), // 1GB
            compaction_enabled: true,
        };

        DistributedLogSystem {
            node_id: node_id.to_string(),
            storage,
            replication_factor,
            partitions,
            retention_policy,
            peers: RwLock::new(HashMap::new()),
        }
    }

    fn append(&self, topic: &str, key: Option<&[u8]>, value: &[u8], headers: Option<HashMap<String, String>>) -> Result<(u32, u64), String> {
        // è®¡ç®—åˆ†åŒº
        let partition = self.calculate_partition(topic, key);

        // åˆ›å»ºæ—¥å¿—æ¡ç›®
        let entry = LogEntry {
            offset: 0, // å ä½ç¬¦ï¼Œå®é™…å€¼å°†ç”±å­˜å‚¨å±‚è®¾ç½®
            timestamp: SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs(),
            key: key.map(|k| k.to_vec()),
            value: value.to_vec(),
            headers: headers.unwrap_or_default(),
        };

        // è¿½åŠ åˆ°æœ¬åœ°å­˜å‚¨
        let offset = self.storage.append(partition, &[entry])?;

        // å°†æ—¥å¿—æ¡ç›®å¤åˆ¶åˆ°å‰¯æœ¬
        self.replicate_to_peers(partition, &[entry])?;

        Ok((partition, offset))
    }

    fn read(&self, topic: &str, partition: u32, start_offset: u64, max_entries: usize) -> Result<Vec<LogEntry>, String> {
        // éªŒè¯åˆ†åŒº
        if partition >= self.partitions {
            return Err(format!("æ— æ•ˆçš„åˆ†åŒº: {}", partition));
        }

        // ä»å­˜å‚¨ä¸­è¯»å–æ—¥å¿—æ¡ç›®
        self.storage.read(partition, start_offset, max_entries)
    }

    fn calculate_partition(&self, topic: &str, key: Option<&[u8]>) -> u32 {
        if let Some(key) = key {
            // ä½¿ç”¨é”®çš„å“ˆå¸Œå€¼è®¡ç®—åˆ†åŒº
            let mut hasher = DefaultHasher::new();
            key.hash(&mut hasher);
            hasher.finish() as u32 % self.partitions
        } else {
            // å¦‚æœæ²¡æœ‰é”®ï¼Œä½¿ç”¨ç®€å•çš„è½®è¯¢ç­–ç•¥
            let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos();
            (timestamp % self.partitions as u128) as u32
        }
    }

    fn replicate_to_peers(&self, partition: u32, entries: &[LogEntry]) -> Result<(), String> {
        let peers = self.peers.read().unwrap();

        // æŸ¥æ‰¾è´Ÿè´£æ­¤åˆ†åŒºçš„å¯¹ç­‰èŠ‚ç‚¹
        let responsible_peers: Vec<_> = peers.values()
            .filter(|p| p.partitions.contains(&partition) && p.status == PeerStatus::Connected)
            .collect();

        if responsible_peers.len() < self.replication_factor - 1 {
            // è­¦å‘Šï¼šæ²¡æœ‰è¶³å¤Ÿçš„å¯¹ç­‰èŠ‚ç‚¹è¿›è¡Œå¤åˆ¶
            println!("è­¦å‘Šï¼šå¯ç”¨çš„å¤åˆ¶å¯¹ç­‰èŠ‚ç‚¹æ•°é‡ä¸è¶³ï¼Œå½“å‰ä¸º {}, éœ€è¦ {}",
                    responsible_peers.len(), self.replication_factor - 1);
        }

        // å‘å¯¹ç­‰èŠ‚ç‚¹å¤åˆ¶æ¡ç›®
        for peer in &responsible_peers {
            println!("å°†åˆ†åŒº {} çš„æ¡ç›®å¤åˆ¶åˆ°å¯¹ç­‰èŠ‚ç‚¹ {}", partition, peer.node_id);

            // å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€šè¿‡ç½‘ç»œå‘é€æ¡ç›®
            // self.send_entries_to_peer(peer, partition, entries)?;
        }

        Ok(())
    }

    fn apply_retention_policy(&self) -> Result<(), String> {
        println!("åº”ç”¨ä¿ç•™ç­–ç•¥...");

        for partition in 0..self.partitions {
            // åº”ç”¨åŸºäºæ—¶é—´çš„ä¿ç•™ç­–ç•¥
            if let Some(time_limit) = self.retention_policy.time_based {
                let cutoff_time = SystemTime::now().duration_since(UNIX_EPOCH).unwrap()
                    .checked_sub(time_limit).unwrap().as_secs();

                // è¯»å–æ—¥å¿—ä»¥æ‰¾åˆ°æ—¶é—´æˆ³å°äºcutoff_timeçš„æœ€æ–°åç§»é‡
                let entries = self.storage.read(partition, 0, 1000)?;

                if let Some(cutoff_offset) = entries.iter()
                    .filter(|e| e.timestamp < cutoff_time)
                    .map(|e| e.offset)
                    .last() {
                    // åˆ é™¤æ­¤åç§»é‡ä¹‹å‰çš„æ‰€æœ‰æ¡ç›®
                    self.storage.delete_before(partition, cutoff_offset)?;
                }
            }

            // åº”ç”¨åŸºäºå¤§å°çš„ä¿ç•™ç­–ç•¥ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼‰
            // ...

            // åº”ç”¨å‹ç¼©ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼‰
            // ...
        }

        Ok(())
    }

    fn add_peer(&self, peer: LogPeer) -> Result<(), String> {
        let mut peers = self.peers.write().unwrap();

        if peers.contains_key(&peer.node_id) {
            return Err(format!("å¯¹ç­‰èŠ‚ç‚¹ {} å·²å­˜åœ¨", peer.node_id));
        }

        peers.insert(peer.node_id.clone(), peer);

        Ok(())
    }

    fn rebalance_partitions(&self) -> Result<(), String> {
        println!("é‡æ–°å¹³è¡¡åˆ†åŒº...");

        let mut peers = self.peers.write().unwrap();
        let peer_count = peers.len();

        if peer_count == 0 {
            return Ok(());
        }

        // ä¸ºæ¯ä¸ªåˆ†åŒºåˆ†é…å‰¯æœ¬
        for partition in 0..self.partitions {
            // é€‰æ‹© replication_factor ä¸ªèŠ‚ç‚¹ä½œä¸ºå‰¯æœ¬
            let replica_count = std::cmp::min(self.replication_factor, peer_count);

            // ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œæˆ–å…¶ä»–ç­–ç•¥é€‰æ‹©èŠ‚ç‚¹
            let partition_key = format!("partition:{}", partition);
            let mut selected_peers = Vec::new();

            // ç®€å•çš„ç¤ºä¾‹ï¼šä½¿ç”¨å¾ªç¯åˆ†é…
            for (i, (peer_id, peer)) in peers.iter_mut().enumerate() {
                if i % self.partitions as usize == partition as usize % peer_count {
                    peer.partitions.push(partition);
                    selected_peers.push(peer_id.clone());

                    if selected_peers.len() >= replica_count {
                        break;
                    }
                }
            }

            println!("åˆ†åŒº {} çš„å‰¯æœ¬: {:?}", partition, selected_peers);
        }

        Ok(())
    }
}

// åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ
struct DistributedTracingSystem {
    node_id: String,
    trace_storage: Box<dyn TraceStorage>,
    sampler: Box<dyn Sampler>,
    exporter: Box<dyn TraceExporter>,
}

trait TraceStorage: Send + Sync {
    fn store_span(&self, span: &Span) -> Result<(), String>;
    fn get_trace(&self, trace_id: &str) -> Result<Vec<Span>, String>;
    fn query_traces(&self, query: &TraceQuery) -> Result<Vec<Trace>, String>;
}

trait Sampler: Send + Sync {
    fn should_sample(&self, trace_id: &str, operation_name: &str) -> bool;
}

trait TraceExporter: Send + Sync {
    fn export_traces(&self, traces: &[Trace]) -> Result<(), String>;
}

struct Span {
    span_id: String,
    trace_id: String,
    parent_span_id: Option<String>,
    operation_name: String,
    start_time: SystemTime,
    end_time: Option<SystemTime>,
    tags: HashMap<String, String>,
    logs: Vec<SpanLog>,
    service_name: String,
}

struct SpanLog {
    timestamp: SystemTime,
    fields: HashMap<String, String>,
}

struct Trace {
    trace_id: String,
    root_span: Span,
    child_spans: Vec<Span>,
}

struct TraceQuery {
    service_name: Option<String>,
    operation_name: Option<String>,
    tags: HashMap<String, String>,
    min_duration: Option<Duration>,
    max_duration: Option<Duration>,
    start_time: Option<SystemTime>,
    end_time: Option<SystemTime>,
    limit: usize,
}

struct ProbabilisticSampler {
    sampling_rate: f64,
}

impl Sampler for ProbabilisticSampler {
    fn should_sample(&self, trace_id: &str, operation_name: &str) -> bool {
        // ç®€å•çš„æ¦‚ç‡é‡‡æ ·
        let mut rng = rand::thread_rng();
        rng.gen::<f64>() < self.sampling_rate
    }
}

struct RateLimitingSampler {
    traces_per_second: u32,
    counter: AtomicUsize,
    last_reset: Mutex<Instant>,
}

impl Sampler for RateLimitingSampler {
    fn should_sample(&self, trace_id: &str, operation_name: &str) -> bool {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®è®¡æ•°å™¨
        let mut last_reset = self.last_reset.lock().unwrap();
        let now = Instant::now();

        if now.duration_since(*last_reset).as_secs() >= 1 {
            // é‡ç½®è®¡æ•°å™¨
            self.counter.store(0, Ordering::SeqCst);
            *last_reset = now;
        }

        // å¢åŠ è®¡æ•°å™¨å¹¶æ£€æŸ¥æ˜¯å¦ä½äºé™åˆ¶
        let count = self.counter.fetch_add(1, Ordering::SeqCst);
        count < self.traces_per_second as usize
    }
}

impl DistributedTracingSystem {
    fn new(
        node_id: &str,
        trace_storage: Box<dyn TraceStorage>,
        sampler: Box<dyn Sampler>,
        exporter: Box<dyn TraceExporter>,
    ) -> Self {
        DistributedTracingSystem {
            node_id: node_id.to_string(),
            trace_storage,
            sampler,
            exporter,
        }
    }

    fn start_span(&self, operation_name: &str, parent_span: Option<&Span>) -> Span {
        // ç”Ÿæˆæˆ–ç»§æ‰¿trace_id
        let trace_id = if let Some(parent) = parent_span {
            parent.trace_id.clone()
        } else {
            uuid::Uuid::new_v4().to_string()
        };

        // ç”Ÿæˆspan_id
        let span_id = uuid::Uuid::new_v4().to_string();

        // æå–çˆ¶span_id
        let parent_span_id = parent_span.map(|p| p.span_id.clone());

        // åˆ›å»ºspan
        Span {
            span_id,
            trace_id,
            parent_span_id,
            operation_name: operation_name.to_string(),
            start_time: SystemTime::now(),
            end_time: None,
            tags: HashMap::new(),
            logs: Vec::new(),
            service_name: self.node_id.clone(),
        }
    }

    fn finish_span(&self, mut span: Span) -> Result<(), String> {
        // è®¾ç½®ç»“æŸæ—¶é—´
        span.end_time = Some(SystemTime::now());

        // å­˜å‚¨span
        self.trace_storage.store_span(&span)?;

        // å¦‚æœæ˜¯æ ¹spanï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥å¯¼å‡º
        if span.parent_span_id.is_none() && self.sampler.should_sample(&span.trace_id, &span.operation_name) {
            // è·å–å®Œæ•´çš„trace
            let spans = self.trace_storage.get_trace(&span.trace_id)?;

            // æ„å»ºtraceç»“æ„
            if !spans.is_empty() {
                let root_span = spans.iter().find(|s| s.parent_span_id.is_none()).unwrap().clone();
                let child_spans = spans.iter().filter(|s| s.parent_span_id.is_some()).cloned().collect();

                let trace = Trace {
                    trace_id: span.trace_id.clone(),
                    root_span,
                    child_spans,
                };

                // å¯¼å‡ºtrace
                self.exporter.export_traces(&[trace])?;
            }
        }

        Ok(())
    }

    fn add_tag(&self, span: &mut Span, key: &str, value: &str) {
        span.tags.insert(key.to_string(), value.to_string());
    }

    fn log(&self, span: &mut Span, fields: HashMap<String, String>) {
        let log = SpanLog {
            timestamp: SystemTime::now(),
            fields,
        };

        span.logs.push(log);
    }

    fn query_traces(&self, query: &TraceQuery) -> Result<Vec<Trace>, String> {
        self.trace_storage.query_traces(query)
    }
}

// åˆ†å¸ƒå¼é”ç®¡ç†å™¨
struct DistributedLockManager {
    node_id: String,
    locks: RwLock<HashMap<String, LockInfo>>,
    lock_timeout: Duration,
    heartbeat_interval: Duration,
    storage: Box<dyn LockStorage>,
}

trait LockStorage: Send + Sync {
    fn acquire_lock(&self, lock_name: &str, owner: &str, ttl: Duration) -> Result<bool, String>;
    fn release_lock(&self, lock_name: &str, owner: &str) -> Result<bool, String>;
    fn refresh_lock(&self, lock_name: &str, owner: &str, ttl: Duration) -> Result<bool, String>;
    fn get_lock_info(&self, lock_name: &str) -> Result<Option<LockInfo>, String>;
}

struct LockInfo {
    name: String,
    owner: String,
    acquired_at: SystemTime,
    expires_at: SystemTime,
    metadata: HashMap<String, String>,
}

impl DistributedLockManager {
    fn new(node_id: &str, storage: Box<dyn LockStorage>, lock_timeout: Duration) -> Self {
        DistributedLockManager {
            node_id: node_id.to_string(),
            locks: RwLock::new(HashMap::new()),
            lock_timeout,
            heartbeat_interval: lock_timeout.div_f32(3.0), // å¿ƒè·³é—´éš”ä¸ºè¶…æ—¶æ—¶é—´çš„1/3
            storage,
        }
    }

    fn acquire_lock(&self, lock_name: &str, wait_timeout: Option<Duration>) -> Result<bool, String> {
        println!("å°è¯•è·å–é”: {}", lock_name);

        let start_time = Instant::now();
        let wait_timeout = wait_timeout.unwrap_or(Duration::from_secs(0));

        loop {
            // å°è¯•è·å–é”
            let acquired = self.storage.acquire_lock(lock_name, &self.node_id, self.lock_timeout)?;

            if acquired {
                println!("æˆåŠŸè·å–é”: {}", lock_name);

                // æ·»åŠ åˆ°æœ¬åœ°é”é›†åˆ
                let now = SystemTime::now();
                let expires_at = now + self.lock_timeout;

                let lock_info = LockInfo {
                    name: lock_name.to_string(),
                    owner: self.node_id.clone(),
                    acquired_at: now,
                    expires_at,
                    metadata: HashMap::new(),
                };

                let mut locks = self.locks.write().unwrap();
                locks.insert(lock_name.to_string(), lock_info);

                // å¯åŠ¨å¿ƒè·³åˆ·æ–°
                let node_id = self.node_id.clone();
                let lock_name = lock_name.to_string();
                let heartbeat_interval = self.heartbeat_interval;
                let lock_timeout = self.lock_timeout;
                let storage = Arc::clone(&self.storage);

                thread::spawn(move || {
                    let mut heartbeat_failed = false;

                    while !heartbeat_failed {
                        // ç­‰å¾…å¿ƒè·³é—´éš”
                        thread::sleep(heartbeat_interval);

                        // åˆ·æ–°é”
                        match storage.refresh_lock(&lock_name, &node_id, lock_timeout) {
                            Ok(true) => {
                                println!("åˆ·æ–°é”: {}", lock_name);
                            },
                            Ok(false) => {
                                println!("é”å·²è¢«å…¶ä»–èŠ‚ç‚¹è·å–: {}", lock_name);
                                heartbeat_failed = true;
                            },
                            Err(err) => {
                                println!("åˆ·æ–°é”å¤±è´¥: {}, é”™è¯¯: {}", lock_name, err);
                                heartbeat_failed = true;
                            },
                        }
                    }
                });

                return Ok(true);
            }

            // æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if wait_timeout == Duration::from_secs(0) || start_time.elapsed() >= wait_timeout {
                println!("è·å–é”è¶…æ—¶: {}", lock_name);
                return Ok(false);
            }

            // ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            thread::sleep(Duration::from_millis(100));
        }
    }

    fn release_lock(&self, lock_name: &str) -> Result<bool, String> {
        println!("é‡Šæ”¾é”: {}", lock_name);

        // ä»æœ¬åœ°é”é›†åˆä¸­ç§»é™¤
        let mut locks = self.locks.write().unwrap();
        locks.remove(lock_name);

        // ä»å­˜å‚¨ä¸­é‡Šæ”¾
        self.storage.release_lock(lock_name, &self.node_id)
    }

    fn get_lock_info(&self, lock_name: &str) -> Result<Option<LockInfo>, String> {
        self.storage.get_lock_info(lock_name)
    }
}

// å¤šæ´»æ•°æ®ä¸­å¿ƒç®¡ç†å™¨
struct MultiDCManager {
    node_id: String,
    data_centers: RwLock<HashMap<String, DataCenterInfo>>,
    current_dc: String,
    replication_manager: ReplicationManager,
    service_registry: Arc<MicroserviceRegistry>,
}

struct DataCenterInfo {
    id: String,
    name: String,
    location: String,
    status: DataCenterStatus,
    services: Vec<String>,
    priority: u32,
}

enum DataCenterStatus {
    Active,
    Standby,
    Degraded,
    Down,
}

struct DCFailoverPolicy {
    automatic: bool,
    failover_threshold: u32,
    min_healthy_services: f64, // ç™¾åˆ†æ¯”
    cooldown_period: Duration,
    last_failover: Option<SystemTime>,
}

impl MultiDCManager {
    fn new(
        node_id: &str,
        current_dc: &str,
        service_registry: Arc<MicroserviceRegistry>,
        replication_factor: usize,
    ) -> Self {
        let replication_manager = ReplicationManager {
            node_id: node_id.to_string(),
            replication_factor,
            replication_strategy: ReplicationStrategy::Asynchronous,
            peers: RwLock::new(HashMap::new()),
        };

        MultiDCManager {
            node_id: node_id.to_string(),
            data_centers: RwLock::new(HashMap::new()),
            current_dc: current_dc.to_string(),
            replication_manager,
            service_registry,
        }
    }

    fn register_data_center(&self, dc: DataCenterInfo) -> Result<(), String> {
        println!("æ³¨å†Œæ•°æ®ä¸­å¿ƒ: {}", dc.name);

        let mut data_centers = self.data_centers.write().unwrap();
        data_centers.insert(dc.id.clone(), dc);

        Ok(())
    }

    fn update_data_center_status(&self, dc_id: &str, status: DataCenterStatus) -> Result<(), String> {
        println!("æ›´æ–°æ•°æ®ä¸­å¿ƒçŠ¶æ€: {}, çŠ¶æ€: {:?}", dc_id, status);

        let mut data_centers = self.data_centers.write().unwrap();

        if let Some(dc) = data_centers.get_mut(dc_id) {
            dc.status = status;
            Ok(())
        } else {
            Err(format!("æ•°æ®ä¸­å¿ƒä¸å­˜åœ¨: {}", dc_id))
        }
    }

    fn get_active_data_centers(&self) -> Vec<DataCenterInfo> {
        let data_centers = self.data_centers.read().unwrap();

        data_centers.values()
            .filter(|dc| dc.status == DataCenterStatus::Active)
            .cloned()
            .collect()
    }

    fn route_request(&self, service_name: &str, affinity_dc: Option<&str>) -> Result<String, String> {
        // é¦–å…ˆå°è¯•ä½¿ç”¨æŒ‡å®šçš„äº²å’Œæ€§æ•°æ®ä¸­å¿ƒ
        if let Some(dc_id) = affinity_dc {
            let data_centers = self.data_centers.read().unwrap();

            if let Some(dc) = data_centers.get(dc_id) {
                if dc.status == DataCenterStatus::Active && dc.services.contains(&service_name.to_string()) {
                    return Ok(dc_id.to_string());
                }
            }
        }

        // ç„¶åå°è¯•ä½¿ç”¨å½“å‰æ•°æ®ä¸­å¿ƒ
        {
            let data_centers = self.data_centers.read().unwrap();

            if let Some(dc) = data_centers.get(&self.current_dc) {
                if dc.status == DataCenterStatus::Active && dc.services.contains(&service_name.to_string()) {
                    return Ok(self.current_dc.clone());
                }
            }
        }

        // æœ€åå°è¯•ä½¿ç”¨ä»»ä½•æ´»åŠ¨çš„æ•°æ®ä¸­å¿ƒ
        let active_dcs = self.get_active_data_centers();

        for dc in active_dcs {
            if dc.services.contains(&service_name.to_string()) {
                return Ok(dc.id);
            }
        }

        Err(format!("æ²¡æœ‰å¯ç”¨çš„æ•°æ®ä¸­å¿ƒæä¾›æœåŠ¡: {}", service_name))
    }

    fn handle_data_center_failure(&self, dc_id: &str, policy: &DCFailoverPolicy) -> Result<(), String> {
        println!("å¤„ç†æ•°æ®ä¸­å¿ƒæ•…éšœ: {}", dc_id);

        // æ£€æŸ¥æ˜¯å¦å…è®¸è‡ªåŠ¨æ•…éšœè½¬ç§»
        if !policy.automatic {
            println!("è‡ªåŠ¨æ•…éšœè½¬ç§»å·²ç¦ç”¨ï¼Œéœ€è¦æ‰‹åŠ¨å¹²é¢„");
            return Ok(());
        }

        // æ£€æŸ¥å†·å´æœŸ
        if let Some(last_failover) = policy.last_failover {
            let elapsed = SystemTime::now().duration_since(last_failover).unwrap();
            if elapsed < policy.cooldown_period {
                println!("åœ¨å†·å´æœŸå†…ï¼Œä¸æ‰§è¡Œæ•…éšœè½¬ç§»");
                return Ok(());
            }
        }

        // æ›´æ–°æ•°æ®ä¸­å¿ƒçŠ¶æ€
        self.update_data_center_status(dc_id, DataCenterStatus::Down)?;

        // è·å–æ•°æ®ä¸­å¿ƒä¿¡æ¯
        let dc_info = {
            let data_centers = self.data_centers.read().unwrap();
            data_centers.get(dc_id).cloned().ok_or_else(|| format!("æ•°æ®ä¸­å¿ƒä¸å­˜åœ¨: {}", dc_id))?
        };

        // æŸ¥æ‰¾å¤‡ç”¨æ•°æ®ä¸­å¿ƒ
        let standby_dc = {
            let data_centers = self.data_centers.read().unwrap();
            data_centers.values()
                .filter(|dc| dc.status == DataCenterStatus::Standby)
                .max_by_key(|dc| dc.priority)
                .cloned()
        };

        // å¦‚æœæ‰¾åˆ°å¤‡ç”¨æ•°æ®ä¸­å¿ƒï¼Œæ¿€æ´»å®ƒ
        if let Some(standby_dc) = standby_dc {
            println!("å°†å¤‡ç”¨æ•°æ®ä¸­å¿ƒæ¿€æ´»: {}", standby_dc.id);
            self.update_data_center_status(&standby_dc.id, DataCenterStatus::Active)?;

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæœ‰æ›´å¤šçš„æ­¥éª¤æ¥ç¡®ä¿æœåŠ¡çš„è¿ç§»å’Œæ•°æ®åŒæ­¥
        } else {
            println!("æ²¡æœ‰å¯ç”¨çš„å¤‡ç”¨æ•°æ®ä¸­å¿ƒè¿›è¡Œæ•…éšœè½¬ç§»");
        }

        // æ›´æ–°å¤±æ•ˆæ•°æ®ä¸­å¿ƒçš„æœåŠ¡å®ä¾‹çŠ¶æ€
        for service_id in &dc_info.services {
            let instances = self.service_registry.get_instances(service_id)?;

            for instance in instances {
                // ç®€åŒ–ï¼šå®é™…å®ç°ä¸­ä¼šæ£€æŸ¥å®ä¾‹æ˜¯å¦åœ¨å¤±æ•ˆçš„æ•°æ®ä¸­å¿ƒ
                self.service_registry.set_instance_status(service_id, &instance.id, InstanceStatus::DOWN)?;
            }
        }

        println!("æ•°æ®ä¸­å¿ƒæ•…éšœå¤„ç†å®Œæˆ: {}", dc_id);

        Ok(())
    }

    fn sync_data_between_dcs(&self, source_dc: &str, target_dc: &str, data_type: &str) -> Result<(), String> {
        println!("åŒæ­¥æ•°æ®ä» {} åˆ° {}, ç±»å‹: {}", source_dc, target_dc, data_type);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®æ•°æ®ç±»å‹æ‰§è¡Œä¸åŒçš„åŒæ­¥ç­–ç•¥
        // ä¾‹å¦‚ï¼ŒæœåŠ¡æ³¨å†Œä¿¡æ¯ã€é…ç½®æ•°æ®ã€ä¸šåŠ¡æ•°æ®ç­‰

        // éªŒè¯æºå’Œç›®æ ‡æ•°æ®ä¸­å¿ƒ
        {
            let data_centers = self.data_centers.read().unwrap();

            if !data_centers.contains_key(source_dc) {
                return Err(format!("æºæ•°æ®ä¸­å¿ƒä¸å­˜åœ¨: {}", source_dc));
            }

            if !data_centers.contains_key(target_dc) {
                return Err(format!("ç›®æ ‡æ•°æ®ä¸­å¿ƒä¸å­˜åœ¨: {}", target_dc));
            }
        }

        // æ ¹æ®æ•°æ®ç±»å‹æ‰§è¡ŒåŒæ­¥
        match data_type {
            "service_registry" => {
                // åŒæ­¥æœåŠ¡æ³¨å†Œä¿¡æ¯
                println!("åŒæ­¥æœåŠ¡æ³¨å†Œä¿¡æ¯");
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»æºæ•°æ®ä¸­å¿ƒè·å–æœåŠ¡ä¿¡æ¯å¹¶æ›´æ–°åˆ°ç›®æ ‡æ•°æ®ä¸­å¿ƒ
            },
            "configuration" => {
                // åŒæ­¥é…ç½®æ•°æ®
                println!("åŒæ­¥é…ç½®æ•°æ®");
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»æºæ•°æ®ä¸­å¿ƒè·å–é…ç½®æ•°æ®å¹¶æ›´æ–°åˆ°ç›®æ ‡æ•°æ®ä¸­å¿ƒ
            },
            "business_data" => {
                // åŒæ­¥ä¸šåŠ¡æ•°æ®
                println!("åŒæ­¥ä¸šåŠ¡æ•°æ®");
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨ç‰¹å®šçš„æ•°æ®å¤åˆ¶æœºåˆ¶åŒæ­¥ä¸šåŠ¡æ•°æ®
            },
            _ => {
                return Err(format!("ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {}", data_type));
            },
        }

        println!("æ•°æ®åŒæ­¥å®Œæˆ");

        Ok(())
    }
}

// å¼¹æ€§ä¼¸ç¼©ç®¡ç†å™¨
struct AutoScalingManager {
    node_id: String,
    cluster_manager: Box<dyn ClusterManager>,
    scaling_policies: RwLock<HashMap<String, ScalingPolicy>>,
    metrics_collector: Box<dyn MetricsCollector>,
    cooldown_periods: RwLock<HashMap<String, SystemTime>>,
}

trait ClusterManager: Send + Sync {
    fn add_node(&self, node_type: &str, count: usize) -> Result<Vec<String>, String>;
    fn remove_node(&self, node_id: &str) -> Result<bool, String>;
    fn get_nodes(&self, node_type: Option<&str>) -> Result<Vec<NodeInfo>, String>;
    fn get_node_metrics(&self, node_id: &str) -> Result<NodeMetrics, String>;
}

trait MetricsCollector: Send + Sync {
    fn collect_metrics(&self, service_id: &str) -> Result<ServiceMetrics, String>;
    fn collect_node_metrics(&self, node_id: &str) -> Result<NodeMetrics, String>;
    fn collect_cluster_metrics(&self) -> Result<ClusterMetrics, String>;
}

struct ScalingPolicy {
    id: String,
    service_id: String,
    node_type: String,
    min_nodes: usize,
    max_nodes: usize,
    target_cpu_utilization: f64,
    target_memory_utilization: f64,
    cooldown_period: Duration,
    scaling_step: usize,
}

struct NodeInfo {
    id: String,
    node_type: String,
    address: SocketAddr,
    status: NodeStatus,
    start_time: SystemTime,
    tags: HashMap<String, String>,
}

enum NodeStatus {
    Starting,
    Running,
    Draining,
    Stopped,
    Failed,
}

struct NodeMetrics {
    node_id: String,
    cpu_utilization: f64,
    memory_utilization: f64,
    disk_utilization: f64,
    network_in_bytes: u64,
    network_out_bytes: u64,
    timestamp: SystemTime,
}

struct ServiceMetrics {
    service_id: String,
    instance_count: usize,
    avg_cpu_utilization: f64,
    avg_memory_utilization: f64,
    requests_per_second: f64,
    avg_response_time: Duration,
    error_rate: f64,
    timestamp: SystemTime,
}

struct ClusterMetrics {
    nodes_count: usize,
    avg_cpu_utilization: f64,
    avg_memory_utilization: f64,
    total_network_in_bytes: u64,
    total_network_out_bytes: u64,
    timestamp: SystemTime,
}

impl AutoScalingManager {
    fn new(
        node_id: &str,
        cluster_manager: Box<dyn ClusterManager>,
        metrics_collector: Box<dyn MetricsCollector>,
    ) -> Self {
        AutoScalingManager {
            node_id: node_id.to_string(),
            cluster_manager,
            scaling_policies: RwLock::new(HashMap::new()),
            metrics_collector,
            cooldown_periods: RwLock::new(HashMap::new()),
        }
    }

    fn add_scaling_policy(&self, policy: ScalingPolicy) -> Result<(), String> {
        println!("æ·»åŠ ä¼¸ç¼©ç­–ç•¥: {}", policy.id);

        let mut policies = self.scaling_policies.write().unwrap();
        policies.insert(policy.id.clone(), policy);

        Ok(())
    }

    fn remove_scaling_policy(&self, policy_id: &str) -> Result<bool, String> {
        println!("ç§»é™¤ä¼¸ç¼©ç­–ç•¥: {}", policy_id);

        let mut policies = self.scaling_policies.write().unwrap();
        Ok(policies.remove(policy_id).is_some())
    }

    fn check_scaling_policies(&self) -> Result<(), String> {
        println!("æ£€æŸ¥ä¼¸ç¼©ç­–ç•¥...");

        let policies = self.scaling_policies.read().unwrap();
        let mut cooldown_periods = self.cooldown_periods.write().unwrap();

        for policy in policies.values() {
            // æ£€æŸ¥å†·å´æœŸ
            if let Some(last_scaling) = cooldown_periods.get(&policy.id) {
                let elapsed = SystemTime::now().duration_since(*last_scaling).unwrap();
                if elapsed < policy.cooldown_period {
                    println!("ç­–ç•¥ {} åœ¨å†·å´æœŸå†…ï¼Œè·³è¿‡", policy.id);
                    continue;
                }
            }

            // è·å–æœåŠ¡æŒ‡æ ‡
            let metrics = self.metrics_collector.collect_metrics(&policy.service_id)?;

            // è·å–å½“å‰èŠ‚ç‚¹æ•°
            let nodes = self.cluster_manager.get_nodes(Some(&policy.node_type))?;
            let current_nodes = nodes.len();

            // æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•
            if metrics.avg_cpu_utilization > policy.target_cpu_utilization && current_nodes < policy.max_nodes {
                // éœ€è¦æ‰©å±•
                let nodes_to_add = policy.scaling_step.min(policy.max_nodes - current_nodes);
                println!("è§¦å‘æ‰©å±•: æœåŠ¡ {}, æ·»åŠ  {} ä¸ªèŠ‚ç‚¹", policy.service_id, nodes_to_add);

                let new_nodes = self.cluster_manager.add_node(&policy.node_type, nodes_to_add)?;
                println!("æ–°èŠ‚ç‚¹å·²æ·»åŠ : {:?}", new_nodes);

                // æ›´æ–°å†·å´æœŸ
                cooldown_periods.insert(policy.id.clone(), SystemTime::now());
            }
            // æ£€æŸ¥æ˜¯å¦éœ€è¦æ”¶ç¼©
            else if metrics.avg_cpu_utilization < policy.target_cpu_utilization * 0.5 && current_nodes > policy.min_nodes {
                // éœ€è¦æ”¶ç¼©
                let nodes_to_remove = policy.scaling_step.min(current_nodes - policy.min_nodes);
                println!("è§¦å‘æ”¶ç¼©: æœåŠ¡ {}, ç§»é™¤ {} ä¸ªèŠ‚ç‚¹", policy.service_id, nodes_to_remove);

                // é€‰æ‹©è¦ç§»é™¤çš„èŠ‚ç‚¹ï¼ˆæŒ‰CPUä½¿ç”¨ç‡æœ€ä½çš„é¡ºåºï¼‰
                let mut node_metrics: Vec<_> = nodes.iter()
                    .map(|node| {
                        let metrics = self.cluster_manager.get_node_metrics(&node.id).unwrap_or(NodeMetrics {
                            node_id: node.id.clone(),
                            cpu_utilization: 0.0,
                            memory_utilization: 0.0,
                            disk_utilization: 0.0,
                            network_in_bytes: 0,
                            network_out_bytes: 0,
                            timestamp: SystemTime::now(),
                        });
                        (node, metrics)
                    })
                    .collect();

                node_metrics.sort_by(|a, b| a.1.cpu_utilization.partial_cmp(&b.1.cpu_utilization).unwrap());

                // ç§»é™¤èŠ‚ç‚¹
                for i in 0..nodes_to_remove {
                    if i < node_metrics.len() {
                        let node_id = &node_metrics[i].0.id;
                        println!("ç§»é™¤èŠ‚ç‚¹: {}", node_id);
                        self.cluster_manager.remove_node(node_id)?;
                    }
                }

                // æ›´æ–°å†·å´æœŸ
                cooldown_periods.insert(policy.id.clone(), SystemTime::now());
            }
        }

        Ok(())
    }

    fn manual_scale(&self, service_id: &str, node_count: usize) -> Result<(), String> {
        println!("æ‰‹åŠ¨ä¼¸ç¼©æœåŠ¡ {} åˆ° {} ä¸ªèŠ‚ç‚¹", service_id, node

```rust
        println!("æ‰‹åŠ¨ä¼¸ç¼©æœåŠ¡ {} åˆ° {} ä¸ªèŠ‚ç‚¹", service_id, node_count);

        // æŸ¥æ‰¾æœåŠ¡å¯¹åº”çš„ä¼¸ç¼©ç­–ç•¥
        let policies = self.scaling_policies.read().unwrap();
        let policy = policies.values().find(|p| p.service_id == service_id)
            .ok_or_else(|| format!("æ²¡æœ‰æ‰¾åˆ°æœåŠ¡çš„ä¼¸ç¼©ç­–ç•¥: {}", service_id))?;

        // éªŒè¯èŠ‚ç‚¹æ•°æ˜¯å¦åœ¨å…è®¸èŒƒå›´å†…
        if node_count < policy.min_nodes || node_count > policy.max_nodes {
            return Err(format!("èŠ‚ç‚¹æ•° {} è¶…å‡ºå…è®¸èŒƒå›´ ({} - {})",
                node_count, policy.min_nodes, policy.max_nodes));
        }

        // è·å–å½“å‰èŠ‚ç‚¹
        let nodes = self.cluster_manager.get_nodes(Some(&policy.node_type))?;
        let current_count = nodes.len();

        if current_count < node_count {
            // éœ€è¦æ·»åŠ èŠ‚ç‚¹
            let nodes_to_add = node_count - current_count;
            println!("æ·»åŠ  {} ä¸ªèŠ‚ç‚¹", nodes_to_add);

            let new_nodes = self.cluster_manager.add_node(&policy.node_type, nodes_to_add)?;
            println!("æ–°èŠ‚ç‚¹å·²æ·»åŠ : {:?}", new_nodes);
        } else if current_count > node_count {
            // éœ€è¦ç§»é™¤èŠ‚ç‚¹
            let nodes_to_remove = current_count - node_count;
            println!("ç§»é™¤ {} ä¸ªèŠ‚ç‚¹", nodes_to_remove);

            // é€‰æ‹©è¦ç§»é™¤çš„èŠ‚ç‚¹ï¼ˆæŒ‰CPUä½¿ç”¨ç‡æœ€ä½çš„é¡ºåºï¼‰
            let mut node_metrics: Vec<_> = nodes.iter()
                .map(|node| {
                    let metrics = self.cluster_manager.get_node_metrics(&node.id).unwrap_or(NodeMetrics {
                        node_id: node.id.clone(),
                        cpu_utilization: 0.0,
                        memory_utilization: 0.0,
                        disk_utilization: 0.0,
                        network_in_bytes: 0,
                        network_out_bytes: 0,
                        timestamp: SystemTime::now(),
                    });
                    (node, metrics)
                })
                .collect();

            node_metrics.sort_by(|a, b| a.1.cpu_utilization.partial_cmp(&b.1.cpu_utilization).unwrap());

            // ç§»é™¤èŠ‚ç‚¹
            for i in 0..nodes_to_remove {
                if i < node_metrics.len() {
                    let node_id = &node_metrics[i].0.id;
                    println!("ç§»é™¤èŠ‚ç‚¹: {}", node_id);
                    self.cluster_manager.remove_node(node_id)?;
                }
            }
        } else {
            println!("å½“å‰èŠ‚ç‚¹æ•°å·²ç»æ˜¯ç›®æ ‡æ•°é‡: {}", node_count);
        }

        // æ›´æ–°å†·å´æœŸ
        let mut cooldown_periods = self.cooldown_periods.write().unwrap();
        cooldown_periods.insert(policy.id.clone(), SystemTime::now());

        Ok(())
    }

    fn get_scaling_recommendations(&self) -> Result<Vec<ScalingRecommendation>, String> {
        println!("ç”Ÿæˆä¼¸ç¼©å»ºè®®...");

        let mut recommendations = Vec::new();
        let policies = self.scaling_policies.read().unwrap();

        for policy in policies.values() {
            // è·å–æœåŠ¡æŒ‡æ ‡
            let metrics = self.metrics_collector.collect_metrics(&policy.service_id)?;

            // è·å–å½“å‰èŠ‚ç‚¹æ•°
            let nodes = self.cluster_manager.get_nodes(Some(&policy.node_type))?;
            let current_nodes = nodes.len();

            // è®¡ç®—å»ºè®®çš„èŠ‚ç‚¹æ•°
            let cpu_factor = metrics.avg_cpu_utilization / policy.target_cpu_utilization;
            let memory_factor = metrics.avg_memory_utilization / policy.target_memory_utilization;
            let scale_factor = cpu_factor.max(memory_factor);

            let recommended_nodes = (current_nodes as f64 * scale_factor).round() as usize;
            let clamped_nodes = recommended_nodes.clamp(policy.min_nodes, policy.max_nodes);

            if (clamped_nodes as f64 / current_nodes as f64) > 1.1 || (clamped_nodes as f64 / current_nodes as f64) < 0.9 {
                // åªæœ‰å½“æ¨èçš„å˜åŒ–è¶…è¿‡10%æ—¶æ‰æ¨è
                recommendations.push(ScalingRecommendation {
                    service_id: policy.service_id.clone(),
                    current_nodes,
                    recommended_nodes: clamped_nodes,
                    cpu_utilization: metrics.avg_cpu_utilization,
                    memory_utilization: metrics.avg_memory_utilization,
                    requests_per_second: metrics.requests_per_second,
                });
            }
        }

        Ok(recommendations)
    }
}

struct ScalingRecommendation {
    service_id: String,
    current_nodes: usize,
    recommended_nodes: usize,
    cpu_utilization: f64,
    memory_utilization: f64,
    requests_per_second: f64,
}

// è¾¹ç¼˜è®¡ç®—ç½‘å…³
struct EdgeComputeGateway {
    node_id: String,
    location: GeoLocation,
    devices: RwLock<HashMap<String, EdgeDevice>>,
    functions: RwLock<HashMap<String, EdgeFunction>>,
    data_router: DataRouter,
    metrics_collector: Box<dyn MetricsCollector>,
    central_sync_manager: CentralSyncManager,
}

struct GeoLocation {
    latitude: f64,
    longitude: f64,
    region: String,
    datacenter: Option<String>,
}

struct EdgeDevice {
    id: String,
    device_type: String,
    location: Option<GeoLocation>,
    status: DeviceStatus,
    capabilities: DeviceCapabilities,
    last_heartbeat: SystemTime,
    connected_at: SystemTime,
}

struct DeviceCapabilities {
    cpu_cores: u32,
    memory_mb: u64,
    storage_mb: u64,
    supported_protocols: Vec<String>,
    features: HashMap<String, String>,
}

enum DeviceStatus {
    Online,
    Offline,
    Degraded,
    Maintenance,
}

struct EdgeFunction {
    id: String,
    name: String,
    version: String,
    runtime: String,
    code: Vec<u8>,
    memory_mb: u64,
    timeout: Duration,
    environment: HashMap<String, String>,
    triggers: Vec<FunctionTrigger>,
}

enum FunctionTrigger {
    Http { path: String, method: String },
    Timer { schedule: String },
    Event { source: String, type_name: String },
    DeviceData { device_type: String, data_type: String },
}

struct DataRouter {
    rules: RwLock<Vec<RoutingRule>>,
    local_cache: RwLock<HashMap<String, CachedData>>,
    offload_policy: OffloadPolicy,
}

struct RoutingRule {
    id: String,
    priority: u32,
    condition: String,
    action: RoutingAction,
    description: String,
}

enum RoutingAction {
    Forward { target: String },
    Transform { transformation: String, then: Box<RoutingAction> },
    Split { targets: Vec<String>, strategy: SplitStrategy },
    Cache { ttl: Duration, then: Box<RoutingAction> },
    Filter { criteria: String, then: Box<RoutingAction> },
    Execute { function_id: String },
}

enum SplitStrategy {
    RoundRobin,
    Percentile { weights: Vec<u32> },
    ContentBased { field: String },
}

struct CentralSyncManager {
    central_endpoint: String,
    sync_interval: Duration,
    last_sync: RwLock<SystemTime>,
    sync_items: RwLock<HashMap<String, SystemTime>>,
    offline_queue: RwLock<Vec<SyncItem>>,
}

struct SyncItem {
    id: String,
    item_type: String,
    data: Vec<u8>,
    timestamp: SystemTime,
    priority: u32,
}

struct OffloadPolicy {
    cpu_threshold: f64,
    memory_threshold: f64,
    bandwidth_threshold: f64,
    latency_requirement: Option<Duration>,
    preferred_targets: Vec<String>,
}

impl EdgeComputeGateway {
    fn new(
        node_id: &str,
        location: GeoLocation,
        central_endpoint: &str,
        metrics_collector: Box<dyn MetricsCollector>,
    ) -> Self {
        let data_router = DataRouter {
            rules: RwLock::new(Vec::new()),
            local_cache: RwLock::new(HashMap::new()),
            offload_policy: OffloadPolicy {
                cpu_threshold: 0.8,
                memory_threshold: 0.8,
                bandwidth_threshold: 0.7,
                latency_requirement: Some(Duration::from_millis(100)),
                preferred_targets: Vec::new(),
            },
        };

        let central_sync_manager = CentralSyncManager {
            central_endpoint: central_endpoint.to_string(),
            sync_interval: Duration::from_secs(300), // 5åˆ†é’Ÿ
            last_sync: RwLock::new(SystemTime::now()),
            sync_items: RwLock::new(HashMap::new()),
            offline_queue: RwLock::new(Vec::new()),
        };

        EdgeComputeGateway {
            node_id: node_id.to_string(),
            location,
            devices: RwLock::new(HashMap::new()),
            functions: RwLock::new(HashMap::new()),
            data_router,
            metrics_collector,
            central_sync_manager,
        }
    }

    fn register_device(&self, device: EdgeDevice) -> Result<(), String> {
        println!("æ³¨å†Œè¾¹ç¼˜è®¾å¤‡: {}", device.id);

        let mut devices = self.devices.write().unwrap();
        devices.insert(device.id.clone(), device);

        // æ·»åŠ åˆ°åŒæ­¥é¡¹ç›®
        let mut sync_items = self.central_sync_manager.sync_items.write().unwrap();
        sync_items.insert(format!("device:{}", device.id), SystemTime::now());

        Ok(())
    }

    fn update_device_status(&self, device_id: &str, status: DeviceStatus) -> Result<(), String> {
        println!("æ›´æ–°è®¾å¤‡çŠ¶æ€: {}, çŠ¶æ€: {:?}", device_id, status);

        let mut devices = self.devices.write().unwrap();

        if let Some(device) = devices.get_mut(device_id) {
            device.status = status;
            device.last_heartbeat = SystemTime::now();

            // æ·»åŠ åˆ°åŒæ­¥é¡¹ç›®
            let mut sync_items = self.central_sync_manager.sync_items.write().unwrap();
            sync_items.insert(format!("device:{}", device_id), SystemTime::now());

            Ok(())
        } else {
            Err(format!("è®¾å¤‡ä¸å­˜åœ¨: {}", device_id))
        }
    }

    fn register_function(&self, function: EdgeFunction) -> Result<(), String> {
        println!("æ³¨å†Œè¾¹ç¼˜å‡½æ•°: {}", function.name);

        let mut functions = self.functions.write().unwrap();
        functions.insert(function.id.clone(), function);

        // æ·»åŠ åˆ°åŒæ­¥é¡¹ç›®
        let mut sync_items = self.central_sync_manager.sync_items.write().unwrap();
        sync_items.insert(format!("function:{}", function.id), SystemTime::now());

        Ok(())
    }

    fn execute_function(&self, function_id: &str, input: &[u8]) -> Result<Vec<u8>, String> {
        println!("æ‰§è¡Œå‡½æ•°: {}", function_id);

        let functions = self.functions.read().unwrap();

        let function = functions.get(function_id)
            .ok_or_else(|| format!("å‡½æ•°ä¸å­˜åœ¨: {}", function_id))?;

        // æ£€æŸ¥æ˜¯å¦åº”è¯¥å¸è½½å‡½æ•°æ‰§è¡Œ
        let should_offload = self.should_offload_function(function)?;

        if should_offload {
            println!("å¸è½½å‡½æ•°æ‰§è¡Œåˆ°äº‘ç«¯");
            return self.offload_function_execution(function_id, input);
        }

        // åœ¨æœ¬åœ°æ‰§è¡Œå‡½æ•°
        println!("åœ¨æœ¬åœ°æ‰§è¡Œå‡½æ•°: {}", function.name);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåˆ›å»ºé€‚å½“çš„è¿è¡Œæ—¶ç¯å¢ƒå¹¶æ‰§è¡Œå‡½æ•°ä»£ç 
        // è¿™é‡Œç®€å•æ¨¡æ‹Ÿå‡½æ•°æ‰§è¡Œ
        let result = format!("å‡½æ•° {} æ‰§è¡Œç»“æœï¼šè¾“å…¥å¤§å° {}", function.name, input.len()).into_bytes();

        Ok(result)
    }

    fn should_offload_function(&self, function: &EdgeFunction) -> Result<bool, String> {
        // è·å–å½“å‰ç³»ç»ŸæŒ‡æ ‡
        let metrics = self.metrics_collector.collect_node_metrics(&self.node_id)?;

        // æ£€æŸ¥èµ„æºä½¿ç”¨ç‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if metrics.cpu_utilization > self.data_router.offload_policy.cpu_threshold ||
           metrics.memory_utilization > self.data_router.offload_policy.memory_threshold {
            return Ok(true);
        }

        // æ£€æŸ¥å‡½æ•°å†…å­˜éœ€æ±‚æ˜¯å¦è¶…è¿‡å¯ç”¨å†…å­˜
        let available_memory_mb = (1.0 - metrics.memory_utilization) * 1024.0; // å‡è®¾æ€»å†…å­˜ä¸º1GB
        if function.memory_mb as f64 > available_memory_mb {
            return Ok(true);
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿˜å¯ä»¥è€ƒè™‘æ›´å¤šå› ç´ ï¼Œå¦‚ç½‘ç»œå»¶è¿Ÿã€å¸¦å®½ä½¿ç”¨ç‡ç­‰

        Ok(false)
    }

    fn offload_function_execution(&self, function_id: &str, input: &[u8]) -> Result<Vec<u8>, String> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨äº‘ç«¯APIæ‰§è¡Œå‡½æ•°
        // è¿™é‡Œç®€å•æ¨¡æ‹Ÿ
        println!("å°†å‡½æ•° {} å¸è½½åˆ°äº‘ç«¯æ‰§è¡Œ", function_id);

        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        thread::sleep(Duration::from_millis(100));

        let result = format!("äº‘ç«¯æ‰§è¡Œå‡½æ•° {} çš„ç»“æœï¼šè¾“å…¥å¤§å° {}", function_id, input.len()).into_bytes();

        Ok(result)
    }

    fn process_device_data(&self, device_id: &str, data_type: &str, data: &[u8]) -> Result<(), String> {
        println!("å¤„ç†è®¾å¤‡æ•°æ®: è®¾å¤‡ {}, æ•°æ®ç±»å‹ {}", device_id, data_type);

        // éªŒè¯è®¾å¤‡
        let devices = self.devices.read().unwrap();
        let device = devices.get(device_id)
            .ok_or_else(|| format!("è®¾å¤‡ä¸å­˜åœ¨: {}", device_id))?;

        if device.status != DeviceStatus::Online {
            return Err(format!("è®¾å¤‡ä¸åœ¨çº¿: {}", device_id));
        }

        // æŸ¥æ‰¾ä¸æ•°æ®ç±»å‹åŒ¹é…çš„å‡½æ•°è§¦å‘å™¨
        let functions = self.functions.read().unwrap();
        let matching_functions: Vec<_> = functions.values()
            .filter(|f| f.triggers.iter().any(|t| matches!(t, FunctionTrigger::DeviceData { device_type, data_type: dt } if *device_type == device.device_type && *dt == data_type)))
            .collect();

        // æ‰§è¡ŒåŒ¹é…çš„å‡½æ•°
        for function in matching_functions {
            println!("è§¦å‘å‡½æ•° {} å¤„ç†è®¾å¤‡æ•°æ®", function.name);
            self.execute_function(&function.id, data)?;
        }

        // åº”ç”¨è·¯ç”±è§„åˆ™
        self.route_data(device_id, data_type, data)?;

        Ok(())
    }

    fn route_data(&self, source: &str, data_type: &str, data: &[u8]) -> Result<(), String> {
        println!("è·¯ç”±æ•°æ®: æ¥æº {}, ç±»å‹ {}", source, data_type);

        let rules = self.data_router.rules.read().unwrap();

        // æŒ‰ä¼˜å…ˆçº§æ’åºè§„åˆ™
        let mut sorted_rules = rules.clone();
        sorted_rules.sort_by(|a, b| b.priority.cmp(&a.priority));

        // æŸ¥æ‰¾åŒ¹é…çš„è§„åˆ™
        for rule in &sorted_rules {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè¯„ä¼°æ¡ä»¶è¡¨è¾¾å¼
            let condition_matches = true; // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰è§„åˆ™éƒ½åŒ¹é…

            if condition_matches {
                println!("æ•°æ®åŒ¹é…è§„åˆ™: {}", rule.id);
                self.apply_routing_action(&rule.action, source, data_type, data)?;
                break;
            }
        }

        Ok(())
    }

    fn apply_routing_action(&self, action: &RoutingAction, source: &str, data_type: &str, data: &[u8]) -> Result<(), String> {
        match action {
            RoutingAction::Forward { target } => {
                println!("è½¬å‘æ•°æ®åˆ°: {}", target);
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€æ•°æ®åˆ°ç›®æ ‡
            },
            RoutingAction::Transform { transformation, then } => {
                println!("è½¬æ¢æ•°æ®ï¼Œä½¿ç”¨è½¬æ¢: {}", transformation);
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåº”ç”¨æ•°æ®è½¬æ¢
                let transformed_data = data.to_vec(); // ç®€åŒ–ï¼šä¸åšå®é™…è½¬æ¢
                self.apply_routing_action(then, source, data_type, &transformed_data)?;
            },
            RoutingAction::Split { targets, strategy } => {
                println!("æ‹†åˆ†æ•°æ®åˆ°å¤šä¸ªç›®æ ‡ï¼Œä½¿ç”¨ç­–ç•¥: {:?}", strategy);
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®ç­–ç•¥å°†æ•°æ®å‘é€åˆ°å¤šä¸ªç›®æ ‡
                for target in targets {
                    println!("æ‹†åˆ†æ•°æ®åˆ°ç›®æ ‡: {}", target);
                }
            },
            RoutingAction::Cache { ttl, then } => {
                println!("ç¼“å­˜æ•°æ®ï¼ŒTTL: {:?}", ttl);

                // å°†æ•°æ®æ·»åŠ åˆ°æœ¬åœ°ç¼“å­˜
                let key = format!("{}:{}", source, data_type);
                let mut local_cache = self.data_router.local_cache.write().unwrap();
                local_cache.insert(key, CachedData {
                    id: format!("{}:{}", source, data_type),
                    data: data.to_vec(),
                    size: data.len() as u64,
                    last_accessed: SystemTime::now(),
                    access_count: 1,
                });

                // ç»§ç»­å¤„ç†
                self.apply_routing_action(then, source, data_type, data)?;
            },
            RoutingAction::Filter { criteria, then } => {
                println!("è¿‡æ»¤æ•°æ®ï¼Œä½¿ç”¨æ¡ä»¶: {}", criteria);

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè¯„ä¼°è¿‡æ»¤æ¡ä»¶
                let passes_filter = true; // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰æ•°æ®éƒ½é€šè¿‡è¿‡æ»¤

                if passes_filter {
                    self.apply_routing_action(then, source, data_type, data)?;
                } else {
                    println!("æ•°æ®è¢«è¿‡æ»¤æ‰");
                }
            },
            RoutingAction::Execute { function_id } => {
                println!("æ‰§è¡Œå‡½æ•°: {}", function_id);
                self.execute_function(function_id, data)?;
            },
        }

        Ok(())
    }

    fn sync_with_central(&self) -> Result<(), String> {
        println!("ä¸ä¸­å¤®æœåŠ¡å™¨åŒæ­¥...");

        // æ›´æ–°ä¸Šæ¬¡åŒæ­¥æ—¶é—´
        {
            let mut last_sync = self.central_sync_manager.last_sync.write().unwrap();
            *last_sync = SystemTime::now();
        }

        // è·å–éœ€è¦åŒæ­¥çš„é¡¹ç›®
        let sync_items = self.central_sync_manager.sync_items.read().unwrap();
        let items_to_sync: Vec<_> = sync_items.iter().collect();

        for (item_key, timestamp) in items_to_sync {
            println!("åŒæ­¥é¡¹ç›®: {}, æ—¶é—´æˆ³: {:?}", item_key, timestamp);

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†é¡¹ç›®æ•°æ®å‘é€åˆ°ä¸­å¤®æœåŠ¡å™¨
        }

        // å¤„ç†ç¦»çº¿é˜Ÿåˆ—
        let mut offline_queue = self.central_sync_manager.offline_queue.write().unwrap();

        if !offline_queue.is_empty() {
            println!("å¤„ç†ç¦»çº¿é˜Ÿåˆ—ä¸­çš„ {} ä¸ªé¡¹ç›®", offline_queue.len());

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°è¯•å°†ç¦»çº¿é˜Ÿåˆ—ä¸­çš„é¡¹ç›®å‘é€åˆ°ä¸­å¤®æœåŠ¡å™¨
            offline_queue.clear();
        }

        Ok(())
    }
}

// ç‰©è”ç½‘ç½‘å…³
struct IoTGateway {
    node_id: String,
    protocols: RwLock<HashMap<String, Box<dyn IoTProtocol>>>,
    devices: RwLock<HashMap<String, IoTDevice>>,
    message_broker: Box<dyn MessageBroker>,
    data_processor: DataProcessor,
    security_manager: SecurityManager,
}

trait IoTProtocol: Send + Sync {
    fn get_name(&self) -> String;
    fn initialize(&self) -> Result<(), String>;
    fn start(&self) -> Result<(), String>;
    fn stop(&self) -> Result<(), String>;
    fn send_message(&self, device_id: &str, payload: &[u8]) -> Result<(), String>;
}

struct IoTDevice {
    id: String,
    name: String,
    device_type: String,
    protocol: String,
    status: DeviceStatus,
    last_seen: SystemTime,
    metadata: HashMap<String, String>,
    telemetry: RwLock<HashMap<String, TelemetryValue>>,
}

struct TelemetryValue {
    name: String,
    value: serde_json::Value,
    timestamp: SystemTime,
    quality: DataQuality,
}

enum DataQuality {
    Good,
    Uncertain,
    Bad,
}

trait MessageBroker: Send + Sync {
    fn publish(&self, topic: &str, payload: &[u8], qos: u8) -> Result<(), String>;
    fn subscribe(&self, topic: &str, qos: u8, callback: Box<dyn Fn(&str, &[u8]) + Send + Sync>) -> Result<String, String>;
    fn unsubscribe(&self, subscription_id: &str) -> Result<(), String>;
}

struct DataProcessor {
    pipelines: RwLock<HashMap<String, DataPipeline>>,
    rules_engine: RulesEngine,
}

struct DataPipeline {
    id: String,
    name: String,
    stages: Vec<PipelineStage>,
    source: String,
    sink: String,
}

enum PipelineStage {
    Filter { condition: String },
    Transform { transformation: String },
    Enrich { sources: Vec<String> },
    Aggregate { window: String, function: String },
    Split { by: String },
}

struct RulesEngine {
    rules: RwLock<Vec<Rule>>,
    triggers: RwLock<HashMap<String, Vec<String>>>,
}

struct Rule {
    id: String,
    name: String,
    condition: String,
    actions: Vec<RuleAction>,
    enabled: bool,
}

enum RuleAction {
    SendCommand { device_id: String, command: String, params: HashMap<String, String> },
    PublishEvent { topic: String, payload: String },
    UpdateState { key: String, value: String },
    InvokeWebhook { url: String, method: String, headers: HashMap<String, String>, body: String },
    SendAlert { severity: AlertSeverity, message: String, targets: Vec<String> },
}

enum AlertSeverity {
    Info,
    Warning,
    Error,
    Critical,
}

struct SecurityManager {
    device_credentials: RwLock<HashMap<String, DeviceCredential>>,
    certificate_manager: CertificateManager,
    access_control: AccessControlManager,
}

struct DeviceCredential {
    device_id: String,
    credential_type: CredentialType,
    credential_data: HashMap<String, String>,
    created_at: SystemTime,
    expires_at: Option<SystemTime>,
}

enum CredentialType {
    Password,
    Token,
    X509Certificate,
    SymmetricKey,
}

struct CertificateManager {
    ca_certificate: String,
    device_certificates: RwLock<HashMap<String, String>>,
}

struct AccessControlManager {
    policies: RwLock<HashMap<String, AccessPolicy>>,
}

struct AccessPolicy {
    id: String,
    name: String,
    resources: Vec<String>,
    permissions: Vec<String>,
    subjects: Vec<String>,
}

impl IoTGateway {
    fn new(
        node_id: &str,
        message_broker: Box<dyn MessageBroker>,
    ) -> Self {
        let data_processor = DataProcessor {
            pipelines: RwLock::new(HashMap::new()),
            rules_engine: RulesEngine {
                rules: RwLock::new(Vec::new()),
                triggers: RwLock::new(HashMap::new()),
            },
        };

        let security_manager = SecurityManager {
            device_credentials: RwLock::new(HashMap::new()),
            certificate_manager: CertificateManager {
                ca_certificate: String::new(),
                device_certificates: RwLock::new(HashMap::new()),
            },
            access_control: AccessControlManager {
                policies: RwLock::new(HashMap::new()),
            },
        };

        IoTGateway {
            node_id: node_id.to_string(),
            protocols: RwLock::new(HashMap::new()),
            devices: RwLock::new(HashMap::new()),
            message_broker,
            data_processor,
            security_manager,
        }
    }

    fn register_protocol(&self, protocol: Box<dyn IoTProtocol>) -> Result<(), String> {
        let protocol_name = protocol.get_name();
        println!("æ³¨å†Œåè®®: {}", protocol_name);

        let mut protocols = self.protocols.write().unwrap();
        protocols.insert(protocol_name, protocol);

        Ok(())
    }

    fn start_protocols(&self) -> Result<(), String> {
        println!("å¯åŠ¨æ‰€æœ‰åè®®...");

        let protocols = self.protocols.read().unwrap();

        for (name, protocol) in protocols.iter() {
            println!("åˆå§‹åŒ–åè®®: {}", name);
            protocol.initialize()?;

            println!("å¯åŠ¨åè®®: {}", name);
            protocol.start()?;
        }

        Ok(())
    }

    fn register_device(&self, device: IoTDevice) -> Result<(), String> {
        println!("æ³¨å†Œè®¾å¤‡: {}", device.name);

        // éªŒè¯è®¾å¤‡åè®®
        {
            let protocols = self.protocols.read().unwrap();
            if !protocols.contains_key(&device.protocol) {
                return Err(format!("ä¸æ”¯æŒçš„åè®®: {}", device.protocol));
            }
        }

        // æ·»åŠ è®¾å¤‡
        let mut devices = self.devices.write().unwrap();
        devices.insert(device.id.clone(), device);

        Ok(())
    }

    fn process_device_message(&self, device_id: &str, payload: &[u8]) -> Result<(), String> {
        println!("å¤„ç†è®¾å¤‡æ¶ˆæ¯: {}", device_id);

        // éªŒè¯è®¾å¤‡
        let mut devices = self.devices.write().unwrap();
        let device = devices.get_mut(device_id)
            .ok_or_else(|| format!("è®¾å¤‡ä¸å­˜åœ¨: {}", device_id))?;

        // æ›´æ–°è®¾å¤‡çŠ¶æ€
        device.status = DeviceStatus::Online;
        device.last_seen = SystemTime::now();

        // è§£ææ¶ˆæ¯ï¼ˆç®€åŒ–ï¼Œå®é™…å®ç°ä¸­éœ€è¦æ ¹æ®åè®®è§£æï¼‰
        // è¿™é‡Œå‡è®¾æ¶ˆæ¯æ˜¯JSONæ ¼å¼
        let json_result: Result<serde_json::Value, _> = serde_json::from_slice(payload);

        match json_result {
            Ok(json) => {
                // æå–é¥æµ‹æ•°æ®
                if let Some(telemetry) = json.get("telemetry").and_then(|t| t.as_object()) {
                    let mut device_telemetry = device.telemetry.write().unwrap();

                    for (key, value) in telemetry {
                        device_telemetry.insert(key.clone(), TelemetryValue {
                            name: key.clone(),
                            value: value.clone(),
                            timestamp: SystemTime::now(),
                            quality: DataQuality::Good,
                        });
                    }
                }

                // å‘å¸ƒæ¶ˆæ¯åˆ°æ¶ˆæ¯ä»£ç†
                let topic = format!("devices/{}/messages", device_id);
                self.message_broker.publish(&topic, payload, 1)?;

                // å¤„ç†è§„åˆ™
                self.process_rules(device_id, &json)?;
            },
            Err(err) => {
                println!("è§£æè®¾å¤‡æ¶ˆæ¯å¤±è´¥: {}", err);
                return Err(format!("è§£ææ¶ˆæ¯å¤±è´¥: {}", err));
            }
        }

        Ok(())
    }

    fn process_rules(&self, device_id: &str, data: &serde_json::Value) -> Result<(), String> {
        println!("å¤„ç†è®¾å¤‡è§„åˆ™: {}", device_id);

        let rules = self.data_processor.rules_engine.rules.read().unwrap();

        for rule in rules.iter() {
            if !rule.enabled {
                continue;
            }

            // è¯„ä¼°è§„åˆ™æ¡ä»¶ï¼ˆç®€åŒ–ï¼Œå®é™…å®ç°ä¸­éœ€è¦è§£æå’Œè¯„ä¼°æ¡ä»¶è¡¨è¾¾å¼ï¼‰
            let condition_met = true; // å‡è®¾æ¡ä»¶æ€»æ˜¯æ»¡è¶³

            if condition_met {
                println!("è§„åˆ™æ¡ä»¶æ»¡è¶³: {}", rule.name);

                // æ‰§è¡Œè§„åˆ™åŠ¨ä½œ
                for action in &rule.actions {
                    self.execute_rule_action(action, device_id, data)?;
                }
            }
        }

        Ok(())
    }

    fn execute_rule_action(&self, action: &RuleAction, device_id: &str, data: &serde_json::Value) -> Result<(), String> {
        match action {
            RuleAction::SendCommand { device_id, command, params } => {
                println!("å‘é€å‘½ä»¤ '{}' åˆ°è®¾å¤‡ {}", command, device_id);

                // æ„é€ å‘½ä»¤æ¶ˆæ¯
                let command_data = serde_json::json!({
                    "command": command,
                    "params": params,
                    "timestamp": SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs()
                });

                // è·å–è®¾å¤‡åè®®
                let devices = self.devices.read().unwrap();
                let device = devices.get(device_id)
                    .ok_or_else(|| format!("è®¾å¤‡ä¸å­˜åœ¨: {}", device_id))?;

                let protocols = self.protocols.read().unwrap();
                let protocol = protocols.get(&device.protocol)
                    .ok_or_else(|| format!("åè®®ä¸å­˜åœ¨: {}", device.protocol))?;

                // å‘é€å‘½ä»¤
                protocol.send_message(device_id, &command_data.to_string().into_bytes())?;
            },
            RuleAction::PublishEvent { topic, payload } => {
                println!("å‘å¸ƒäº‹ä»¶åˆ°ä¸»é¢˜: {}", topic);

                // æ›¿æ¢è´Ÿè½½ä¸­çš„å ä½ç¬¦ï¼ˆç®€åŒ–ï¼‰
                let payload = payload.replace("${deviceId}", device_id);

                // å‘å¸ƒäº‹ä»¶
                self.message_broker.publish(topic, payload.as_bytes(), 1)?;
            },
            RuleAction::UpdateState { key, value } => {
                println!("æ›´æ–°çŠ¶æ€: {} = {}", key, value);

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°è®¾å¤‡æˆ–ç³»ç»ŸçŠ¶æ€
            },
            RuleAction::InvokeWebhook { url, method, headers, body } => {
                println!("è°ƒç”¨Webhook: {} {}", method, url);

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€HTTPè¯·æ±‚
            },
            RuleAction::SendAlert { severity, message, targets } => {
                println!("å‘é€è­¦æŠ¥: {:?} - {}", severity, message);

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘æŒ‡å®šç›®æ ‡å‘é€è­¦æŠ¥
                for target in targets {
                    println!("å‘é€è­¦æŠ¥åˆ°ç›®æ ‡: {}", target);
                }
            },
        }

        Ok(())
    }

    fn send_command_to_device(&self, device_id: &str, command: &str, params: &HashMap<String, String>) -> Result<(), String> {
        println!("å‘é€å‘½ä»¤åˆ°è®¾å¤‡: {}, å‘½ä»¤: {}", device_id, command);

        // éªŒè¯è®¾å¤‡
        let devices = self.devices.read().unwrap();
        let device = devices.get(device_id)
            .ok_or_else(|| format!("è®¾å¤‡ä¸å­˜åœ¨: {}", device_id))?;

        if device.status != DeviceStatus::Online {
            return Err(format!("è®¾å¤‡ä¸åœ¨çº¿: {}", device_id));
        }

        // æ„é€ å‘½ä»¤æ¶ˆæ¯
        let command_data = serde_json::json!({
            "command": command,
            "params": params,
            "timestamp": SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs()
        });

        // è·å–è®¾å¤‡åè®®
        let protocols = self.protocols.read().unwrap();
        let protocol = protocols.get(&device.protocol)
            .ok_or_else(|| format!("åè®®ä¸å­˜åœ¨: {}", device.protocol))?;

        // å‘é€å‘½ä»¤
        protocol.send_message(device_id, &command_data.to_string().into_bytes())?;

        Ok(())
    }

    fn add_device_credential(&self, credential: DeviceCredential) -> Result<(), String> {
        println!("æ·»åŠ è®¾å¤‡å‡­è¯: {}", credential.device_id);

        // éªŒè¯è®¾å¤‡
        {
            let devices = self.devices.read().unwrap();
            if !devices.contains_key(&credential.device_id) {
                return Err(format!("è®¾å¤‡ä¸å­˜åœ¨: {}", credential.device_id));
            }
        }

        // æ·»åŠ å‡­è¯
        let mut credentials = self.security_manager.device_credentials.write().unwrap();
        credentials.insert(credential.device_id.clone(), credential);

        Ok(())
    }

    fn authenticate_device(&self, device_id: &str, credential_type: CredentialType, credential_data: &HashMap<String, String>) -> Result<bool, String> {
        println!("éªŒè¯è®¾å¤‡: {}", device_id);

        let credentials = self.security_manager.device_credentials.read().unwrap();

        if let Some(stored_credential) = credentials.get(device_id) {
            // æ£€æŸ¥å‡­è¯ç±»å‹
            if std::mem::discriminant(&stored_credential.credential_type) != std::mem::discriminant(&credential_type) {
                return Ok(false);
            }

            // æ£€æŸ¥è¿‡æœŸæ—¶é—´
            if let Some(expires_at) = stored_credential.expires_at {
                if SystemTime::now() > expires_at {
                    return Ok(false);
                }
            }

            // éªŒè¯å‡­è¯æ•°æ®ï¼ˆç®€åŒ–ï¼Œå®é™…å®ç°ä¸­éœ€è¦æ ¹æ®å‡­è¯ç±»å‹è¿›è¡Œé€‚å½“çš„éªŒè¯ï¼‰
            match credential_type {
                CredentialType::Password => {
                    let stored_password = stored_credential.credential_data.get("password")
                        .ok_or_else(|| "å­˜å‚¨çš„å‡­è¯ä¸­æ²¡æœ‰å¯†ç ".to_string())?;
                    let provided_password = credential_data.get("password")
                        .ok_or_else(|| "æä¾›çš„å‡­è¯ä¸­æ²¡æœ‰å¯†ç ".to_string())?;

                    Ok(stored_password == provided_password)
                },
                CredentialType::Token => {
                    let stored_token = stored_credential.credential_data.get("token")
                        .ok_or_else(|| "å­˜å‚¨çš„å‡­è¯ä¸­æ²¡æœ‰ä»¤ç‰Œ".to_string())?;
                    let provided_token = credential_data.get("token")

```rust
                    let provided_token = credential_data.get("token")
                        .ok_or_else(|| "æä¾›çš„å‡­è¯ä¸­æ²¡æœ‰ä»¤ç‰Œ".to_string())?;

                    Ok(stored_token == provided_token)
                },
                CredentialType::X509Certificate => {
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šéªŒè¯X.509è¯ä¹¦
                    // ç®€åŒ–å¤„ç†ï¼Œä»…æ£€æŸ¥è¯ä¹¦æŒ‡çº¹
                    let stored_fingerprint = stored_credential.credential_data.get("fingerprint")
                        .ok_or_else(|| "å­˜å‚¨çš„å‡­è¯ä¸­æ²¡æœ‰è¯ä¹¦æŒ‡çº¹".to_string())?;
                    let provided_fingerprint = credential_data.get("fingerprint")
                        .ok_or_else(|| "æä¾›çš„å‡­è¯ä¸­æ²¡æœ‰è¯ä¹¦æŒ‡çº¹".to_string())?;

                    Ok(stored_fingerprint == provided_fingerprint)
                },
                CredentialType::SymmetricKey => {
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨å¯¹ç§°å¯†é’¥è¿›è¡Œè®¤è¯
                    // ç®€åŒ–å¤„ç†ï¼Œä»…æ¯”è¾ƒå¯†é’¥
                    let stored_key = stored_credential.credential_data.get("key")
                        .ok_or_else(|| "å­˜å‚¨çš„å‡­è¯ä¸­æ²¡æœ‰å¯¹ç§°å¯†é’¥".to_string())?;
                    let provided_key = credential_data.get("key")
                        .ok_or_else(|| "æä¾›çš„å‡­è¯ä¸­æ²¡æœ‰å¯¹ç§°å¯†é’¥".to_string())?;

                    Ok(stored_key == provided_key)
                },
            }
        } else {
            Ok(false)
        }
    }

    fn add_access_policy(&self, policy: AccessPolicy) -> Result<(), String> {
        println!("æ·»åŠ è®¿é—®ç­–ç•¥: {}", policy.name);

        let mut policies = self.security_manager.access_control.policies.write().unwrap();
        policies.insert(policy.id.clone(), policy);

        Ok(())
    }

    fn check_access(&self, subject: &str, resource: &str, permission: &str) -> Result<bool, String> {
        println!("æ£€æŸ¥è®¿é—®æƒé™: ä¸»ä½“ {}, èµ„æº {}, æƒé™ {}", subject, resource, permission);

        let policies = self.security_manager.access_control.policies.read().unwrap();

        for policy in policies.values() {
            // æ£€æŸ¥ä¸»ä½“æ˜¯å¦åŒ¹é…
            let subject_matches = policy.subjects.contains(&subject.to_string()) ||
                                 policy.subjects.contains(&"*".to_string());

            // æ£€æŸ¥èµ„æºæ˜¯å¦åŒ¹é…
            let resource_matches = policy.resources.iter().any(|r| {
                if r.ends_with("*") {
                    // é€šé…ç¬¦åŒ¹é…
                    let prefix = &r[0..r.len()-1];
                    resource.starts_with(prefix)
                } else {
                    r == resource
                }
            });

            // æ£€æŸ¥æƒé™æ˜¯å¦åŒ¹é…
            let permission_matches = policy.permissions.contains(&permission.to_string()) ||
                                    policy.permissions.contains(&"*".to_string());

            if subject_matches && resource_matches && permission_matches {
                return Ok(true);
            }
        }

        Ok(false)
    }

    fn add_data_pipeline(&self, pipeline: DataPipeline) -> Result<(), String> {
        println!("æ·»åŠ æ•°æ®å¤„ç†ç®¡é“: {}", pipeline.name);

        let mut pipelines = self.data_processor.pipelines.write().unwrap();
        pipelines.insert(pipeline.id.clone(), pipeline);

        Ok(())
    }

    fn process_data_through_pipeline(&self, pipeline_id: &str, data: &[u8]) -> Result<Vec<u8>, String> {
        println!("é€šè¿‡ç®¡é“å¤„ç†æ•°æ®: {}", pipeline_id);

        let pipelines = self.data_processor.pipelines.read().unwrap();

        let pipeline = pipelines.get(pipeline_id)
            .ok_or_else(|| format!("ç®¡é“ä¸å­˜åœ¨: {}", pipeline_id))?;

        // è§£æè¾“å…¥æ•°æ®
        let mut processed_data: serde_json::Value = serde_json::from_slice(data)
            .map_err(|e| format!("è§£ææ•°æ®å¤±è´¥: {}", e))?;

        // é€šè¿‡ç®¡é“çš„æ¯ä¸ªé˜¶æ®µå¤„ç†æ•°æ®
        for stage in &pipeline.stages {
            match stage {
                PipelineStage::Filter { condition } => {
                    println!("è¿‡æ»¤é˜¶æ®µï¼Œæ¡ä»¶: {}", condition);
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè¯„ä¼°è¿‡æ»¤æ¡ä»¶
                    // å¦‚æœæ¡ä»¶ä¸æ»¡è¶³ï¼Œè¿”å›ç©ºæ•°æ®
                    // ç®€åŒ–å¤„ç†ï¼Œè¿™é‡Œæ€»æ˜¯é€šè¿‡
                },
                PipelineStage::Transform { transformation } => {
                    println!("è½¬æ¢é˜¶æ®µï¼Œè½¬æ¢: {}", transformation);
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåº”ç”¨æ•°æ®è½¬æ¢
                    // ç®€åŒ–å¤„ç†ï¼Œåªæ·»åŠ å¤„ç†æ—¶é—´æˆ³
                    let obj = processed_data.as_object_mut().unwrap();
                    obj.insert("processed_at".to_string(),
                        serde_json::Value::Number(serde_json::Number::from(
                            SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs())));
                },
                PipelineStage::Enrich { sources } => {
                    println!("æ•°æ®å¯ŒåŒ–é˜¶æ®µï¼Œæ¥æº: {:?}", sources);
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»å¤–éƒ¨æºè·å–é¢å¤–æ•°æ®
                    // ç®€åŒ–å¤„ç†ï¼Œåªæ·»åŠ å¯ŒåŒ–æ ‡è®°
                    let obj = processed_data.as_object_mut().unwrap();
                    obj.insert("enriched".to_string(), serde_json::Value::Bool(true));
                },
                PipelineStage::Aggregate { window, function } => {
                    println!("èšåˆé˜¶æ®µï¼Œçª—å£: {}, å‡½æ•°: {}", window, function);
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåº”ç”¨çª—å£èšåˆ
                    // ç®€åŒ–å¤„ç†ï¼Œåªæ·»åŠ èšåˆæ ‡è®°
                    let obj = processed_data.as_object_mut().unwrap();
                    obj.insert("aggregated".to_string(), serde_json::Value::Bool(true));
                },
                PipelineStage::Split { by } => {
                    println!("æ‹†åˆ†é˜¶æ®µï¼ŒæŒ‰: {}", by);
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ•°æ®æ‹†åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ
                    // ç®€åŒ–å¤„ç†ï¼Œåªæ·»åŠ æ‹†åˆ†æ ‡è®°
                    let obj = processed_data.as_object_mut().unwrap();
                    obj.insert("split".to_string(), serde_json::Value::Bool(true));
                },
            }
        }

        // å°†å¤„ç†åçš„æ•°æ®è½¬æ¢å›å­—èŠ‚
        let result = serde_json::to_vec(&processed_data)
            .map_err(|e| format!("åºåˆ—åŒ–æ•°æ®å¤±è´¥: {}", e))?;

        Ok(result)
    }

    fn add_rule(&self, rule: Rule) -> Result<(), String> {
        println!("æ·»åŠ è§„åˆ™: {}", rule.name);

        // æ·»åŠ è§„åˆ™
        let mut rules = self.data_processor.rules_engine.rules.write().unwrap();
        rules.push(rule.clone());

        // æ›´æ–°è§¦å‘å™¨æ˜ å°„
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æè§„åˆ™æ¡ä»¶ï¼Œæå–è§¦å‘å™¨
        // ç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ‰€æœ‰è§„åˆ™éƒ½ç”±è®¾å¤‡æ¶ˆæ¯è§¦å‘
        let mut triggers = self.data_processor.rules_engine.triggers.write().unwrap();

        let trigger_key = "device_message".to_string();
        let rule_list = triggers.entry(trigger_key).or_insert_with(Vec::new);
        rule_list.push(rule.id);

        Ok(())
    }
}

// åˆ†å¸ƒå¼æ•°æ®æµå¤„ç†ç³»ç»Ÿ
struct DataFlowSystem {
    node_id: String,
    stream_manager: StreamManager,
    topology_manager: TopologyManager,
    state_manager: StateManager,
    scheduler: Scheduler,
}

struct StreamManager {
    streams: RwLock<HashMap<String, Stream>>,
}

struct Stream {
    id: String,
    name: String,
    partitions: u32,
    retention_policy: RetentionPolicy,
    schema: Option<String>,
    created_at: SystemTime,
}

struct TopologyManager {
    topologies: RwLock<HashMap<String, Topology>>,
}

struct Topology {
    id: String,
    name: String,
    sources: Vec<Source>,
    processors: Vec<Processor>,
    sinks: Vec<Sink>,
    connections: Vec<Connection>,
    status: TopologyStatus,
    config: HashMap<String, String>,
}

enum TopologyStatus {
    Created,
    Running,
    Paused,
    Failed,
    Stopped,
}

struct Source {
    id: String,
    source_type: String,
    config: HashMap<String, String>,
    output_streams: Vec<String>,
}

struct Processor {
    id: String,
    processor_type: String,
    config: HashMap<String, String>,
    input_streams: Vec<String>,
    output_streams: Vec<String>,
    state_stores: Vec<String>,
}

struct Sink {
    id: String,
    sink_type: String,
    config: HashMap<String, String>,
    input_streams: Vec<String>,
}

struct Connection {
    from_id: String,
    to_id: String,
    stream_id: String,
}

struct StateManager {
    state_stores: RwLock<HashMap<String, StateStore>>,
}

struct StateStore {
    id: String,
    store_type: String,
    config: HashMap<String, String>,
    persistent: bool,
}

struct Scheduler {
    workers: RwLock<HashMap<String, Worker>>,
    scheduling_policy: SchedulingPolicy,
}

struct Worker {
    id: String,
    address: SocketAddr,
    capacity: WorkerCapacity,
    assigned_tasks: Vec<String>,
    status: WorkerStatus,
    last_heartbeat: SystemTime,
}

struct WorkerCapacity {
    cpu_cores: u32,
    memory_mb: u64,
    network_bandwidth_mbps: u32,
}

impl DataFlowSystem {
    fn new(node_id: &str) -> Self {
        let stream_manager = StreamManager {
            streams: RwLock::new(HashMap::new()),
        };

        let topology_manager = TopologyManager {
            topologies: RwLock::new(HashMap::new()),
        };

        let state_manager = StateManager {
            state_stores: RwLock::new(HashMap::new()),
        };

        let scheduler = Scheduler {
            workers: RwLock::new(HashMap::new()),
            scheduling_policy: SchedulingPolicy::ResourceAware,
        };

        DataFlowSystem {
            node_id: node_id.to_string(),
            stream_manager,
            topology_manager,
            state_manager,
            scheduler,
        }
    }

    fn create_stream(&self, stream: Stream) -> Result<(), String> {
        println!("åˆ›å»ºæ•°æ®æµ: {}", stream.name);

        let mut streams = self.stream_manager.streams.write().unwrap();

        if streams.contains_key(&stream.id) {
            return Err(format!("æ•°æ®æµå·²å­˜åœ¨: {}", stream.id));
        }

        streams.insert(stream.id.clone(), stream);

        Ok(())
    }

    fn delete_stream(&self, stream_id: &str) -> Result<bool, String> {
        println!("åˆ é™¤æ•°æ®æµ: {}", stream_id);

        // æ£€æŸ¥æ˜¯å¦æœ‰æ‹“æ‰‘ä¾èµ–æ­¤æ•°æ®æµ
        {
            let topologies = self.topology_manager.topologies.read().unwrap();

            for topology in topologies.values() {
                // æ£€æŸ¥æº
                for source in &topology.sources {
                    if source.output_streams.contains(&stream_id.to_string()) {
                        return Err(format!("æ•°æ®æµè¢«æ‹“æ‰‘ {} çš„æº {} ä½¿ç”¨ï¼Œæ— æ³•åˆ é™¤",
                            topology.name, source.id));
                    }
                }

                // æ£€æŸ¥å¤„ç†å™¨
                for processor in &topology.processors {
                    if processor.input_streams.contains(&stream_id.to_string()) ||
                       processor.output_streams.contains(&stream_id.to_string()) {
                        return Err(format!("æ•°æ®æµè¢«æ‹“æ‰‘ {} çš„å¤„ç†å™¨ {} ä½¿ç”¨ï¼Œæ— æ³•åˆ é™¤",
                            topology.name, processor.id));
                    }
                }

                // æ£€æŸ¥æ¥æ”¶å™¨
                for sink in &topology.sinks {
                    if sink.input_streams.contains(&stream_id.to_string()) {
                        return Err(format!("æ•°æ®æµè¢«æ‹“æ‰‘ {} çš„æ¥æ”¶å™¨ {} ä½¿ç”¨ï¼Œæ— æ³•åˆ é™¤",
                            topology.name, sink.id));
                    }
                }
            }
        }

        // åˆ é™¤æ•°æ®æµ
        let mut streams = self.stream_manager.streams.write().unwrap();
        Ok(streams.remove(stream_id).is_some())
    }

    fn create_topology(&self, topology: Topology) -> Result<(), String> {
        println!("åˆ›å»ºæ‹“æ‰‘: {}", topology.name);

        // éªŒè¯æ‹“æ‰‘é…ç½®
        self.validate_topology(&topology)?;

        let mut topologies = self.topology_manager.topologies.write().unwrap();

        if topologies.contains_key(&topology.id) {
            return Err(format!("æ‹“æ‰‘å·²å­˜åœ¨: {}", topology.id));
        }

        topologies.insert(topology.id.clone(), topology);

        Ok(())
    }

    fn validate_topology(&self, topology: &Topology) -> Result<(), String> {
        // éªŒè¯æ‰€æœ‰å¼•ç”¨çš„æ•°æ®æµéƒ½å­˜åœ¨
        let streams = self.stream_manager.streams.read().unwrap();

        for source in &topology.sources {
            for stream_id in &source.output_streams {
                if !streams.contains_key(stream_id) {
                    return Err(format!("æº {} å¼•ç”¨çš„æ•°æ®æµ {} ä¸å­˜åœ¨",
                        source.id, stream_id));
                }
            }
        }

        for processor in &topology.processors {
            for stream_id in &processor.input_streams {
                if !streams.contains_key(stream_id) {
                    return Err(format!("å¤„ç†å™¨ {} å¼•ç”¨çš„è¾“å…¥æ•°æ®æµ {} ä¸å­˜åœ¨",
                        processor.id, stream_id));
                }
            }

            for stream_id in &processor.output_streams {
                if !streams.contains_key(stream_id) {
                    return Err(format!("å¤„ç†å™¨ {} å¼•ç”¨çš„è¾“å‡ºæ•°æ®æµ {} ä¸å­˜åœ¨",
                        processor.id, stream_id));
                }
            }
        }

        for sink in &topology.sinks {
            for stream_id in &sink.input_streams {
                if !streams.contains_key(stream_id) {
                    return Err(format!("æ¥æ”¶å™¨ {} å¼•ç”¨çš„æ•°æ®æµ {} ä¸å­˜åœ¨",
                        sink.id, stream_id));
                }
            }
        }

        // éªŒè¯æ‰€æœ‰å¼•ç”¨çš„çŠ¶æ€å­˜å‚¨éƒ½å­˜åœ¨
        let state_stores = self.state_manager.state_stores.read().unwrap();

        for processor in &topology.processors {
            for store_id in &processor.state_stores {
                if !state_stores.contains_key(store_id) {
                    return Err(format!("å¤„ç†å™¨ {} å¼•ç”¨çš„çŠ¶æ€å­˜å‚¨ {} ä¸å­˜åœ¨",
                        processor.id, store_id));
                }
            }
        }

        // éªŒè¯æ‹“æ‰‘è¿æ¥
        for connection in &topology.connections {
            // éªŒè¯æºèŠ‚ç‚¹å­˜åœ¨
            let source_exists = topology.sources.iter().any(|s| s.id == connection.from_id) ||
                               topology.processors.iter().any(|p| p.id == connection.from_id);

            if !source_exists {
                return Err(format!("è¿æ¥ä¸­çš„æºèŠ‚ç‚¹ {} ä¸å­˜åœ¨", connection.from_id));
            }

            // éªŒè¯ç›®æ ‡èŠ‚ç‚¹å­˜åœ¨
            let target_exists = topology.processors.iter().any(|p| p.id == connection.to_id) ||
                               topology.sinks.iter().any(|s| s.id == connection.to_id);

            if !target_exists {
                return Err(format!("è¿æ¥ä¸­çš„ç›®æ ‡èŠ‚ç‚¹ {} ä¸å­˜åœ¨", connection.to_id));
            }

            // éªŒè¯æ•°æ®æµå­˜åœ¨
            if !streams.contains_key(&connection.stream_id) {
                return Err(format!("è¿æ¥å¼•ç”¨çš„æ•°æ®æµ {} ä¸å­˜åœ¨", connection.stream_id));
            }
        }

        Ok(())
    }

    fn start_topology(&self, topology_id: &str) -> Result<(), String> {
        println!("å¯åŠ¨æ‹“æ‰‘: {}", topology_id);

        let mut topologies = self.topology_manager.topologies.write().unwrap();

        let topology = topologies.get_mut(topology_id)
            .ok_or_else(|| format!("æ‹“æ‰‘ä¸å­˜åœ¨: {}", topology_id))?;

        if topology.status == TopologyStatus::Running {
            return Err(format!("æ‹“æ‰‘å·²åœ¨è¿è¡Œä¸­: {}", topology_id));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒåº¦æ‹“æ‰‘åˆ°å·¥ä½œèŠ‚ç‚¹
        // åˆ†é…å·¥ä½œèŠ‚ç‚¹
        let assigned_workers = self.assign_workers_to_topology(topology)?;

        // å°†ä»»åŠ¡éƒ¨ç½²åˆ°å·¥ä½œèŠ‚ç‚¹
        self.deploy_topology_to_workers(topology, &assigned_workers)?;

        // æ›´æ–°æ‹“æ‰‘çŠ¶æ€
        topology.status = TopologyStatus::Running;

        Ok(())
    }

    fn assign_workers_to_topology(&self, topology: &Topology) -> Result<HashMap<String, String>, String> {
        println!("ä¸ºæ‹“æ‰‘åˆ†é…å·¥ä½œèŠ‚ç‚¹: {}", topology.name);

        let mut assigned_workers = HashMap::new();
        let mut workers = self.scheduler.workers.write().unwrap();

        // è®¡ç®—æ‰€éœ€èµ„æº
        let sources_count = topology.sources.len();
        let processors_count = topology.processors.len();
        let sinks_count = topology.sinks.len();
        let total_tasks = sources_count + processors_count + sinks_count;

        // ç®€å•ç­–ç•¥ï¼šå°è¯•å‡åŒ€åˆ†é…ä»»åŠ¡åˆ°å¯ç”¨å·¥ä½œèŠ‚ç‚¹
        let available_workers: Vec<_> = workers.values_mut()
            .filter(|w| w.status == WorkerStatus::Running)
            .collect();

        if available_workers.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹".to_string());
        }

        let tasks_per_worker = (total_tasks as f64 / available_workers.len() as f64).ceil() as usize;

        let mut current_worker_index = 0;

        // åˆ†é…æº
        for source in &topology.sources {
            let worker = &mut available_workers[current_worker_index];
            assigned_workers.insert(source.id.clone(), worker.id.clone());
            worker.assigned_tasks.push(source.id.clone());

            current_worker_index = (current_worker_index + 1) % available_workers.len();
        }

        // åˆ†é…å¤„ç†å™¨
        for processor in &topology.processors {
            let worker = &mut available_workers[current_worker_index];
            assigned_workers.insert(processor.id.clone(), worker.id.clone());
            worker.assigned_tasks.push(processor.id.clone());

            current_worker_index = (current_worker_index + 1) % available_workers.len();
        }

        // åˆ†é…æ¥æ”¶å™¨
        for sink in &topology.sinks {
            let worker = &mut available_workers[current_worker_index];
            assigned_workers.insert(sink.id.clone(), worker.id.clone());
            worker.assigned_tasks.push(sink.id.clone());

            current_worker_index = (current_worker_index + 1) % available_workers.len();
        }

        Ok(assigned_workers)
    }

    fn deploy_topology_to_workers(&self, topology: &Topology, assigned_workers: &HashMap<String, String>) -> Result<(), String> {
        println!("å°†æ‹“æ‰‘éƒ¨ç½²åˆ°å·¥ä½œèŠ‚ç‚¹: {}", topology.name);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†ä»»åŠ¡éƒ¨ç½²åˆ°å¯¹åº”çš„å·¥ä½œèŠ‚ç‚¹
        // åŒ…æ‹¬å‘é€é…ç½®ã€å»ºç«‹è¿æ¥ç­‰

        for (task_id, worker_id) in assigned_workers {
            println!("ä»»åŠ¡ {} éƒ¨ç½²åˆ°å·¥ä½œèŠ‚ç‚¹ {}", task_id, worker_id);
        }

        Ok(())
    }

    fn stop_topology(&self, topology_id: &str) -> Result<(), String> {
        println!("åœæ­¢æ‹“æ‰‘: {}", topology_id);

        let mut topologies = self.topology_manager.topologies.write().unwrap();

        let topology = topologies.get_mut(topology_id)
            .ok_or_else(|| format!("æ‹“æ‰‘ä¸å­˜åœ¨: {}", topology_id))?;

        if topology.status != TopologyStatus::Running &&
           topology.status != TopologyStatus::Paused {
            return Err(format!("æ‹“æ‰‘æœªè¿è¡Œæˆ–æš‚åœ: {}", topology_id));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘å·¥ä½œèŠ‚ç‚¹å‘é€åœæ­¢å‘½ä»¤

        // æ›´æ–°æ‹“æ‰‘çŠ¶æ€
        topology.status = TopologyStatus::Stopped;

        // ä»å·¥ä½œèŠ‚ç‚¹é‡Šæ”¾ä»»åŠ¡
        self.release_topology_tasks(topology)?;

        Ok(())
    }

    fn release_topology_tasks(&self, topology: &Topology) -> Result<(), String> {
        println!("ä»å·¥ä½œèŠ‚ç‚¹é‡Šæ”¾æ‹“æ‰‘ä»»åŠ¡: {}", topology.name);

        let mut workers = self.scheduler.workers.write().unwrap();

        // æ”¶é›†æ‰€æœ‰ä»»åŠ¡ID
        let mut task_ids = Vec::new();

        for source in &topology.sources {
            task_ids.push(source.id.clone());
        }

        for processor in &topology.processors {
            task_ids.push(processor.id.clone());
        }

        for sink in &topology.sinks {
            task_ids.push(sink.id.clone());
        }

        // ä»å·¥ä½œèŠ‚ç‚¹ç§»é™¤ä»»åŠ¡
        for worker in workers.values_mut() {
            worker.assigned_tasks.retain(|id| !task_ids.contains(id));
        }

        Ok(())
    }

    fn create_state_store(&self, store: StateStore) -> Result<(), String> {
        println!("åˆ›å»ºçŠ¶æ€å­˜å‚¨: {}", store.id);

        let mut stores = self.state_manager.state_stores.write().unwrap();

        if stores.contains_key(&store.id) {
            return Err(format!("çŠ¶æ€å­˜å‚¨å·²å­˜åœ¨: {}", store.id));
        }

        stores.insert(store.id.clone(), store);

        Ok(())
    }

    fn register_worker(&self, worker: Worker) -> Result<(), String> {
        println!("æ³¨å†Œå·¥ä½œèŠ‚ç‚¹: {}", worker.id);

        let mut workers = self.scheduler.workers.write().unwrap();

        if workers.contains_key(&worker.id) {
            return Err(format!("å·¥ä½œèŠ‚ç‚¹å·²å­˜åœ¨: {}", worker.id));
        }

        workers.insert(worker.id.clone(), worker);

        Ok(())
    }

    fn process_metrics(&self, worker_id: &str, metrics: &WorkerMetrics) -> Result<(), String> {
        println!("å¤„ç†å·¥ä½œèŠ‚ç‚¹æŒ‡æ ‡: {}", worker_id);

        let mut workers = self.scheduler.workers.write().unwrap();

        let worker = workers.get_mut(worker_id)
            .ok_or_else(|| format!("å·¥ä½œèŠ‚ç‚¹ä¸å­˜åœ¨: {}", worker_id))?;

        // æ›´æ–°å·¥ä½œèŠ‚ç‚¹çŠ¶æ€
        worker.last_heartbeat = SystemTime::now();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å’Œå­˜å‚¨æŒ‡æ ‡

        // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¹³è¡¡
        if self.should_rebalance(metrics) {
            self.rebalance_topologies()?;
        }

        Ok(())
    }

    fn should_rebalance(&self, metrics: &WorkerMetrics) -> bool {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè¯„ä¼°æ˜¯å¦éœ€è¦é‡æ–°å¹³è¡¡
        // ä¾‹å¦‚ï¼Œæ£€æŸ¥CPUæˆ–å†…å­˜ä½¿ç”¨ç‡æ˜¯å¦è¶…è¿‡é˜ˆå€¼

        // ç®€åŒ–å®ç°ï¼Œæ€»æ˜¯è¿”å›false
        false
    }

    fn rebalance_topologies(&self) -> Result<(), String> {
        println!("é‡æ–°å¹³è¡¡æ‹“æ‰‘...");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé‡æ–°åˆ†é…ä»»åŠ¡åˆ°å·¥ä½œèŠ‚ç‚¹

        Ok(())
    }
}

struct WorkerMetrics {
    worker_id: String,
    cpu_usage: f64,
    memory_usage: f64,
    network_in: u64,
    network_out: u64,
    task_metrics: HashMap<String, TaskMetrics>,
    timestamp: SystemTime,
}

struct TaskMetrics {
    task_id: String,
    processed_records: u64,
    processing_rate: f64,
    average_latency: Duration,
    error_count: u64,
}

// æœ€åï¼Œæµ‹è¯•ä»£ç 
fn main() {
    println!("åˆ†å¸ƒå¼ç³»ç»Ÿç¤ºä¾‹");

    // è¿™é‡Œå¯ä»¥æ·»åŠ å„ç§ç»„ä»¶çš„ç¤ºä¾‹ä»£ç 

    println!("ç¤ºä¾‹ç»“æŸ");
}
```

### 1.6 ç»¼åˆåº”ç”¨06-åˆ†å¸ƒå¼æœç´¢å¼•æ“

```rust
// åˆ†å¸ƒå¼æœç´¢å¼•æ“
struct DistributedSearchEngine {
    node_id: String,
    index_manager: IndexManager,
    document_store: DocumentStore,
    query_processor: SearchQueryProcessor,
    shard_manager: ShardManager,
    replica_manager: ReplicaManager,
}

struct IndexManager {
    indices: RwLock<HashMap<String, IndexMetadata>>,
}

struct IndexMetadata {
    name: String,
    mappings: HashMap<String, FieldMapping>,
    settings: IndexSettings,
    created_at: SystemTime,
    stats: IndexStats,
}

struct FieldMapping {
    field_type: FieldType,
    analyzer: Option<String>,
    index_options: IndexOptions,
    stored: bool,
    doc_values: bool,
}

enum FieldType {
    Text,
    Keyword,
    Integer,
    Float,
    Boolean,
    Date,
    Object,
    Nested,
    GeoPoint,
}

struct IndexSettings {
    number_of_shards: u32,
    number_of_replicas: u32,
    refresh_interval: Duration,
    analysis: AnalysisSettings,
}

struct AnalysisSettings {
    analyzers: HashMap<String, Analyzer>,
    tokenizers: HashMap<String, Tokenizer>,
    filters: HashMap<String, TokenFilter>,
}

struct Analyzer {
    name: String,
    tokenizer: String,
    filters: Vec<String>,
}

struct Tokenizer {
    name: String,
    tokenizer_type: String,
    settings: HashMap<String, String>,
}

struct TokenFilter {
    name: String,
    filter_type: String,
    settings: HashMap<String, String>,
}

enum IndexOptions {
    Docs,
    FreqsAndPositions,
    FreqsAndPositionsAndOffsets,
}

struct IndexStats {
    doc_count: u64,
    size_in_bytes: u64,
    indexing_rate: f64,
    search_rate: f64,
    query_latency: Duration,
}

struct DocumentStore {
    store_type: StoreType,
    connection_string: String,
    cache: Arc<LruCache<String, Document>>,
}

enum StoreType {
    Memory,
    FileSystem,
    Database,
}

struct Document {
    id: String,
    source: HashMap<String, serde_json::Value>,
    indexed_at: SystemTime,
    version: u64,
}

struct SearchQueryProcessor {
    parsers: HashMap<String, Box<dyn QueryParser + Send + Sync>>,
    execution_engine: ExecutionEngine,
}

trait QueryParser: Send + Sync {
    fn parse(&self, query_string: &str) -> Result<Query, String>;
}

struct Query {
    query_type: QueryType,
    field: Option<String>,
    value: Option<String>,
    boost: f32,
    children: Vec<Query>,
}

enum QueryType {
    Term,
    Match,
    Range,
    Prefix,
    Wildcard,
    Bool,
    Phrase,
    FunctionScore,
}

struct ExecutionEngine {
    max_clauses: usize,
    timeout: Duration,
    max_concurrent_searches: usize,
}

struct ShardManager {
    shards: RwLock<HashMap<String, ShardInfo>>,
    allocation_strategy: AllocationStrategy,
}

struct ShardInfo {
    id: String,
    index_name: String,
    shard_num: u32,
    state: ShardState,
    size_in_bytes: u64,
    doc_count: u64,
    primary: bool,
    node_id: String,
}

enum ShardState {
    Initializing,
    Started,
    Relocating,
    Unassigned,
}

enum AllocationStrategy {
    Balanced,
    DiskAware,
    RackAware,
    Custom,
}

struct ReplicaManager {
    replicas: RwLock<HashMap<String, ReplicaInfo>>,
    recovery_strategy: RecoveryStrategy,
}

struct ReplicaInfo {
    id: String,
    shard_id: String,
    node_id: String,
    state: ReplicaState,
    sync_status: SyncStatus,
}

enum ReplicaState {
    Initializing,
    Started,
    Stale,
    Failed,
}

struct SyncStatus {
    last_sync_time: SystemTime,
    pending_operations: u64,
    bytes_behind: u64,
}

enum RecoveryStrategy {
    Full,
    Incremental,
    Snapshot,
}

impl DistributedSearchEngine {
    fn new(node_id: &str) -> Self {
        let index_manager = IndexManager {
            indices: RwLock::new(HashMap::new()),
        };

        let document_store = DocumentStore {
            store_type: StoreType::Memory,
            connection_string: String::new(),
            cache: Arc::new(LruCache::new(10000)),
        };

        let search_query_processor = SearchQueryProcessor {
            parsers: HashMap::new(),
            execution_engine: ExecutionEngine {
                max_clauses: 1024,
                timeout: Duration::from_secs(30),
                max_concurrent_searches: 8,
            },
        };

        let shard_manager = ShardManager {
            shards: RwLock::new(HashMap::new()),
            allocation_strategy: AllocationStrategy::Balanced,
        };

        let replica_manager = ReplicaManager {
            replicas: RwLock::new(HashMap::new()),
            recovery_strategy: RecoveryStrategy::Incremental,
        };

        DistributedSearchEngine {
            node_id: node_id.to_string(),
            index_manager,
            document_store,
            query_processor: search_query_processor,
            shard_manager,
            replica_manager,
        }
    }

    fn create_index(&self, name: &str, mappings: HashMap<String, FieldMapping>, settings: IndexSettings) -> Result<(), String> {
        println!("åˆ›å»ºç´¢å¼•: {}", name);

        let mut indices = self.index_manager.indices.write().unwrap();

        if indices.contains_key(name) {
            return Err(format!("ç´¢å¼•å·²å­˜åœ¨: {}", name));
        }

        let metadata = IndexMetadata {
            name: name.to_string(),
            mappings,
            settings: settings.clone(),
            created_at: SystemTime::now(),
            stats: IndexStats {
                doc_count: 0,
                size_in_bytes: 0,
                indexing_rate: 0.0,
                search_rate: 0.0,
                query_latency: Duration::from_millis(0),
            },
        };

        indices.insert(name.to_string(), metadata);

        // åˆ›å»ºåˆ†ç‰‡
        self.create_shards(name, settings.number_of_shards, settings.number_of_replicas)?;

        Ok(())
    }

    fn create_shards(&self, index_name: &str, num_shards: u32, num_replicas: u32) -> Result<(), String> {
        println!("ä¸ºç´¢å¼• {} åˆ›å»º {} ä¸ªåˆ†ç‰‡å’Œ {} ä¸ªå‰¯æœ¬", index_name, num_shards, num_replicas);

        let mut shards = self.shard_manager.shards.write().unwrap();
        let mut replicas = self.replica_manager.replicas.write().unwrap();

        // åˆ›å»ºä¸»åˆ†ç‰‡
        for shard_num in 0..num_shards {
            let shard_id = format!("{}_{}", index_name, shard_num);

            let shard_info = ShardInfo {
                id: shard_id.clone(),
                index_name: index_name.to_string(),
                shard_num,
                state: ShardState::Initializing,
                size_in_bytes: 0,
                doc_count: 0,
                primary: true,
                node_id: self.node_id.clone(), // ç®€åŒ–ï¼šæ‰€æœ‰ä¸»åˆ†ç‰‡éƒ½åˆ†é…ç»™å½“å‰èŠ‚ç‚¹
            };

            shards.insert(shard_id.clone(), shard_info);

            // åˆ›å»ºå‰¯æœ¬
            for replica_num in 0..num_replicas {
                let replica_id = format!("{}_{}_{}", index_name, shard_num, replica_num);

                let replica_info = ReplicaInfo {
                    id: replica_id.clone(),
                    shard_id: shard_id.clone(),
                    node_id: format!("node_{}", replica_num), // ç®€åŒ–ï¼šå‰¯æœ¬åˆ†é…ç»™å…¶ä»–èŠ‚ç‚¹
                    state: ReplicaState::Initializing,
                    sync_status: SyncStatus {
                        last_sync_time: SystemTime::now(),
                        pending_operations: 0,
                        bytes_behind: 0,
                    },
                };

                replicas.insert(replica_id, replica_info);
            }
        }

        // å¯åŠ¨æ‰€æœ‰åˆ†ç‰‡
        for shard in shards.values_mut() {
            if shard.index_name == index_name {
                shard.state = ShardState::Started;
            }
        }

        Ok(())
    }

    fn index_document(&self, index_name: &str, doc_id: &str, document: HashMap<String, serde_json::Value>) -> Result<(), String> {
        println!("ä¸ºç´¢å¼• {} ç´¢å¼•æ–‡æ¡£ {}", index_name, doc_id);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indices = self.index_manager.indices.read().unwrap();
        let index = indices.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        // éªŒè¯æ–‡æ¡£ç»“æ„ä¸ç´¢å¼•æ˜ å°„åŒ¹é…
        self.validate_document(&document, &index.mappings)?;

        // è·¯ç”±æ–‡æ¡£åˆ°é€‚å½“çš„åˆ†ç‰‡
        let shard_num = self.route_document(index_name, doc_id, &document)?;

        // è·å–åˆ†ç‰‡ä¿¡æ¯
        let mut shards = self.shard_manager.shards.write().unwrap();
        let shard_id = format!("{}_{}", index_name, shard_num);

        let shard = shards.get_mut(&shard_id)
            .ok_or_else(|| format!("åˆ†ç‰‡ä¸å­˜åœ¨: {}", shard_id))?;

        // æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€
        if shard.state != ShardState::Started {
            return Err(format!("åˆ†ç‰‡ä¸å¯ç”¨: {}", shard_id));
        }

        // å­˜å‚¨æ–‡æ¡£
        let doc = Document {
            id: doc_id.to_string(),
            source: document,
            indexed_at: SystemTime::now(),
            version: 1, // ç®€åŒ–ï¼šå§‹ç»ˆä½¿ç”¨ç‰ˆæœ¬1
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ–‡æ¡£æ·»åŠ åˆ°ç´¢å¼•ä¸­
        // å¹¶æ›´æ–°æ–‡æ¡£å­˜å‚¨

        // æ›´æ–°åˆ†ç‰‡ç»Ÿè®¡ä¿¡æ¯
        shard.doc_count += 1;
        shard.size_in_bytes += 1000; // å‡è®¾æ¯ä¸ªæ–‡æ¡£çº¦1KB

        // æ›´æ–°ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        let mut indices = self.index_manager.indices.write().unwrap();
        if let Some(index) = indices.get_mut(index_name) {
            index.stats.doc_count += 1;
            index.stats.size_in_bytes += 1000;
        }

        // å°†æ›´æ”¹å¤åˆ¶åˆ°å‰¯æœ¬
        self.replicate_changes(shard_id, &doc)?;

        Ok(())
    }

    fn validate_document(&self, document: &HashMap<String, serde_json::Value>, mappings: &HashMap<String, FieldMapping>) -> Result<(), String> {
        // ç®€åŒ–ï¼šåœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šéªŒè¯æ–‡æ¡£å­—æ®µä¸æ˜ å°„å®šä¹‰æ˜¯å¦å…¼å®¹
        Ok(())
    }

    fn route_document(&self, index_name: &str, doc_id: &str, document: &HashMap<String, serde_json::Value>) -> Result<u32, String> {
        // ç®€åŒ–ï¼šä½¿ç”¨æ–‡æ¡£IDçš„å“ˆå¸Œæ¥ç¡®å®šåˆ†ç‰‡
        let indices = self.index_manager.indices.read().unwrap();
        let index = indices.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        let mut hasher = DefaultHasher::new();
        doc_id.hash(&mut hasher);
        let hash = hasher.finish();

        Ok((hash % index.settings.number_of_shards as u64) as u32)
    }

    fn replicate_changes(&self, shard_id: String, document: &Document) -> Result<(), String> {
        // æ‰¾åˆ°åˆ†ç‰‡çš„æ‰€æœ‰å‰¯æœ¬
        let replicas = self.replica_manager.replicas.read().unwrap();
        let shard_replicas: Vec<_> = replicas.values()
            .filter(|r| r.shard_id == shard_id)
            .collect();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†å˜æ›´å¼‚æ­¥å¤åˆ¶åˆ°æ‰€æœ‰å‰¯æœ¬
        for replica in shard_replicas {
            println!("å°†æ–‡æ¡£ {} å¤åˆ¶åˆ°å‰¯æœ¬ {}", document.id, replica.id);
        }

        Ok(())
    }

    fn search(&self, index_name: &str, query_string: &str, size: usize, from: usize) -> Result<SearchResults, String> {
        println!("åœ¨ç´¢å¼• {} ä¸­æœç´¢: {}", index_name, query_string);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indices = self.index_manager.indices.read().unwrap();
        let index = indices.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        // è§£ææŸ¥è¯¢
        let query = self.parse_query(query_string)?;

        // ç¡®å®šè¦æœç´¢çš„åˆ†ç‰‡
        let shards = self.shard_manager.shards.read().unwrap();
        let index_shards: Vec<_> = shards.values()
            .filter(|s| s.index_name == index_name && s.state == ShardState::Started)
            .collect();

        if index_shards.is_empty() {
            return Err(format!("ç´¢å¼• {} æ²¡æœ‰å¯ç”¨åˆ†ç‰‡", index_name));
        }

        // åœ¨æ¯ä¸ªåˆ†ç‰‡ä¸Šæ‰§è¡Œæœç´¢
        let mut all_hits = Vec::new();

        for shard in &index_shards {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåœ¨åˆ†ç‰‡ä¸Šæ‰§è¡ŒæŸ¥è¯¢
            // ç®€åŒ–ï¼šç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿç»“æœ
            let hits = self.execute_query_on_shard(shard, &query)?;
            all_hits.extend(hits);
        }

        // åˆå¹¶å’Œæ’åºç»“æœ
        all_hits.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());

        // åˆ†é¡µ
        let total = all_hits.len();
        let hits = if from < all_hits.len() {
            all_hits.into_iter()
                .skip(from)
                .take(size)
                .collect()
        } else {
            Vec::new()
        };

        // æ„å»ºç»“æœ
        let results = SearchResults {
            took: Duration::from_millis(100), // æ¨¡æ‹Ÿè€—æ—¶
            timed_out: false,
            total,
            hits,
        };

        Ok(results)
    }

    fn parse_query(&self, query_string: &str) -> Result<Query, String> {
        // ç®€åŒ–ï¼šè§£æä¸ºç®€å•çš„åŒ¹é…æŸ¥è¯¢
        let parts: Vec<_> = query_string.splitn(2, ':').collect();

        if parts.len() == 2 {
            // å­—æ®µ:å€¼ æ ¼å¼
            Ok(Query {
                query_type: QueryType::Match,
                field: Some(parts[0].to_string()),
                value: Some(parts[1].to_string()),
                boost: 1.0,
                children: Vec::new(),
            })
        } else {
            // æ²¡æœ‰æŒ‡å®šå­—æ®µï¼Œå‡è®¾æ˜¯å…¨æ–‡æœç´¢
            Ok(Query {
                query_type: QueryType::Match,
                field: None,
                value: Some(query_string.to_string()),
                boost: 1.0,
                children: Vec::new(),
            })
        }
    }

    fn execute_query_on_shard(&self, shard: &ShardInfo, query: &Query) -> Result<Vec<SearchHit>, String> {
        // ç®€åŒ–ï¼šç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿç»“æœ
        let mut hits = Vec::new();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåœ¨å€’æ’ç´¢å¼•ä¸Šæ‰§è¡ŒæŸ¥è¯¢
        for i in 0..5 {
            let hit = SearchHit {
                id: format!("doc_{}_shard_{}", i, shard.shard_num),
                index: shard.index_name.clone(),
                score: 1.0 - (i as f32 * 0.1),
                source: HashMap::new(), // å®é™…å®ç°ä¸­ä¼šå¡«å……æ–‡æ¡£å†…å®¹
            };

            hits.push(hit);
        }

        Ok(hits)
    }

    fn get_document(&self, index_name: &str, doc_id: &str) -> Result<Option<Document>, String> {
        println!("è·å–ç´¢å¼• {} ä¸­çš„æ–‡æ¡£ {}", index_name, doc_id);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indices = self.index_manager.indices.read().unwrap();
        if !indices.contains_key(index_name) {
            return Err(format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name));
        }

        // ç¡®å®šæ–‡æ¡£æ‰€åœ¨çš„åˆ†ç‰‡
        let shard_num = self.route_document(index_name, doc_id, &HashMap::new())?;
        let shard_id = format!("{}_{}", index_name, shard_num);

        // æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€
        let shards = self.shard_manager.shards.read().unwrap();
        let shard = match shards.get(&shard_id) {
            Some(s) => s,
            None => return Err(format!("åˆ†ç‰‡ä¸å­˜åœ¨: {}", shard_id)),
        };

        if shard.state != ShardState::Started {
            return Err(format!("åˆ†ç‰‡ä¸å¯ç”¨: {}", shard_id));
        }

        // å°è¯•ä»ç¼“å­˜è·å–
        if let Some(doc) = self.document_store.cache.borrow().get(&format!("{}_{}", index_name, doc_id)) {
            return Ok(Some(doc.clone()));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»æ–‡æ¡£å­˜å‚¨ä¸­è·å–æ–‡æ¡£
        // ç®€åŒ–ï¼šè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿæ–‡æ¡£
        let doc = Document {
            id: doc_id.to_string(),
            source: HashMap::new(),
            indexed_at: SystemTime::now(),
            version: 1,
        };

        Ok(Some(doc))
    }

    fn delete_document(&self, index_name: &str, doc_id: &str) -> Result<bool, String> {
        println!("ä»ç´¢å¼• {} ä¸­åˆ é™¤æ–‡æ¡£ {}", index_name, doc_id);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indices = self.index_manager.indices.read().unwrap();
        if !indices.contains_key(index_name) {
            return Err(format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name));
        }

        // ç¡®å®šæ–‡æ¡£æ‰€åœ¨çš„åˆ†ç‰‡
        let shard_num = self.route_document(index_name, doc_id, &HashMap::new())?;
        let shard_id = format!("{}_{}", index_name, shard_num);

        // æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€
        let mut shards = self.shard_manager.shards.write().unwrap();
        let shard = match shards.get_mut(&shard_id) {
            Some(s) => s,
            None => return Err(format!("åˆ†ç‰‡ä¸å­˜åœ¨: {}", shard_id)),
        };

        if shard.state != ShardState::Started {
            return Err(format!("åˆ†ç‰‡ä¸å¯ç”¨: {}", shard_id));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»ç´¢å¼•ä¸­åˆ é™¤æ–‡æ¡£
        // æ›´æ–°åˆ†ç‰‡ç»Ÿè®¡ä¿¡æ¯
        if shard.doc_count > 0 {
            shard.doc_count -= 1;
        }

        // æ›´æ–°ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        let mut indices = self.index_manager.indices.write().unwrap();
        if let Some(index) = indices.get_mut(index_name) {
            if index.stats.doc_count > 0 {
                index.stats.doc_count -= 1;
            }
        }

        // å°†åˆ é™¤æ“ä½œå¤åˆ¶åˆ°å‰¯æœ¬
        let delete_op = Document {
            id: doc_id.to_string(),
            source: HashMap::new(),
            indexed_at: SystemTime::now(),
            version: 0, // ç‰ˆæœ¬0è¡¨ç¤ºåˆ é™¤
        };

        self.replicate_changes(shard_id, &delete_op)?;

        Ok(true)
    }

    fn delete_index(&self, index_name: &str) -> Result<bool, String> {
        println!("åˆ é™¤ç´¢å¼•: {}", index_name);

        // åˆ é™¤ç´¢å¼•å…ƒæ•°æ®
        let mut indices = self.index_manager.indices.write().unwrap();
        if !indices.contains_key(index_name) {
            return Err(format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name));
        }

        indices.remove(index_name);

        // åˆ é™¤ç´¢å¼•çš„åˆ†ç‰‡
        let mut shards = self.shard_manager.shards.write().unwrap();
        let shard_ids: Vec<_> = shards.keys()
            .filter(|id| id.starts_with(&format!("{}_", index_name)))
            .cloned()
            .collect();

        for shard_id in &shard_ids {
            shards.remove(shard_id);
        }

        // åˆ é™¤ç´¢å¼•çš„å‰¯æœ¬
        let mut replicas = self.replica_manager.replicas.write().unwrap();
        let replica_ids: Vec<_> = replicas.keys()
            .filter(|id| id.starts_with(&format!("{}_", index_name)))
            .cloned()
            .collect();

        for replica_id in &replica_ids {
            replicas.remove(replica_id);
        }

        Ok(true)
    }
}

struct SearchResults {
    took: Duration,
    timed_out: bool,
    total: usize,
    hits: Vec<SearchHit>,
}

struct SearchHit {
    id: String,
    index: String,
    score: f32,
    source: HashMap<String, serde_json::Value>,
}

struct LruCache<K, V> {
    capacity: usize,
    cache: HashMap<K, V>,
}

impl<K: Eq + Hash + Clone, V: Clone> LruCache<K, V> {
    fn new(capacity: usize) -> Self {
        LruCache {
            capacity,
            cache: HashMap::with_capacity(capacity),
        }
    }

    fn get(&self, key: &K) -> Option<&V> {
        self.cache.get(key)
    }

    fn insert(&mut self, key: K, value: V) {
        if self.cache.len() >= self.capacity {
            // ç®€åŒ–ï¼šåœ¨å®é™…å®ç°ä¸­åº”è¯¥ç§»é™¤æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹
            if let Some(k) = self.cache.keys().next().cloned() {
                self.cache.remove(&k);
            }
        }

        self.cache.insert(key, value);
    }
}

// åˆ†å¸ƒå¼å·¥ä½œæµå¼•æ“
struct WorkflowEngine {
    node_id: String,
    workflow_store: WorkflowStore,
    execution_engine: WorkflowExecutionEngine,
    scheduler: WorkflowScheduler,
    task_queue: TaskQueue,
}

struct WorkflowStore {
    workflows: RwLock<HashMap<String, Workflow>>,
    workflow_instances: RwLock<HashMap<String, WorkflowInstance>>,
    task_definitions: RwLock<HashMap<String, TaskDefinition>>,
}

struct Workflow {
    id: String,
    name: String,
    version: String,
    description: Option<String>,
    tasks: Vec<WorkflowTask>,
    transitions: Vec<TaskTransition>,
    input_schema: Option<String>,
    output_schema: Option<String>,
    timeout: Option<Duration>,
    created_at: SystemTime,
    updated_at: SystemTime,
}

struct WorkflowTask {
    id: String,
    name: String,
    task_type: String,
    config: HashMap<String, serde_json::Value>,
    retry_policy: RetryPolicy,
    timeout: Option<Duration>,
}

struct TaskTransition {
    from_task_id: String,
    to_task_id: String,
    condition: Option<String>,
}

struct TaskDefinition {
    id: String,
    name: String,
    description: Option<String>,
    handler: String,
    input_schema: Option<String>,
    output_schema: Option<String>,
    timeout: Duration,
}

struct WorkflowInstance {
    id: String,
    workflow_id: String,
    status: WorkflowStatus,
    current_tasks: Vec<String>,
    completed_tasks: HashMap<String, TaskResult>,
    input: HashMap<String, serde_json::Value>,
    output: Option<HashMap<String, serde_json::Value>>,
    error: Option<String>,
    started_at: SystemTime,
    updated_at: SystemTime,
    completed_at: Option<SystemTime>,
}

enum WorkflowStatus {
    Created,
    Running,
    Completed,
    Failed,
    Cancelled,
    TimedOut,
}

struct TaskResult {
    task_id: String,
    status: TaskStatus,
    output: Option<HashMap<String, serde_json::Value>>,
    error: Option<String>,
    started_at: SystemTime,
    completed_at: SystemTime,
    attempts: u32,
}

enum TaskStatus {
    Success,
    Failed,
    Cancelled,
    TimedOut,
}

struct WorkflowExecutionEngine {
    max_concurrent_workflows: usize,
    task_handlers: HashMap<String, Box<dyn TaskHandler + Send + Sync>>,
}

trait TaskHandler: Send + Sync {
    fn execute(&self, task: &WorkflowTask, input: &HashMap<String, serde_json::Value>)
        -> Result<HashMap<String, serde_json::Value>, String>;
}

struct WorkflowScheduler {
    schedule_configs: RwLock<HashMap<String, ScheduleConfig>>,
    next_execution_times: RwLock<BinaryHeap<ScheduledExecution>>,
}

struct ScheduleConfig {
    id: String,
    workflow_id: String,
    cron_expression: String,
    timezone: String,
    input: HashMap<String, serde_json::Value>,
    enabled: bool,
    last_execution: Option<SystemTime>,
    next_execution: Option<SystemTime>,
}

struct ScheduledExecution {
    schedule_id: String,
    execution_time: SystemTime,
}

impl Ord for ScheduledExecution {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        // åå‘æ¯”è¾ƒï¼Œä½¿æœ€å°çš„æ—¶é—´åœ¨å †é¡¶
        other.execution_time.cmp(&self.execution_time)
    }
}

impl PartialOrd for ScheduledExecution {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl PartialEq for ScheduledExecution {
    fn eq(&self, other: &Self) -> bool {
        self.execution_time == other.execution_time
    }
}

impl Eq for ScheduledExecution {}

struct TaskQueue {
    pending_tasks: RwLock<Vec<QueuedTask>>,
    processing_tasks: RwLock<HashMap<String, QueuedTask>>,
    max_concurrent_tasks: usize,
}

struct QueuedTask {
    id: String,
    workflow_instance_id: String,
    task_id: String,
    priority: u32,
    input: HashMap<String, serde_json::Value>,
    queued_at: SystemTime,
}

impl WorkflowEngine {
    fn new(node_id: &str) -> Self {
        let workflow_store = WorkflowStore {
            workflows: RwLock::new(HashMap::new()),
            workflow_instances: RwLock::new(HashMap::new()),
            task_definitions: RwLock::new(HashMap::new()),
        };

        let execution_engine = WorkflowExecutionEngine {
            max_concurrent_workflows: 100,
            task_handlers: HashMap::new(),
        };

        let scheduler = WorkflowScheduler {
            schedule_configs: RwLock::new(HashMap::new()),
            next_execution_times: RwLock::new(BinaryHeap::new()),
        };

        let task_queue = TaskQueue {
            pending_tasks: RwLock::new(Vec::new()),
            processing_tasks: RwLock::new(HashMap::new()),
            max_concurrent_tasks: 100,
        };

        WorkflowEngine {
            node_id: node_id.to_string(),
            workflow_store,
            execution_engine,
            scheduler,
            task_queue,
        }
    }

    fn register_workflow(&self, workflow: Workflow) -> Result<(), String> {
        println!("æ³¨å†Œå·¥ä½œæµ: {}", workflow.name);

        // éªŒè¯å·¥ä½œæµ
        self.validate_workflow(&workflow)?;

        // å­˜å‚¨å·¥ä½œæµ
        let mut workflows = self.workflow_store.workflows.write().unwrap();

        // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåä½†ä¸åŒç‰ˆæœ¬çš„å·¥ä½œæµ
        for (_, existing) in workflows.iter() {
            if existing.name == workflow.name && existing.version != workflow.version {
                println!("å‘ç°å·¥ä½œæµ {} çš„æ–°ç‰ˆæœ¬: {}", workflow.name, workflow.version);
            }
        }

        workflows.insert(workflow.id.clone(), workflow);

        Ok(())
    }

    fn validate_workflow(&self, workflow: &Workflow) -> Result<(), String> {
        // æ£€æŸ¥ä»»åŠ¡å®šä¹‰
        let task_definitions = self.workflow_store.task_definitions.read().unwrap();

        for task in &workflow.tasks {
            if !task_definitions.contains_key(&task.task_type) {
                return Err(format!("æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {}", task.task_type));
            }
        }

        // æ£€æŸ¥ä»»åŠ¡è½¬æ¢
        let task_ids: HashSet<_> = workflow.tasks.iter().map(|t| &t.id).collect();

        for transition in &workflow.transitions {
            if !task_ids.contains(&transition.from_task_id) {
                return Err(format!("è½¬æ¢ä¸­çš„æºä»»åŠ¡ä¸å­˜åœ¨: {}", transition.from_task_id));
            }

            if !task_ids.contains(&transition.to_task_id) {
                return Err(format!("è½¬æ¢ä¸­çš„ç›®æ ‡ä»»åŠ¡ä¸å­˜åœ¨: {}", transition.to_task_id));
            }
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰ç¯
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æµ‹å·¥ä½œæµå›¾ä¸­çš„ç¯

        Ok(())
    }

    fn register_task_definition(&self, task_def: TaskDefinition) -> Result<(), String> {
        println!("æ³¨å†Œä»»åŠ¡å®šä¹‰: {}", task_def.name);

        let mut task_definitions = self.workflow_store.task_definitions.write().unwrap();
        task_definitions.insert(task_def.id.clone(), task_def);

        Ok(())
    }

    fn start_workflow(&self, workflow_id: &str, input: HashMap<String, serde_json::Value>) -> Result<String, String> {
        println!("å¯åŠ¨å·¥ä½œæµ: {}", workflow_id);

        // æŸ¥æ‰¾å·¥ä½œæµ
        let workflows = self.workflow_store.workflows.read().unwrap();
        let workflow = workflows.get(workflow_id)
            .ok_or_else(|| format!("å·¥ä½œæµä¸å­˜åœ¨: {}", workflow_id))?;

        // éªŒè¯è¾“å…¥
        if let Some(schema) = &workflow.input_schema {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šéªŒè¯è¾“å…¥æ˜¯å¦ç¬¦åˆæ¨¡å¼
        }

        // åˆ›å»ºå·¥ä½œæµå®ä¾‹
        let instance_id = uuid::Uuid::new_v4().to_string();

        // æ‰¾åˆ°èµ·å§‹ä»»åŠ¡ï¼ˆæ²¡æœ‰å…¥è¾¹çš„ä»»åŠ¡ï¼‰
        let mut start_tasks = Vec::new();
        let task_ids_with_incoming: HashSet<_> = workflow.transitions.iter()
            .map(|t| &t.to_task_id)
            .collect();

        for task in &workflow.tasks {
            if !task_ids_with_incoming.contains(&task.id) {
                start_tasks.push(task.id.clone());
            }
        }

        if start_tasks.is_empty() {
            return Err("å·¥ä½œæµæ²¡æœ‰èµ·å§‹ä»»åŠ¡".to_string());
        }

        let now = SystemTime::now();

        let instance = WorkflowInstance {
            id: instance_id.clone(),
            workflow_id: workflow_id.to_string(),
            status: WorkflowStatus::Running,
            current_tasks: start_tasks.clone(),
            completed_tasks: HashMap::new(),
            input: input.clone(),
            output: None,
            error: None,
            started_at: now,
            updated_at: now,
            completed_at: None,
        };

        // å­˜å‚¨å·¥ä½œæµå®ä¾‹
        let mut instances = self.workflow_store.workflow_instances.write().unwrap();
        instances.insert(instance_id.clone(), instance);

        // å°†èµ·å§‹ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        for task_id in start_tasks {
            self.queue_task(&instance_id, &task_id, &input)?;
        }

        Ok(instance_id)
    }

    fn queue_task(&self, instance_id: &str, task_id: &str, input: &HashMap<String, serde_json::Value>) -> Result<(), String> {
        println!("å°†ä»»åŠ¡ {} åŠ å…¥é˜Ÿåˆ—ï¼Œå®ä¾‹ {}", task_id, instance_id);

        // åˆ›å»ºé˜Ÿåˆ—ä»»åŠ¡
        let queued_task = QueuedTask {
            id: uuid::Uuid::new_v4().to_string(),
            workflow_instance_id: instance_id.to_string(),
            task_id: task_id.to_string(),
            priority: 0, // é»˜è®¤ä¼˜å…ˆçº§
            input: input.clone(),
            queued_at: SystemTime::now(),
        };

        // æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ—
        let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
        pending_tasks.push(queued_task);

        // æŒ‰ä¼˜å…ˆçº§æ’åº
        pending_tasks.sort_by(|a, b| b.priority.cmp(&a.priority));

        Ok(())
    }

    fn process_next_task(&self) -> Result<bool, String> {
        // æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨ä»»åŠ¡æ§½
        let processing_tasks = self.task_queue.processing_tasks.read().unwrap();
        if processing_tasks.len() >= self.task_queue.max_concurrent_tasks {
            return Ok(false);
        }
        drop(processing_tasks);

        // è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡
        let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
        if pending_tasks.is_empty() {
            return Ok(false);
        }

        let task = pending_tasks.remove(0);
        drop(pending_tasks);

        // å°†ä»»åŠ¡ç§»åŠ¨åˆ°å¤„ç†ä¸­é˜Ÿåˆ—
        let mut processing_tasks = self.task_queue.processing_tasks.write().unwrap();


```rust
        processing_tasks.insert(task.id.clone(), task.clone());
        drop(processing_tasks);

        // è·å–å·¥ä½œæµå®ä¾‹
        let instances = self.workflow_store.workflow_instances.read().unwrap();
        let instance = instances.get(&task.workflow_instance_id)
            .ok_or_else(|| format!("å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {}", task.workflow_instance_id))?;

        // æ£€æŸ¥å·¥ä½œæµå®ä¾‹çŠ¶æ€
        if instance.status != WorkflowStatus::Running {
            return Err(format!("å·¥ä½œæµå®ä¾‹ä¸åœ¨è¿è¡ŒçŠ¶æ€: {}", task.workflow_instance_id));
        }

        // è·å–å·¥ä½œæµ
        let workflows = self.workflow_store.workflows.read().unwrap();
        let workflow = workflows.get(&instance.workflow_id)
            .ok_or_else(|| format!("å·¥ä½œæµä¸å­˜åœ¨: {}", instance.workflow_id))?;

        // æŸ¥æ‰¾ä»»åŠ¡å®šä¹‰
        let workflow_task = workflow.tasks.iter()
            .find(|t| t.id == task.task_id)
            .ok_or_else(|| format!("ä»»åŠ¡ä¸å­˜åœ¨: {}", task.task_id))?;

        // è·å–ä»»åŠ¡å¤„ç†å™¨
        let task_handler = self.execution_engine.task_handlers.get(&workflow_task.task_type)
            .ok_or_else(|| format!("ä»»åŠ¡å¤„ç†å™¨ä¸å­˜åœ¨: {}", workflow_task.task_type))?;

        println!("æ‰§è¡Œä»»åŠ¡: {}", task.task_id);

        // æ‰§è¡Œä»»åŠ¡
        let result = match task_handler.execute(workflow_task, &task.input) {
            Ok(output) => {
                TaskResult {
                    task_id: task.task_id.clone(),
                    status: TaskStatus::Success,
                    output: Some(output),
                    error: None,
                    started_at: task.queued_at,
                    completed_at: SystemTime::now(),
                    attempts: 1,
                }
            },
            Err(err) => {
                println!("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {}, é”™è¯¯: {}", task.task_id, err);

                // æ£€æŸ¥é‡è¯•ç­–ç•¥
                if workflow_task.retry_policy.max_attempts > 1 {
                    // é‡æ–°åŠ å…¥é˜Ÿåˆ—è¿›è¡Œé‡è¯•
                    let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
                    pending_tasks.push(QueuedTask {
                        id: uuid::Uuid::new_v4().to_string(),
                        workflow_instance_id: task.workflow_instance_id.clone(),
                        task_id: task.task_id.clone(),
                        priority: task.priority,
                        input: task.input.clone(),
                        queued_at: SystemTime::now(),
                    });
                    pending_tasks.sort_by(|a, b| b.priority.cmp(&a.priority));

                    // ä»»åŠ¡å°†é‡è¯•ï¼Œæš‚æ—¶ä¸æ›´æ–°å·¥ä½œæµå®ä¾‹
                    let mut processing_tasks = self.task_queue.processing_tasks.write().unwrap();
                    processing_tasks.remove(&task.id);

                    return Ok(true);
                }

                TaskResult {
                    task_id: task.task_id.clone(),
                    status: TaskStatus::Failed,
                    output: None,
                    error: Some(err),
                    started_at: task.queued_at,
                    completed_at: SystemTime::now(),
                    attempts: 1,
                }
            }
        };

        // æ›´æ–°å·¥ä½œæµå®ä¾‹
        self.update_workflow_instance(&task.workflow_instance_id, &task.task_id, result)?;

        // ä»å¤„ç†ä¸­é˜Ÿåˆ—ç§»é™¤ä»»åŠ¡
        let mut processing_tasks = self.task_queue.processing_tasks.write().unwrap();
        processing_tasks.remove(&task.id);

        Ok(true)
    }

    fn update_workflow_instance(&self, instance_id: &str, task_id: &str, result: TaskResult) -> Result<(), String> {
        println!("æ›´æ–°å·¥ä½œæµå®ä¾‹: {}, ä»»åŠ¡: {}", instance_id, task_id);

        let mut instances = self.workflow_store.workflow_instances.write().unwrap();
        let instance = instances.get_mut(instance_id)
            .ok_or_else(|| format!("å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {}", instance_id))?;

        // æ›´æ–°å½“å‰ä»»åŠ¡å’Œå·²å®Œæˆä»»åŠ¡
        instance.current_tasks.retain(|id| id != task_id);
        instance.completed_tasks.insert(task_id.to_string(), result.clone());
        instance.updated_at = SystemTime::now();

        // è·å–å·¥ä½œæµ
        let workflows = self.workflow_store.workflows.read().unwrap();
        let workflow = workflows.get(&instance.workflow_id)
            .ok_or_else(|| format!("å·¥ä½œæµä¸å­˜åœ¨: {}", instance.workflow_id))?;

        // å¦‚æœä»»åŠ¡å¤±è´¥ä¸”æ²¡æœ‰é‡è¯•ï¼Œåˆ™å°†å·¥ä½œæµæ ‡è®°ä¸ºå¤±è´¥
        if result.status == TaskStatus::Failed {
            instance.status = WorkflowStatus::Failed;
            instance.error = result.error.clone();
            instance.completed_at = Some(SystemTime::now());
            return Ok(());
        }

        // æŸ¥æ‰¾ä¸‹ä¸€ä¸ªä»»åŠ¡
        let next_tasks = self.find_next_tasks(workflow, task_id, &result)?;

        // å¦‚æœæ²¡æœ‰ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼Œå¹¶ä¸”å½“å‰æ²¡æœ‰å…¶ä»–ä»»åŠ¡åœ¨è¿è¡Œï¼Œåˆ™å·¥ä½œæµå®Œæˆ
        if next_tasks.is_empty() && instance.current_tasks.is_empty() {
            instance.status = WorkflowStatus::Completed;
            instance.completed_at = Some(SystemTime::now());

            // æ”¶é›†æ‰€æœ‰è¾“å‡º
            let mut final_output = HashMap::new();
            for (_, task_result) in &instance.completed_tasks {
                if let Some(output) = &task_result.output {
                    for (key, value) in output {
                        final_output.insert(key.clone(), value.clone());
                    }
                }
            }

            instance.output = Some(final_output);
        } else {
            // å°†ä¸‹ä¸€ä¸ªä»»åŠ¡æ·»åŠ åˆ°å½“å‰ä»»åŠ¡åˆ—è¡¨å¹¶åŠ å…¥é˜Ÿåˆ—
            for task_id in &next_tasks {
                instance.current_tasks.push(task_id.clone());

                // å‡†å¤‡ä»»åŠ¡è¾“å…¥
                let mut task_input = instance.input.clone();

                // æ·»åŠ å‰ä¸€ä¸ªä»»åŠ¡çš„è¾“å‡º
                if let Some(output) = result.output.as_ref() {
                    for (key, value) in output {
                        task_input.insert(format!("previous.{}", key), value.clone());
                    }
                }

                // åŠ å…¥é˜Ÿåˆ—
                self.queue_task(instance_id, task_id, &task_input)?;
            }
        }

        Ok(())
    }

    fn find_next_tasks(&self, workflow: &Workflow, task_id: &str, result: &TaskResult) -> Result<Vec<String>, String> {
        let mut next_tasks = Vec::new();

        for transition in &workflow.transitions {
            if transition.from_task_id == task_id {
                // æ£€æŸ¥æ¡ä»¶
                if let Some(condition) = &transition.condition {
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè¯„ä¼°æ¡ä»¶è¡¨è¾¾å¼
                    // ç®€åŒ–ï¼šå‡è®¾æ¡ä»¶æ€»æ˜¯æ»¡è¶³
                    next_tasks.push(transition.to_task_id.clone());
                } else {
                    // æ— æ¡ä»¶è½¬æ¢
                    next_tasks.push(transition.to_task_id.clone());
                }
            }
        }

        Ok(next_tasks)
    }

    fn schedule_workflow(&self, config: ScheduleConfig) -> Result<(), String> {
        println!("è°ƒåº¦å·¥ä½œæµ: {}", config.workflow_id);

        // éªŒè¯å·¥ä½œæµæ˜¯å¦å­˜åœ¨
        let workflows = self.workflow_store.workflows.read().unwrap();
        if !workflows.contains_key(&config.workflow_id) {
            return Err(format!("å·¥ä½œæµä¸å­˜åœ¨: {}", config.workflow_id));
        }

        // éªŒè¯cronè¡¨è¾¾å¼
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šéªŒè¯cronè¡¨è¾¾å¼çš„æœ‰æ•ˆæ€§

        // è®¡ç®—ä¸‹ä¸€ä¸ªæ‰§è¡Œæ—¶é—´
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŸºäºcronè¡¨è¾¾å¼è®¡ç®—ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´
        let next_execution = SystemTime::now() + Duration::from_secs(60);

        let mut config = config;
        config.next_execution = Some(next_execution);

        // å­˜å‚¨è°ƒåº¦é…ç½®
        let mut schedule_configs = self.scheduler.schedule_configs.write().unwrap();
        schedule_configs.insert(config.id.clone(), config.clone());

        // æ›´æ–°æ‰§è¡Œé˜Ÿåˆ—
        let mut next_executions = self.scheduler.next_execution_times.write().unwrap();
        next_executions.push(ScheduledExecution {
            schedule_id: config.id.clone(),
            execution_time: next_execution,
        });

        Ok(())
    }

    fn process_scheduled_workflows(&self) -> Result<usize, String> {
        println!("å¤„ç†è°ƒåº¦çš„å·¥ä½œæµ...");

        let now = SystemTime::now();
        let mut count = 0;

        // è·å–æ‰€æœ‰åº”è¯¥æ‰§è¡Œçš„å·¥ä½œæµ
        let mut to_execute = Vec::new();

        {
            let mut next_executions = self.scheduler.next_execution_times.write().unwrap();

            while let Some(scheduled) = next_executions.peek() {
                if scheduled.execution_time <= now {
                    let scheduled = next_executions.pop().unwrap();
                    to_execute.push(scheduled);
                } else {
                    break;
                }
            }
        }

        // æ‰§è¡Œå·¥ä½œæµ
        for scheduled in to_execute {
            let mut schedule_configs = self.scheduler.schedule_configs.write().unwrap();

            if let Some(config) = schedule_configs.get_mut(&scheduled.schedule_id) {
                if config.enabled {
                    // å¯åŠ¨å·¥ä½œæµ
                    match self.start_workflow(&config.workflow_id, config.input.clone()) {
                        Ok(instance_id) => {
                            println!("å·²è°ƒåº¦å·¥ä½œæµ {}, å®ä¾‹ {}", config.workflow_id, instance_id);
                            count += 1;
                        },
                        Err(err) => {
                            println!("è°ƒåº¦å·¥ä½œæµ {} å¤±è´¥: {}", config.workflow_id, err);
                        }
                    }

                    // æ›´æ–°ä¸Šæ¬¡æ‰§è¡Œæ—¶é—´
                    config.last_execution = Some(now);

                    // è®¡ç®—ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŸºäºcronè¡¨è¾¾å¼è®¡ç®—ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´
                    let next_execution = now + Duration::from_secs(60);
                    config.next_execution = Some(next_execution);

                    // é‡æ–°åŠ å…¥æ‰§è¡Œé˜Ÿåˆ—
                    let mut next_executions = self.scheduler.next_execution_times.write().unwrap();
                    next_executions.push(ScheduledExecution {
                        schedule_id: config.id.clone(),
                        execution_time: next_execution,
                    });
                }
            }
        }

        Ok(count)
    }

    fn get_workflow_instance(&self, instance_id: &str) -> Result<WorkflowInstance, String> {
        let instances = self.workflow_store.workflow_instances.read().unwrap();

        instances.get(instance_id)
            .cloned()
            .ok_or_else(|| format!("å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {}", instance_id))
    }

    fn cancel_workflow_instance(&self, instance_id: &str) -> Result<(), String> {
        println!("å–æ¶ˆå·¥ä½œæµå®ä¾‹: {}", instance_id);

        let mut instances = self.workflow_store.workflow_instances.write().unwrap();

        let instance = instances.get_mut(instance_id)
            .ok_or_else(|| format!("å·¥ä½œæµå®ä¾‹ä¸å­˜åœ¨: {}", instance_id))?;

        if instance.status != WorkflowStatus::Running {
            return Err(format!("å·¥ä½œæµå®ä¾‹ä¸åœ¨è¿è¡ŒçŠ¶æ€: {}", instance_id));
        }

        // æ›´æ–°å·¥ä½œæµçŠ¶æ€
        instance.status = WorkflowStatus::Cancelled;
        instance.completed_at = Some(SystemTime::now());

        // ä»é˜Ÿåˆ—ä¸­ç§»é™¤è¯¥å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡
        let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
        pending_tasks.retain(|task| task.workflow_instance_id != instance_id);

        // å–æ¶ˆå¤„ç†ä¸­çš„ä»»åŠ¡
        let processing_tasks = self.task_queue.processing_tasks.read().unwrap();
        let tasks_to_cancel: Vec<_> = processing_tasks.values()
            .filter(|task| task.workflow_instance_id == instance_id)
            .map(|task| task.id.clone())
            .collect();

        drop(processing_tasks);

        for task_id in tasks_to_cancel {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€å–æ¶ˆä¿¡å·ç»™ä»»åŠ¡å¤„ç†å™¨
            let mut processing_tasks = self.task_queue.processing_tasks.write().unwrap();
            processing_tasks.remove(&task_id);
        }

        Ok(())
    }
}

// åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿ
struct DistributedMLSystem {
    node_id: String,
    model_registry: ModelRegistry,
    training_manager: TrainingManager,
    inference_service: InferenceService,
    data_manager: MLDataManager,
}

struct ModelRegistry {
    models: RwLock<HashMap<String, ModelMetadata>>,
    model_versions: RwLock<HashMap<String, Vec<ModelVersion>>>,
}

struct ModelMetadata {
    id: String,
    name: String,
    description: Option<String>,
    model_type: String,
    created_at: SystemTime,
    created_by: String,
    tags: HashMap<String, String>,
}

struct ModelVersion {
    model_id: String,
    version: String,
    location: String,
    status: ModelStatus,
    metrics: HashMap<String, f64>,
    hyperparameters: HashMap<String, String>,
    created_at: SystemTime,
    experiment_id: Option<String>,
}

enum ModelStatus {
    Draft,
    Training,
    Trained,
    Validated,
    Deployed,
    Archived,
    Failed,
}

struct TrainingManager {
    jobs: RwLock<HashMap<String, TrainingJob>>,
    job_queue: RwLock<Vec<String>>,
    worker_pool: WorkerPool,
}

struct TrainingJob {
    id: String,
    model_id: String,
    version: String,
    dataset_id: String,
    hyperparameters: HashMap<String, String>,
    status: JobStatus,
    progress: f64,
    metrics: HashMap<String, f64>,
    logs: Vec<String>,
    created_at: SystemTime,
    started_at: Option<SystemTime>,
    completed_at: Option<SystemTime>,
    error: Option<String>,
}

enum JobStatus {
    Queued,
    Running,
    Completed,
    Failed,
    Cancelled,
}

struct WorkerPool {
    workers: RwLock<HashMap<String, Worker>>,
    max_workers: usize,
}

struct InferenceService {
    endpoints: RwLock<HashMap<String, InferenceEndpoint>>,
    request_handler: RequestHandler,
}

struct InferenceEndpoint {
    id: String,
    name: String,
    model_id: String,
    model_version: String,
    status: EndpointStatus,
    scaling_config: ScalingConfig,
    metrics: HashMap<String, f64>,
    created_at: SystemTime,
    last_request: Option<SystemTime>,
}

enum EndpointStatus {
    Creating,
    Running,
    Updating,
    Deleting,
    Failed,
}

struct ScalingConfig {
    min_instances: u32,
    max_instances: u32,
    target_utilization: f64,
}

struct RequestHandler {
    model_cache: LruCache<String, LoadedModel>,
    max_concurrent_requests: usize,
    timeout: Duration,
}

struct LoadedModel {
    model_id: String,
    version: String,
    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŒ…å«æ¨¡å‹æ•°æ®æˆ–å¼•ç”¨
    loaded_at: SystemTime,
    last_used: SystemTime,
    memory_usage: u64,
}

struct MLDataManager {
    datasets: RwLock<HashMap<String, Dataset>>,
    features: RwLock<HashMap<String, FeatureSet>>,
}

struct Dataset {
    id: String,
    name: String,
    description: Option<String>,
    format: String,
    location: String,
    size_bytes: u64,
    row_count: u64,
    schema: Option<String>,
    created_at: SystemTime,
    tags: HashMap<String, String>,
}

struct FeatureSet {
    id: String,
    name: String,
    features: Vec<Feature>,
    source: String,
    created_at: SystemTime,
    last_updated: SystemTime,
}

struct Feature {
    name: String,
    feature_type: FeatureType,
    description: Option<String>,
    statistics: Option<FeatureStatistics>,
}

enum FeatureType {
    Numeric,
    Categorical,
    Text,
    Image,
    Embedding,
    Timestamp,
}

struct FeatureStatistics {
    count: u64,
    missing: u64,
    min: Option<f64>,
    max: Option<f64>,
    mean: Option<f64>,
    std_dev: Option<f64>,
    unique_count: Option<u64>,
}

impl DistributedMLSystem {
    fn new(node_id: &str) -> Self {
        let model_registry = ModelRegistry {
            models: RwLock::new(HashMap::new()),
            model_versions: RwLock::new(HashMap::new()),
        };

        let worker_pool = WorkerPool {
            workers: RwLock::new(HashMap::new()),
            max_workers: 10,
        };

        let training_manager = TrainingManager {
            jobs: RwLock::new(HashMap::new()),
            job_queue: RwLock::new(Vec::new()),
            worker_pool,
        };

        let request_handler = RequestHandler {
            model_cache: LruCache::new(10),
            max_concurrent_requests: 100,
            timeout: Duration::from_secs(30),
        };

        let inference_service = InferenceService {
            endpoints: RwLock::new(HashMap::new()),
            request_handler,
        };

        let data_manager = MLDataManager {
            datasets: RwLock::new(HashMap::new()),
            features: RwLock::new(HashMap::new()),
        };

        DistributedMLSystem {
            node_id: node_id.to_string(),
            model_registry,
            training_manager,
            inference_service,
            data_manager,
        }
    }

    fn register_model(&self, metadata: ModelMetadata) -> Result<(), String> {
        println!("æ³¨å†Œæ¨¡å‹: {}", metadata.name);

        let mut models = self.model_registry.models.write().unwrap();

        if models.contains_key(&metadata.id) {
            return Err(format!("æ¨¡å‹å·²å­˜åœ¨: {}", metadata.id));
        }

        models.insert(metadata.id.clone(), metadata);

        // åˆå§‹åŒ–ç‰ˆæœ¬åˆ—è¡¨
        let mut versions = self.model_registry.model_versions.write().unwrap();
        versions.insert(metadata.id.clone(), Vec::new());

        Ok(())
    }

    fn create_model_version(&self, version: ModelVersion) -> Result<(), String> {
        println!("åˆ›å»ºæ¨¡å‹ç‰ˆæœ¬: {}:{}", version.model_id, version.version);

        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        let models = self.model_registry.models.read().unwrap();
        if !models.contains_key(&version.model_id) {
            return Err(format!("æ¨¡å‹ä¸å­˜åœ¨: {}", version.model_id));
        }

        // æ·»åŠ ç‰ˆæœ¬
        let mut versions = self.model_registry.model_versions.write().unwrap();

        let model_versions = versions.get_mut(&version.model_id)
            .ok_or_else(|| format!("æ¨¡å‹ç‰ˆæœ¬åˆ—è¡¨ä¸å­˜åœ¨: {}", version.model_id))?;

        // æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦å·²å­˜åœ¨
        if model_versions.iter().any(|v| v.version == version.version) {
            return Err(format!("æ¨¡å‹ç‰ˆæœ¬å·²å­˜åœ¨: {}:{}", version.model_id, version.version));
        }

        model_versions.push(version);

        Ok(())
    }

    fn get_model(&self, model_id: &str) -> Result<ModelMetadata, String> {
        let models = self.model_registry.models.read().unwrap();

        models.get(model_id)
            .cloned()
            .ok_or_else(|| format!("æ¨¡å‹ä¸å­˜åœ¨: {}", model_id))
    }

    fn get_model_versions(&self, model_id: &str) -> Result<Vec<ModelVersion>, String> {
        let versions = self.model_registry.model_versions.read().unwrap();

        versions.get(model_id)
            .cloned()
            .ok_or_else(|| format!("æ¨¡å‹ç‰ˆæœ¬åˆ—è¡¨ä¸å­˜åœ¨: {}", model_id))
    }

    fn register_dataset(&self, dataset: Dataset) -> Result<(), String> {
        println!("æ³¨å†Œæ•°æ®é›†: {}", dataset.name);

        let mut datasets = self.data_manager.datasets.write().unwrap();

        if datasets.contains_key(&dataset.id) {
            return Err(format!("æ•°æ®é›†å·²å­˜åœ¨: {}", dataset.id));
        }

        datasets.insert(dataset.id.clone(), dataset);

        Ok(())
    }

    fn create_training_job(&self, job: TrainingJob) -> Result<(), String> {
        println!("åˆ›å»ºè®­ç»ƒä»»åŠ¡: {}", job.id);

        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        let models = self.model_registry.models.read().unwrap();
        if !models.contains_key(&job.model_id) {
            return Err(format!("æ¨¡å‹ä¸å­˜åœ¨: {}", job.model_id));
        }

        // æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        let datasets = self.data_manager.datasets.read().unwrap();
        if !datasets.contains_key(&job.dataset_id) {
            return Err(format!("æ•°æ®é›†ä¸å­˜åœ¨: {}", job.dataset_id));
        }

        // æ·»åŠ è®­ç»ƒä»»åŠ¡
        let mut jobs = self.training_manager.jobs.write().unwrap();

        if jobs.contains_key(&job.id) {
            return Err(format!("è®­ç»ƒä»»åŠ¡å·²å­˜åœ¨: {}", job.id));
        }

        jobs.insert(job.id.clone(), job.clone());

        // å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—
        let mut job_queue = self.training_manager.job_queue.write().unwrap();
        job_queue.push(job.id.clone());

        Ok(())
    }

    fn process_training_job(&self) -> Result<bool, String> {
        // æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹
        let workers = self.training_manager.worker_pool.workers.read().unwrap();
        let available_workers: Vec<_> = workers.values()
            .filter(|w| w.status == WorkerStatus::Running && w.current_tasks.is_empty())
            .collect();

        if available_workers.is_empty() {
            return Ok(false);
        }

        // è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†ä»»åŠ¡
        let mut job_queue = self.training_manager.job_queue.write().unwrap();
        if job_queue.is_empty() {
            return Ok(false);
        }

        let job_id = job_queue.remove(0);
        drop(job_queue);

        // è·å–è®­ç»ƒä»»åŠ¡
        let mut jobs = self.training_manager.jobs.write().unwrap();
        let job = jobs.get_mut(&job_id)
            .ok_or_else(|| format!("è®­ç»ƒä»»åŠ¡ä¸å­˜åœ¨: {}", job_id))?;

        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
        job.status = JobStatus::Running;
        job.started_at = Some(SystemTime::now());

        println!("å¼€å§‹å¤„ç†è®­ç»ƒä»»åŠ¡: {}", job_id);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†ä»»åŠ¡åˆ†é…ç»™å·¥ä½œèŠ‚ç‚¹
        // ç®€åŒ–ï¼šæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹

        // åˆ›å»ºæ¨¡å‹ç‰ˆæœ¬
        let model_version = ModelVersion {
            model_id: job.model_id.clone(),
            version: job.version.clone(),
            location: format!("models/{}/{}", job.model_id, job.version),
            status: ModelStatus::Training,
            metrics: HashMap::new(),
            hyperparameters: job.hyperparameters.clone(),
            created_at: SystemTime::now(),
            experiment_id: None,
        };

        self.create_model_version(model_version)?;

        Ok(true)
    }

    fn create_inference_endpoint(&self, endpoint: InferenceEndpoint) -> Result<(), String> {
        println!("åˆ›å»ºæ¨ç†ç«¯ç‚¹: {}", endpoint.name);

        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        let versions = self.model_registry.model_versions.read().unwrap();
        let model_versions = versions.get(&endpoint.model_id)
            .ok_or_else(|| format!("æ¨¡å‹ç‰ˆæœ¬åˆ—è¡¨ä¸å­˜åœ¨: {}", endpoint.model_id))?;

        let model_version = model_versions.iter()
            .find(|v| v.version == endpoint.model_version)
            .ok_or_else(|| format!("æ¨¡å‹ç‰ˆæœ¬ä¸å­˜åœ¨: {}:{}", endpoint.model_id, endpoint.model_version))?;

        if model_version.status != ModelStatus::Trained &&
           model_version.status != ModelStatus::Validated &&
           model_version.status != ModelStatus::Deployed {
            return Err(format!("æ¨¡å‹ç‰ˆæœ¬ä¸å¯ç”¨äºæ¨ç†: {}:{}", endpoint.model_id, endpoint.model_version));
        }

        // æ·»åŠ ç«¯ç‚¹
        let mut endpoints = self.inference_service.endpoints.write().unwrap();

        if endpoints.contains_key(&endpoint.id) {
            return Err(format!("æ¨ç†ç«¯ç‚¹å·²å­˜åœ¨: {}", endpoint.id));
        }

        endpoints.insert(endpoint.id.clone(), endpoint);

        Ok(())
    }

    fn predict(&self, endpoint_id: &str, input: &HashMap<String, serde_json::Value>) -> Result<HashMap<String, serde_json::Value>, String> {
        println!("ä½¿ç”¨ç«¯ç‚¹ {} è¿›è¡Œé¢„æµ‹", endpoint_id);

        // è·å–ç«¯ç‚¹ä¿¡æ¯
        let endpoints = self.inference_service.endpoints.read().unwrap();
        let endpoint = endpoints.get(endpoint_id)
            .ok_or_else(|| format!("æ¨ç†ç«¯ç‚¹ä¸å­˜åœ¨: {}", endpoint_id))?;

        if endpoint.status != EndpointStatus::Running {
            return Err(format!("æ¨ç†ç«¯ç‚¹ä¸å¯ç”¨: {}", endpoint_id));
        }

        // è·å–æ¨¡å‹ç‰ˆæœ¬
        let versions = self.model_registry.model_versions.read().unwrap();
        let model_versions = versions.get(&endpoint.model_id)
            .ok_or_else(|| format!("æ¨¡å‹ç‰ˆæœ¬åˆ—è¡¨ä¸å­˜åœ¨: {}", endpoint.model_id))?;

        let model_version = model_versions.iter()
            .find(|v| v.version == endpoint.model_version)
            .ok_or_else(|| format!("æ¨¡å‹ç‰ˆæœ¬ä¸å­˜åœ¨: {}:{}", endpoint.model_id, endpoint.model_version))?;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½æ¨¡å‹å¹¶æ‰§è¡Œé¢„æµ‹
        // ç®€åŒ–ï¼šè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„é¢„æµ‹ç»“æœ

        let mut result = HashMap::new();
        result.insert("prediction".to_string(), serde_json::json!([0.1, 0.2, 0.7]));
        result.insert("model_version".to_string(), serde_json::json!(model_version.version));

        Ok(result)
    }
}

// æµ‹è¯•ä¸»å‡½æ•°
fn main() {
    println!("åˆ†å¸ƒå¼ç³»ç»Ÿç»„ä»¶ç¤ºä¾‹å®Œæˆã€‚");
}
```

### 1.7 ç»¼åˆåº”ç”¨07-åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“

```rust
// åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“
struct TimeSeriesDB {
    node_id: String,
    storage_engine: TimeSeriesStorage,
    retention_policies: RwLock<HashMap<String, RetentionPolicy>>,
    schema_manager: SchemaManager,
    query_processor: TSQueryProcessor,
    partition_manager: TimePartitionManager,
    compaction_manager: CompactionManager,
}

struct TimeSeriesStorage {
    storage_type: TSStorageType,
    base_path: PathBuf,
    partitions: RwLock<HashMap<String, TimePartition>>,
    wal: WriteAheadLog,
}

enum TSStorageType {
    Memory,
    File,
    Hybrid,
}

struct TimePartition {
    id: String,
    time_range: (DateTime<Utc>, DateTime<Utc>),
    shards: Vec<TimeSeriesShard>,
    status: PartitionStatus,
    stats: PartitionStats,
}

struct TimeSeriesShard {
    id: String,
    partition_id: String,
    series_count: u64,
    points_count: u64,
    size_bytes: u64,
    index_size_bytes: u64,
    time_range: (DateTime<Utc>, DateTime<Utc>),
    compression_ratio: f64,
}

struct PartitionStats {
    points_count: u64,
    series_count: u64,
    size_bytes: u64,
    earliest_time: DateTime<Utc>,
    latest_time: DateTime<Utc>,
    avg_points_per_series: f64,
    compression_ratio: f64,
}

struct WriteAheadLog {
    enabled: bool,
    path: PathBuf,
    sync_interval: Duration,
    max_size_bytes: u64,
}

struct SchemaManager {
    measurements: RwLock<HashMap<String, Measurement>>,
    field_dictionary: RwLock<HashMap<String, Field>>,
    tags_dictionary: RwLock<HashMap<String, TagInfo>>,
}

struct Measurement {
    name: String,
    fields: HashMap<String, Field>,
    tags: Vec<String>,
    retention_policy: String,
    created_at: DateTime<Utc>,
}

struct Field {
    name: String,
    field_type: FieldType,
    is_indexed: bool,
}

enum FieldType {
    Float,
    Integer,
    String,
    Boolean,
}

struct TagInfo {
    name: String,
    cardinality: u64,
    is_indexed: bool,
}

struct TSQueryProcessor {
    parser: TSQueryParser,
    planner: TSQueryPlanner,
    executor: TSQueryExecutor,
}

struct TSQueryParser {
    max_query_length: usize,
    timeout: Duration,
}

struct TSQueryPlanner {
    optimizers: Vec<Box<dyn QueryOptimizer>>,
}

trait QueryOptimizer: Send + Sync {
    fn optimize(&self, plan: &mut QueryPlan) -> Result<(), String>;
}

struct TSQueryExecutor {
    max_concurrent_queries: usize,
    max_points_per_query: u64,
    timeout: Duration,
}

struct QueryPlan {
    id: String,
    query_type: TSQueryType,
    measurements: Vec<String>,
    time_range: (DateTime<Utc>, DateTime<Utc>),
    filters: Vec<Filter>,
    projections: Vec<String>,
    group_by: Option<GroupBy>,
    limit: Option<usize>,
    offset: Option<usize>,
    order_by: Option<OrderBy>,
}

enum TSQueryType {
    Select,
    Aggregate,
    ShowMeasurements,
    ShowTagKeys,
    ShowTagValues,
    ShowFieldKeys,
}

struct Filter {
    field: String,
    operator: FilterOperator,
    value: serde_json::Value,
}

enum FilterOperator {
    Eq,
    NotEq,
    Gt,
    GtEq,
    Lt,
    LtEq,
    In,
    NotIn,
    Regex,
    NotRegex,
}

struct GroupBy {
    time_interval: Option<Duration>,
    tags: Vec<String>,
}

struct OrderBy {
    field: String,
    direction: SortDirection,
}

enum SortDirection {
    Asc,
    Desc,
}

struct TimePartitionManager {
    partition_interval: Duration,
    max_partitions: usize,
    active_partitions: RwLock<Vec<String>>,
}

struct CompactionManager {
    enabled: bool,
    compaction_interval: Duration,
    max_concurrent_compactions: usize,
    threshold_size_bytes: u64,
    threshold_series_count: u64,
    compaction_worker: Option<JoinHandle<()>>,
}

struct TimeSeriesPoint {
    measurement: String,
    tags: HashMap<String, String>,
    fields: HashMap<String, serde_json::Value>,
    timestamp: DateTime<Utc>,
}

impl TimeSeriesDB {
    fn new(node_id: &str, base_path: PathBuf, storage_type: TSStorageType) -> Self {
        let wal = WriteAheadLog {
            enabled: true,
            path: base_path.join("wal"),
            sync_interval: Duration::from_secs(1),
            max_size_bytes: 1024 * 1024 * 1024, // 1GB
        };

        let storage_engine = TimeSeriesStorage {
            storage_type,
            base_path,
            partitions: RwLock::new(HashMap::new()),
            wal,
        };

        let schema_manager = SchemaManager {
            measurements: RwLock::new(HashMap::new()),
            field_dictionary: RwLock::new(HashMap::new()),
            tags_dictionary: RwLock::new(HashMap::new()),
        };

        let parser = TSQueryParser {
            max_query_length: 10000,
            timeout: Duration::from_secs(30),
        };

        let planner = TSQueryPlanner {
            optimizers: Vec::new(),
        };

        let executor = TSQueryExecutor {
            max_concurrent_queries: 10,
            max_points_per_query: 1_000_000,
            timeout: Duration::from_secs(60),
        };

        let query_processor = TSQueryProcessor {
            parser,
            planner,
            executor,
        };

        let partition_manager = TimePartitionManager {
            partition_interval: Duration::from_secs(86400), // 1 day
            max_partitions: 100,
            active_partitions: RwLock::new(Vec::new()),
        };

        let compaction_manager = CompactionManager {
            enabled: true,
            compaction_interval: Duration::from_secs(3600), // 1 hour
            max_concurrent_compactions: 2,
            threshold_size_bytes: 100 * 1024 * 1024, // 100MB
            threshold_series_count: 1_000_000,
            compaction_worker: None,
        };

        TimeSeriesDB {
            node_id: node_id.to_string(),
            storage_engine,
            retention_policies: RwLock::new(HashMap::new()),
            schema_manager,
            query_processor,
            partition_manager,
            compaction_manager,
        }
    }

    fn create_retention_policy(&self, name: &str, duration: Duration, replication_factor: u32, default: bool) -> Result<(), String> {
        println!("åˆ›å»ºä¿ç•™ç­–ç•¥: {}", name);

        let policy = RetentionPolicy {
            name: name.to_string(),
            duration,
            replication_factor,
        };

        let mut policies = self.retention_policies.write().unwrap();

        if policies.contains_key(name) {
            return Err(format!("ä¿ç•™ç­–ç•¥å·²å­˜åœ¨: {}", name));
        }

        policies.insert(name.to_string(), policy);

        Ok(())
    }

    fn write_point(&self, point: TimeSeriesPoint) -> Result<(), String> {
        // éªŒè¯ç‚¹
        self.validate_point(&point)?;

        // ç¡®ä¿æµ‹é‡å­˜åœ¨
        self.ensure_measurement_exists(&point)?;

        // è·å–æ—¶é—´åˆ†åŒº
        let partition_id = self.get_partition_for_time(point.timestamp)?;

        // å†™å…¥åˆ°å­˜å‚¨å¼•æ“
        self.write_to_storage(partition_id, point)?;

        Ok(())
    }

    fn validate_point(&self, point: &TimeSeriesPoint) -> Result<(), String> {
        if point.measurement.is_empty() {
            return Err("æµ‹é‡åç§°ä¸èƒ½ä¸ºç©º".to_string());
        }

        if point.fields.is_empty() {
            return Err("ç‚¹å¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªå­—æ®µ".to_string());
        }

        Ok(())
    }

    fn ensure_measurement_exists(&self, point: &TimeSeriesPoint) -> Result<(), String> {
        let measurements = self.schema_manager.measurements.read().unwrap();

        if !measurements.contains_key(&point.measurement) {
            drop(measurements);

            // åˆ›å»ºæ–°æµ‹é‡
            let mut fields = HashMap::new();
            for (field_name, field_value) in &point.fields {
                let field_type = match field_value {
                    serde_json::Value::Number(n) if n.is_i64() => FieldType::Integer,
                    serde_json::Value::Number(_) => FieldType::Float,
                    serde_json::Value::String(_) => FieldType::String,
                    serde_json::Value::Bool(_) => FieldType::Boolean,
                    _ => return Err(format!("ä¸æ”¯æŒçš„å­—æ®µç±»å‹: {:?}", field_value)),
                };

                fields.insert(field_name.clone(), Field {
                    name: field_name.clone(),
                    field_type,
                    is_indexed: false,
                });

                // æ›´æ–°å­—æ®µå­—å…¸
                let mut field_dict = self.schema_manager.field_dictionary.write().unwrap();
                field_dict.insert(field_name.clone(), Field {
                    name: field_name.clone(),
                    field_type,
                    is_indexed: false,
                });
            }

            // æ›´æ–°æ ‡ç­¾å­—å…¸
            let mut tags_dict = self.schema_manager.tags_dictionary.write().unwrap();
            for tag_name in point.tags.keys() {
                if !tags_dict.contains_key(tag_name) {
                    tags_dict.insert(tag_name.clone(), TagInfo {
                        name: tag_name.clone(),
                        cardinality: 1,
                        is_indexed: true,
                    });
                }
            }

            let measurement = Measurement {
                name: point.measurement.clone(),
                fields,
                tags: point.tags.keys().cloned().collect(),
                retention_policy: "default".to_string(), // ä½¿ç”¨é»˜è®¤ä¿ç•™ç­–ç•¥
                created_at: Utc::now(),
            };

            let mut measurements = self.schema_manager.measurements.write().unwrap();
            measurements.insert(point.measurement.clone(), measurement);
        }

        Ok(())
    }

    fn get_partition_for_time(&self, timestamp: DateTime<Utc>) -> Result<String, String> {
        let partition_interval = self.partition_manager.partition_interval;

        // è®¡ç®—åˆ†åŒºæ—¶é—´èŒƒå›´
        let start_time = timestamp - Duration::seconds(timestamp.timestamp() % partition_interval.as_secs() as i64);
        let end_time = start_time + partition_interval;

        let partition_id = format!("{}_{}", start_time.format("%Y%m%d%H"), end_time.format("%Y%m%d%H"));

        // æ£€æŸ¥åˆ†åŒºæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        let mut partitions = self.storage_engine.partitions.write().unwrap();

        if !partitions.contains_key(&partition_id) {
            println!("åˆ›å»ºæ–°åˆ†åŒº: {}", partition_id);

            let partition = TimePartition {
                id: partition_id.clone(),
                time_range: (start_time, end_time),
                shards: Vec::new(), // åˆ›å»ºåˆå§‹åˆ†ç‰‡
                status: PartitionStatus::Active,
                stats: PartitionStats {
                    points_count: 0,
                    series_count: 0,
                    size_bytes: 0,
                    earliest_time: start_time,
                    latest_time: start_time,
                    avg_points_per_series: 0.0,
                    compression_ratio: 1.0,
                },
            };

            partitions.insert(partition_id.clone(), partition);

            // æ›´æ–°æ´»åŠ¨åˆ†åŒºåˆ—è¡¨
            let mut active_partitions = self.partition_manager.active_partitions.write().unwrap();
            active_partitions.push(partition_id.clone());

            // å¦‚æœæ´»åŠ¨åˆ†åŒºè¶…è¿‡é™åˆ¶ï¼Œæ¸…ç†æœ€æ—§çš„
            if active_partitions.len() > self.partition_manager.max_partitions {
                // æŒ‰æ—¶é—´æ’åº
                active_partitions.sort();
                while active_partitions.len() > self.partition_manager.max_partitions {
                    let oldest = active_partitions.remove(0);
                    println!("å°†åˆ†åŒºæ ‡è®°ä¸ºéæ´»åŠ¨: {}", oldest);
                    if let Some(partition) = partitions.get_mut(&oldest) {
                        partition.status = PartitionStatus::Inactive;
                    }
                }
            }
        }

        Ok(partition_id)
    }

    fn write_to_storage(&self, partition_id: String, point: TimeSeriesPoint) -> Result<(), String> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†ç‚¹å†™å…¥å­˜å‚¨å¼•æ“
        println!("å†™å…¥ç‚¹åˆ°åˆ†åŒº {}: {} @ {}",
                partition_id, point.measurement, point.timestamp);

        // æ›´æ–°åˆ†åŒºç»Ÿè®¡ä¿¡æ¯
        let mut partitions = self.storage_engine.partitions.write().unwrap();
        if let Some(partition) = partitions.get_mut(&partition_id) {
            partition.stats.points_count += 1;
            partition.stats.latest_time = std::cmp::max(partition.stats.latest_time, point.timestamp);
            // å…¶ä»–ç»Ÿè®¡æ›´æ–°...
        }

        Ok(())
    }

    fn query(&self, query_string: &str) -> Result<TSQueryResult, String> {
        println!("æ‰§è¡ŒæŸ¥è¯¢: {}", query_string);

        // è§£ææŸ¥è¯¢
        let plan = self.query_processor.parser.parse(query_string)?;

        // ä¼˜åŒ–æŸ¥è¯¢
        let mut optimized_plan = plan.clone();
        for optimizer in &self.query_processor.planner.optimizers {
            optimizer.optimize(&mut optimized_plan)?;
        }

        // æ‰§è¡ŒæŸ¥è¯¢
        self.query_processor.executor.execute(&optimized_plan)
    }
}

struct TSQueryResult {
    columns: Vec<String>,
    data: Vec<Vec<serde_json::Value>>,
    series: Vec<Series>,
    execution_time: Duration,
}

struct Series {
    name: String,
    tags: HashMap<String, String>,
    columns: Vec<String>,
    values: Vec<Vec<serde_json::Value>>,
}

impl TSQueryParser {
    fn parse(&self, query_string: &str) -> Result<QueryPlan, String> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£ææŸ¥è¯¢è¯­è¨€
        // ç®€åŒ–ï¼šåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„æŸ¥è¯¢è®¡åˆ’

        if query_string.len() > self.max_query_length {
            return Err(format!("æŸ¥è¯¢é•¿åº¦è¶…è¿‡é™åˆ¶: {} > {}",
                    query_string.len(), self.max_query_length));
        }

        let plan = QueryPlan {
            id: uuid::Uuid::new_v4().to_string(),
            query_type: TSQueryType::Select,
            measurements: vec!["cpu".to_string()],
            time_range: (Utc::now() - Duration::from_secs(3600), Utc::now()),
            filters: Vec::new(),
            projections: vec!["usage_user".to_string()],
            group_by: None,
            limit: Some(100),
            offset: None,
            order_by: None,
        };

        Ok(plan)
    }
}

impl TSQueryExecutor {
    fn execute(&self, plan: &QueryPlan) -> Result<TSQueryResult, String> {
        println!("æ‰§è¡ŒæŸ¥è¯¢è®¡åˆ’: {:?}", plan.query_type);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®è®¡åˆ’ä»å­˜å‚¨ä¸­è¯»å–æ•°æ®
        // ç®€åŒ–ï¼šè¿”å›ä¸€äº›æ¨¡æ‹Ÿæ•°æ®

        let columns = plan.projections.clone();
        let mut data = Vec::new();

        // æ¨¡æ‹Ÿæ•°æ®ç‚¹
        for i in 0..10 {
            let timestamp = (Utc::now() - Duration::from_secs(i * 60)).to_rfc3339();
            let value = rand::random::<f64>() * 100.0;

            let row = vec![
                serde_json::Value::String(timestamp),
                serde_json::Value::Number(serde_json::Number::from_f64(value).unwrap()),
            ];

            data.push(row);
        }

        let mut tags = HashMap::new();
        tags.insert("host".to_string(), "server01".to_string());

        let series = Series {
            name: plan.measurements[0].clone(),
            tags,
            columns: vec!["time".to_string(), plan.projections[0].clone()],
            values: data.clone(),
        };

        let result = TSQueryResult {
            columns,
            data,
            series: vec![series],
            execution_time: Duration::from_millis(10),
        };

        Ok(result)
    }
}

// å®æ—¶äº‹ä»¶å¤„ç†ç³»ç»Ÿ
struct RealTimeEventProcessingSystem {
    node_id: String,
    event_sources: RwLock<HashMap<String, EventSource>>,
    processors: RwLock<HashMap<String, EventProcessor>>,
    sinks: RwLock<HashMap<String, EventSink>>,
    pipelines: RwLock<HashMap<String, Pipeline>>,
    execution_engine: ExecutionEngine,
    monitoring: MonitoringSystem,
}

struct EventSource {
    id: String,
    name: String,
    source_type: SourceType,
    config: HashMap<String, String>,
    schema: Option<String>,
    status: ComponentStatus,
    metrics: SourceMetrics,
}

enum SourceType {
    Kafka,
    MQTT,
    HTTP,
    Websocket,
    FileSystem,
    Database,
    Custom,
}

struct SourceMetrics {
    events_received: u64,
    events_processed: u64,
    bytes_received: u64,
    last_event_time: Option<DateTime<Utc>>,
    error_count: u64,
    average_latency: Duration,
}

struct EventProcessor {
    id: String,
    name: String,
    processor_type: ProcessorType,
    config: HashMap<String, String>,
    status: ComponentStatus,
    metrics: ProcessorMetrics,
}

enum ProcessorType {
    Filter,
    Transform,
    Aggregate,
    Enrich,
    Join,
    Window,
    CEP,
    ML,
    Custom,
}

struct ProcessorMetrics {
    events_in: u64,
    events_out: u64,
    processing_time: Duration,
    error_count: u64,
    backpressure_count: u64,
}

struct EventSink {
    id: String,
    name: String,
    sink_type: SinkType,
    config: HashMap<String, String>,
    status: ComponentStatus,
    metrics: SinkMetrics,
}

enum SinkType {
    Kafka,
    MQTT,
    HTTP,
    Websocket,
    FileSystem,
    Database,
    ElasticSearch,
    Custom,
}

struct SinkMetrics {
    events_received: u64,
    events_sent: u64,
    bytes_sent: u64,
    error_count: u64,
    retry_count: u64,
    average_latency: Duration,
}

enum ComponentStatus {
    Running,
    Stopped,
    Failed,
    Degraded,
}

struct Pipeline {
    id: String,
    name: String,
    sources: Vec<String>,
    processors: Vec<PipelineProcessor>,
    sinks: Vec<String>,
    status: PipelineStatus,
    metrics: PipelineMetrics,
}

struct PipelineProcessor {
    processor_id: String,
    inputs: Vec<String>,
    outputs: Vec<String>,
}

enum PipelineStatus {
    Running,
    Stopped,
    Failed,
    Deploying,
    Undeploying,
}

struct PipelineMetrics {
    events_in: u64,
    events_out: u64,
    events_dropped: u64,
    processing_time: Duration,
    throughput: f64,
}

struct ExecutionEngine {
    thread_pool: ThreadPool,
    scheduler: Scheduler,
    buffer_manager: BufferManager,
}

struct ThreadPool {
    min_threads: usize,
    max_threads: usize,
    keep_alive: Duration,
}

struct BufferManager {
    buffers: RwLock<HashMap<String, EventBuffer>>,
}

struct EventBuffer {
    id: String,
    capacity: usize,
    current_size: AtomicUsize,
    overflow_strategy: OverflowStrategy,
}

enum OverflowStrategy {
    Block,
    DropOldest,
    DropNewest,
    Error,
}

struct MonitoringSystem {
    metrics_collector: MetricsCollector,
    alerting: AlertingSystem,
}

struct MetricsCollector {
    collection_interval: Duration,
    retention_period: Duration,
    metrics_store: MetricsStore,
}

enum MetricsStore {
    Memory,
    Prometheus,
    InfluxDB,
    Custom,
}

struct AlertingSystem {
    rules: RwLock<HashMap<String, AlertRule>>,
    notifiers: RwLock<HashMap<String, Notifier>>,
}

struct AlertRule {
    id: String,
    name: String,
    condition: String,
    severity: AlertSeverity,
    notifiers: Vec<String>,
}

struct Notifier {
    id: String,
    name: String,
    notifier_type: NotifierType,
    config: HashMap<String, String>,
}

enum NotifierType {
    Email,
    Slack,
    PagerDuty,
    Webhook,
    SMS,
    Custom,
}

struct Event {
    id: String,
    source: String,
    timestamp: DateTime<Utc>,
    payload: HashMap<String, serde_json::Value>,
    metadata: HashMap<String, String>,
}

impl RealTimeEventProcessingSystem {
    fn new(node_id: &str) -> Self {
        let thread_pool = ThreadPool {
            min_threads: 4,
            max_threads: 32,
            keep_alive: Duration::from_secs(60),
        };

        let scheduler = Scheduler {
            // ...çœç•¥å®ç°ç»†èŠ‚
        };

        let buffer_manager = BufferManager {
            buffers: RwLock::new(HashMap::new()),
        };

        let execution_engine = ExecutionEngine {
            thread_pool,
            scheduler,
            buffer_manager,
        };

        let metrics_collector = MetricsCollector {
            collection_interval: Duration::from_secs(10),
            retention_period: Duration::from_secs(86400), // 1 day
            metrics_store: MetricsStore::Memory,
        };

        let alerting_system = AlertingSystem {
            rules: RwLock::new(HashMap::new()),
            notifiers: RwLock::new(HashMap::new()),
        };

        let monitoring = MonitoringSystem {
            metrics_collector,
            alerting: alerting_system,
        };

        RealTimeEventProcessingSystem {
            node_id: node_id.to_string(),
            event_sources: RwLock::new(HashMap::new()),
            processors: RwLock::new(HashMap::new()),
            sinks: RwLock::new(HashMap::new()),
            pipelines: RwLock::new(HashMap::new()),
            execution_engine,
            monitoring,
        }
    }

    fn add_source(&self, source: EventSource) -> Result<(), String> {
        println!("æ·»åŠ äº‹ä»¶æº: {}", source.name);

        let mut sources = self.event_sources.write().unwrap();

        if sources.contains_key(&source.id) {
            return Err(format!("äº‹ä»¶æºå·²å­˜åœ¨: {}", source.id));
        }

        sources.insert(source.id.clone(), source);

        Ok(())
    }

    fn add_processor(&self, processor: EventProcessor) -> Result<(), String> {
        println!("æ·»åŠ äº‹ä»¶å¤„ç†å™¨: {}", processor.name);

        let mut processors = self.processors.write().unwrap();

        if processors.contains_key(&processor.id) {
            return Err(format!("äº‹ä»¶å¤„ç†å™¨å·²å­˜åœ¨: {}", processor.id));
        }

        processors.insert(processor.id.clone(), processor);

        Ok(())
    }

    fn add_sink(&self, sink: EventSink) -> Result<(), String> {
        println!("æ·»åŠ äº‹ä»¶æ¥æ”¶å™¨: {}", sink.name);

        let mut sinks = self.sinks.write().unwrap();

        if sinks.contains_key(&sink.id) {
            return Err(format!("äº‹ä»¶æ¥æ”¶å™¨å·²å­˜åœ¨: {}", sink.id));
        }

        sinks.insert(sink.id.clone(), sink);

        Ok(())
    }

    fn create_pipeline(&self, pipeline: Pipeline) -> Result<(), String> {
        println!("åˆ›å»ºäº‹ä»¶ç®¡é“: {}", pipeline.name);

        // éªŒè¯ç®¡é“é…ç½®
        self.validate_pipeline(&pipeline)?;

        let mut pipelines = self.pipelines.write().unwrap();

        if pipelines.contains_key(&pipeline.id) {
            return Err(format!("äº‹ä»¶ç®¡é“å·²å­˜åœ¨: {}", pipeline.id));
        }

        pipelines.insert(pipeline.id.clone(), pipeline);

        Ok(())
    }

    fn validate_pipeline(&self, pipeline: &Pipeline) -> Result<(), String> {
        // æ£€æŸ¥æ‰€æœ‰æºæ˜¯å¦å­˜åœ¨
        let sources = self.event_sources.read().unwrap();
        for source_id in &pipeline.sources {
            if !sources.contains_key(source_id) {
                return Err(format!("äº‹ä»¶æºä¸å­˜åœ¨: {}", source_id));
            }
        }

        // æ£€æŸ¥æ‰€æœ‰å¤„ç†å™¨æ˜¯å¦å­˜åœ¨
        let processors = self.processors.read().unwrap();
        for processor in &pipeline.processors {
            if !processors.contains_key(&processor.processor_id) {
                return Err(format!("äº‹ä»¶å¤„ç†å™¨ä¸å­˜åœ¨: {}", processor.processor_id));
            }
        }

        // æ£€æŸ¥æ‰€æœ‰æ¥æ”¶å™¨æ˜¯å¦å­˜åœ¨
        let sinks = self.sinks.read().unwrap();
        for sink_id in &pipeline.sinks {
            if !sinks.contains_key(sink_id) {
                return Err(format!("äº‹ä»¶æ¥æ”¶å™¨ä¸å­˜åœ¨: {}", sink_id));
            }
        }

        // éªŒè¯å›¾è¿æ¥
        // ...çœç•¥å…·ä½“å®ç°

        Ok(())
    }

    fn start_pipeline(&self, pipeline_id: &str) -> Result<(), String> {
        println!("å¯åŠ¨äº‹ä»¶ç®¡é“: {}", pipeline_id);

        let mut pipelines = self.pipelines.write().unwrap();

        let pipeline = pipelines.get_mut(pipeline_id)
            .ok_or_else(|| format!("äº‹ä»¶ç®¡é“ä¸å­˜åœ¨: {}", pipeline_id))?;

        if pipeline.status == PipelineStatus::Running {
            return Err(format!("äº‹ä»¶ç®¡é“å·²åœ¨è¿è¡Œä¸­: {}", pipeline_id));
        }

        // åˆ›å»ºç®¡é“æ‰€éœ€çš„ç¼“å†²åŒº
        self.create_pipeline_buffers(pipeline)?;

        // å¯åŠ¨æ‰€æœ‰ç»„ä»¶
        self.start_pipeline_components(pipeline)?;

        // æ›´æ–°ç®¡é“çŠ¶æ€
        pipeline.status = PipelineStatus::Running;

        Ok(())
    }

    fn create_pipeline_buffers(&self, pipeline: &Pipeline) -> Result<(), String> {
        let mut buffers = self.execution_engine.buffer_manager.buffers.write().unwrap();

        // ä¸ºæ¯ä¸ªæºå’Œå¤„ç†å™¨è¾“å‡ºåˆ›å»ºç¼“å†²åŒº
        for source_id in &pipeline.sources {
            let buffer_id = format!("source_{}_{}", source_id, pipeline.id);

            if !buffers.contains_key(&buffer_id) {
                buffers.insert(buffer_id, EventBuffer {
                    id: buffer_id.clone(),
                    capacity: 10000,
                    current_size: AtomicUsize::new(0),
                    overflow_strategy: OverflowStrategy::Block,
                });
            }
        }

        for processor in &pipeline.processors {
            for output in &processor.outputs {
                let buffer_id = format!("processor_{}_{}_{}", processor.processor_id, output, pipeline.id);

                if !buffers.contains_key(&buffer_id) {
                    buffers.insert(buffer_id, EventBuffer {
                        id: buffer_id.clone(),
                        capacity: 10000,
                        current_size: AtomicUsize::new(0),
                        overflow_strategy: OverflowStrategy::Block,
                    });
                }
            }
        }

        Ok(())
    }

    fn start_pipeline_components(&self, pipeline: &Pipeline) -> Result<(), String> {
        // å¯åŠ¨æ‰€æœ‰æº
        let mut sources = self.event_sources.write().unwrap();
        for source_id in &pipeline.sources {
            if let Some(source) = sources.get_mut(source_id) {
                source.status = ComponentStatus::Running;
                println!("å¯åŠ¨äº‹ä»¶æº: {}", source.name);
            }
        }

        // å¯åŠ¨æ‰€æœ‰å¤„ç†å™¨
        let mut processors = self.processors.write().unwrap();
        for pipeline_processor in &pipeline.processors {
            if let Some(processor) = processors.get_mut(&pipeline_processor.processor_id) {
                processor.status = ComponentStatus::Running;
                println!("å¯åŠ¨äº‹ä»¶å¤„ç†å™¨: {}", processor.name);
            }
        }

        // å¯åŠ¨æ‰€æœ‰æ¥æ”¶å™¨
        let mut sinks = self.sinks.write().unwrap();
        for sink_id in &pipeline.sinks {
            if let Some(sink) = sinks.get_mut(sink_id) {
                sink.status = ComponentStatus::Running;
                println!("å¯åŠ¨äº‹ä»¶æ¥æ”¶å™¨: {}", sink.name);
            }
        }

        Ok(())
    }

    fn stop_pipeline(&self, pipeline_id: &str) -> Result<(), String> {
        println!("åœæ­¢äº‹ä»¶ç®¡é“: {}", pipeline_id);

        let mut pipelines = self.pipelines.write().unwrap();

        let pipeline = pipelines.get_mut(pipeline_id)
            .ok_or_else(|| format!("äº‹ä»¶ç®¡é“ä¸å­˜åœ¨: {}", pipeline_id))?;

        if pipeline.status != PipelineStatus::Running {
            return Err(format!("äº‹ä»¶ç®¡é“æœªè¿è¡Œ: {}", pipeline_id));
        }

        // åœæ­¢æ‰€æœ‰ç»„ä»¶
        self.stop_pipeline_components(pipeline)?;

        // æ›´æ–°ç®¡é“çŠ¶æ€
        pipeline.status = PipelineStatus::Stopped;

        Ok(())
    }

    fn stop_pipeline_components(&self, pipeline: &Pipeline) -> Result<(), String> {
        // åœæ­¢æ‰€æœ‰æº
        let mut sources = self.event_sources.write().unwrap();
        for source_id in &pipeline.sources {
            if let Some(source) = sources.get_mut(source_id) {
                source.status = ComponentStatus::Stopped;
                println!("åœæ­¢äº‹ä»¶æº: {}", source.name);
            }
        }

        // åœæ­¢æ‰€æœ‰å¤„ç†å™¨
        let mut processors = self.processors.write().unwrap();
        for pipeline_processor in &pipeline.processors {
            if let Some(processor) = processors.get_mut(&pipeline_processor.processor_id) {
                processor.status = ComponentStatus::Stopped;
                println!("åœæ­¢äº‹ä»¶å¤„ç†å™¨: {}", processor.name);
            }
        }

        // åœæ­¢æ‰€æœ‰æ¥æ”¶å™¨
        let mut sinks = self.sinks.write().unwrap();
        for sink_id in &pipeline.sinks {
            if let Some(sink) = sinks.get_mut(sink_id) {
                sink.status = ComponentStatus::Stopped;
                println!("åœæ­¢äº‹ä»¶æ¥æ”¶å™¨: {}", sink.name);
            }
        }

        Ok(())
    }

    fn process_event(&self, event: Event) -> Result<(), String> {
        println!("å¤„ç†äº‹ä»¶: {}", event.id);

        // æŸ¥æ‰¾åŒ…å«æ­¤äº‹ä»¶æºçš„ç®¡é“
        let pipelines = self.pipelines.read().unwrap();
        let matching_pipelines: Vec<_> = pipelines.values()
            .filter(|p| p.status == PipelineStatus::Running && p.sources.contains(&event.source))
            .collect();

        if matching_pipelines.is_empty() {
            return Err(format!("æ²¡æœ‰æ‰¾åˆ°å¤„ç†äº‹ä»¶æº {} çš„è¿è¡Œä¸­çš„ç®¡é“", event.source));
        }

        // å°†äº‹ä»¶å‘é€åˆ°æ‰€æœ‰åŒ¹é…çš„ç®¡é“
        for pipeline in matching_pipelines {
            println!("å°†äº‹ä»¶ {} å‘é€åˆ°ç®¡é“ {}", event.id, pipeline.name);

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†äº‹ä»¶æ”¾å…¥ç›¸åº”çš„ç¼“å†²åŒºï¼Œç„¶åç”±å¤„ç†å™¨å¤„ç†
        }

        Ok(())
    }

    fn add_alert_rule(&self, rule: AlertRule) -> Result<(), String> {
        println!("æ·»åŠ å‘Šè­¦è§„åˆ™: {}", rule.name);

        let mut rules = self.monitoring.alerting.rules.write().unwrap();

        if rules.contains_key(&rule.id) {
            return Err(format!("å‘Šè­¦è§„åˆ™å·²å­˜åœ¨: {}", rule.id));
        }

        // éªŒè¯é€šçŸ¥å™¨æ˜¯å¦å­˜åœ¨
        let notifiers = self.monitoring.alerting.notifiers.read().unwrap();
        for notifier_id in &rule.notifiers {
            if !notifiers.contains_key(notifier_id) {
                return Err(format!("é€šçŸ¥å™¨ä¸å­˜åœ¨: {}", notifier_id));
            }
        }

        rules.insert(rule.id.clone(), rule);

        Ok(())
    }

    fn add_notifier(&self, notifier: Notifier) -> Result<(), String> {
        println!("æ·»åŠ é€šçŸ¥å™¨: {}", notifier.name);

        let mut notifiers = self.monitoring.alerting.notifiers.write().unwrap();

        if notifiers.contains_key(&notifier.id) {
            return Err(format!("é€šçŸ¥å™¨å·²å­˜åœ¨: {}", notifier.id));
        }

        notifiers.insert(notifier.id.clone(), notifier);

        Ok(())
    }
}

// ç©ºé—´æ•°æ®åº“
struct SpatialDatabase {
    node_id: String,
    storage_engine: SpatialStorageEngine,
    index_manager: SpatialIndexManager,
    query_processor: SpatialQueryProcessor,
    geo_functions: GeoFunctionLibrary,
}

struct SpatialStorageEngine {
    storage_type: SpatialStorageType,
    tables: RwLock<HashMap<String, SpatialTable>>,
}

enum Spat

```rust
enum SpatialStorageType {
    Memory,
    File,
    Hybrid,
}

struct SpatialTable {
    name: String,
    columns: Vec<ColumnDefinition>,
    geometry_column: String,
    srid: i32,
    row_count: u64,
    bbox: BoundingBox,
    storage_size_bytes: u64,
}

struct ColumnDefinition {
    name: String,
    data_type: DataType,
    not_null: bool,
    default_value: Option<String>,
}

enum DataType {
    Integer,
    Float,
    Text,
    Boolean,
    Date,
    Timestamp,
    Geometry(GeometryType),
}

enum GeometryType {
    Point,
    LineString,
    Polygon,
    MultiPoint,
    MultiLineString,
    MultiPolygon,
    GeometryCollection,
}

struct BoundingBox {
    min_x: f64,
    min_y: f64,
    max_x: f64,
    max_y: f64,
}

struct SpatialIndexManager {
    indices: RwLock<HashMap<String, SpatialIndex>>,
}

struct SpatialIndex {
    id: String,
    table_name: String,
    column_name: String,
    index_type: SpatialIndexType,
    parameters: HashMap<String, String>,
    stats: IndexStats,
}

enum SpatialIndexType {
    RTree,
    QuadTree,
    GridIndex,
    GeoHash,
}

struct IndexStats {
    creation_time: DateTime<Utc>,
    last_updated: DateTime<Utc>,
    node_count: u64,
    depth: u32,
    coverage_ratio: f64,
    query_time_avg: Duration,
}

struct SpatialQueryProcessor {
    parser: SpatialQueryParser,
    optimizer: SpatialQueryOptimizer,
    executor: SpatialQueryExecutor,
}

struct SpatialQueryParser {
    supported_langs: Vec<String>,
    max_query_length: usize,
}

struct SpatialQueryOptimizer {
    optimization_level: u32,
    strategies: Vec<OptimizationStrategy>,
}

enum OptimizationStrategy {
    IndexSelection,
    SpatialPredicatePushdown,
    BoundingBoxFiltering,
    JoinOrdering,
}

struct SpatialQueryExecutor {
    max_concurrent_queries: usize,
    max_memory_usage: u64,
    timeout: Duration,
}

struct GeoFunctionLibrary {
    functions: HashMap<String, GeoFunction>,
}

struct GeoFunction {
    name: String,
    description: String,
    parameters: Vec<FunctionParameter>,
    return_type: DataType,
}

struct FunctionParameter {
    name: String,
    param_type: DataType,
    optional: bool,
    default_value: Option<String>,
}

struct Geometry {
    geom_type: GeometryType,
    srid: i32,
    coordinates: Vec<Coordinate>,
    bbox: Option<BoundingBox>,
}

struct Coordinate {
    x: f64,
    y: f64,
    z: Option<f64>,
    m: Option<f64>,
}

struct SpatialQuery {
    query_type: SpatialQueryType,
    target_table: String,
    columns: Vec<String>,
    spatial_filter: Option<SpatialFilter>,
    non_spatial_filter: Option<String>,
    order_by: Option<String>,
    limit: Option<usize>,
    offset: Option<usize>,
}

enum SpatialQueryType {
    Select,
    Insert,
    Update,
    Delete,
}

enum SpatialFilter {
    Intersects(Geometry),
    Contains(Geometry),
    Within(Geometry),
    Distance(Geometry, f64),
    DWithin(Geometry, f64),
    BoundingBox(BoundingBox),
}

struct SpatialQueryResult {
    columns: Vec<String>,
    rows: Vec<Vec<serde_json::Value>>,
    row_count: usize,
    execution_time: Duration,
}

impl SpatialDatabase {
    fn new(node_id: &str) -> Self {
        let storage_engine = SpatialStorageEngine {
            storage_type: SpatialStorageType::Memory,
            tables: RwLock::new(HashMap::new()),
        };

        let index_manager = SpatialIndexManager {
            indices: RwLock::new(HashMap::new()),
        };

        let parser = SpatialQueryParser {
            supported_langs: vec!["SQL".to_string(), "GeoJSON".to_string()],
            max_query_length: 10000,
        };

        let optimizer = SpatialQueryOptimizer {
            optimization_level: 2,
            strategies: vec![
                OptimizationStrategy::IndexSelection,
                OptimizationStrategy::BoundingBoxFiltering,
                OptimizationStrategy::SpatialPredicatePushdown,
            ],
        };

        let executor = SpatialQueryExecutor {
            max_concurrent_queries: 10,
            max_memory_usage: 1024 * 1024 * 1024, // 1GB
            timeout: Duration::from_secs(30),
        };

        let query_processor = SpatialQueryProcessor {
            parser,
            optimizer,
            executor,
        };

        let geo_functions = GeoFunctionLibrary {
            functions: HashMap::new(),
        };

        SpatialDatabase {
            node_id: node_id.to_string(),
            storage_engine,
            index_manager,
            query_processor,
            geo_functions,
        }
    }

    fn create_table(&self, table: SpatialTable) -> Result<(), String> {
        println!("åˆ›å»ºç©ºé—´è¡¨: {}", table.name);

        // éªŒè¯è¡¨ç»“æ„
        self.validate_table(&table)?;

        let mut tables = self.storage_engine.tables.write().unwrap();

        if tables.contains_key(&table.name) {
            return Err(format!("è¡¨å·²å­˜åœ¨: {}", table.name));
        }

        tables.insert(table.name.clone(), table);

        Ok(())
    }

    fn validate_table(&self, table: &SpatialTable) -> Result<(), String> {
        // æ£€æŸ¥è¡¨å
        if table.name.is_empty() {
            return Err("è¡¨åä¸èƒ½ä¸ºç©º".to_string());
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰åˆ—
        if table.columns.is_empty() {
            return Err("è¡¨å¿…é¡»è‡³å°‘æœ‰ä¸€åˆ—".to_string());
        }

        // éªŒè¯å‡ ä½•åˆ—
        let geom_column = table.columns.iter()
            .find(|c| c.name == table.geometry_column);

        if let Some(column) = geom_column {
            match &column.data_type {
                DataType::Geometry(_) => {}, // æœ‰æ•ˆçš„å‡ ä½•ç±»å‹
                _ => return Err(format!("å‡ ä½•åˆ— {} å¿…é¡»æ˜¯å‡ ä½•ç±»å‹", table.geometry_column)),
            }
        } else {
            return Err(format!("æ‰¾ä¸åˆ°å‡ ä½•åˆ—: {}", table.geometry_column));
        }

        // éªŒè¯SRID
        if table.srid < 0 {
            return Err(format!("æ— æ•ˆçš„SRID: {}", table.srid));
        }

        Ok(())
    }

    fn create_spatial_index(&self, index: SpatialIndex) -> Result<(), String> {
        println!("åˆ›å»ºç©ºé—´ç´¢å¼•: {}", index.id);

        // éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();
        if !tables.contains_key(&index.table_name) {
            return Err(format!("è¡¨ä¸å­˜åœ¨: {}", index.table_name));
        }

        // éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        let table = tables.get(&index.table_name).unwrap();
        let column = table.columns.iter()
            .find(|c| c.name == index.column_name);

        if let Some(column) = column {
            match &column.data_type {
                DataType::Geometry(_) => {}, // æœ‰æ•ˆçš„å‡ ä½•ç±»å‹
                _ => return Err(format!("åˆ— {} ä¸æ˜¯å‡ ä½•ç±»å‹", index.column_name)),
            }
        } else {
            return Err(format!("è¡¨ {} ä¸­æ‰¾ä¸åˆ°åˆ—: {}", index.table_name, index.column_name));
        }

        let mut indices = self.index_manager.indices.write().unwrap();

        if indices.contains_key(&index.id) {
            return Err(format!("ç´¢å¼•å·²å­˜åœ¨: {}", index.id));
        }

        indices.insert(index.id.clone(), index);

        Ok(())
    }

    fn execute_query(&self, query: SpatialQuery) -> Result<SpatialQueryResult, String> {
        println!("æ‰§è¡Œç©ºé—´æŸ¥è¯¢: {:?} on {}", query.query_type, query.target_table);

        let start_time = Instant::now();

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();
        if !tables.contains_key(&query.target_table) {
            return Err(format!("è¡¨ä¸å­˜åœ¨: {}", query.target_table));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š
        // 1. è§£æå’Œä¼˜åŒ–æŸ¥è¯¢
        // 2. é€‰æ‹©åˆé€‚çš„ç©ºé—´ç´¢å¼•
        // 3. æ‰§è¡Œç©ºé—´è¿‡æ»¤
        // 4. åº”ç”¨éç©ºé—´è¿‡æ»¤æ¡ä»¶
        // 5. è¿”å›ç»“æœ

        // ç®€åŒ–ï¼šè¿”å›ä¸€äº›æ¨¡æ‹Ÿæ•°æ®
        let mut rows = Vec::new();
        let columns = if query.columns.is_empty() {
            vec!["id".to_string(), "name".to_string(), "geom".to_string()]
        } else {
            query.columns.clone()
        };

        for i in 0..5 {
            let mut row = Vec::new();
            row.push(serde_json::json!(i + 1)); // id
            row.push(serde_json::json!(format!("Feature {}", i + 1))); // name
            row.push(serde_json::json!({
                "type": "Point",
                "coordinates": [10.0 + i as f64, 20.0 + i as f64]
            })); // geom

            rows.push(row);
        }

        let execution_time = start_time.elapsed();

        let result = SpatialQueryResult {
            columns,
            rows,
            row_count: rows.len(),
            execution_time,
        };

        Ok(result)
    }

    fn register_geo_function(&self, function: GeoFunction) -> Result<(), String> {
        println!("æ³¨å†Œåœ°ç†å‡½æ•°: {}", function.name);

        let mut functions = self.geo_functions.functions.clone();

        if functions.contains_key(&function.name) {
            return Err(format!("å‡½æ•°å·²å­˜åœ¨: {}", function.name));
        }

        functions.insert(function.name.clone(), function);

        Ok(())
    }

    fn import_data(&self, table_name: &str, data: &str, format: &str) -> Result<usize, String> {
        println!("å¯¼å…¥æ•°æ®åˆ°è¡¨ {}, æ ¼å¼: {}", table_name, format);

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let mut tables = self.storage_engine.tables.write().unwrap();
        let table = tables.get_mut(table_name)
            .ok_or_else(|| format!("è¡¨ä¸å­˜åœ¨: {}", table_name))?;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š
        // 1. è§£æè¾“å…¥æ•°æ®ï¼ˆGeoJSONã€WKTã€Shapefileç­‰ï¼‰
        // 2. éªŒè¯æ•°æ®æ˜¯å¦ç¬¦åˆè¡¨ç»“æ„
        // 3. å°†æ•°æ®è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼å¹¶å­˜å‚¨
        // 4. æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯

        // ç®€åŒ–ï¼šå‡è®¾å¯¼å…¥äº†5æ¡è®°å½•
        let imported_count = 5;

        // æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
        table.row_count += imported_count as u64;

        // æ›´æ–°è¡¨çš„è¾¹ç•Œæ¡†
        let new_bbox = BoundingBox {
            min_x: -180.0,
            min_y: -90.0,
            max_x: 180.0,
            max_y: 90.0,
        };

        table.bbox = new_bbox;

        Ok(imported_count)
    }

    fn export_data(&self, query: SpatialQuery, format: &str) -> Result<String, String> {
        println!("å¯¼å‡ºæŸ¥è¯¢ç»“æœ, æ ¼å¼: {}", format);

        // æ‰§è¡ŒæŸ¥è¯¢
        let result = self.execute_query(query)?;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š
        // 1. å°†æŸ¥è¯¢ç»“æœè½¬æ¢ä¸ºè¯·æ±‚çš„è¾“å‡ºæ ¼å¼ï¼ˆGeoJSONã€WKTã€Shapefileç­‰ï¼‰

        // ç®€åŒ–ï¼šè¿”å›GeoJSONæ ¼å¼çš„æ•°æ®
        match format.to_lowercase().as_str() {
            "geojson" => {
                let mut features = Vec::new();

                for row in &result.rows {
                    if row.len() >= 3 {
                        let id = &row[0];
                        let name = &row[1];
                        let geom = &row[2];

                        let feature = serde_json::json!({
                            "type": "Feature",
                            "id": id,
                            "properties": {
                                "name": name
                            },
                            "geometry": geom
                        });

                        features.push(feature);
                    }
                }

                let geojson = serde_json::json!({
                    "type": "FeatureCollection",
                    "features": features
                });

                Ok(geojson.to_string())
            },
            _ => Err(format!("ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {}", format)),
        }
    }
}

// åˆ†å¸ƒå¼äº‹åŠ¡ç®¡ç†å™¨
struct DistributedTransactionManager {
    node_id: String,
    transaction_store: TransactionStore,
    coordinator: TransactionCoordinator,
    participant_manager: ParticipantManager,
    recovery_manager: RecoveryManager,
}

struct TransactionStore {
    transactions: RwLock<HashMap<String, Transaction>>,
    storage: Box<dyn TransactionStorage>,
}

struct Transaction {
    id: String,
    participants: Vec<ParticipantInfo>,
    status: TransactionStatus,
    timeout: Duration,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
    completed_at: Option<DateTime<Utc>>,
}

struct ParticipantInfo {
    id: String,
    resource_id: String,
    status: ParticipantStatus,
    vote: Option<Vote>,
    prepare_time: Option<DateTime<Utc>>,
    commit_time: Option<DateTime<Utc>>,
}

enum ParticipantStatus {
    Registered,
    Prepared,
    Committed,
    Aborted,
    TimedOut,
}

enum Vote {
    Commit,
    Abort,
}

enum TransactionStatus {
    Active,
    Preparing,
    Prepared,
    Committing,
    Committed,
    Aborting,
    Aborted,
    TimedOut,
    Unknown,
}

struct TransactionCoordinator {
    prepare_timeout: Duration,
    commit_timeout: Duration,
    heartbeat_interval: Duration,
}

struct ParticipantManager {
    participants: RwLock<HashMap<String, ParticipantConnection>>,
}

struct ParticipantConnection {
    id: String,
    endpoint: String,
    status: ConnectionStatus,
    last_heartbeat: DateTime<Utc>,
}

enum ConnectionStatus {
    Connected,
    Disconnected,
    Unknown,
}

struct RecoveryManager {
    recovery_interval: Duration,
    max_recovery_attempts: u32,
}

impl DistributedTransactionManager {
    fn new(node_id: &str, storage: Box<dyn TransactionStorage>) -> Self {
        let transaction_store = TransactionStore {
            transactions: RwLock::new(HashMap::new()),
            storage,
        };

        let coordinator = TransactionCoordinator {
            prepare_timeout: Duration::from_secs(30),
            commit_timeout: Duration::from_secs(30),
            heartbeat_interval: Duration::from_secs(5),
        };

        let participant_manager = ParticipantManager {
            participants: RwLock::new(HashMap::new()),
        };

        let recovery_manager = RecoveryManager {
            recovery_interval: Duration::from_secs(60),
            max_recovery_attempts: 3,
        };

        DistributedTransactionManager {
            node_id: node_id.to_string(),
            transaction_store,
            coordinator,
            participant_manager,
            recovery_manager,
        }
    }

    fn start_transaction(&self) -> Result<String, String> {
        let tx_id = uuid::Uuid::new_v4().to_string();

        println!("å¯åŠ¨äº‹åŠ¡: {}", tx_id);

        let now = Utc::now();
        let transaction = Transaction {
            id: tx_id.clone(),
            participants: Vec::new(),
            status: TransactionStatus::Active,
            timeout: Duration::from_secs(60),
            created_at: now,
            updated_at: now,
            completed_at: None,
        };

        // å­˜å‚¨äº‹åŠ¡
        let mut transactions = self.transaction_store.transactions.write().unwrap();
        transactions.insert(tx_id.clone(), transaction.clone());

        // æŒä¹…åŒ–äº‹åŠ¡
        self.transaction_store.storage.create_transaction(&tx_id)?;

        Ok(tx_id)
    }

    fn register_participant(&self, tx_id: &str, participant_id: &str, resource_id: &str) -> Result<(), String> {
        println!("æ³¨å†Œå‚ä¸è€… {} åˆ°äº‹åŠ¡ {}", participant_id, tx_id);

        let mut transactions = self.transaction_store.transactions.write().unwrap();

        let transaction = transactions.get_mut(tx_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸å¤„äºæ´»åŠ¨çŠ¶æ€: {}", tx_id));
        }

        // æ£€æŸ¥å‚ä¸è€…æ˜¯å¦å·²æ³¨å†Œ
        if transaction.participants.iter().any(|p| p.id == participant_id) {
            return Err(format!("å‚ä¸è€…å·²æ³¨å†Œ: {}", participant_id));
        }

        // æ·»åŠ å‚ä¸è€…
        let participant = ParticipantInfo {
            id: participant_id.to_string(),
            resource_id: resource_id.to_string(),
            status: ParticipantStatus::Registered,
            vote: None,
            prepare_time: None,
            commit_time: None,
        };

        transaction.participants.push(participant);
        transaction.updated_at = Utc::now();

        // æ›´æ–°äº‹åŠ¡å­˜å‚¨
        self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Active {
            participants: transaction.participants.iter().map(|p| p.id.clone()).collect(),
        })?;

        Ok(())
    }

    fn prepare(&self, tx_id: &str) -> Result<bool, String> {
        println!("å‡†å¤‡äº‹åŠ¡: {}", tx_id);

        let mut transactions = self.transaction_store.transactions.write().unwrap();

        let transaction = transactions.get_mut(tx_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸å¤„äºæ´»åŠ¨çŠ¶æ€: {}", tx_id));
        }

        if transaction.participants.is_empty() {
            return Err(format!("äº‹åŠ¡æ²¡æœ‰å‚ä¸è€…: {}", tx_id));
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.status = TransactionStatus::Preparing;
        transaction.updated_at = Utc::now();

        // æ›´æ–°äº‹åŠ¡å­˜å‚¨
        self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Preparing {
            started_at: transaction.updated_at,
        })?;

        // å‡†å¤‡æ‰€æœ‰å‚ä¸è€…
        let prepared = self.prepare_participants(transaction)?;

        if prepared {
            // æ‰€æœ‰å‚ä¸è€…éƒ½å‡†å¤‡å¥½äº†
            transaction.status = TransactionStatus::Prepared;
            transaction.updated_at = Utc::now();

            // æ›´æ–°äº‹åŠ¡å­˜å‚¨
            self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Prepared {
                prepared_at: transaction.updated_at,
            })?;
        } else {
            // è‡³å°‘æœ‰ä¸€ä¸ªå‚ä¸è€…æŠ•ç¥¨ä¸­æ­¢
            transaction.status = TransactionStatus::Aborting;
            transaction.updated_at = Utc::now();

            // æ›´æ–°äº‹åŠ¡å­˜å‚¨
            self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Aborting {
                abort_reason: "å‚ä¸è€…æŠ•ç¥¨ä¸­æ­¢".to_string(),
                started_at: transaction.updated_at,
            })?;
        }

        Ok(prepared)
    }

    fn prepare_participants(&self, transaction: &mut Transaction) -> Result<bool, String> {
        let all_prepared = true;

        for participant in &mut transaction.participants {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘å‚ä¸è€…å‘é€å‡†å¤‡è¯·æ±‚
            println!("è¯·æ±‚å‚ä¸è€… {} å‡†å¤‡", participant.id);

            // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰å‚ä¸è€…éƒ½å‡†å¤‡å¥½å¹¶æŠ•ç¥¨æäº¤
            participant.status = ParticipantStatus::Prepared;
            participant.vote = Some(Vote::Commit);
            participant.prepare_time = Some(Utc::now());
        }

        Ok(all_prepared)
    }

    fn commit_transaction(&self, tx_id: &str) -> Result<bool, String> {
        println!("æäº¤äº‹åŠ¡: {}", tx_id);

        let mut transactions = self.transaction_store.transactions.write().unwrap();

        let transaction = transactions.get_mut(tx_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id))?;

        if transaction.status != TransactionStatus::Prepared {
            return Err(format!("äº‹åŠ¡æœªå‡†å¤‡å¥½: {}", tx_id));
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.status = TransactionStatus::Committing;
        transaction.updated_at = Utc::now();

        // æ›´æ–°äº‹åŠ¡å­˜å‚¨
        self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Committing {
            started_at: transaction.updated_at,
        })?;

        // æäº¤æ‰€æœ‰å‚ä¸è€…
        let all_committed = self.commit_participants(transaction)?;

        if all_committed {
            // æ‰€æœ‰å‚ä¸è€…éƒ½å·²æäº¤
            transaction.status = TransactionStatus::Committed;
            transaction.updated_at = Utc::now();
            transaction.completed_at = Some(transaction.updated_at);

            // æ›´æ–°äº‹åŠ¡å­˜å‚¨
            self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Committed {
                committed_at: transaction.updated_at,
            })?;
        } else {
            // æäº¤é˜¶æ®µå‡ºç°é”™è¯¯
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™å¯èƒ½éœ€è¦åè°ƒå‘˜å¹²é¢„
            return Err("æäº¤é˜¶æ®µå‡ºç°é”™è¯¯ï¼Œéœ€è¦æ‰‹åŠ¨å¹²é¢„".to_string());
        }

        Ok(all_committed)
    }

    fn commit_participants(&self, transaction: &mut Transaction) -> Result<bool, String> {
        let mut all_committed = true;

        for participant in &mut transaction.participants {
            if participant.status != ParticipantStatus::Prepared || participant.vote != Some(Vote::Commit) {
                continue;
            }

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘å‚ä¸è€…å‘é€æäº¤è¯·æ±‚
            println!("è¯·æ±‚å‚ä¸è€… {} æäº¤", participant.id);

            // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰å‚ä¸è€…éƒ½æˆåŠŸæäº¤
            participant.status = ParticipantStatus::Committed;
            participant.commit_time = Some(Utc::now());
        }

        Ok(all_committed)
    }

    fn abort_transaction(&self, tx_id: &str, reason: &str) -> Result<bool, String> {
        println!("ä¸­æ­¢äº‹åŠ¡: {}, åŸå› : {}", tx_id, reason);

        let mut transactions = self.transaction_store.transactions.write().unwrap();

        let transaction = transactions.get_mut(tx_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id))?;

        // åªæœ‰æ´»åŠ¨ã€å‡†å¤‡ä¸­æˆ–å·²å‡†å¤‡çš„äº‹åŠ¡å¯ä»¥è¢«ä¸­æ­¢
        match transaction.status {
            TransactionStatus::Active |
            TransactionStatus::Preparing |
            TransactionStatus::Prepared => {},
            _ => return Err(format!("æ— æ³•ä¸­æ­¢å¤„äº {:?} çŠ¶æ€çš„äº‹åŠ¡", transaction.status)),
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.status = TransactionStatus::Aborting;
        transaction.updated_at = Utc::now();

        // æ›´æ–°äº‹åŠ¡å­˜å‚¨
        self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Aborting {
            abort_reason: reason.to_string(),
            started_at: transaction.updated_at,
        })?;

        // ä¸­æ­¢æ‰€æœ‰å‚ä¸è€…
        let all_aborted = self.abort_participants(transaction)?;

        if all_aborted {
            // æ‰€æœ‰å‚ä¸è€…éƒ½å·²ä¸­æ­¢
            transaction.status = TransactionStatus::Aborted;
            transaction.updated_at = Utc::now();
            transaction.completed_at = Some(transaction.updated_at);

            // æ›´æ–°äº‹åŠ¡å­˜å‚¨
            self.transaction_store.storage.update_transaction_state(tx_id, &TransactionState::Aborted {
                aborted_at: transaction.updated_at,
                reason: reason.to_string(),
            })?;
        } else {
            // ä¸­æ­¢é˜¶æ®µå‡ºç°é”™è¯¯
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™å¯èƒ½éœ€è¦åè°ƒå‘˜å¹²é¢„
            return Err("ä¸­æ­¢é˜¶æ®µå‡ºç°é”™è¯¯ï¼Œéœ€è¦æ‰‹åŠ¨å¹²é¢„".to_string());
        }

        Ok(all_aborted)
    }

    fn abort_participants(&self, transaction: &mut Transaction) -> Result<bool, String> {
        let mut all_aborted = true;

        for participant in &mut transaction.participants {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘å‚ä¸è€…å‘é€ä¸­æ­¢è¯·æ±‚
            println!("è¯·æ±‚å‚ä¸è€… {} ä¸­æ­¢", participant.id);

            // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰å‚ä¸è€…éƒ½æˆåŠŸä¸­æ­¢
            participant.status = ParticipantStatus::Aborted;
        }

        Ok(all_aborted)
    }

    fn get_transaction_status(&self, tx_id: &str) -> Result<TransactionStatus, String> {
        let transactions = self.transaction_store.transactions.read().unwrap();

        let transaction = transactions.get(tx_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id))?;

        Ok(transaction.status.clone())
    }

    fn recover_transactions(&self) -> Result<usize, String> {
        println!("æ¢å¤æœªå®Œæˆçš„äº‹åŠ¡...");

        // è·å–æ‰€æœ‰æœªå®Œæˆçš„äº‹åŠ¡
        let active_transactions = self.transaction_store.storage.list_active_transactions()?;

        let mut recovered_count = 0;

        for (tx_id, state) in active_transactions {
            println!("æ¢å¤äº‹åŠ¡: {}", tx_id);

            match state {
                TransactionState::Prepared { .. } => {
                    // å·²å‡†å¤‡å¥½çš„äº‹åŠ¡åº”è¯¥ç»§ç»­æäº¤
                    match self.commit_transaction(&tx_id) {
                        Ok(_) => {
                            println!("æˆåŠŸæ¢å¤å¹¶æäº¤äº‹åŠ¡: {}", tx_id);
                            recovered_count += 1;
                        },
                        Err(err) => {
                            println!("æ¢å¤äº‹åŠ¡ {} å¤±è´¥: {}", tx_id, err);
                        }
                    }
                },
                TransactionState::Committing { .. } => {
                    // æ­£åœ¨æäº¤çš„äº‹åŠ¡åº”è¯¥ç»§ç»­æäº¤
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥å“ªäº›å‚ä¸è€…å°šæœªæäº¤
                    match self.commit_transaction(&tx_id) {
                        Ok(_) => {
                            println!("æˆåŠŸæ¢å¤å¹¶æäº¤äº‹åŠ¡: {}", tx_id);
                            recovered_count += 1;
                        },
                        Err(err) => {
                            println!("æ¢å¤äº‹åŠ¡ {} å¤±è´¥: {}", tx_id, err);
                        }
                    }
                },
                TransactionState::Aborting { .. } => {
                    // æ­£åœ¨ä¸­æ­¢çš„äº‹åŠ¡åº”è¯¥ç»§ç»­ä¸­æ­¢
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥å“ªäº›å‚ä¸è€…å°šæœªä¸­æ­¢
                    match self.abort_transaction(&tx_id, "æ¢å¤æœŸé—´ä¸­æ­¢") {
                        Ok(_) => {
                            println!("æˆåŠŸæ¢å¤å¹¶ä¸­æ­¢äº‹åŠ¡: {}", tx_id);
                            recovered_count += 1;
                        },
                        Err(err) => {
                            println!("æ¢å¤äº‹åŠ¡ {} å¤±è´¥: {}", tx_id, err);
                        }
                    }
                },
                _ => {
                    // å…¶ä»–çŠ¶æ€çš„äº‹åŠ¡å¯èƒ½éœ€è¦æ ¹æ®è¶…æ—¶æƒ…å†µåˆ¤æ–­æ˜¯æäº¤è¿˜æ˜¯ä¸­æ­¢
                    // ç®€åŒ–ï¼šä¸­æ­¢æ‰€æœ‰å…¶ä»–çŠ¶æ€çš„äº‹åŠ¡
                    match self.abort_transaction(&tx_id, "æ¢å¤æœŸé—´é»˜è®¤ä¸­æ­¢") {
                        Ok(_) => {
                            println!("æˆåŠŸæ¢å¤å¹¶ä¸­æ­¢äº‹åŠ¡: {}", tx_id);
                            recovered_count += 1;
                        },
                        Err(err) => {
                            println!("æ¢å¤äº‹åŠ¡ {} å¤±è´¥: {}", tx_id, err);
                        }
                    }
                }
            }
        }

        println!("æ¢å¤äº† {} ä¸ªäº‹åŠ¡", recovered_count);

        Ok(recovered_count)
    }
}

fn main() {
    println!("åˆ†å¸ƒå¼ç³»ç»Ÿç»„ä»¶ç¤ºä¾‹å®Œæˆï¼");
}
```

### 1.8 ç»¼åˆåº”ç”¨08-åˆ†å¸ƒå¼å›¾æ•°æ®åº“

```rust
// åˆ†å¸ƒå¼å›¾æ•°æ®åº“
struct DistributedGraphDB {
    node_id: String,
    storage_engine: GraphStorageEngine,
    query_processor: GraphQueryProcessor,
    index_manager: GraphIndexManager,
    partition_manager: GraphPartitionManager,
    transaction_manager: GraphTransactionManager,
}

struct GraphStorageEngine {
    storage_type: GraphStorageType,
    vertices: RwLock<HashMap<String, Vertex>>,
    edges: RwLock<HashMap<String, Edge>>,
    properties: RwLock<HashMap<String, Property>>,
}

enum GraphStorageType {
    Memory,
    Disk,
    Hybrid,
}

struct Vertex {
    id: String,
    label: String,
    properties: HashMap<String, PropertyValue>,
    in_edges: Vec<String>,
    out_edges: Vec<String>,
    partition_id: String,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct Edge {
    id: String,
    label: String,
    from_vertex: String,
    to_vertex: String,
    properties: HashMap<String, PropertyValue>,
    partition_id: String,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct Property {
    key: String,
    value_type: PropertyType,
    indexed: bool,
    unique: bool,
}

enum PropertyType {
    String,
    Integer,
    Float,
    Boolean,
    Date,
    DateTime,
    List,
    Map,
}

enum PropertyValue {
    String(String),
    Integer(i64),
    Float(f64),
    Boolean(bool),
    Date(NaiveDate),
    DateTime(DateTime<Utc>),
    List(Vec<PropertyValue>),
    Map(HashMap<String, PropertyValue>),
}

struct GraphQueryProcessor {
    parser: GraphQueryParser,
    optimizer: GraphQueryOptimizer,
    executor: GraphQueryExecutor,
}

struct GraphQueryParser {
    supported_languages: Vec<GraphQueryLanguage>,
    max_query_length: usize,
}

enum GraphQueryLanguage {
    Gremlin,
    Cypher,
    SPARQL,
    GraphQL,
    Custom,
}

struct GraphQueryOptimizer {
    optimization_rules: Vec<OptimizationRule>,
    statistics: GraphStatistics,
}

enum OptimizationRule {
    IndexUsage,
    PatternRewriting,
    JoinOrdering,
    PathSimplification,
    PredicatePushdown,
}

struct GraphStatistics {
    vertex_count: u64,
    edge_count: u64,
    label_statistics: HashMap<String, LabelStatistics>,
    property_statistics: HashMap<String, PropertyStatistics>,
}

struct LabelStatistics {
    count: u64,
    avg_properties: f64,
    avg_edges: f64,
}

struct PropertyStatistics {
    count: u64,
    distinct_values: u64,
    min_value: Option<PropertyValue>,
    max_value: Option<PropertyValue>,
}

struct GraphQueryExecutor {
    max_concurrent_queries: usize,
    timeout: Duration,
    max_result_size: usize,
}

struct GraphIndexManager {
    indices: RwLock<HashMap<String, GraphIndex>>,
}

struct GraphIndex {
    id: String,
    index_type: GraphIndexType,
    element_type: ElementType,
    label: String,
    properties: Vec<String>,
    unique: bool,
    created_at: DateTime<Utc>,
    stats: IndexStatistics,
}

enum GraphIndexType {
    BTree,
    Hash,
    Fulltext,
    Spatial,
    Custom,
}

enum ElementType {
    Vertex,
    Edge,
}

struct IndexStatistics {
    size: u64,
    entries: u64,
    lookups: u64,
    avg_lookup_time: Duration,
}

struct GraphPartitionManager {
    partitions: RwLock<HashMap<String, GraphPartition>>,
    partitioning_strategy: PartitioningStrategy,
}

struct GraphPartition {
    id: String,
    node_id: String,
    vertex_count: u64,
    edge_count: u64,
    size_bytes: u64,
    status: PartitionStatus,
}

enum PartitioningStrategy {
    Hash,
    Range,
    Consistent,
    EdgeCut,
    VertexCut,
    Metis,
    Custom,
}

struct GraphTransactionManager {
    active_transactions: RwLock<HashMap<String, GraphTransaction>>,
    isolation_level: IsolationLevel,
}

struct GraphTransaction {
    id: String,
    operations: Vec<GraphOperation>,
    status: TransactionStatus,
    started_at: DateTime<Utc>,
    timeout: Duration,
}

enum GraphOperation {
    AddVertex(Vertex),
    AddEdge(Edge),
    UpdateVertex { id: String, properties: HashMap<String, PropertyValue> },
    UpdateEdge { id: String, properties: HashMap<String, PropertyValue> },
    RemoveVertex(String),
    RemoveEdge(String),
    AddProperty(Property),
}

struct GraphQuery {
    query_text: String,
    language: GraphQueryLanguage,
    parameters: HashMap<String, PropertyValue>,
    timeout: Option<Duration>,
    max_results: Option<usize>,
}

struct GraphQueryResult {
    vertices: Vec<Vertex>,
    edges: Vec<Edge>,
    paths: Vec<GraphPath>,
    values: Vec<PropertyValue>,
    execution_time: Duration,
}

struct GraphPath {
    vertices: Vec<String>,
    edges: Vec<String>,
}

impl DistributedGraphDB {
    fn new(node_id: &str) -> Self {
        let storage_engine = GraphStorageEngine {
            storage_type: GraphStorageType::Memory,
            vertices: RwLock::new(HashMap::new()),
            edges: RwLock::new(HashMap::new()),
            properties: RwLock::new(HashMap::new()),
        };

        let parser = GraphQueryParser {
            supported_languages: vec![
                GraphQueryLanguage::Gremlin,
                GraphQueryLanguage::Cypher,
            ],
            max_query_length: 10000,
        };

        let statistics = GraphStatistics {
            vertex_count: 0,
            edge_count: 0,
            label_statistics: HashMap::new(),
            property_statistics: HashMap::new(),
        };

        let optimizer = GraphQueryOptimizer {
            optimization_rules: vec![
                OptimizationRule::IndexUsage,
                OptimizationRule::PatternRewriting,
                OptimizationRule::JoinOrdering,
            ],
            statistics,
        };

        let executor = GraphQueryExecutor {
            max_concurrent_queries: 10,
            timeout: Duration::from_secs(60),
            max_result_size: 10000,
        };

        let query_processor = GraphQueryProcessor {
            parser,
            optimizer,
            executor,
        };

        let index_manager = GraphIndexManager {
            indices: RwLock::new(HashMap::new()),
        };

        let partition_manager = GraphPartitionManager {
            partitions: RwLock::new(HashMap::new()),
            partitioning_strategy: PartitioningStrategy::Hash,
        };

        let transaction_manager = GraphTransactionManager {
            active_transactions: RwLock::new(HashMap::new()),
            isolation_level: IsolationLevel::ReadCommitted,
        };

        DistributedGraphDB {
            node_id: node_id.to_string(),
            storage_engine,
            query_processor,
            index_manager,
            partition_manager,
            transaction_manager,
        }
    }

    fn add_vertex(&self, label: &str, properties: HashMap<String, PropertyValue>) -> Result<String, String> {
        println!("æ·»åŠ é¡¶ç‚¹: {}", label);

        // ç”Ÿæˆé¡¶ç‚¹ID
        let vertex_id = uuid::Uuid::new_v4().to_string();

        // åˆ†é…åˆ†åŒº
        let partition_id = self.assign_partition_for_vertex(&vertex_id, label, &properties)?;

        let now = Utc::now();

        // åˆ›å»ºé¡¶ç‚¹
        let vertex = Vertex {
            id: vertex_id.clone(),
            label: label.to_string(),
            properties,
            in_edges: Vec::new(),
            out_edges: Vec::new(),
            partition_id,
            created_at: now,
            updated_at: now,
        };

        // å­˜å‚¨é¡¶ç‚¹
        let mut vertices = self.storage_engine.vertices.write().unwrap();
        vertices.insert(vertex_id.clone(), vertex);

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å„ç§ç»Ÿè®¡ä¿¡æ¯

        Ok(vertex_id)
    }

    fn assign_partition_for_vertex(&self, vertex_id: &str, label: &str, properties: &HashMap<String, PropertyValue>) -> Result<String, String> {
        // æ ¹æ®åˆ†åŒºç­–ç•¥åˆ†é…åˆ†åŒº
        match self.partition_manager.partitioning_strategy {
            PartitioningStrategy::Hash => {
                // ä½¿ç”¨é¡¶ç‚¹IDçš„å“ˆå¸Œå€¼åˆ†é…åˆ†åŒº
                let mut hasher = DefaultHasher::new();
                vertex_id.hash(&mut hasher);
                let hash = hasher.finish();

                // å‡è®¾æœ‰Nä¸ªåˆ†åŒºï¼Œé€‰æ‹©ä¸€ä¸ª
                let partitions = self.partition_manager.partitions.read().unwrap();
                let partition_count = partitions.len().max(1);
                let partition_index = (hash % partition_count as u64) as usize;

                let partition_id = partitions.keys().nth(partition_index)
                    .cloned()
                    .unwrap_or_else(|| "default".to_string());

                Ok(partition_id)
            },
            PartitioningStrategy::Range => {
                // å‡è®¾æ ¹æ®æŸä¸ªå±æ€§å€¼çš„èŒƒå›´åˆ†é…åˆ†åŒº
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
                Ok("default".to_string())
            },
            _ => {
                // ç®€åŒ–ï¼šå§‹ç»ˆè¿”å›é»˜è®¤åˆ†åŒº
                Ok("default".to_string())
            }
        }
    }

    fn add_edge(&self, from_vertex: &str, to_vertex: &str, label: &str, properties: HashMap<String, PropertyValue>) -> Result<String, String> {
        println!("æ·»åŠ è¾¹: {} -> {}, æ ‡ç­¾: {}", from_vertex, to_vertex, label);

        // æ£€æŸ¥é¡¶ç‚¹æ˜¯å¦å­˜åœ¨
        let vertices = self.storage_engine.vertices.read().unwrap();

        if !vertices.contains_key(from_vertex) {
            return Err(format!("æºé¡¶ç‚¹ä¸å­˜åœ¨: {}", from_vertex));
        }

        if !vertices.contains_key(to_vertex) {
            return Err(format!("ç›®æ ‡é¡¶ç‚¹ä¸å­˜åœ¨: {}", to_vertex));
        }

        // ç”Ÿæˆè¾¹ID
        let edge_id = uuid::Uuid::new_v4().to_string();

        // åˆ†é…åˆ†åŒº
        let partition_id = self.assign_partition_for_edge(from_vertex, to_vertex, label, &properties)?;

        let now = Utc::now();

        // åˆ›å»ºè¾¹
        let edge = Edge {
            id: edge_id.clone(),
            label: label.to_string(),
            from_vertex: from_vertex.to_string(),
            to_vertex: to_vertex.to_string(),
            properties,
            partition_id,
            created_at: now,
            updated_at: now,
        };

        // å­˜å‚¨è¾¹
        let mut edges = self.storage_engine.edges.write().unwrap();
        edges.insert(edge_id.clone(), edge);

        // æ›´æ–°é¡¶ç‚¹çš„è¾¹å¼•ç”¨
        drop(vertices);
        let mut vertices = self.storage_engine.vertices.write().unwrap();

        if let Some(from) = vertices.get_mut(from_vertex) {
            from.out_edges.push(edge_id.clone());
            from.updated_at = now;
        }

        if let Some(to) = vertices.get_mut(to_vertex) {
            to.in_edges.push(edge_id.clone());
            to.updated_at = now;
        }

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å„ç§ç»Ÿè®¡ä¿¡æ¯

        Ok(edge_id)
    }

    fn assign_partition_for_edge(&self, from_vertex: &str, to_vertex: &str, label: &str, properties: &HashMap<String, PropertyValue>) -> Result<String, String> {
        // æ ¹æ®åˆ†åŒºç­–ç•¥åˆ†é…åˆ†åŒº
        match self.partition_manager.partitioning_strategy {
            PartitioningStrategy::EdgeCut => {
                // ä½¿ç”¨æºé¡¶ç‚¹çš„åˆ†åŒº
                let vertices = self.storage_engine.vertices.read().unwrap();
                if let Some(from) = vertices.get(from_vertex) {
                    return Ok(from.partition_id.clone());
                }

                Ok("default".to_string())
            },
            PartitioningStrategy::VertexCut => {
                // åœ¨è¾¹åˆ‡åˆ†ç­–ç•¥ä¸­ï¼Œè¾¹å¯ä»¥è·¨åˆ†åŒº
                // è¿™é‡Œç®€åŒ–ï¼Œä½¿ç”¨è¾¹IDçš„å“ˆå¸Œå€¼åˆ†é…åˆ†åŒº
                let edge_id = format!("{}_{}", from_vertex, to_vertex);
                let mut hasher = DefaultHasher::new();
                edge_id.hash(&mut hasher);
                let hash = hasher.finish();

                let partitions = self.partition_manager.partitions.read().unwrap();
                let partition_count = partitions.len().max(1);
                let partition_index = (hash % partition_count as u64) as usize;

                let partition_id = partitions.keys().nth(partition_index)
                    .cloned()
                    .unwrap_or_else(|| "default".to_string());

                Ok(partition_id)
            },
            _ => {
                // ç®€åŒ–ï¼šå§‹ç»ˆè¿”å›é»˜è®¤åˆ†åŒº
                Ok("default".to_string())
            }
        }
    }

    fn get_vertex(&self, vertex_id: &str) -> Result<Vertex, String> {
        let vertices = self.storage_engine.vertices.read().unwrap();

        vertices.get(vertex_id)
            .cloned()
            .ok_or_else(|| format!("é¡¶ç‚¹ä¸å­˜åœ¨: {}", vertex_id))
    }

    fn get_edge(&self, edge_id: &str) -> Result<Edge, String> {
        let edges = self.storage_engine.edges.read().unwrap();

        edges.get(edge_id)
            .cloned()
            .ok_or_else(|| format!("è¾¹ä¸å­˜åœ¨: {}", edge_id))
    }

    fn query(&self, query: GraphQuery) -> Result<GraphQueryResult, String> {
        println!("æ‰§è¡Œå›¾æŸ¥è¯¢: {}", query.query_text);

        // è§£ææŸ¥è¯¢
        let parsed_query = self.query_processor.parser.parse(&query)?;

        // ä¼˜åŒ–æŸ¥è¯¢
        let optimized_query = self.query_processor.optimizer.optimize(parsed_query)?;

        // æ‰§è¡ŒæŸ¥è¯¢
        let result = self.query_processor.executor.execute(optimized_query, query.parameters)?;

        Ok(result)
    }

    fn create_index(&self, index: GraphIndex) -> Result<(), String> {
        println!("åˆ›å»ºç´¢å¼•: {}", index.id);

        let mut indices = self.index_manager.indices.write().unwrap();

        if indices.contains_key(&index.id) {
            return Err(format!("ç´¢å¼•å·²å­˜åœ¨: {}", index.id));
        }

        // éªŒè¯ç´¢å¼•é…ç½®
        self.validate_index(&index)?;

        // æ„å»ºç´¢å¼•
        self.build_index(&index)?;

        // å­˜å‚¨ç´¢å¼•
        indices.insert(index.id.clone(), index);

        Ok(())
    }

    fn validate_index(&self, index: &GraphIndex) -> Result<(), String> {
        // æ£€æŸ¥ç´¢å¼•ç±»å‹
        match index.index_type {
            GraphIndexType::Fulltext => {
                // éªŒè¯å…¨æ–‡ç´¢å¼•åªèƒ½åº”ç”¨äºå­—ç¬¦ä¸²å±æ€§
                for prop_name in &index.properties {
                    let properties = self.storage_engine.properties.read().unwrap();
                    if let Some(prop) = properties.get(prop_name) {
                        if !matches!(prop.value_type, PropertyType::String) {
                            return Err(format!("å…¨æ–‡ç´¢å¼•åªèƒ½åº”ç”¨äºå­—ç¬¦ä¸²å±æ€§ï¼Œä½† {} çš„ç±»å‹æ˜¯ {:?}", prop_name, prop.value_type));
                        }
                    } else {
                        return Err(format!("å±æ€§ä¸å­˜åœ¨: {}", prop_name));
                    }
                }
            },
            GraphIndexType::Spatial => {
                // éªŒè¯ç©ºé—´ç´¢å¼•éœ€è¦åˆé€‚çš„å±æ€§ç±»å‹
                // ...çœç•¥å®ç°
            },
            _ => {
                // å…¶ä»–ç´¢å¼•ç±»å‹çš„éªŒè¯
                // ...çœç•¥å®ç°
            }
        }

        Ok(())
    }

    fn build_index(&self, index: &GraphIndex) -> Result<(), String> {
        println!("æ„å»ºç´¢å¼•: {}", index.id);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰«æå›¾æ•°æ®ï¼Œæ„å»ºç´¢å¼•
        // ç®€åŒ–ï¼šå‡è®¾ç´¢å¼•å·²æˆåŠŸæ„å»º

        Ok(())
    }

    fn begin_transaction(&self) -> Result<String, String> {
        let tx_id = uuid::Uuid::new_v4().to_string();

        println!("å¼€å§‹äº‹åŠ¡: {}", tx_id);

        let transaction = GraphTransaction {
            id: tx_id.clone(),
            operations: Vec::new(),
            status: TransactionStatus::Active,
            started_at: Utc::now(),
            timeout: Duration::from_secs(60),
        };

        let mut transactions = self.transaction_manager.active_transactions.write().unwrap();
        transactions.insert(tx_id.clone(), transaction);

        Ok(tx_id)
    }

    fn commit_transaction(&self, tx_id: &str) -> Result<(), String> {
        println!("æäº¤äº‹åŠ¡: {}", tx_id);

        let mut transactions = self.transaction_manager.active_transactions.write().unwrap();

        let transaction = transactions.get_mut(tx_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸å¤„äºæ´»åŠ¨çŠ¶æ€: {}", tx_id));
        }

        // æ‰§è¡Œäº‹åŠ¡ä¸­çš„æ‰€æœ‰æ“ä½œ
        for operation in &transaction.operations {
            self.execute_operation(operation)?;
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.status = TransactionStatus::Committed;

        // åˆ é™¤äº‹åŠ¡
        transactions.remove(tx_id);

        Ok(())
    }

    fn rollback_transaction(&self, tx_id: &str) -> Result<(), String> {
        println!("å›æ»šäº‹åŠ¡: {}", tx_id);

        let mut transactions = self.transaction_manager.active_transactions.write().unwrap();

        if !transactions.contains_key(tx_id) {
            return Err(format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id));
        }

        // åˆ é™¤äº‹åŠ¡ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
        transactions.remove(tx_id);

        Ok(())
    }

    fn execute_operation(&self, operation: &GraphOperation) -> Result<(), String> {
        match operation {
            GraphOperation::AddVertex(vertex) => {
                let mut vertices = self.storage_engine.vertices.write().unwrap();
                vertices.insert(vertex.id.clone(), vertex.clone());
            },
            GraphOperation::AddEdge(edge) => {
                let mut edges = self.storage_engine.edges.write().unwrap();
                edges.insert(edge.id.clone(), edge.clone());

                // æ›´æ–°é¡¶ç‚¹çš„è¾¹å¼•ç”¨
                let mut vertices = self.storage_engine.vertices.write().unwrap();

                if let Some(from) = vertices.get_mut(&edge.from_vertex) {
                    from.out_edges.push(edge.id.clone());
                    from.updated_at = Utc::now();
                }

                if let Some(to) = vertices.get_mut(&edge.to_vertex) {
                    to.in_edges.push(edge.id.clone());
                    to.updated_at = Utc::now();
                }
            },
            GraphOperation::UpdateVertex { id, properties } => {
                let mut vertices = self.storage_engine.vertices.write().unwrap();

                if let Some(vertex) = vertices.get_mut(id) {
                    for (key, value) in properties {
                        vertex.properties.insert(key.clone(), value.clone());
                    }
                    vertex.updated_at = Utc::now();
                }
            },
            GraphOperation::UpdateEdge { id, properties } => {
                let mut edges = self.storage_engine.edges.write().unwrap();

                if let Some(edge) = edges.get_mut(id) {
                    for (key, value) in properties {
                        edge.properties.insert(key.clone(), value.clone());
                    }
                    edge.updated_at = Utc::now();
                }
            },
            GraphOperation::RemoveVertex(vertex_id) => {
                // è·å–é¡¶ç‚¹
                let mut vertices = self.storage_engine.vertices.write().unwrap();
                let vertex = vertices.get(vertex_id).cloned();

                if let Some(vertex) = vertex {
                    // åˆ é™¤æ‰€æœ‰å…³è”çš„è¾¹
                    let mut edges = self.storage_engine.edges.write().unwrap();

                    // åˆ é™¤å…¥è¾¹
                    for edge_id in &vertex.in_edges {
                        if let Some(edge) = edges.get(edge_id) {
                            if let Some(from_vertex) = vertices.get_mut(&edge.from_vertex) {
                                from_vertex.out_edges.retain(|id| id != edge_id);
                            }
                        }
                        edges.remove(edge_id);
                    }

                    // åˆ é™¤å‡ºè¾¹
                    for edge_id in &vertex.out_edges {
                        if let Some(edge) = edges.get(edge_id) {
                            if let Some(to_vertex) = vertices.get_mut(&edge.to_vertex) {
                                to_vertex.in_edges.retain(|id| id != edge_id);
                            }
                        }
                        edges.remove(edge_id);
                    }

                    // åˆ é™¤é¡¶ç‚¹
                    vertices.remove(vertex_id);
                }
            },
            GraphOperation::RemoveEdge(edge_id) => {
                // è·å–è¾¹
                let edges = self.storage_engine.edges.read().unwrap();
                let edge = edges.get(edge_id).cloned();

                if let Some(edge) = edge {
                    // ä»é¡¶ç‚¹ä¸­ç§»é™¤è¾¹å¼•ç”¨
                    let mut vertices = self.storage_engine.vertices.write().unwrap();

                    if let Some(from_vertex) = vertices.get_mut(&edge.from_vertex) {
                        from_vertex.out_edges.retain(|id| id != edge_id);
                    }

                    if let Some(to_vertex) = vertices.get_mut(&edge.to_vertex) {
                        to_vertex.in_edges.retain(|id| id != edge_id);
                    }

                    // åˆ é™¤è¾¹
                    drop(edges);
                    let mut edges = self.storage_engine.edges.write().unwrap();
                    edges.remove(edge_id);
                }
            },
            GraphOperation::AddProperty(property) => {
                let mut properties = self.storage_engine.properties.write().unwrap();
                properties.insert(property.key.clone(), property.clone());
            },
        }

        Ok(())
    }
}

impl GraphQueryParser {
    fn parse(&self, query: &GraphQuery) -> Result<ParsedQuery, String> {
        println!("è§£ææŸ¥è¯¢: {}", query.query_text);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®æŸ¥è¯¢è¯­è¨€è§£ææŸ¥è¯¢
        // ç®€åŒ–ï¼šè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„è§£æç»“æœ

        let parsed_query = ParsedQuery {
            query_type: QueryType::PathQuery,
            start_vertices: vec!["v1".to_string()],
            patterns: vec![
                Pattern::EdgePattern {
                    direction: Direction::Outgoing,
                    edge_label: Some("knows".to_string()),
                    vertex_label: Some("person".to_string()),
                    conditions: Vec::new(),
                },
            ],
            projections: vec!["name".to_string()],
            limit: query.max_results.unwrap_or(100),
        };

        Ok(parsed_query)
    }
}

impl GraphQueryOptimizer {
    fn optimize(&self, query: ParsedQuery) -> Result<OptimizedQuery, String> {
        println!("ä¼˜åŒ–æŸ¥è¯¢");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåº”ç”¨å„ç§ä¼˜åŒ–è§„åˆ™
        // ç®€åŒ–ï¼šè¿”å›ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–æŸ¥è¯¢

        let optimized_query = OptimizedQuery {
            execution_plan: vec![
                ExecutionStep::GetVertices {
                    vertices: query.start_vertices,
                },
                ExecutionStep::ExpandEdges {
                    direction: Direction::Outgoing,
                    edge_label: Some("knows".to_string()),
                },
                ExecutionStep::FilterVertices {
                    label: Some("person".to_string()),
                    conditions: Vec::new(),
                },
                ExecutionStep::Project {
                    properties: query.projections,
                },
                ExecutionStep::Limit {
                    count: query.limit,
                },
            ],
        };

        Ok(optimized_query)
    }
}

impl GraphQueryExecutor {
    fn execute(&self, query: OptimizedQuery, parameters: HashMap<String, PropertyValue>) -> Result<GraphQueryResult, String> {
        println!("æ‰§è¡ŒæŸ¥è¯¢");

        let start_time = Instant::now();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡ŒæŸ¥è¯¢è®¡åˆ’
        // ç®€åŒ–ï¼šè¿”å›ä¸€äº›æ¨¡æ‹Ÿæ•°æ®

        let mut vertices = Vec::new();
        let mut edges = Vec::new();
        let mut values = Vec::new();

        // æ¨¡æ‹Ÿæ‰§è¡Œç»“æœ
        for i in 0..5 {
            let vertex_id = format!("v{}", i + 1);
            let now = Utc::now();

            let mut properties = HashMap::new();
            properties.insert("name".to_string(), PropertyValue::String(format!("Person {}", i + 1)));
            properties.insert("age".to_string(), PropertyValue::Integer(20 + i));

            let vertex = Vertex {
                id: vertex_id.clone(),
                label: "person".to_string(),
                properties,
                in_edges: Vec::new(),
                out_edges: Vec::new(),
                partition_id: "default".to_string(),
                created_at: now,
                updated_at: now,
            };

            vertices.push(vertex);

            // æ·»åŠ ä¸€äº›å€¼ä½œä¸ºç»“æœ
            values.push(PropertyValue::String(format!("Person {}", i + 1)));
        }

        let execution_time = start_time.elapsed();

        let result = GraphQueryResult {
            vertices,
            edges,
            paths: Vec::new(),
            values,
            execution_time,
        };

        Ok(result)
    }
}

struct ParsedQuery {
    query_type: QueryType,
    start_vertices: Vec<String>,
    patterns: Vec<Pattern>,
    projections: Vec<String>,
    limit: usize,
}

enum QueryType {
    VertexQuery,
    EdgeQuery,
    PathQuery,
    AggregationQuery,
}

enum Pattern {
    EdgePattern {
        direction: Direction,
        edge_label: Option<String>,
        vertex_label: Option<String>,
        conditions: Vec<Condition>,
    },
    PathPattern {
        min_length: usize,
        max_length: Option<usize>,
        edge_labels: Option<Vec<String>>,
    },
}

enum Direction {
    Incoming,
    Outgoing,
    Both,
}

struct Condition {
    property: String,
    operator: ConditionOperator,
    value: PropertyValue,
}

enum ConditionOperator {
    Equals,
    NotEquals,
    GreaterThan,
    GreaterThanOrEqual,
    LessThan,
    LessThanOrEqual,
    Contains,
    StartsWith,
    EndsWith,
    In,
    NotIn,
    Regex,
}

struct OptimizedQuery {
    execution_plan: Vec<ExecutionStep>,
}

enum ExecutionStep {
    GetVertices {
        vertices: Vec<String>,
    },
    GetEdges {
        edges: Vec<String>,
    },
    ExpandEdges {
        direction: Direction,
        edge_label: Option<String>,
    },
    FilterVertices {
        label: Option<String>,
        conditions: Vec<Condition>,
    },
    FilterEdges {
        label: Option<String>,
        conditions: Vec<Condition>,
    },
    Project {
        properties: Vec<String>,
    },
    Limit {
        count: usize,
    },
    OrderBy {
        property: String,
        ascending: bool,
    },
    Aggregate {
        group_by: Vec<String>,
        aggregations: Vec<Aggregation>,
    },
}

struct Aggregation {
    function: AggregationFunction,
    property: String,
    alias: String,
}

enum AggregationFunction {
    Count,
    Sum,
    Min,
    Max,
    Avg,
}

// å‘é‡æ•°æ®åº“
struct VectorDatabase {
    node_id: String,
    storage_engine: VectorStorageEngine,
    index_manager: VectorIndexManager,
    query_processor: VectorQueryProcessor,
}

struct VectorStorageEngine {
    collections: RwLock<HashMap<String, VectorCollection>>,
}

struct VectorCollection {
    name: String,
    dimension: usize,
    vectors: HashMap<String, Vector>,
    metadata: HashMap<String, HashMap<String, serde_json::Value>>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct Vector {
    id: String,
    values: Vec<f32>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct VectorIndexManager {
    indices: RwLock<HashMap<String, VectorIndex>>,
}

struct VectorIndex {
    id: String,
    collection_name: String,
    index_type: VectorIndexType,
    parameters: HashMap<String, serde_json::Value>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

enum VectorIndexType {
    Flat,
    HNSW,
    IVF,
    PQ,
    ScaNN,
    Faiss,
    Annoy,
}

struct VectorQueryProcessor {
    max_concurrent_queries: usize,
    timeout: Duration,
}

struct VectorQuery {
    collection_name: String,
    query_vector: Vec<f32>,
    top_k: usize,
    filter: Option<String>,
    include_metadata: bool,
    include_values: bool,
}

struct VectorSearchResult {
    matches: Vec<VectorMatch>,
    execution_time: Duration,
}

struct VectorMatch {
    id: String,
    score: f32,
    values: Option<Vec<f32>>,
    metadata: Option<HashMap<String, serde_json::Value>>,
}

impl VectorDatabase {
    fn new(node_id: &str) -> Self {
        let storage_engine = VectorStorageEngine {
            collections: RwLock::new(HashMap::new()),
        };

        let index_manager = VectorIndexManager {
            indices: RwLock::new(HashMap::new()),
        };

        let query_processor = VectorQueryProcessor {
            max_concurrent_queries: 10,
            timeout: Duration::from_secs(30),
        };

        VectorDatabase {
            node_id: node_id.to_string(),
            storage_engine,
            index_manager,
            query_processor,
        }
    }

    fn create_collection(&self, name: &str, dimension: usize) -> Result<(), String> {
        println!("åˆ›å»ºå‘é‡é›†åˆ: {}", name);

        if dimension == 0 {
            return Err("ç»´åº¦å¿…é¡»å¤§äº0".to_string());
        }

        let mut collections = self.storage_engine.collections.write().unwrap();

        if collections.contains_key(name) {
            return Err(format!("é›†åˆå·²å­˜åœ¨: {}", name));
        }

        let now = Utc::now();

        let collection = VectorCollection {
            name: name.to_string(),
            dimension,
            vectors: HashMap::new(),
            metadata: HashMap::new(),
            created_at: now,
            updated_at: now,
        };

        collections.insert(name.to_string(), collection);

        Ok(())
    }

    fn add_vector(&self, collection_name: &str, id: &str, values: Vec<f32>, metadata: Option<HashMap<String, serde_json::Value>>) -> Result<(), String> {
        println!("æ·»åŠ å‘é‡: {} åˆ°é›†åˆ {}", id, collection_name);

        let mut collections = self.storage_engine.collections.write().unwrap();

        let collection = collections.get_mut(collection_name)
            .ok_or_else(|| format!("é›†åˆä¸å­˜åœ¨: {}", collection_name))?;

        // éªŒè¯å‘é‡ç»´åº¦
        if values.len() != collection.dimension {
            return Err(format!("å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {}, å®é™… {}", collection.dimension, values.len()));
        }

        // æ£€æŸ¥IDæ˜¯å¦å·²å­˜åœ¨
        if collection.vectors.contains_key(id) {
            return Err(format!("å‘é‡IDå·²å­˜åœ¨: {}", id));
        }

        let now = Utc::now();

        // åˆ›å»ºå‘é‡
        let vector = Vector {
            id: id.to_string(),
            values,
            created_at: now,
            updated_at: now,
        };

        // å­˜å‚¨å‘é‡
        collection.vectors.insert(id.to_string(), vector);

        // å­˜å‚¨å…ƒæ•°æ®
        if let Some(metadata) = metadata {
            collection.metadata.insert(id.to_string(), metadata);
        }

        collection.updated_at = now;

        // æ›´æ–°ç´¢å¼•
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å‘é‡ç´¢å¼•

        Ok(())
    }

    fn search(&self, query: VectorQuery) -> Result<VectorSearchResult, String> {
        println!("æœç´¢å‘é‡, é›†åˆ: {}, è¿”å›å‰ {} ä¸ª", query.collection_name, query.top_k);

        let start_time = Instant::now();

        let collections = self.storage_engine.collections.read().unwrap();

        let collection = collections.get(&query.collection_name)
            .ok_or_else(|| format!("é›†åˆä¸å­˜åœ¨: {}", query.collection_name))?;

        // éªŒè¯æŸ¥è¯¢å‘é‡ç»´åº¦
        if query.query_vector.len() != collection.dimension {
            return Err(format!("æŸ¥è¯¢å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {}, å®é™… {}",
                     collection.dimension, query.query_vector.len()));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨å‘é‡ç´¢å¼•è¿›è¡Œå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢
        // ç®€åŒ–ï¼šè®¡ç®—æ‰€æœ‰å‘é‡ä¸æŸ¥è¯¢å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
        let mut matches = Vec::new();

        for (id, vector) in &collection.vectors {
            // è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            let similarity = compute_cosine_similarity(&query.query_vector, &vector.values);

            // åº”ç”¨è¿‡æ»¤å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if let Some(filter) = &query.filter {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æå¹¶åº”ç”¨è¿‡æ»¤æ¡ä»¶
                // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰å‘é‡éƒ½é€šè¿‡è¿‡æ»¤å™¨
            }

            // åˆ›å»ºåŒ¹é…é¡¹
            let mut match_item = VectorMatch {
                id: id.clone(),
                score: similarity,
                values: None,
                metadata: None,
            };

            // åŒ…å«å‘é‡å€¼ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
            if query.include_values {
                match_item.values = Some(vector.

```rust
                match_item.values = Some(vector.values.clone());
            }

            // åŒ…å«å…ƒæ•°æ®ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
            if query.include_metadata {
                if let Some(meta) = collection.metadata.get(id) {
                    match_item.metadata = Some(meta.clone());
                }
            }

            matches.push(match_item);
        }

        // æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆé™åºï¼‰
        matches.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));

        // æˆªå–å‰top_kä¸ªç»“æœ
        if matches.len() > query.top_k {
            matches.truncate(query.top_k);
        }

        let execution_time = start_time.elapsed();

        let result = VectorSearchResult {
            matches,
            execution_time,
        };

        Ok(result)
    }

    fn compute_cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
        if a.len() != b.len() {
            return 0.0;
        }

        let mut dot_product = 0.0;
        let mut norm_a = 0.0;
        let mut norm_b = 0.0;

        for i in 0..a.len() {
            dot_product += a[i] * b[i];
            norm_a += a[i] * a[i];
            norm_b += b[i] * b[i];
        }

        norm_a = norm_a.sqrt();
        norm_b = norm_b.sqrt();

        if norm_a == 0.0 || norm_b == 0.0 {
            return 0.0;
        }

        dot_product / (norm_a * norm_b)
    }

    fn create_index(&self, collection_name: &str, index_type: VectorIndexType, parameters: HashMap<String, serde_json::Value>) -> Result<String, String> {
        println!("ä¸ºé›†åˆ {} åˆ›å»ºå‘é‡ç´¢å¼•", collection_name);

        // éªŒè¯é›†åˆæ˜¯å¦å­˜åœ¨
        let collections = self.storage_engine.collections.read().unwrap();
        if !collections.contains_key(collection_name) {
            return Err(format!("é›†åˆä¸å­˜åœ¨: {}", collection_name));
        }

        // ä¸ºç´¢å¼•ç”ŸæˆID
        let index_id = uuid::Uuid::new_v4().to_string();

        let now = Utc::now();

        // åˆ›å»ºç´¢å¼•
        let index = VectorIndex {
            id: index_id.clone(),
            collection_name: collection_name.to_string(),
            index_type,
            parameters,
            created_at: now,
            updated_at: now,
        };

        // éªŒè¯ç´¢å¼•å‚æ•°
        self.validate_index_parameters(&index)?;

        // æ„å»ºç´¢å¼•
        self.build_index(&index, &collections.get(collection_name).unwrap())?;

        // å­˜å‚¨ç´¢å¼•
        let mut indices = self.index_manager.indices.write().unwrap();
        indices.insert(index_id.clone(), index);

        Ok(index_id)
    }

    fn validate_index_parameters(&self, index: &VectorIndex) -> Result<(), String> {
        match index.index_type {
            VectorIndexType::HNSW => {
                // éªŒè¯HNSWç´¢å¼•å‚æ•°
                if !index.parameters.contains_key("M") {
                    return Err("HNSWç´¢å¼•éœ€è¦å‚æ•° 'M'".to_string());
                }
                if !index.parameters.contains_key("ef_construction") {
                    return Err("HNSWç´¢å¼•éœ€è¦å‚æ•° 'ef_construction'".to_string());
                }
            },
            VectorIndexType::IVF => {
                // éªŒè¯IVFç´¢å¼•å‚æ•°
                if !index.parameters.contains_key("nlist") {
                    return Err("IVFç´¢å¼•éœ€è¦å‚æ•° 'nlist'".to_string());
                }
            },
            VectorIndexType::PQ => {
                // éªŒè¯PQç´¢å¼•å‚æ•°
                if !index.parameters.contains_key("m") {
                    return Err("PQç´¢å¼•éœ€è¦å‚æ•° 'm'".to_string());
                }
                if !index.parameters.contains_key("nbits") {
                    return Err("PQç´¢å¼•éœ€è¦å‚æ•° 'nbits'".to_string());
                }
            },
            _ => {
                // å…¶ä»–ç´¢å¼•ç±»å‹çš„éªŒè¯
            }
        }

        Ok(())
    }

    fn build_index(&self, index: &VectorIndex, collection: &VectorCollection) -> Result<(), String> {
        println!("æ„å»ºç´¢å¼•: {}", index.id);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ„å»ºå‘é‡ç´¢å¼•
        // ä¸åŒç±»å‹çš„ç´¢å¼•éœ€è¦ä¸åŒçš„æ„å»ºé€»è¾‘
        match index.index_type {
            VectorIndexType::Flat => {
                // å¹³é¢ç´¢å¼•ä¸éœ€è¦ç‰¹åˆ«æ„å»º
                println!("æ„å»ºå¹³é¢ç´¢å¼•");
            },
            VectorIndexType::HNSW => {
                println!("æ„å»ºHNSWç´¢å¼•");
                // ä»å‚æ•°ä¸­è·å–Må’Œef_construction
                let m = index.parameters.get("M")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(16) as usize;
                let ef_construction = index.parameters.get("ef_construction")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(200) as usize;

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ„å»ºHNSWç´¢å¼•
            },
            VectorIndexType::IVF => {
                println!("æ„å»ºIVFç´¢å¼•");
                // ä»å‚æ•°ä¸­è·å–nlist
                let nlist = index.parameters.get("nlist")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(100) as usize;

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ„å»ºIVFç´¢å¼•
            },
            VectorIndexType::PQ => {
                println!("æ„å»ºPQç´¢å¼•");
                // ä»å‚æ•°ä¸­è·å–må’Œnbits
                let m = index.parameters.get("m")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(8) as usize;
                let nbits = index.parameters.get("nbits")
                    .and_then(|v| v.as_u64())
                    .unwrap_or(8) as usize;

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ„å»ºPQç´¢å¼•
            },
            _ => {
                return Err(format!("ä¸æ”¯æŒçš„ç´¢å¼•ç±»å‹: {:?}", index.index_type));
            }
        }

        // ç®€åŒ–ï¼šå‡è®¾ç´¢å¼•å·²æˆåŠŸæ„å»º

        Ok(())
    }

    fn delete_vector(&self, collection_name: &str, id: &str) -> Result<(), String> {
        println!("ä»é›†åˆ {} ä¸­åˆ é™¤å‘é‡: {}", collection_name, id);

        let mut collections = self.storage_engine.collections.write().unwrap();

        let collection = collections.get_mut(collection_name)
            .ok_or_else(|| format!("é›†åˆä¸å­˜åœ¨: {}", collection_name))?;

        // åˆ é™¤å‘é‡
        if !collection.vectors.contains_key(id) {
            return Err(format!("å‘é‡ä¸å­˜åœ¨: {}", id));
        }

        collection.vectors.remove(id);

        // åˆ é™¤å…ƒæ•°æ®
        collection.metadata.remove(id);

        // æ›´æ–°é›†åˆæ—¶é—´æˆ³
        collection.updated_at = Utc::now();

        // æ›´æ–°ç´¢å¼•
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å‘é‡ç´¢å¼•

        Ok(())
    }
}

// åˆ†å¸ƒå¼åˆ†æå¼•æ“
struct DistributedAnalyticsEngine {
    node_id: String,
    query_engine: AnalyticsQueryEngine,
    execution_engine: AnalyticsExecutionEngine,
    storage_manager: AnalyticsStorageManager,
    scheduler: AnalyticsScheduler,
    monitor: AnalyticsMonitor,
}

struct AnalyticsQueryEngine {
    parsers: HashMap<String, Box<dyn AnalyticsQueryParser>>,
    optimizer: AnalyticsQueryOptimizer,
}

trait AnalyticsQueryParser: Send + Sync {
    fn parse(&self, query: &str) -> Result<AnalyticsQueryPlan, String>;
}

struct AnalyticsQueryOptimizer {
    optimization_rules: Vec<Box<dyn OptimizationRule>>,
    statistics: AnalyticsStatistics,
}

struct AnalyticsStatistics {
    table_statistics: HashMap<String, TableStatistics>,
    column_statistics: HashMap<String, HashMap<String, ColumnStatistics>>,
}

struct AnalyticsExecutionEngine {
    executors: HashMap<String, Box<dyn QueryExecutor>>,
    max_memory: usize,
    max_concurrent_queries: usize,
}

trait QueryExecutor: Send + Sync {
    fn execute(&self, plan: &AnalyticsQueryPlan) -> Result<QueryResult, String>;
}

struct AnalyticsStorageManager {
    storage_adapters: HashMap<String, Box<dyn StorageAdapter>>,
    cache_manager: CacheManager,
}

trait StorageAdapter: Send + Sync {
    fn read_data(&self, location: &str, schema: &Schema, filters: &[Filter]) -> Result<DataBatch, String>;
    fn write_data(&self, location: &str, data: &DataBatch) -> Result<(), String>;
    fn get_metadata(&self, location: &str) -> Result<Metadata, String>;
}

struct CacheManager {
    cache_size: usize,
    cache_policy: CachePolicy,
    cached_data: RwLock<HashMap<String, CachedData>>,
}

struct AnalyticsScheduler {
    max_workers: usize,
    scheduling_policy: SchedulingPolicy,
    priority_queue: PriorityQueue<AnalyticsJob>,
}

struct AnalyticsJob {
    id: String,
    query: String,
    priority: u32,
    status: JobStatus,
    created_at: DateTime<Utc>,
    started_at: Option<DateTime<Utc>>,
    completed_at: Option<DateTime<Utc>>,
}

struct AnalyticsMonitor {
    metrics: Metrics,
    alerts: Vec<Alert>,
}

struct Metrics {
    query_count: u64,
    query_errors: u64,
    avg_query_time: Duration,
    memory_usage: usize,
    disk_usage: usize,
    cache_hit_ratio: f64,
}

struct Alert {
    name: String,
    condition: String,
    actions: Vec<AlertAction>,
    triggered: bool,
    last_triggered: Option<DateTime<Utc>>,
}

enum AlertAction {
    Email(String),
    Webhook(String),
    Log,
}

struct AnalyticsQueryPlan {
    operations: Vec<QueryOperation>,
    output_schema: Schema,
    estimated_cost: QueryCost,
}

enum QueryOperation {
    Scan {
        source: String,
        schema: Schema,
        filters: Vec<Filter>,
        projection: Vec<String>,
    },
    Join {
        left: Box<QueryOperation>,
        right: Box<QueryOperation>,
        join_type: JoinType,
        condition: JoinCondition,
    },
    Aggregate {
        input: Box<QueryOperation>,
        group_by: Vec<String>,
        aggregations: Vec<Aggregation>,
    },
    Sort {
        input: Box<QueryOperation>,
        sort_exprs: Vec<SortExpr>,
    },
    Limit {
        input: Box<QueryOperation>,
        limit: usize,
        offset: usize,
    },
    Union {
        left: Box<QueryOperation>,
        right: Box<QueryOperation>,
        union_type: UnionType,
    },
}

enum JoinType {
    Inner,
    Left,
    Right,
    Full,
    Semi,
    Anti,
}

struct JoinCondition {
    left_cols: Vec<String>,
    right_cols: Vec<String>,
}

struct SortExpr {
    expr: String,
    ascending: bool,
}

enum UnionType {
    All,
    Distinct,
}

struct QueryCost {
    cpu_cost: f64,
    memory_cost: f64,
    io_cost: f64,
    network_cost: f64,
}

struct Schema {
    fields: Vec<Field>,
}

struct Field {
    name: String,
    data_type: DataType,
    nullable: bool,
}

struct DataBatch {
    schema: Schema,
    columns: Vec<Column>,
    row_count: usize,
}

struct Column {
    name: String,
    data: ColumnData,
}

enum ColumnData {
    Int32(Vec<Option<i32>>),
    Int64(Vec<Option<i64>>),
    Float32(Vec<Option<f32>>),
    Float64(Vec<Option<f64>>),
    Boolean(Vec<Option<bool>>),
    String(Vec<Option<String>>),
    Date(Vec<Option<NaiveDate>>),
    Timestamp(Vec<Option<DateTime<Utc>>>),
}

struct Metadata {
    schema: Schema,
    statistics: HashMap<String, ColumnStatistics>,
    partitioning: Option<PartitioningInfo>,
    format: String,
    compression: Option<String>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct PartitioningInfo {
    columns: Vec<String>,
    partitions: Vec<Partition>,
}

struct Partition {
    id: String,
    values: HashMap<String, String>,
    location: String,
    row_count: u64,
    size_bytes: u64,
}

struct QueryResult {
    schema: Schema,
    data: DataBatch,
    execution_time: Duration,
    execution_stats: ExecutionStats,
}

struct ExecutionStats {
    bytes_scanned: u64,
    bytes_returned: u64,
    rows_scanned: u64,
    rows_returned: u64,
    execution_stages: Vec<StageStats>,
}

struct StageStats {
    stage_name: String,
    start_time: DateTime<Utc>,
    end_time: DateTime<Utc>,
    input_rows: u64,
    output_rows: u64,
    input_bytes: u64,
    output_bytes: u64,
    metrics: HashMap<String, f64>,
}

impl DistributedAnalyticsEngine {
    fn new(node_id: &str) -> Self {
        let query_engine = AnalyticsQueryEngine {
            parsers: HashMap::new(),
            optimizer: AnalyticsQueryOptimizer {
                optimization_rules: Vec::new(),
                statistics: AnalyticsStatistics {
                    table_statistics: HashMap::new(),
                    column_statistics: HashMap::new(),
                },
            },
        };

        let execution_engine = AnalyticsExecutionEngine {
            executors: HashMap::new(),
            max_memory: 1024 * 1024 * 1024, // 1GB
            max_concurrent_queries: 10,
        };

        let storage_manager = AnalyticsStorageManager {
            storage_adapters: HashMap::new(),
            cache_manager: CacheManager {
                cache_size: 1024 * 1024 * 1024, // 1GB
                cache_policy: CachePolicy::LRU,
                cached_data: RwLock::new(HashMap::new()),
            },
        };

        let scheduler = AnalyticsScheduler {
            max_workers: 10,
            scheduling_policy: SchedulingPolicy::FIFO,
            priority_queue: PriorityQueue::new(),
        };

        let monitor = AnalyticsMonitor {
            metrics: Metrics {
                query_count: 0,
                query_errors: 0,
                avg_query_time: Duration::from_secs(0),
                memory_usage: 0,
                disk_usage: 0,
                cache_hit_ratio: 0.0,
            },
            alerts: Vec::new(),
        };

        DistributedAnalyticsEngine {
            node_id: node_id.to_string(),
            query_engine,
            execution_engine,
            storage_manager,
            scheduler,
            monitor,
        }
    }

    fn submit_query(&self, query: &str, priority: u32) -> Result<String, String> {
        println!("æäº¤æŸ¥è¯¢: {}", query);

        // ç”Ÿæˆä½œä¸šID
        let job_id = uuid::Uuid::new_v4().to_string();

        // åˆ›å»ºåˆ†æä½œä¸š
        let job = AnalyticsJob {
            id: job_id.clone(),
            query: query.to_string(),
            priority,
            status: JobStatus::Queued,
            created_at: Utc::now(),
            started_at: None,
            completed_at: None,
        };

        // æ·»åŠ åˆ°è°ƒåº¦å™¨é˜Ÿåˆ—
        self.scheduler.priority_queue.push(job, priority);

        // å°è¯•è°ƒåº¦ä½œä¸š
        self.schedule_next_job()?;

        Ok(job_id)
    }

    fn schedule_next_job(&self) -> Result<bool, String> {
        // æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å·¥ä½œçº¿ç¨‹
        if self.scheduler.priority_queue.is_empty() {
            return Ok(false);
        }

        // è·å–ä¸‹ä¸€ä¸ªä½œä¸š
        if let Some((mut job, _)) = self.scheduler.priority_queue.pop() {
            println!("è°ƒåº¦ä½œä¸š: {}", job.id);

            // æ›´æ–°ä½œä¸šçŠ¶æ€
            job.status = JobStatus::Running;
            job.started_at = Some(Utc::now());

            // è§£ææŸ¥è¯¢
            let parser = self.select_parser(&job.query)?;
            let plan = parser.parse(&job.query)?;

            // ä¼˜åŒ–æŸ¥è¯¢
            let optimized_plan = self.query_engine.optimizer.optimize(&plan)?;

            // æ‰§è¡ŒæŸ¥è¯¢
            let executor = self.select_executor(&optimized_plan)?;
            let result = executor.execute(&optimized_plan)?;

            // æ›´æ–°ä½œä¸šçŠ¶æ€
            job.status = JobStatus::Completed;
            job.completed_at = Some(Utc::now());

            // æ›´æ–°ç›‘æ§æŒ‡æ ‡
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°å„ç§ç»Ÿè®¡ä¿¡æ¯

            return Ok(true);
        }

        Ok(false)
    }

    fn select_parser(&self, query: &str) -> Result<&Box<dyn AnalyticsQueryParser>, String> {
        // æ ¹æ®æŸ¥è¯¢è¯­æ³•é€‰æ‹©åˆé€‚çš„è§£æå™¨
        // ç®€åŒ–ï¼šå‡è®¾åªæœ‰SQLè§£æå™¨
        self.query_engine.parsers.get("SQL")
            .ok_or_else(|| "æ²¡æœ‰å¯ç”¨çš„æŸ¥è¯¢è§£æå™¨".to_string())
    }

    fn select_executor(&self, plan: &AnalyticsQueryPlan) -> Result<&Box<dyn QueryExecutor>, String> {
        // æ ¹æ®æŸ¥è¯¢è®¡åˆ’é€‰æ‹©åˆé€‚çš„æ‰§è¡Œå™¨
        // ç®€åŒ–ï¼šå‡è®¾åªæœ‰ä¸€ä¸ªé€šç”¨æ‰§è¡Œå™¨
        self.execution_engine.executors.get("general")
            .ok_or_else(|| "æ²¡æœ‰å¯ç”¨çš„æŸ¥è¯¢æ‰§è¡Œå™¨".to_string())
    }
}

impl AnalyticsQueryOptimizer {
    fn optimize(&self, plan: &AnalyticsQueryPlan) -> Result<AnalyticsQueryPlan, String> {
        println!("ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’");

        let mut optimized_plan = plan.clone();

        // åº”ç”¨ä¼˜åŒ–è§„åˆ™
        for rule in &self.optimization_rules {
            rule.optimize(&mut optimized_plan)?;
        }

        Ok(optimized_plan)
    }
}

struct PriorityQueue<T> {
    items: Vec<(T, u32)>,
}

impl<T> PriorityQueue<T> {
    fn new() -> Self {
        PriorityQueue {
            items: Vec::new(),
        }
    }

    fn push(&mut self, item: T, priority: u32) {
        self.items.push((item, priority));
        self.items.sort_by(|a, b| b.1.cmp(&a.1)); // æŒ‰ä¼˜å…ˆçº§é™åºæ’åº
    }

    fn pop(&mut self) -> Option<(T, u32)> {
        self.items.pop()
    }

    fn is_empty(&self) -> bool {
        self.items.is_empty()
    }
}

impl Clone for AnalyticsQueryPlan {
    fn clone(&self) -> Self {
        AnalyticsQueryPlan {
            operations: self.operations.clone(),
            output_schema: self.output_schema.clone(),
            estimated_cost: self.estimated_cost.clone(),
        }
    }
}

impl Clone for QueryOperation {
    fn clone(&self) -> Self {
        match self {
            QueryOperation::Scan { source, schema, filters, projection } => {
                QueryOperation::Scan {
                    source: source.clone(),
                    schema: schema.clone(),
                    filters: filters.clone(),
                    projection: projection.clone(),
                }
            },
            QueryOperation::Join { left, right, join_type, condition } => {
                QueryOperation::Join {
                    left: Box::new((*left).clone()),
                    right: Box::new((*right).clone()),
                    join_type: join_type.clone(),
                    condition: condition.clone(),
                }
            },
            QueryOperation::Aggregate { input, group_by, aggregations } => {
                QueryOperation::Aggregate {
                    input: Box::new((*input).clone()),
                    group_by: group_by.clone(),
                    aggregations: aggregations.clone(),
                }
            },
            QueryOperation::Sort { input, sort_exprs } => {
                QueryOperation::Sort {
                    input: Box::new((*input).clone()),
                    sort_exprs: sort_exprs.clone(),
                }
            },
            QueryOperation::Limit { input, limit, offset } => {
                QueryOperation::Limit {
                    input: Box::new((*input).clone()),
                    limit: *limit,
                    offset: *offset,
                }
            },
            QueryOperation::Union { left, right, union_type } => {
                QueryOperation::Union {
                    left: Box::new((*left).clone()),
                    right: Box::new((*right).clone()),
                    union_type: union_type.clone(),
                }
            },
        }
    }
}

impl Clone for JoinType {
    fn clone(&self) -> Self {
        match self {
            JoinType::Inner => JoinType::Inner,
            JoinType::Left => JoinType::Left,
            JoinType::Right => JoinType::Right,
            JoinType::Full => JoinType::Full,
            JoinType::Semi => JoinType::Semi,
            JoinType::Anti => JoinType::Anti,
        }
    }
}

impl Clone for JoinCondition {
    fn clone(&self) -> Self {
        JoinCondition {
            left_cols: self.left_cols.clone(),
            right_cols: self.right_cols.clone(),
        }
    }
}

impl Clone for SortExpr {
    fn clone(&self) -> Self {
        SortExpr {
            expr: self.expr.clone(),
            ascending: self.ascending,
        }
    }
}

impl Clone for UnionType {
    fn clone(&self) -> Self {
        match self {
            UnionType::All => UnionType::All,
            UnionType::Distinct => UnionType::Distinct,
        }
    }
}

impl Clone for QueryCost {
    fn clone(&self) -> Self {
        QueryCost {
            cpu_cost: self.cpu_cost,
            memory_cost: self.memory_cost,
            io_cost: self.io_cost,
            network_cost: self.network_cost,
        }
    }
}

impl Clone for Schema {
    fn clone(&self) -> Self {
        Schema {
            fields: self.fields.clone(),
        }
    }
}

impl Clone for Field {
    fn clone(&self) -> Self {
        Field {
            name: self.name.clone(),
            data_type: self.data_type.clone(),
            nullable: self.nullable,
        }
    }
}

impl Clone for DataType {
    fn clone(&self) -> Self {
        match self {
            DataType::Integer => DataType::Integer,
            DataType::Float => DataType::Float,
            DataType::Text => DataType::Text,
            DataType::Boolean => DataType::Boolean,
            DataType::Date => DataType::Date,
            DataType::Timestamp => DataType::Timestamp,
            DataType::Geometry(geo_type) => DataType::Geometry(geo_type.clone()),
        }
    }
}

// ä¸»å‡½æ•°
fn main() {
    println!("åˆ†å¸ƒå¼ç³»ç»Ÿç¤ºä¾‹ä»£ç ");
}
```

### 1.9 ç»¼åˆåº”ç”¨09-åˆ†å¸ƒå¼é”æœåŠ¡

```rust
// åˆ†å¸ƒå¼é”æœåŠ¡
struct DistributedLockService {
    node_id: String,
    lock_manager: LockManager,
    watch_manager: WatchManager,
    session_manager: SessionManager,
    storage_provider: Box<dyn LockStorageProvider>,
    heartbeat_manager: HeartbeatManager,
}

struct LockManager {
    locks: RwLock<HashMap<String, LockEntry>>,
}

struct LockEntry {
    resource_id: String,
    owner: String,
    lease_duration: Duration,
    acquired_at: DateTime<Utc>,
    expires_at: DateTime<Utc>,
    lock_type: LockType,
    data: Option<Vec<u8>>,
    version: u64,
}

enum LockType {
    Exclusive,
    Shared,
    Read,
    Write,
}

struct WatchManager {
    watches: RwLock<HashMap<String, Vec<WatchEntry>>>,
}

struct WatchEntry {
    id: String,
    resource_id: String,
    owner: String,
    callback: Box<dyn Fn(WatchEvent) -> Result<(), String> + Send + Sync>,
    created_at: DateTime<Utc>,
}

enum WatchEvent {
    Acquired { resource_id: String, owner: String },
    Released { resource_id: String, owner: String },
    Expired { resource_id: String, owner: String },
}

struct SessionManager {
    sessions: RwLock<HashMap<String, SessionInfo>>,
    session_timeout: Duration,
}

struct SessionInfo {
    id: String,
    owner: String,
    created_at: DateTime<Utc>,
    last_heartbeat: DateTime<Utc>,
    resources: Vec<String>,
}

trait LockStorageProvider: Send + Sync {
    fn save_lock(&self, lock: &LockEntry) -> Result<(), String>;
    fn get_lock(&self, resource_id: &str) -> Result<Option<LockEntry>, String>;
    fn delete_lock(&self, resource_id: &str, version: u64) -> Result<bool, String>;
    fn list_locks(&self) -> Result<Vec<LockEntry>, String>;
}

struct HeartbeatManager {
    running: AtomicBool,
    heartbeat_interval: Duration,
    heartbeat_thread: Option<JoinHandle<()>>,
}

impl DistributedLockService {
    fn new(node_id: &str, storage_provider: Box<dyn LockStorageProvider>) -> Self {
        let lock_manager = LockManager {
            locks: RwLock::new(HashMap::new()),
        };

        let watch_manager = WatchManager {
            watches: RwLock::new(HashMap::new()),
        };

        let session_manager = SessionManager {
            sessions: RwLock::new(HashMap::new()),
            session_timeout: Duration::from_secs(30),
        };

        let heartbeat_manager = HeartbeatManager {
            running: AtomicBool::new(false),
            heartbeat_interval: Duration::from_secs(10),
            heartbeat_thread: None,
        };

        DistributedLockService {
            node_id: node_id.to_string(),
            lock_manager,
            watch_manager,
            session_manager,
            storage_provider,
            heartbeat_manager,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼é”æœåŠ¡");

        // ä»å­˜å‚¨ä¸­æ¢å¤é”
        self.recover_locks()?;

        // å¯åŠ¨å¿ƒè·³çº¿ç¨‹
        self.start_heartbeat()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼é”æœåŠ¡");

        // åœæ­¢å¿ƒè·³çº¿ç¨‹
        self.heartbeat_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.heartbeat_manager.heartbeat_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¿ƒè·³çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn recover_locks(&self) -> Result<(), String> {
        println!("ä»å­˜å‚¨ä¸­æ¢å¤é”");

        let locks = self.storage_provider.list_locks()?;
        let mut lock_map = self.lock_manager.locks.write().unwrap();

        for lock in locks {
            // æ£€æŸ¥é”æ˜¯å¦è¿‡æœŸ
            if lock.expires_at > Utc::now() {
                lock_map.insert(lock.resource_id.clone(), lock);
            }
        }

        println!("æ¢å¤äº† {} ä¸ªé”", lock_map.len());

        Ok(())
    }

    fn start_heartbeat(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¿ƒè·³çº¿ç¨‹");

        let node_id = self.node_id.clone();
        let lock_manager = self.lock_manager.locks.clone();
        let storage_provider = Arc::new(Mutex::new(self.storage_provider.clone()));
        let interval = self.heartbeat_manager.heartbeat_interval;

        self.heartbeat_manager.running.store(true, Ordering::SeqCst);

        let running = self.heartbeat_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // ç§»é™¤è¿‡æœŸçš„é”
                let now = Utc::now();
                let mut expired_resources = Vec::new();

                {
                    let locks = lock_manager.read().unwrap();
                    for (resource_id, lock) in locks.iter() {
                        if lock.expires_at <= now {
                            expired_resources.push((resource_id.clone(), lock.version));
                        }
                    }
                }

                if !expired_resources.is_empty() {
                    let mut locks = lock_manager.write().unwrap();
                    for (resource_id, version) in expired_resources {
                        if let Some(lock) = locks.get(&resource_id) {
                            if lock.version == version {
                                locks.remove(&resource_id);
                                println!("é”è¿‡æœŸ: {}", resource_id);

                                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€šçŸ¥å­˜å‚¨æä¾›è€…åˆ é™¤é”
                                let mut provider = storage_provider.lock().unwrap();
                                match provider.delete_lock(&resource_id, version) {
                                    Ok(_) => {},
                                    Err(e) => println!("åˆ é™¤è¿‡æœŸé”å¤±è´¥: {}", e),
                                }
                            }
                        }
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.heartbeat_manager.heartbeat_thread = Some(thread);

        Ok(())
    }

    fn create_session(&self, owner: &str) -> Result<String, String> {
        println!("åˆ›å»ºä¼šè¯: {}", owner);

        let session_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let session = SessionInfo {
            id: session_id.clone(),
            owner: owner.to_string(),
            created_at: now,
            last_heartbeat: now,
            resources: Vec::new(),
        };

        let mut sessions = self.session_manager.sessions.write().unwrap();
        sessions.insert(session_id.clone(), session);

        Ok(session_id)
    }

    fn session_heartbeat(&self, session_id: &str) -> Result<(), String> {
        let mut sessions = self.session_manager.sessions.write().unwrap();

        let session = sessions.get_mut(session_id)
            .ok_or_else(|| format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id))?;

        session.last_heartbeat = Utc::now();

        Ok(())
    }

    fn acquire_lock(&self, resource_id: &str, owner: &str, session_id: &str, lock_type: LockType, lease_duration: Duration) -> Result<bool, String> {
        println!("å°è¯•è·å–é”: {}, æ‰€æœ‰è€…: {}", resource_id, owner);

        // éªŒè¯ä¼šè¯
        let mut sessions = self.session_manager.sessions.write().unwrap();

        let session = sessions.get_mut(session_id)
            .ok_or_else(|| format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id))?;

        // æ£€æŸ¥ä¼šè¯æ˜¯å¦è¿‡æœŸ
        let now = Utc::now();
        if session.last_heartbeat + self.session_manager.session_timeout < now {
            return Err(format!("ä¼šè¯å·²è¿‡æœŸ: {}", session_id));
        }

        // æ›´æ–°ä¼šè¯å¿ƒè·³
        session.last_heartbeat = now;

        // å°è¯•è·å–é”
        let mut locks = self.lock_manager.locks.write().unwrap();

        // æ£€æŸ¥é”æ˜¯å¦å·²å­˜åœ¨
        if let Some(existing_lock) = locks.get(resource_id) {
            // å¦‚æœæ˜¯åŒä¸€æ‰€æœ‰è€…ï¼Œå¯ä»¥é‡æ–°è·å–æˆ–å‡çº§é”
            if existing_lock.owner == owner {
                // æ›´æ–°é”è¿‡æœŸæ—¶é—´
                let expires_at = now + lease_duration;
                let mut updated_lock = existing_lock.clone();
                updated_lock.lock_type = lock_type;
                updated_lock.lease_duration = lease_duration;
                updated_lock.expires_at = expires_at;
                updated_lock.version += 1;

                // ä¿å­˜åˆ°å­˜å‚¨
                self.storage_provider.save_lock(&updated_lock)?;

                // æ›´æ–°å†…å­˜ä¸­çš„é”
                locks.insert(resource_id.to_string(), updated_lock);

                return Ok(true);
            }

            // æ£€æŸ¥é”æ˜¯å¦å¯å…±äº«
            if let (LockType::Shared, LockType::Shared) = (&existing_lock.lock_type, &lock_type) {
                // å¯ä»¥å…±äº«é”ï¼ˆå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
                return Ok(true);
            }

            // é”è¢«å…¶ä»–æ‰€æœ‰è€…æŒæœ‰
            return Ok(false);
        }

        // åˆ›å»ºæ–°é”
        let expires_at = now + lease_duration;
        let lock = LockEntry {
            resource_id: resource_id.to_string(),
            owner: owner.to_string(),
            lease_duration,
            acquired_at: now,
            expires_at,
            lock_type,
            data: None,
            version: 1,
        };

        // ä¿å­˜åˆ°å­˜å‚¨
        self.storage_provider.save_lock(&lock)?;

        // æ›´æ–°å†…å­˜ä¸­çš„é”
        locks.insert(resource_id.to_string(), lock.clone());

        // æ›´æ–°ä¼šè¯èµ„æº
        session.resources.push(resource_id.to_string());

        // è§¦å‘ç›‘è§†äº‹ä»¶
        drop(locks);
        drop(sessions);
        self.trigger_watch_event(WatchEvent::Acquired {
            resource_id: resource_id.to_string(),
            owner: owner.to_string(),
        })?;

        Ok(true)
    }

    fn release_lock(&self, resource_id: &str, owner: &str, session_id: &str) -> Result<bool, String> {
        println!("é‡Šæ”¾é”: {}, æ‰€æœ‰è€…: {}", resource_id, owner);

        // éªŒè¯ä¼šè¯
        let mut sessions = self.session_manager.sessions.write().unwrap();

        let session = sessions.get_mut(session_id)
            .ok_or_else(|| format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id))?;

        // å°è¯•é‡Šæ”¾é”
        let mut locks = self.lock_manager.locks.write().unwrap();

        if let Some(lock) = locks.get(resource_id) {
            if lock.owner != owner {
                return Err(format!("é”ä¸å±äºæ­¤æ‰€æœ‰è€…: {}", owner));
            }

            // ä»å†…å­˜ä¸­ç§»é™¤é”
            let version = lock.version;
            locks.remove(resource_id);

            // ä»å­˜å‚¨ä¸­åˆ é™¤é”
            self.storage_provider.delete_lock(resource_id, version)?;

            // ä»ä¼šè¯èµ„æºä¸­ç§»é™¤
            session.resources.retain(|r| r != resource_id);

            // è§¦å‘ç›‘è§†äº‹ä»¶
            drop(locks);
            drop(sessions);
            self.trigger_watch_event(WatchEvent::Released {
                resource_id: resource_id.to_string(),
                owner: owner.to_string(),
            })?;

            return Ok(true);
        }

        Ok(false)
    }

    fn add_watch(&self, resource_id: &str, owner: &str, callback: Box<dyn Fn(WatchEvent) -> Result<(), String> + Send + Sync>) -> Result<String, String> {
        println!("æ·»åŠ ç›‘è§†: {}, æ‰€æœ‰è€…: {}", resource_id, owner);

        let watch_id = uuid::Uuid::new_v4().to_string();

        let watch = WatchEntry {
            id: watch_id.clone(),
            resource_id: resource_id.to_string(),
            owner: owner.to_string(),
            callback,
            created_at: Utc::now(),
        };

        let mut watches = self.watch_manager.watches.write().unwrap();

        let resource_watches = watches.entry(resource_id.to_string())
            .or_insert_with(Vec::new);

        resource_watches.push(watch);

        Ok(watch_id)
    }

    fn remove_watch(&self, watch_id: &str, resource_id: &str) -> Result<bool, String> {
        println!("ç§»é™¤ç›‘è§†: {}, èµ„æº: {}", watch_id, resource_id);

        let mut watches = self.watch_manager.watches.write().unwrap();

        if let Some(resource_watches) = watches.get_mut(resource_id) {
            let len_before = resource_watches.len();
            resource_watches.retain(|w| w.id != watch_id);

            let removed = len_before > resource_watches.len();

            // å¦‚æœæ²¡æœ‰æ›´å¤šçš„ç›‘è§†ï¼Œç§»é™¤èµ„æºæ¡ç›®
            if resource_watches.is_empty() {
                watches.remove(resource_id);
            }

            return Ok(removed);
        }

        Ok(false)
    }

    fn trigger_watch_event(&self, event: WatchEvent) -> Result<(), String> {
        let resource_id = match &event {
            WatchEvent::Acquired { resource_id, .. } => resource_id,
            WatchEvent::Released { resource_id, .. } => resource_id,
            WatchEvent::Expired { resource_id, .. } => resource_id,
        };

        let watches = self.watch_manager.watches.read().unwrap();

        if let Some(resource_watches) = watches.get(resource_id) {
            for watch in resource_watches {
                match (watch.callback)(event.clone()) {
                    Ok(_) => {},
                    Err(e) => println!("è§¦å‘ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                }
            }
        }

        Ok(())
    }
}

impl Clone for LockEntry {
    fn clone(&self) -> Self {
        LockEntry {
            resource_id: self.resource_id.clone(),
            owner: self.owner.clone(),
            lease_duration: self.lease_duration,
            acquired_at: self.acquired_at,
            expires_at: self.expires_at,
            lock_type: self.lock_type.clone(),
            data: self.data.clone(),
            version: self.version,
        }
    }
}

impl Clone for LockType {
    fn clone(&self) -> Self {
        match self {
            LockType::Exclusive => LockType::Exclusive,
            LockType::Shared => LockType::Shared,
            LockType::Read => LockType::Read,
            LockType::Write => LockType::Write,
        }
    }
}

impl Clone for WatchEvent {
    fn clone(&self) -> Self {
        match self {
            WatchEvent::Acquired { resource_id, owner } => WatchEvent::Acquired {
                resource_id: resource_id.clone(),
                owner: owner.clone(),
            },
            WatchEvent::Released { resource_id, owner } => WatchEvent::Released {
                resource_id: resource_id.clone(),
                owner: owner.clone(),
            },
            WatchEvent::Expired { resource_id, owner } => WatchEvent::Expired {
                resource_id: resource_id.clone(),
                owner: owner.clone(),
            },
        }
    }
}

// åˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿ
struct DistributedScheduler {
    node_id: String,
    job_manager: JobManager,
    worker_manager: WorkerManager,
    task_queue: TaskQueue,
    scheduler_policy: SchedulerPolicy,
    persistence: Box<dyn SchedulerPersistence>,
    failure_detector: FailureDetector,
}

struct JobManager {
    jobs: RwLock<HashMap<String, Job>>,
}

struct Job {
    id: String,
    name: String,
    description: Option<String>,
    tasks: Vec<Task>,
    dependencies: HashMap<String, Vec<String>>, // ä»»åŠ¡ID -> ä¾èµ–ä»»åŠ¡ID
    schedule: Option<Schedule>,
    status: JobStatus,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
    last_run: Option<JobRun>,
}

struct Task {
    id: String,
    job_id: String,
    name: String,
    task_type: TaskType,
    config: HashMap<String, String>,
    retry_policy: RetryPolicy,
    timeout: Duration,
}

enum TaskType {
    Shell,
    HTTP,
    SQL,
    Spark,
    Custom(String),
}

struct Schedule {
    cron_expression: String,
    timezone: String,
    start_time: Option<DateTime<Utc>>,
    end_time: Option<DateTime<Utc>>,
    next_run: Option<DateTime<Utc>>,
}

struct JobRun {
    id: String,
    job_id: String,
    start_time: DateTime<Utc>,
    end_time: Option<DateTime<Utc>>,
    status: RunStatus,
    task_runs: HashMap<String, TaskRun>,
}

struct TaskRun {
    id: String,
    task_id: String,
    worker_id: Option<String>,
    start_time: DateTime<Utc>,
    end_time: Option<DateTime<Utc>>,
    status: RunStatus,
    attempt: u32,
    output: Option<String>,
    error: Option<String>,
}

enum RunStatus {
    Pending,
    Running,
    Succeeded,
    Failed,
    Cancelled,
    Timeout,
}

struct WorkerManager {
    workers: RwLock<HashMap<String, Worker>>,
}

struct Worker {
    id: String,
    name: String,
    address: String,
    capabilities: HashSet<String>,
    resources: Resources,
    status: WorkerStatus,
    current_tasks: HashMap<String, DateTime<Utc>>,
    last_heartbeat: DateTime<Utc>,
}

struct Resources {
    cpu: u32,
    memory: u64,
    disk: u64,
}

enum WorkerStatus {
    Active,
    Busy,
    Offline,
    Maintenance,
}

struct TaskQueue {
    pending_tasks: RwLock<Vec<QueuedTask>>,
    running_tasks: RwLock<HashMap<String, QueuedTask>>,
}

struct QueuedTask {
    id: String,
    job_id: String,
    task_id: String,
    priority: u32,
    dependencies: Vec<String>,
    created_at: DateTime<Utc>,
}

enum SchedulerPolicy {
    FIFO,
    Priority,
    FairShare,
    ResourceAware,
}

trait SchedulerPersistence: Send + Sync {
    fn save_job(&self, job: &Job) -> Result<(), String>;
    fn get_job(&self, job_id: &str) -> Result<Option<Job>, String>;
    fn list_jobs(&self) -> Result<Vec<Job>, String>;
    fn save_job_run(&self, job_run: &JobRun) -> Result<(), String>;
    fn get_job_run(&self, run_id: &str) -> Result<Option<JobRun>, String>;
    fn list_job_runs(&self, job_id: &str) -> Result<Vec<JobRun>, String>;
}

struct FailureDetector {
    heartbeat_timeout: Duration,
    detection_interval: Duration,
    running: AtomicBool,
    detector_thread: Option<JoinHandle<()>>,
}

impl DistributedScheduler {
    fn new(node_id: &str, persistence: Box<dyn SchedulerPersistence>) -> Self {
        let job_manager = JobManager {
            jobs: RwLock::new(HashMap::new()),
        };

        let worker_manager = WorkerManager {
            workers: RwLock::new(HashMap::new()),
        };

        let task_queue = TaskQueue {
            pending_tasks: RwLock::new(Vec::new()),
            running_tasks: RwLock::new(HashMap::new()),
        };

        let failure_detector = FailureDetector {
            heartbeat_timeout: Duration::from_secs(30),
            detection_interval: Duration::from_secs(10),
            running: AtomicBool::new(false),
            detector_thread: None,
        };

        DistributedScheduler {
            node_id: node_id.to_string(),
            job_manager,
            worker_manager,
            task_queue,
            scheduler_policy: SchedulerPolicy::FIFO,
            persistence,
            failure_detector,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼è°ƒåº¦å™¨");

        // ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½ä½œä¸š
        self.load_jobs()?;

        // å¯åŠ¨å¤±è´¥æ£€æµ‹å™¨
        self.start_failure_detector()?;

        // å¯åŠ¨è°ƒåº¦å¾ªç¯
        self.schedule_pending_tasks()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼è°ƒåº¦å™¨");

        // åœæ­¢å¤±è´¥æ£€æµ‹å™¨
        self.failure_detector.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.failure_detector.detector_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤±è´¥æ£€æµ‹çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn load_jobs(&self) -> Result<(), String> {
        println!("ä»å­˜å‚¨ä¸­åŠ è½½ä½œä¸š");

        let jobs = self.persistence.list_jobs()?;
        let mut job_map = self.job_manager.jobs.write().unwrap();

        for job in jobs {
            job_map.insert(job.id.clone(), job);
        }

        println!("åŠ è½½äº† {} ä¸ªä½œä¸š", job_map.len());

        Ok(())
    }

    fn start_failure_detector(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤±è´¥æ£€æµ‹å™¨");

        let worker_manager = self.worker_manager.workers.clone();
        let task_queue = self.task_queue.running_tasks.clone();
        let heartbeat_timeout = self.failure_detector.heartbeat_timeout;
        let interval = self.failure_detector.detection_interval;

        self.failure_detector.running.store(true, Ordering::SeqCst);

        let running = self.failure_detector.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æµ‹å·¥ä½œèŠ‚ç‚¹å¤±è´¥
                let now = Utc::now();
                let mut failed_workers = Vec::new();

                {
                    let workers = worker_manager.read().unwrap();
                    for (worker_id, worker) in workers.iter() {
                        if worker.status == WorkerStatus::Active &&
                           worker.last_heartbeat + heartbeat_timeout < now {
                            failed_workers.push(worker_id.clone());
                        }
                    }
                }

                if !failed_workers.is_empty() {
                    let mut workers = worker_manager.write().unwrap();
                    for worker_id in &failed_workers {
                        if let Some(worker) = workers.get_mut(worker_id) {
                            println!("å·¥ä½œèŠ‚ç‚¹æ•…éšœ: {}", worker_id);
                            worker.status = WorkerStatus::Offline;

                            // æ¢å¤æ­¤èŠ‚ç‚¹ä¸Šçš„ä»»åŠ¡
                            let mut running_tasks = task_queue.write().unwrap();
                            let mut tasks_to_requeue = Vec::new();

                            for (task_id, task) in running_tasks.iter() {
                                if let Some(worker_tasks) = worker.current_tasks.get(task_id) {
                                    tasks_to_requeue.push(task.clone());
                                }
                            }

                            for task in tasks_to_requeue {
                                running_tasks.remove(&task.id);
                                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†ä»»åŠ¡é‡æ–°åŠ å…¥é˜Ÿåˆ—
                            }
                        }
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.failure_detector.detector_thread = Some(thread);

        Ok(())
    }

    fn register_worker(&self, worker: Worker) -> Result<(), String> {
        println!("æ³¨å†Œå·¥ä½œèŠ‚ç‚¹: {}", worker.name);

        let mut workers = self.worker_manager.workers.write().unwrap();

        if workers.contains_key(&worker.id) {
            return Err(format!("å·¥ä½œèŠ‚ç‚¹å·²å­˜åœ¨: {}", worker.id));
        }

        workers.insert(worker.id.clone(), worker);

        Ok(())
    }

    fn worker_heartbeat(&self, worker_id: &str) -> Result<(), String> {
        let mut workers = self.worker_manager.workers.write().unwrap();

        let worker = workers.get_mut(worker_id)
            .ok_or_else(|| format!("å·¥ä½œèŠ‚ç‚¹ä¸å­˜åœ¨: {}", worker_id))?;

        worker.last_heartbeat = Utc::now();

        Ok(())
    }

    fn create_job(&self, job: Job) -> Result<(), String> {
        println!("åˆ›å»ºä½œä¸š: {}", job.name);

        // éªŒè¯ä½œä¸š
        self.validate_job(&job)?;

        let mut jobs = self.job_manager.jobs.write().unwrap();

        if jobs.contains_key(&job.id) {
            return Err(format!("ä½œä¸šå·²å­˜åœ¨: {}", job.id));
        }

        // ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        self.persistence.save_job(&job)?;

        // ä¿å­˜åˆ°å†…å­˜
        jobs.insert(job.id.clone(), job);

        Ok(())
    }

    fn validate_job(&self, job: &Job) -> Result<(), String> {
        // æ£€æŸ¥ä»»åŠ¡
        if job.tasks.is_empty() {
            return Err("ä½œä¸šå¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªä»»åŠ¡".to_string());
        }

        // æ£€æŸ¥ä¾èµ–å…³ç³»
        for (task_id, deps) in &job.dependencies {
            // æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            if !job.tasks.iter().any(|t| t.id == *task_id) {
                return Err(format!("ä»»åŠ¡ä¸å­˜åœ¨: {}", task_id));
            }

            // æ£€æŸ¥ä¾èµ–ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            for dep_id in deps {
                if !job.tasks.iter().any(|t| t.id == *dep_id) {
                    return Err(format!("ä¾èµ–ä»»åŠ¡ä¸å­˜åœ¨: {}", dep_id));
                }
            }
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰å¾ªç¯ä¾èµ–
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦æ›´å¤æ‚çš„å›¾ç®—æ³•

        Ok(())
    }

    fn schedule_job(&self, job_id: &str) -> Result<String, String> {
        println!("è°ƒåº¦ä½œä¸š: {}", job_id);

        let jobs = self.job_manager.jobs.read().unwrap();

        let job = jobs.get(job_id)
            .ok_or_else(|| format!("ä½œä¸šä¸å­˜åœ¨: {}", job_id))?;

        if job.status == JobStatus::Running {
            return Err(format!("ä½œä¸šå·²åœ¨è¿è¡Œä¸­: {}", job_id));
        }

        // åˆ›å»ºä½œä¸šè¿è¡Œ
        let run_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let job_run = JobRun {
            id: run_id.clone(),
            job_id: job_id.to_string(),
            start_time: now,
            end_time: None,
            status: RunStatus::Running,
            task_runs: HashMap::new(),
        };

        // ä¿å­˜ä½œä¸šè¿è¡Œ
        self.persistence.save_job_run(&job_run)?;

        // å°†ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        self.enqueue_job_tasks(job, &run_id)?;

        // æ›´æ–°ä½œä¸šçŠ¶æ€
        drop(jobs);
        let mut jobs = self.job_manager.jobs.write().unwrap();

        if let Some(job) = jobs.get_mut(job_id) {
            job.status = JobStatus::Running;
            job.updated_at = now;
            job.last_run = Some(job_run);

            // ä¿å­˜æ›´æ–°åçš„ä½œä¸š
            self.persistence.save_job(&job)?;
        }

        // å°è¯•è°ƒåº¦ä»»åŠ¡
        drop(jobs);
        self.schedule_pending_tasks()?;

        Ok(run_id)
    }

    fn enqueue_job_tasks(&self, job: &Job, run_id: &str) -> Result<(), String> {
        println!("å°†ä½œä¸š {} çš„ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—", job.id);

        let now = Utc::now();

        // æ‰¾å‡ºæ²¡æœ‰ä¾èµ–çš„ä»»åŠ¡
        let root_tasks: Vec<_> = job.tasks.iter()
            .filter(|t| !job.dependencies.contains_key(&t.id) || job.dependencies[&t.id].is_empty())
            .collect();

        let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();

        for task in root_tasks {
            let queued_task = QueuedTask {
                id: uuid::Uuid::new_v4().to_string(),
                job_id: job.id.clone(),
                task_id: task.id.clone(),
                priority: 0, // é»˜è®¤ä¼˜å…ˆçº§
                dependencies: Vec::new(),
                created_at: now,
            };

            pending_tasks.push(queued_task);
        }

        // æŒ‰ç­–ç•¥æ’åº
        self.sort_pending_tasks(&mut pending_tasks);

        Ok(())
    }

    fn sort_pending_tasks(&self, tasks: &mut Vec<QueuedTask>) {
        match self.scheduler_policy {
            SchedulerPolicy::FIFO => {
                // æŒ‰åˆ›å»ºæ—¶é—´æ’åº
                tasks.sort_by(|a, b| a.created_at.cmp(&b.created_at));
            },
            SchedulerPolicy::Priority => {
                // æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆé«˜ä¼˜å…ˆçº§åœ¨å‰ï¼‰
                tasks.sort_by(|a, b| b.priority.cmp(&a.priority));
            },
            SchedulerPolicy::FairShare => {
                // æŒ‰ä½œä¸šIDåˆ†ç»„ï¼Œç„¶åäº¤æ›¿è°ƒåº¦
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
            },
            SchedulerPolicy::ResourceAware => {
                // æ ¹æ®èµ„æºéœ€æ±‚æ’åº
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
            },
        }
    }

    fn schedule_pending_tasks(&self) -> Result<usize, String> {
        println!("è°ƒåº¦å¾…å¤„ç†ä»»åŠ¡");

        let mut scheduled_count = 0;

        // è·å–å¯ç”¨å·¥ä½œèŠ‚ç‚¹
        let workers = self.worker_manager.workers.read().unwrap();
        let available_workers: Vec<_> = workers.values()
            .filter(|w| w.status == WorkerStatus::Active)
            .collect();

        if available_workers.is_empty() {
            return Ok(0);
        }

        // è·å–å¾…å¤„ç†ä»»åŠ¡
        let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();

        if pending_tasks.is_empty() {
            return Ok(0);
        }

        // å°è¯•ä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…å·¥ä½œèŠ‚ç‚¹
        let mut i = 0;
        while i < pending_tasks.len() {
            let task = &pending_tasks[i];

            // æ£€æŸ¥æ˜¯å¦æœ‰åˆé€‚çš„å·¥ä½œèŠ‚ç‚¹
            let mut assigned_worker = None;

            for worker in &available_workers {
                // æ£€æŸ¥å·¥ä½œèŠ‚ç‚¹æ˜¯å¦æœ‰æ‰€éœ€çš„èƒ½åŠ›
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæœ‰æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘

                assigned_worker = Some(worker.id.clone());
                break;
            }

            if let Some(worker_id) = assigned_worker {
                println!("å°†ä»»åŠ¡ {} åˆ†é…ç»™å·¥ä½œèŠ‚ç‚¹ {}", task.id, worker_id);

                // ä»å¾…å¤„ç†é˜Ÿåˆ—ç§»é™¤ä»»åŠ¡
                let task = pending_tasks.remove(i);

                // æ·»åŠ åˆ°è¿è¡Œä¸­é˜Ÿåˆ—
                let mut running_tasks = self.task_queue.running_tasks.write().unwrap();
                running_tasks.insert(task.id.clone(), task);

                // æ›´æ–°å·¥ä½œèŠ‚ç‚¹çŠ¶æ€
                drop(workers);
                let mut workers = self.worker_manager.workers.write().unwrap();

                if let Some(worker) = workers.get_mut(&worker_id) {
                    worker.current_tasks.insert(task.id.clone(), Utc::now());

                    if worker.current_tasks.len() >= 10 { // ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªèŠ‚ç‚¹æœ€å¤š10ä¸ªä»»åŠ¡
                        worker.status = WorkerStatus::Busy;
                    }
                }

                scheduled_count += 1;

                // æ¢å¤å·¥ä½œèŠ‚ç‚¹è¯»é”ä»¥ä¾¿ç»§ç»­å¾ªç¯
                drop(workers);
                let workers = self.worker_manager.workers.read().unwrap();
            } else {
                // æ²¡æœ‰åˆé€‚çš„å·¥ä½œèŠ‚ç‚¹ï¼Œè·³åˆ°ä¸‹ä¸€ä¸ªä»»åŠ¡
                i += 1;
            }
        }

        println!("è°ƒåº¦äº† {} ä¸ªä»»åŠ¡", scheduled_count);

        Ok(scheduled_count)
    }

    fn task_completed(&self, task_id: &str, success: bool, output: Option<String>, error: Option<String>) -> Result<(), String> {
        println!("ä»»åŠ¡å®Œæˆ: {}, æˆåŠŸ: {}", task_id, success);

        // è·å–ä»»åŠ¡ä¿¡æ¯
        let mut running_tasks = self.task_queue.running_tasks.write().unwrap();

        let task = running_tasks.remove(task_id)
            .ok_or_else(|| format!("ä»»åŠ¡ä¸åœ¨è¿è¡Œä¸­: {}", task_id))?;

        // æ›´æ–°å·¥ä½œèŠ‚ç‚¹çŠ¶æ€
        let mut workers = self.worker_manager.workers.write().unwrap();

        for worker in workers.values_mut() {
            if worker.current_tasks.contains_key(task_id) {
                worker.current_tasks.remove(task_id);


```rust
                if worker.current_tasks.is_empty() && worker.status == WorkerStatus::Busy {
                    worker.status = WorkerStatus::Active;
                }
                break;
            }
        }

        // æ›´æ–°ä½œä¸šè¿è¡ŒçŠ¶æ€
        let jobs = self.job_manager.jobs.read().unwrap();
        let job = jobs.get(&task.job_id)
            .ok_or_else(|| format!("ä½œä¸šä¸å­˜åœ¨: {}", task.job_id))?;

        if let Some(last_run) = &job.last_run {
            // æ›´æ–°ä»»åŠ¡è¿è¡ŒçŠ¶æ€
            let mut job_run = last_run.clone();

            let now = Utc::now();
            let mut task_run = TaskRun {
                id: uuid::Uuid::new_v4().to_string(),
                task_id: task.task_id.clone(),
                worker_id: None, // å°†åœ¨åé¢å¡«å……
                start_time: now - Duration::from_secs(10), // å‡è®¾ä»»åŠ¡è¿è¡Œäº†10ç§’
                end_time: Some(now),
                status: if success { RunStatus::Succeeded } else { RunStatus::Failed },
                attempt: 1, // å‡è®¾æ˜¯ç¬¬ä¸€æ¬¡å°è¯•
                output,
                error,
            };

            // æ‰¾åˆ°è¿è¡Œæ­¤ä»»åŠ¡çš„å·¥ä½œèŠ‚ç‚¹
            for worker in workers.values() {
                if worker.current_tasks.contains_key(&task.id) {
                    task_run.worker_id = Some(worker.id.clone());
                    break;
                }
            }

            job_run.task_runs.insert(task.task_id.clone(), task_run);

            // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆ
            let all_tasks_completed = job.tasks.iter()
                .all(|t| job_run.task_runs.contains_key(&t.id));

            let any_task_failed = job_run.task_runs.values()
                .any(|tr| tr.status == RunStatus::Failed);

            if all_tasks_completed {
                job_run.end_time = Some(now);
                job_run.status = if any_task_failed { RunStatus::Failed } else { RunStatus::Succeeded };

                // æ›´æ–°ä½œä¸šçŠ¶æ€
                drop(jobs);
                let mut jobs = self.job_manager.jobs.write().unwrap();

                if let Some(job) = jobs.get_mut(&task.job_id) {
                    job.status = if any_task_failed { JobStatus::Failed } else { JobStatus::Succeeded };
                    job.updated_at = now;
                    job.last_run = Some(job_run.clone());

                    // ä¿å­˜æ›´æ–°åçš„ä½œä¸š
                    self.persistence.save_job(job)?;
                }

                // ä¿å­˜ä½œä¸šè¿è¡Œ
                self.persistence.save_job_run(&job_run)?;
            } else if success {
                // ä»»åŠ¡æˆåŠŸï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥è°ƒåº¦ä¾èµ–æ­¤ä»»åŠ¡çš„å…¶ä»–ä»»åŠ¡
                let completed_task_id = task.task_id.clone();

                // æ‰¾å‡ºæ‰€æœ‰ä¾èµ–æ­¤ä»»åŠ¡çš„ä»»åŠ¡
                let dependent_tasks: Vec<_> = job.dependencies.iter()
                    .filter(|(_, deps)| deps.contains(&completed_task_id))
                    .map(|(task_id, _)| task_id.clone())
                    .collect();

                if !dependent_tasks.is_empty() {
                    // æ£€æŸ¥æ¯ä¸ªä¾èµ–ä»»åŠ¡çš„æ‰€æœ‰ä¾èµ–æ˜¯å¦éƒ½å·²å®Œæˆ
                    for dep_task_id in dependent_tasks {
                        let all_deps_completed = if let Some(deps) = job.dependencies.get(&dep_task_id) {
                            deps.iter().all(|dep_id| {
                                job_run.task_runs.contains_key(dep_id) &&
                                job_run.task_runs[dep_id].status == RunStatus::Succeeded
                            })
                        } else {
                            true
                        };

                        if all_deps_completed {
                            // å¯ä»¥è°ƒåº¦æ­¤ä»»åŠ¡
                            let task_def = job.tasks.iter()
                                .find(|t| t.id == dep_task_id)
                                .ok_or_else(|| format!("ä»»åŠ¡ä¸å­˜åœ¨: {}", dep_task_id))?;

                            let queued_task = QueuedTask {
                                id: uuid::Uuid::new_v4().to_string(),
                                job_id: job.id.clone(),
                                task_id: task_def.id.clone(),
                                priority: 0, // é»˜è®¤ä¼˜å…ˆçº§
                                dependencies: Vec::new(), // å·²ç»æ£€æŸ¥äº†ä¾èµ–
                                created_at: now,
                            };

                            let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
                            pending_tasks.push(queued_task);

                            // æŒ‰ç­–ç•¥æ’åº
                            self.sort_pending_tasks(&mut pending_tasks);
                        }
                    }
                }

                // ä¿å­˜ä½œä¸šè¿è¡Œ
                self.persistence.save_job_run(&job_run)?;
            }
        }

        // å°è¯•è°ƒåº¦æ›´å¤šä»»åŠ¡
        self.schedule_pending_tasks()?;

        Ok(())
    }
}

enum JobStatus {
    Pending,
    Scheduled,
    Running,
    Succeeded,
    Failed,
    Cancelled,
}

struct RetryPolicy {
    max_retries: u32,
    retry_interval: Duration,
    max_retry_interval: Duration,
    backoff_factor: f64,
}

impl Clone for JobRun {
    fn clone(&self) -> Self {
        JobRun {
            id: self.id.clone(),
            job_id: self.job_id.clone(),
            start_time: self.start_time,
            end_time: self.end_time,
            status: self.status.clone(),
            task_runs: self.task_runs.clone(),
        }
    }
}

impl Clone for TaskRun {
    fn clone(&self) -> Self {
        TaskRun {
            id: self.id.clone(),
            task_id: self.task_id.clone(),
            worker_id: self.worker_id.clone(),
            start_time: self.start_time,
            end_time: self.end_time,
            status: self.status.clone(),
            attempt: self.attempt,
            output: self.output.clone(),
            error: self.error.clone(),
        }
    }
}

impl Clone for RunStatus {
    fn clone(&self) -> Self {
        match self {
            RunStatus::Pending => RunStatus::Pending,
            RunStatus::Running => RunStatus::Running,
            RunStatus::Succeeded => RunStatus::Succeeded,
            RunStatus::Failed => RunStatus::Failed,
            RunStatus::Cancelled => RunStatus::Cancelled,
            RunStatus::Timeout => RunStatus::Timeout,
        }
    }
}

impl Clone for QueuedTask {
    fn clone(&self) -> Self {
        QueuedTask {
            id: self.id.clone(),
            job_id: self.job_id.clone(),
            task_id: self.task_id.clone(),
            priority: self.priority,
            dependencies: self.dependencies.clone(),
            created_at: self.created_at,
        }
    }
}

// åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
struct DistributedFileSystem {
    node_id: String,
    storage_manager: StorageManager,
    chunk_manager: ChunkManager,
    metadata_manager: MetadataManager,
    replication_manager: ReplicationManager,
    client_manager: ClientManager,
}

struct StorageManager {
    storage_nodes: RwLock<HashMap<String, StorageNode>>,
    data_dir: PathBuf,
}

struct StorageNode {
    id: String,
    address: String,
    capacity: u64,
    used: u64,
    status: NodeStatus,
    last_heartbeat: DateTime<Utc>,
}

enum NodeStatus {
    Online,
    Offline,
    Maintenance,
}

struct ChunkManager {
    chunks: RwLock<HashMap<String, ChunkInfo>>,
    chunk_size: u64,
    default_replication: u32,
}

struct ChunkInfo {
    id: String,
    file_id: String,
    offset: u64,
    size: u64,
    locations: Vec<String>, // å­˜å‚¨èŠ‚ç‚¹ID
    checksum: String,
    created_at: DateTime<Utc>,
    modified_at: DateTime<Utc>,
}

struct MetadataManager {
    files: RwLock<HashMap<String, FileMetadata>>,
    directories: RwLock<HashMap<String, DirectoryMetadata>>,
}

struct FileMetadata {
    id: String,
    name: String,
    path: String,
    parent_dir: String,
    size: u64,
    chunks: Vec<String>, // å—ID
    permissions: u32,
    owner: String,
    group: String,
    created_at: DateTime<Utc>,
    modified_at: DateTime<Utc>,
    accessed_at: DateTime<Utc>,
}

struct DirectoryMetadata {
    id: String,
    name: String,
    path: String,
    parent_dir: Option<String>,
    children: Vec<String>, // æ–‡ä»¶å’Œç›®å½•ID
    permissions: u32,
    owner: String,
    group: String,
    created_at: DateTime<Utc>,
    modified_at: DateTime<Utc>,
    accessed_at: DateTime<Utc>,
}

struct ReplicationManager {
    replication_queue: RwLock<Vec<ReplicationTask>>,
    running: AtomicBool,
    replication_thread: Option<JoinHandle<()>>,
}

struct ReplicationTask {
    chunk_id: String,
    source_node: String,
    target_node: String,
    priority: u32,
    created_at: DateTime<Utc>,
}

struct ClientManager {
    clients: RwLock<HashMap<String, ClientInfo>>,
}

struct ClientInfo {
    id: String,
    address: String,
    user: String,
    connection_time: DateTime<Utc>,
    last_activity: DateTime<Utc>,
}

impl DistributedFileSystem {
    fn new(node_id: &str, data_dir: &Path) -> Self {
        let storage_manager = StorageManager {
            storage_nodes: RwLock::new(HashMap::new()),
            data_dir: data_dir.to_path_buf(),
        };

        let chunk_manager = ChunkManager {
            chunks: RwLock::new(HashMap::new()),
            chunk_size: 64 * 1024 * 1024, // 64MB
            default_replication: 3,
        };

        let metadata_manager = MetadataManager {
            files: RwLock::new(HashMap::new()),
            directories: RwLock::new(HashMap::new()),
        };

        let replication_manager = ReplicationManager {
            replication_queue: RwLock::new(Vec::new()),
            running: AtomicBool::new(false),
            replication_thread: None,
        };

        let client_manager = ClientManager {
            clients: RwLock::new(HashMap::new()),
        };

        DistributedFileSystem {
            node_id: node_id.to_string(),
            storage_manager,
            chunk_manager,
            metadata_manager,
            replication_manager,
            client_manager,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ");

        // åˆ›å»ºæ ¹ç›®å½•
        self.create_root_directory()?;

        // å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨
        self.start_replication_manager()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ");

        // åœæ­¢å¤åˆ¶ç®¡ç†å™¨
        self.replication_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.replication_manager.replication_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤åˆ¶çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn create_root_directory(&self) -> Result<(), String> {
        println!("åˆ›å»ºæ ¹ç›®å½•");

        let mut directories = self.metadata_manager.directories.write().unwrap();

        if directories.is_empty() {
            let root_id = "root".to_string();
            let now = Utc::now();

            let root_dir = DirectoryMetadata {
                id: root_id.clone(),
                name: "/".to_string(),
                path: "/".to_string(),
                parent_dir: None,
                children: Vec::new(),
                permissions: 0o755, // rwxr-xr-x
                owner: "root".to_string(),
                group: "root".to_string(),
                created_at: now,
                modified_at: now,
                accessed_at: now,
            };

            directories.insert(root_id, root_dir);

            println!("æ ¹ç›®å½•å·²åˆ›å»º");
        }

        Ok(())
    }

    fn start_replication_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨");

        let replication_queue = self.replication_manager.replication_queue.clone();
        let chunks = self.chunk_manager.chunks.clone();
        let storage_nodes = self.storage_manager.storage_nodes.clone();
        let data_dir = self.storage_manager.data_dir.clone();

        self.replication_manager.running.store(true, Ordering::SeqCst);

        let running = self.replication_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // å¤„ç†å¤åˆ¶ä»»åŠ¡
                let task_option = {
                    let mut queue = replication_queue.write().unwrap();
                    if queue.is_empty() {
                        None
                    } else {
                        // æŒ‰ä¼˜å…ˆçº§æ’åº
                        queue.sort_by(|a, b| b.priority.cmp(&a.priority));
                        Some(queue.remove(0))
                    }
                };

                if let Some(task) = task_option {
                    println!("å¤„ç†å¤åˆ¶ä»»åŠ¡: {} -> {}", task.source_node, task.target_node);

                    // è·å–å—ä¿¡æ¯
                    let chunk_info = {
                        let chunks = chunks.read().unwrap();
                        chunks.get(&task.chunk_id).cloned()
                    };

                    if let Some(chunk) = chunk_info {
                        // è·å–æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹ä¿¡æ¯
                        let (source_node, target_node) = {
                            let nodes = storage_nodes.read().unwrap();
                            let source = nodes.get(&task.source_node).cloned();
                            let target = nodes.get(&task.target_node).cloned();
                            (source, target)
                        };

                        if let (Some(source), Some(target)) = (source_node, target_node) {
                            if source.status == NodeStatus::Online && target.status == NodeStatus::Online {
                                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»æºèŠ‚ç‚¹è¯»å–æ•°æ®å¹¶å†™å…¥ç›®æ ‡èŠ‚ç‚¹
                                // ç®€åŒ–ï¼šå‡è®¾å¤åˆ¶æˆåŠŸ

                                // æ›´æ–°å—ä½ç½®
                                let mut chunks = chunks.write().unwrap();
                                if let Some(chunk) = chunks.get_mut(&task.chunk_id) {
                                    if !chunk.locations.contains(&task.target_node) {
                                        chunk.locations.push(task.target_node.clone());
                                    }
                                }

                                println!("å¤åˆ¶ä»»åŠ¡å®Œæˆ: {}", task.chunk_id);
                            } else {
                                // èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œé‡æ–°åŠ å…¥é˜Ÿåˆ—
                                let mut queue = replication_queue.write().unwrap();
                                queue.push(task);
                            }
                        } else {
                            println!("æºèŠ‚ç‚¹æˆ–ç›®æ ‡èŠ‚ç‚¹ä¸å­˜åœ¨");
                        }
                    } else {
                        println!("å—ä¸å­˜åœ¨: {}", task.chunk_id);
                    }
                }

                // ä¼‘çœ ä¸€ä¼šå„¿
                thread::sleep(Duration::from_millis(100));
            }
        });

        self.replication_manager.replication_thread = Some(thread);

        Ok(())
    }

    fn register_storage_node(&self, node: StorageNode) -> Result<(), String> {
        println!("æ³¨å†Œå­˜å‚¨èŠ‚ç‚¹: {}", node.id);

        let mut nodes = self.storage_manager.storage_nodes.write().unwrap();

        if nodes.contains_key(&node.id) {
            return Err(format!("å­˜å‚¨èŠ‚ç‚¹å·²å­˜åœ¨: {}", node.id));
        }

        nodes.insert(node.id.clone(), node);

        Ok(())
    }

    fn storage_node_heartbeat(&self, node_id: &str, used: u64) -> Result<(), String> {
        let mut nodes = self.storage_manager.storage_nodes.write().unwrap();

        let node = nodes.get_mut(node_id)
            .ok_or_else(|| format!("å­˜å‚¨èŠ‚ç‚¹ä¸å­˜åœ¨: {}", node_id))?;

        node.last_heartbeat = Utc::now();
        node.used = used;

        Ok(())
    }

    fn create_directory(&self, parent_path: &str, name: &str, owner: &str, group: &str, permissions: u32) -> Result<String, String> {
        println!("åˆ›å»ºç›®å½•: {}/{}", parent_path, name);

        // éªŒè¯çˆ¶ç›®å½•
        let parent_id = self.get_directory_by_path(parent_path)?;

        let mut directories = self.metadata_manager.directories.write().unwrap();

        let parent_dir = directories.get_mut(&parent_id)
            .ok_or_else(|| format!("çˆ¶ç›®å½•ä¸å­˜åœ¨: {}", parent_path))?;

        // éªŒè¯ç›®å½•åç§°
        if name.is_empty() || name.contains('/') {
            return Err("æ— æ•ˆçš„ç›®å½•åç§°".to_string());
        }

        // æ£€æŸ¥åŒåæ–‡ä»¶æˆ–ç›®å½•æ˜¯å¦å·²å­˜åœ¨
        let path = if parent_path.ends_with('/') {
            format!("{}{}", parent_path, name)
        } else {
            format!("{}/{}", parent_path, name)
        };

        for child_id in &parent_dir.children {
            if let Some(dir) = directories.get(child_id) {
                if dir.name == name {
                    return Err(format!("ç›®å½•å·²å­˜åœ¨: {}", path));
                }
            } else {
                let files = self.metadata_manager.files.read().unwrap();
                if let Some(file) = files.get(child_id) {
                    if file.name == name {
                        return Err(format!("åŒåæ–‡ä»¶å·²å­˜åœ¨: {}", path));
                    }
                }
            }
        }

        // åˆ›å»ºæ–°ç›®å½•
        let dir_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let new_dir = DirectoryMetadata {
            id: dir_id.clone(),
            name: name.to_string(),
            path: path.clone(),
            parent_dir: Some(parent_id.clone()),
            children: Vec::new(),
            permissions,
            owner: owner.to_string(),
            group: group.to_string(),
            created_at: now,
            modified_at: now,
            accessed_at: now,
        };

        // æ›´æ–°çˆ¶ç›®å½•
        parent_dir.children.push(dir_id.clone());
        parent_dir.modified_at = now;

        // ä¿å­˜æ–°ç›®å½•
        directories.insert(dir_id.clone(), new_dir);

        println!("ç›®å½•å·²åˆ›å»º: {}", path);

        Ok(dir_id)
    }

    fn get_directory_by_path(&self, path: &str) -> Result<String, String> {
        println!("è·å–ç›®å½•: {}", path);

        let directories = self.metadata_manager.directories.read().unwrap();

        // å¤„ç†æ ¹ç›®å½•æƒ…å†µ
        if path == "/" {
            for (id, dir) in directories.iter() {
                if dir.path == "/" {
                    return Ok(id.clone());
                }
            }
            return Err("æ ¹ç›®å½•ä¸å­˜åœ¨".to_string());
        }

        // è§„èŒƒåŒ–è·¯å¾„
        let normalized_path = if path.ends_with('/') {
            path.to_string()
        } else {
            format!("{}/", path)
        };

        // æŸ¥æ‰¾ç›®å½•
        for (id, dir) in directories.iter() {
            if dir.path == path || format!("{}/", dir.path) == normalized_path {
                return Ok(id.clone());
            }
        }

        Err(format!("ç›®å½•ä¸å­˜åœ¨: {}", path))
    }

    fn create_file(&self, parent_path: &str, name: &str, owner: &str, group: &str, permissions: u32) -> Result<String, String> {
        println!("åˆ›å»ºæ–‡ä»¶: {}/{}", parent_path, name);

        // éªŒè¯çˆ¶ç›®å½•
        let parent_id = self.get_directory_by_path(parent_path)?;

        let mut directories = self.metadata_manager.directories.write().unwrap();

        let parent_dir = directories.get_mut(&parent_id)
            .ok_or_else(|| format!("çˆ¶ç›®å½•ä¸å­˜åœ¨: {}", parent_path))?;

        // éªŒè¯æ–‡ä»¶åç§°
        if name.is_empty() || name.contains('/') {
            return Err("æ— æ•ˆçš„æ–‡ä»¶åç§°".to_string());
        }

        // æ£€æŸ¥åŒåæ–‡ä»¶æˆ–ç›®å½•æ˜¯å¦å·²å­˜åœ¨
        let path = if parent_path.ends_with('/') {
            format!("{}{}", parent_path, name)
        } else {
            format!("{}/{}", parent_path, name)
        };

        for child_id in &parent_dir.children {
            let dir_exists = directories.contains_key(child_id);

            if dir_exists {
                let dir = directories.get(child_id).unwrap();
                if dir.name == name {
                    return Err(format!("åŒåç›®å½•å·²å­˜åœ¨: {}", path));
                }
            } else {
                let files = self.metadata_manager.files.read().unwrap();
                if let Some(file) = files.get(child_id) {
                    if file.name == name {
                        return Err(format!("æ–‡ä»¶å·²å­˜åœ¨: {}", path));
                    }
                }
            }
        }

        // åˆ›å»ºæ–°æ–‡ä»¶
        let file_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let new_file = FileMetadata {
            id: file_id.clone(),
            name: name.to_string(),
            path: path.clone(),
            parent_dir: parent_id.clone(),
            size: 0,
            chunks: Vec::new(),
            permissions,
            owner: owner.to_string(),
            group: group.to_string(),
            created_at: now,
            modified_at: now,
            accessed_at: now,
        };

        // æ›´æ–°çˆ¶ç›®å½•
        parent_dir.children.push(file_id.clone());
        parent_dir.modified_at = now;

        // ä¿å­˜æ–°æ–‡ä»¶
        drop(directories);
        let mut files = self.metadata_manager.files.write().unwrap();
        files.insert(file_id.clone(), new_file);

        println!("æ–‡ä»¶å·²åˆ›å»º: {}", path);

        Ok(file_id)
    }

    fn write_chunk(&self, file_id: &str, offset: u64, data: &[u8]) -> Result<String, String> {
        println!("å†™å…¥å—: {}, åç§»é‡: {}, å¤§å°: {}", file_id, offset, data.len());

        // éªŒè¯æ–‡ä»¶
        let mut files = self.metadata_manager.files.write().unwrap();

        let file = files.get_mut(file_id)
            .ok_or_else(|| format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", file_id))?;

        // è®¡ç®—éœ€è¦çš„å—æ•°
        let chunk_size = self.chunk_manager.chunk_size;
        let chunk_index = offset / chunk_size;
        let chunk_offset = offset % chunk_size;

        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°å—
        let chunk_id = if chunk_index as usize >= file.chunks.len() {
            // åˆ›å»ºæ–°å—
            let new_chunk_id = uuid::Uuid::new_v4().to_string();
            file.chunks.push(new_chunk_id.clone());
            new_chunk_id
        } else {
            // ä½¿ç”¨ç°æœ‰å—
            file.chunks[chunk_index as usize].clone()
        };

        // é€‰æ‹©å­˜å‚¨èŠ‚ç‚¹
        let selected_nodes = self.select_storage_nodes(
            self.chunk_manager.default_replication as usize
        )?;

        if selected_nodes.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨çš„å­˜å‚¨èŠ‚ç‚¹".to_string());
        }

        // åˆ›å»ºæˆ–æ›´æ–°å—ä¿¡æ¯
        let now = Utc::now();
        let mut chunks = self.chunk_manager.chunks.write().unwrap();

        let chunk = chunks.entry(chunk_id.clone())
            .or_insert_with(|| ChunkInfo {
                id: chunk_id.clone(),
                file_id: file_id.to_string(),
                offset: chunk_index * chunk_size,
                size: 0,
                locations: Vec::new(),
                checksum: "".to_string(),
                created_at: now,
                modified_at: now,
            });

        // æ›´æ–°å—å¤§å°
        let new_size = chunk_offset + data.len() as u64;
        if new_size > chunk.size {
            chunk.size = new_size;
        }

        // æ›´æ–°å—ä½ç½®
        chunk.locations = selected_nodes.clone();
        chunk.modified_at = now;

        // è®¡ç®—æ ¡éªŒå’Œ
        let mut hasher = sha2::Sha256::new();
        hasher.update(data);
        let checksum = format!("{:x}", hasher.finalize());
        chunk.checksum = checksum;

        // æ›´æ–°æ–‡ä»¶å¤§å°
        let end_offset = offset + data.len() as u64;
        if end_offset > file.size {
            file.size = end_offset;
        }
        file.modified_at = now;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ•°æ®å†™å…¥æ‰€é€‰çš„å­˜å‚¨èŠ‚ç‚¹
        println!("æ•°æ®å·²å†™å…¥é€‰å®šçš„å­˜å‚¨èŠ‚ç‚¹: {:?}", selected_nodes);

        Ok(chunk_id)
    }

    fn select_storage_nodes(&self, count: usize) -> Result<Vec<String>, String> {
        let nodes = self.storage_manager.storage_nodes.read().unwrap();

        if nodes.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨çš„å­˜å‚¨èŠ‚ç‚¹".to_string());
        }

        // ç­›é€‰åœ¨çº¿èŠ‚ç‚¹
        let online_nodes: Vec<_> = nodes.values()
            .filter(|n| n.status == NodeStatus::Online)
            .collect();

        if online_nodes.is_empty() {
            return Err("æ²¡æœ‰åœ¨çº¿çš„å­˜å‚¨èŠ‚ç‚¹".to_string());
        }

        // æŒ‰å¯ç”¨ç©ºé—´æ’åºï¼ˆç®€åŒ–ï¼šæŒ‰å·²ç”¨ç©ºé—´çš„ç™¾åˆ†æ¯”ï¼‰
        let mut sorted_nodes = online_nodes.clone();
        sorted_nodes.sort_by(|a, b| {
            let a_used_percent = a.used as f64 / a.capacity as f64;
            let b_used_percent = b.used as f64 / b.capacity as f64;
            a_used_percent.partial_cmp(&b_used_percent).unwrap()
        });

        // é€‰æ‹©å‰Nä¸ªèŠ‚ç‚¹
        let selected_count = std::cmp::min(count, sorted_nodes.len());
        let selected_nodes: Vec<_> = sorted_nodes.iter()
            .take(selected_count)
            .map(|n| n.id.clone())
            .collect();

        Ok(selected_nodes)
    }

    fn read_chunk(&self, chunk_id: &str, offset: u64, size: u64) -> Result<Vec<u8>, String> {
        println!("è¯»å–å—: {}, åç§»é‡: {}, å¤§å°: {}", chunk_id, offset, size);

        // è·å–å—ä¿¡æ¯
        let chunks = self.chunk_manager.chunks.read().unwrap();

        let chunk = chunks.get(chunk_id)
            .ok_or_else(|| format!("å—ä¸å­˜åœ¨: {}", chunk_id))?;

        if offset >= chunk.size {
            return Err(format!("åç§»é‡è¶…å‡ºå—å¤§å°: {} >= {}", offset, chunk.size));
        }

        // è®¡ç®—å®é™…è¯»å–å¤§å°
        let read_size = std::cmp::min(size, chunk.size - offset);

        // é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„å­˜å‚¨èŠ‚ç‚¹
        let nodes = self.storage_manager.storage_nodes.read().unwrap();

        let mut available_locations = Vec::new();
        for loc in &chunk.locations {
            if let Some(node) = nodes.get(loc) {
                if node.status == NodeStatus::Online {
                    available_locations.push(loc.clone());
                }
            }
        }

        if available_locations.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨çš„å­˜å‚¨èŠ‚ç‚¹æ¥è¯»å–æ•°æ®".to_string());
        }

        // ç®€å•åœ°é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨ä½ç½®
        let selected_node = &available_locations[0];

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»æ‰€é€‰çš„å­˜å‚¨èŠ‚ç‚¹è¯»å–æ•°æ®
        println!("ä»å­˜å‚¨èŠ‚ç‚¹ {} è¯»å–æ•°æ®", selected_node);

        // æ¨¡æ‹Ÿè¯»å–æ•°æ®
        let data = vec![0; read_size as usize];

        Ok(data)
    }

    fn read_file(&self, file_id: &str, offset: u64, size: u64) -> Result<Vec<u8>, String> {
        println!("è¯»å–æ–‡ä»¶: {}, åç§»é‡: {}, å¤§å°: {}", file_id, offset, size);

        // éªŒè¯æ–‡ä»¶
        let mut files = self.metadata_manager.files.write().unwrap();

        let file = files.get_mut(file_id)
            .ok_or_else(|| format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", file_id))?;

        // æ£€æŸ¥åç§»é‡
        if offset >= file.size {
            return Err(format!("åç§»é‡è¶…å‡ºæ–‡ä»¶å¤§å°: {} >= {}", offset, file.size));
        }

        // æ›´æ–°è®¿é—®æ—¶é—´
        file.accessed_at = Utc::now();

        // è®¡ç®—å®é™…è¯»å–å¤§å°
        let read_size = std::cmp::min(size, file.size - offset);

        // è®¡ç®—éœ€è¦è¯»å–çš„å—
        let chunk_size = self.chunk_manager.chunk_size;
        let start_chunk = offset / chunk_size;
        let end_chunk = (offset + read_size - 1) / chunk_size;

        let mut result = Vec::with_capacity(read_size as usize);

        for chunk_index in start_chunk..=end_chunk {
            if chunk_index as usize >= file.chunks.len() {
                break;
            }

            let chunk_id = &file.chunks[chunk_index as usize];

            // è®¡ç®—åœ¨å—å†…çš„åç§»é‡å’Œå¤§å°
            let chunk_offset = if chunk_index == start_chunk {
                offset % chunk_size
            } else {
                0
            };

            let remaining = read_size - result.len() as u64;
            let chunk_size_to_read = std::cmp::min(chunk_size - chunk_offset, remaining);

            // è¯»å–å—æ•°æ®
            let data = self.read_chunk(chunk_id, chunk_offset, chunk_size_to_read)?;

            // æ·»åŠ åˆ°ç»“æœ
            result.extend_from_slice(&data);
        }

        Ok(result)
    }

    fn delete_file(&self, file_id: &str) -> Result<(), String> {
        println!("åˆ é™¤æ–‡ä»¶: {}", file_id);

        // éªŒè¯æ–‡ä»¶
        let mut files = self.metadata_manager.files.write().unwrap();

        let file = files.get(file_id)
            .ok_or_else(|| format!("æ–‡ä»¶ä¸å­˜åœ¨: {}", file_id))?;

        // ä»çˆ¶ç›®å½•ä¸­ç§»é™¤
        let parent_id = file.parent_dir.clone();
        let mut directories = self.metadata_manager.directories.write().unwrap();

        if let Some(parent) = directories.get_mut(&parent_id) {
            parent.children.retain(|id| id != file_id);
            parent.modified_at = Utc::now();
        }

        // åˆ é™¤å—
        let mut chunks = self.chunk_manager.chunks.write().unwrap();
        for chunk_id in &file.chunks {
            chunks.remove(chunk_id);

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€šçŸ¥å­˜å‚¨èŠ‚ç‚¹åˆ é™¤å—æ•°æ®
        }

        // åˆ é™¤æ–‡ä»¶å…ƒæ•°æ®
        files.remove(file_id);

        println!("æ–‡ä»¶å·²åˆ é™¤: {}", file.path);

        Ok(())
    }
}

impl Clone for ChunkInfo {
    fn clone(&self) -> Self {
        ChunkInfo {
            id: self.id.clone(),
            file_id: self.file_id.clone(),
            offset: self.offset,
            size: self.size,
            locations: self.locations.clone(),
            checksum: self.checksum.clone(),
            created_at: self.created_at,
            modified_at: self.modified_at,
        }
    }
}

impl Clone for StorageNode {
    fn clone(&self) -> Self {
        StorageNode {
            id: self.id.clone(),
            address: self.address.clone(),
            capacity: self.capacity,
            used: self.used,
            status: self.status.clone(),
            last_heartbeat: self.last_heartbeat,
        }
    }
}

impl Clone for NodeStatus {
    fn clone(&self) -> Self {
        match self {
            NodeStatus::Online => NodeStatus::Online,
            NodeStatus::Offline => NodeStatus::Offline,
            NodeStatus::Maintenance => NodeStatus::Maintenance,
        }
    }
}
```

### 1.10 ç»¼åˆåº”ç”¨10-åˆ†å¸ƒå¼é…ç½®æœåŠ¡

```rust
// åˆ†å¸ƒå¼é…ç½®æœåŠ¡
struct DistributedConfigService {
    node_id: String,
    config_store: ConfigStore,
    watch_manager: ConfigWatchManager,
    version_manager: VersionManager,
    acl_manager: AccessControlManager,
    namespace_manager: NamespaceManager,
}

struct ConfigStore {
    items: RwLock<HashMap<String, ConfigItem>>,
}

struct ConfigItem {
    path: String,
    value: ConfigValue,
    metadata: ConfigMetadata,
}

enum ConfigValue {
    String(String),
    Number(f64),
    Boolean(bool),
    Object(HashMap<String, ConfigValue>),
    Array(Vec<ConfigValue>),
    Null,
}

struct ConfigMetadata {
    version: u64,
    created_at: DateTime<Utc>,
    modified_at: DateTime<Utc>,
    created_by: String,
    modified_by: String,
    namespace: String,
    labels: HashMap<String, String>,
    annotations: HashMap<String, String>,
}

struct ConfigWatchManager {
    watches: RwLock<HashMap<String, Vec<ConfigWatchEntry>>>,
}

struct ConfigWatchEntry {
    id: String,
    path: String,
    recursive: bool,
    callback: Box<dyn Fn(ConfigWatchEvent) -> Result<(), String> + Send + Sync>,
    created_at: DateTime<Utc>,
}

enum ConfigWatchEvent {
    Created { path: String, value: ConfigValue },
    Updated { path: String, old_value: Option<ConfigValue>, new_value: ConfigValue },
    Deleted { path: String, old_value: Option<ConfigValue> },
}

struct VersionManager {
    versions: RwLock<HashMap<String, Vec<ConfigVersion>>>,
    max_versions: usize,
}

struct ConfigVersion {
    path: String,
    version: u64,
    value: ConfigValue,
    metadata: ConfigMetadata,
    timestamp: DateTime<Utc>,
}

struct AccessControlManager {
    rules: RwLock<HashMap<String, Vec<AccessRule>>>,
}

struct AccessRule {
    path: String,
    namespace: String,
    principal: String,
    actions: HashSet<AccessAction>,
    effect: AccessEffect,
}

enum AccessAction {
    Read,
    Write,
    Delete,
    List,
    ChangeACL,
}

enum AccessEffect {
    Allow,
    Deny,
}

struct NamespaceManager {
    namespaces: RwLock<HashMap<String, NamespaceInfo>>,
}

struct NamespaceInfo {
    name: String,
    description: Option<String>,
    created_at: DateTime<Utc>,
    created_by: String,
    config_count: usize,
}

impl DistributedConfigService {
    fn new(node_id: &str) -> Self {
        let config_store = ConfigStore {
            items: RwLock::new(HashMap::new()),
        };

        let watch_manager = ConfigWatchManager {
            watches: RwLock::new(HashMap::new()),
        };

        let version_manager = VersionManager {
            versions: RwLock::new(HashMap::new()),
            max_versions: 10,
        };

        let acl_manager = AccessControlManager {
            rules: RwLock::new(HashMap::new()),
        };

        let namespace_manager = NamespaceManager {
            namespaces: RwLock::new(HashMap::new()),
        };

        DistributedConfigService {
            node_id: node_id.to_string(),
            config_store,
            watch_manager,
            version_manager,
            acl_manager,
            namespace_manager,
        }
    }

    fn start(&self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼é…ç½®æœåŠ¡");

        // åˆ›å»ºé»˜è®¤å‘½åç©ºé—´
        self.create_namespace("default", None, "system")?;

        // åˆ›å»ºé»˜è®¤è®¿é—®æ§åˆ¶è§„åˆ™
        self.create_access_rule(
            "/**",
            "default",
            "system",
            vec![AccessAction::Read, AccessAction::Write, AccessAction::Delete, AccessAction::List, AccessAction::ChangeACL],
            AccessEffect::Allow
        )?;

        Ok(())
    }

    fn create_namespace(&self, name: &str, description: Option<&str>, creator: &str) -> Result<(), String> {
        println!("åˆ›å»ºå‘½åç©ºé—´: {}", name);

        let mut namespaces = self.namespace_manager.namespaces.write().unwrap();

        if namespaces.contains_key(name) {
            return Err(format!("å‘½åç©ºé—´å·²å­˜åœ¨: {}", name));
        }

        let namespace = NamespaceInfo {
            name: name.to_string(),
            description: description.map(|s| s.to_string()),
            created_at: Utc::now(),
            created_by: creator.to_string(),
            config_count: 0,
        };

        namespaces.insert(name.to_string(), namespace);

        Ok(())
    }

    fn create_access_rule(
        &self,
        path: &str,
        namespace: &str,
        principal: &str,
        actions: Vec<AccessAction>,
        effect: AccessEffect
    ) -> Result<(), String> {
        println!("åˆ›å»ºè®¿é—®æ§åˆ¶è§„åˆ™: {}", path);

        // éªŒè¯å‘½åç©ºé—´
        let namespaces = self.namespace_manager.namespaces.read().unwrap();
        if !namespaces.contains_key(namespace) {
            return Err(format!("å‘½åç©ºé—´ä¸å­˜åœ¨: {}", namespace));
        }

        // åˆ›å»ºè§„åˆ™
        let rule = AccessRule {
            path: path.to_string(),
            namespace: namespace.to_string(),
            principal: principal.to_string(),
            actions: actions.into_iter().collect(),
            effect,
        };

        let mut rules = self.acl_manager.rules.write().unwrap();

        let path_rules = rules.entry(path.to_string())
            .or_insert_with(Vec::new);

        path_rules.push(rule);

        Ok(())
    }

    fn check_access(
        &self,
        path: &str,
        namespace: &str,
        principal: &str,
        action: AccessAction
    ) -> Result<bool, String> {
        // è¯»å–æ‰€æœ‰è§„åˆ™
        let rules = self.acl_manager.rules.read().unwrap();

        // åŒ¹é…è§„åˆ™
        let mut matched_rules = Vec::new();

        for (rule_path, path_rules) in rules.iter() {
            if self.path_matches(rule_path, path) {
                for rule in path_rules {
                    if (rule.namespace == namespace || rule.namespace == "*") &&
                       (rule.principal == principal || rule.principal == "*") &&
                       rule.actions.contains(&action) {
                        matched_rules.push(rule);
                    }
                }
            }
        }

        // æ’åºè§„åˆ™ï¼ˆä¼˜å…ˆçº§ï¼šå…·ä½“è·¯å¾„ > é€šé…è·¯å¾„ï¼ŒDeny > Allowï¼‰
        matched_rules.sort_by(|a, b| {
            // é¦–å…ˆæŒ‰è·¯å¾„å…·ä½“ç¨‹åº¦æ’åº
            let a_specificity = a.path.matches('*').count();
            let b_specificity = b.path.matches('*').count();

            let specificity_cmp = a_specificity.cmp(&b_specificity);

            if specificity_cmp != std::cmp::Ordering::Equal {
                return specificity_cmp;
            }

            // ç„¶åæŒ‰æ•ˆæœæ’åºï¼ˆDeny > Allowï¼‰
            match (&a.effect, &b.effect) {
                (AccessEffect::Deny, AccessEffect::Allow) => std::cmp::Ordering::Less,
                (AccessEffect::Allow, AccessEffect::Deny) => std::cmp::Ordering::Greater,
                _ => std::cmp::Ordering::Equal,
            }
        });

        // åº”ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„è§„åˆ™
        if let Some(rule) = matched_rules.first() {
            return Ok(matches!(rule.effect, AccessEffect::Allow));
        }

        // é»˜è®¤æ‹’ç»
        Ok(false)
    }

    fn path_matches(&self, pattern: &str, path: &str) -> bool {
        if pattern == path {
            return true;
        }

        // ç®€å•çš„é€šé…ç¬¦åŒ¹é…é€»è¾‘
        if pattern.contains('*') {
            let pattern_parts: Vec<_> = pattern.split('/').collect();
            let path_parts: Vec<_> = path.split('/').collect();

            if pattern == "/**" {
                return true;
            }

            if pattern_parts.len() > path_parts.len() {
                return false;
            }

            for (i, p) in pattern_parts.iter().enumerate() {
                if *p == "**" {
                    return true;
                }

                if *p == "*" {
                    continue;
                }

                if i >= path_parts.len() || *p != path_parts[i] {
                    return false;
                }
            }

            return pattern_parts.len() == path_parts.len() || pattern_parts.last() == Some(&"**");
        }

        false
    }

    fn set_config(
        &self,
        path: &str,
        value: ConfigValue,
        namespace: &str,
        principal: &str
    ) -> Result<u64, String> {
        println!("è®¾ç½®é…ç½®: {}", path);

        // éªŒè¯è·¯å¾„
        if !self.validate_path(path) {
            return Err(format!("æ— æ•ˆçš„é…ç½®è·¯å¾„: {}", path));
        }

        // æ£€æŸ¥è®¿é—®æƒé™
        let has_access = self.check_access(path, namespace, principal, AccessAction::Write)?;
        if !has_access {
            return Err(format!("æ²¡æœ‰å†™å…¥æƒé™: {}", path));
        }

        // éªŒè¯å‘½åç©ºé—´
        let mut namespaces = self.namespace_manager.namespaces.write().unwrap();
        let namespace_info = namespaces.get_mut(namespace)
            .ok_or_else(|| format!("å‘½åç©ºé—´ä¸å­˜åœ¨: {}", namespace))?;

        let now = Utc::now();
        let mut items = self.config_store.items.write().unwrap();

        // æ£€æŸ¥æ˜¯å¦æ›´æ–°ç°æœ‰é…ç½®æˆ–åˆ›å»ºæ–°é…ç½®
        let (is_new, new_version) = if let Some(item) = items.get_mut(path) {
            // æ›´æ–°ç°æœ‰é…ç½®
            let old_value = item.value.clone();
            let old_version = item.metadata.version;
            let new_version = old_version + 1;

            item.value = value.clone();
            item.metadata.version = new_version;
            item.metadata.modified_at = now;
            item.metadata.modified_by = principal.to_string();

            // ä¿å­˜ç‰ˆæœ¬å†å²
            self.save_version_history(path, old_version, &old_value, &item.metadata)?;

            // è§¦å‘æ›´æ–°äº‹ä»¶
            self.trigger_watch_event(ConfigWatchEvent::Updated {
                path: path.to_string(),
                old_value: Some(old_value),
                new_value: value.clone(),
            })?;

            (false, new_version)
        } else {
            // åˆ›å»ºæ–°é…ç½®
            let new_version = 1;

            let metadata = ConfigMetadata {
                version: new_version,
                created_at: now,
                modified_at: now,
                created_by: principal.to_string(),
                modified_by: principal.to_string(),
                namespace: namespace.to_string(),
                labels: HashMap::new(),
                annotations: HashMap::new(),
            };

            let item = ConfigItem {
                path: path.to_string(),
                value: value.clone(),
                metadata,
            };

            items.insert(path.to_string(), item);

            // æ›´æ–°å‘½åç©ºé—´é…ç½®è®¡æ•°
            namespace_info.config_count += 1;

            // è§¦å‘åˆ›å»ºäº‹ä»¶
            self.trigger_watch_event(ConfigWatchEvent::Created {
                path: path.to_string(),
                value: value.clone(),
            })?;

            (true, new_version)
        };

        println!("é…ç½®{}ï¼š{}, ç‰ˆæœ¬: {}", if is_new { "å·²åˆ›å»º" } else { "å·²æ›´æ–°" }, path, new_version);

        Ok(new_version)
    }

    fn validate_path(&self, path: &str) -> bool {
        if path.is_empty() || !path.starts_with('/') {
            return false;
        }

        // è·¯å¾„æ ¼å¼éªŒè¯
        let valid_chars = Regex::new(r"^[a-zA-Z0-9_\-./]+$").unwrap();
        if !valid_chars.is_match(path) {
            return false;
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­çš„æ–œæ 
        if path.contains("//") {
            return false;
        }

        true
    }

    fn save_version_history(
        &self,
        path: &str,
        version: u64,
        value: &ConfigValue,
        metadata: &ConfigMetadata
    ) -> Result<(), String> {
        let mut versions = self.version_manager.versions.write().unwrap();

        let path_versions = versions.entry(path.to_string())
            .or_insert_with(Vec::new);

        let config_version = ConfigVersion {
            path: path.to_string(),
            version,
            value: value.clone(),
            metadata: metadata.clone(),
            timestamp: Utc::now(),
        };

        path_versions.push(config_version);

        // é™åˆ¶ç‰ˆæœ¬å†å²æ•°é‡
        if path_versions.len() > self.version_manager.max_versions {
            path_versions.sort_by(|a, b| b.version.cmp(&a.version));
            path_versions.truncate(self.version_manager.max_versions);
        }

        Ok(())
    }

    fn get_config(
        &self,
        path: &str,
        namespace: &str,
        principal: &str
    ) -> Result<Option<ConfigItem>, String> {
        println!("è·å–é…ç½®: {}", path);

        // æ£€æŸ¥è®¿é—®æƒé™
        let has_access = self.check_access(path, namespace, principal, AccessAction::Read)?;
        if !has_access {
            return Err(format!("æ²¡æœ‰è¯»å–æƒé™: {}", path));
        }

        let items = self.config_store.items.read().unwrap();

        if let Some(item) = items.get(path) {
            if item.metadata.namespace == namespace {
                return Ok(Some(item.clone()));
            }
        }

        Ok(None)
    }

    fn get_config_version(
        &self,
        path: &str,
        version: u64,
        namespace: &str,
        principal: &str
    ) -> Result<Option<ConfigVersion>, String> {
        println!("è·å–é…ç½®ç‰ˆæœ¬: {}, ç‰ˆæœ¬: {}", path, version);

        // æ£€æŸ¥è®¿é—®æƒé™
        let has_access = self.check_access(path, namespace, principal, AccessAction::Read)?;
        if !has_access {
            return Err(format!("æ²¡æœ‰è¯»å–æƒé™: {}", path));
        }

        let versions = self.version_manager.versions.read().unwrap();

        if let Some(path_versions) = versions.get(path) {
            for ver in path_versions {
                if ver.version == version && ver.metadata.namespace == namespace {
                    return Ok(Some(ver.clone()));
                }
            }
        }

        Ok(None)
    }

    fn delete_config(
        &self,
        path: &str,
        namespace: &str,
        principal: &str
    ) -> Result<bool, String> {
        println!("åˆ é™¤é…ç½®: {}", path);

        // æ£€æŸ¥è®¿é—®æƒé™
        let has_access = self.check_access(path, namespace, principal, AccessAction::Delete)?;
        if !has_access {
            return Err(format!("æ²¡æœ‰åˆ é™¤æƒé™: {}", path));
        }

        let mut items = self.config_store.items.write().unwrap();

        if let Some(item) = items.get(path) {
            if item.metadata.namespace != namespace {
                return Ok(false);
            }

            let old_value = item.value.clone();

            // åˆ é™¤é…ç½®
            items.remove(path);

            // æ›´æ–°å‘½åç©ºé—´é…ç½®è®¡æ•°
            let mut namespaces = self.namespace_manager.namespaces.write().unwrap();
            if let Some(namespace_info) = namespaces.get_mut(namespace) {
                namespace_info.config_count -= 1;
            }

            // è§¦å‘åˆ é™¤äº‹ä»¶
            self.trigger_watch_event(ConfigWatchEvent::Deleted {
                path: path.to_string(),
                old_value: Some(old_value),
            })?;

            return Ok(true);
        }

        Ok(false)
    }

    fn list_config(
        &self,
        prefix: &str,
        namespace: &str,
        principal: &str
    ) -> Result<Vec<ConfigItem>, String> {
        println!("åˆ—å‡ºé…ç½®: å‰ç¼€ {}", prefix);

        // æ£€æŸ¥è®¿é—®æƒé™
        let has_access = self.check_access(prefix, namespace, principal, AccessAction::List)?;
        if !has_access {
            return Err(format!("æ²¡æœ‰åˆ—è¡¨æƒé™: {}", prefix));
        }

        let items = self.config_store.items.read().unwrap();

        let mut result = Vec::new();

        for (path, item) in items.iter() {
            if path.starts_with(prefix) && item.metadata.namespace == namespace {
                result.push(item.clone());
            }
        }

        Ok(result)
    }

    fn add_watch(
        &self,
        path: &str,
        recursive: bool,
        callback: Box<dyn Fn(ConfigWatchEvent) -> Result<(), String> + Send + Sync>
    ) -> Result<String, String> {
        println!("æ·»åŠ ç›‘è§†: {}, é€’å½’: {}", path, recursive);

        let watch_id = uuid::Uuid::new_v4().to_string();

        let watch = ConfigWatchEntry {
            id: watch_id.clone(),
            path: path.to_string(),
            recursive,
            callback,
            created_at: Utc::now(),
        };

        let mut watches = self.watch_manager.watches.write().unwrap();

        let path_watches = watches.entry(path.to_string())
            .or_insert_with(Vec::new);

        path_watches.push(watch);

        Ok(watch_id)
    }

    fn remove_watch(&self, watch_id: &str, path: &str) -> Result<bool, String> {
        println!("ç§»é™¤ç›‘è§†: {}, è·¯å¾„: {}", watch_id, path);

        let mut watches = self.watch_manager.watches.write().unwrap();

        if let Some(path_watches) = watches.get_mut(path) {
            let len_before = path_watches.len();
            path_watches.retain(|w| w.id != watch_id);

            let removed = len_before > path_watches.len();

            // å¦‚æœæ²¡æœ‰æ›´å¤šçš„ç›‘è§†ï¼Œç§»é™¤è·¯å¾„æ¡ç›®
            if path_watches.is_empty() {
                watches.remove(path);
            }

            return Ok(removed);
        }

        Ok(false)
    }

    fn trigger_watch_event(&self, event: ConfigWatchEvent) -> Result<(), String> {
        let path = match &event {
            ConfigWatchEvent::Created { path, .. } => path,
            ConfigWatchEvent::Updated { path, .. } => path,
            ConfigWatchEvent::Deleted { path, .. } => path,
        };

        let watches = self.watch_manager.watches.read().unwrap();

        // ç›´æ¥è·¯å¾„ç›‘è§†
        if let Some(path_watches) = watches.get(path) {
            for watch in path_watches {
                match (watch.callback)(event.clone()) {
                    Ok(_) => {},
                    Err(e) => println!("è§¦å‘ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                }
            }
        }

        // é€’å½’è·¯å¾„ç›‘è§†
        let path_parts: Vec<_> = path.split('/').collect();
        for i in 1..path_parts.len() {
            let parent_path = path_parts[..i].join("/");
            if parent_path.is_empty() {
                continue;
            }

            if let Some(parent_watches) = watches.get(&parent_path) {
                for watch in parent_watches {
                    if watch.recursive {
                        match (watch.callback)(event.clone()) {
                            Ok(_) => {},
                            Err(e) => println!("è§¦å‘é€’å½’ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                        }
                    }
                }
            }
        }

        Ok(())
    }
}

impl Clone for ConfigItem {
    fn clone(&self) -> Self {
        ConfigItem {
            path: self.path.clone(),
            value: self.value.clone(),
            metadata: self.metadata.clone(),
        }
    }
}

impl Clone for ConfigValue {
    fn clone(&self) -> Self {
        match self {
            ConfigValue::String(s) => ConfigValue::String(s.clone()),
            ConfigValue::Number(n) => ConfigValue::Number(*n),
            ConfigValue::Boolean(b) => ConfigValue::Boolean(*b),
            ConfigValue::Object(o) => ConfigValue::Object(o.clone()),
            ConfigValue::Array(a) => ConfigValue::Array(a.clone()),
            ConfigValue::Null => ConfigValue::Null,
        }
    }
}

impl Clone for ConfigMetadata {
    fn clone(&self) -> Self {
        ConfigMetadata {
            version: self.version,
            created_at: self.created_at,
            modified_at: self.modified_at,
            created_by: self.created_by.clone(),
            modified_by: self.modified_by.clone(),
            namespace: self.namespace.clone(),
            labels: self.labels.clone(),
            annotations: self.annotations.clone(),
        }
    }
}

impl Clone for ConfigWatchEvent {
    fn clone(&self) -> Self {
        match self {
            ConfigWatchEvent::Created { path, value } => ConfigWatchEvent::Created {
                path: path.clone(),
                value: value.clone(),
            },
            ConfigWatchEvent::Updated { path, old_value, new_value } => ConfigWatchEvent::Updated {
                path: path.clone(),
                old_value: old_value.clone(),
                new_value: new_value.clone(),
            },
            ConfigWatchEvent::Deleted { path, old_value } => ConfigWatchEvent::Deleted {
                path: path.clone(),
                old_value: old_value.clone(),
            },
        }
    }
}

impl Clone for ConfigVersion {
    fn clone(&self) -> Self {
        ConfigVersion {
            path: self.path.clone(),
            version: self.version,
            value: self.value.clone(),
            metadata: self.metadata.clone(),
            timestamp: self.timestamp,
        }
    }
}

// åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿ
struct DistributedCacheSystem {
    node_id: String,
    cache_manager: CacheManager,
    eviction_manager: EvictionManager,
    replication_manager: CacheReplicationManager,
    stats_manager: CacheStatsManager,
}

struct CacheManager {
    caches: RwLock<HashMap<String, Cache>>,
}

struct Cache {
    name: String,
    items: RwLock<HashMap<String, CacheItem>>,
    config: CacheConfig,
    stats: CacheStats,
}

struct CacheItem {
    key: String,
    value: Vec<u8>,
    created_at: DateTime<Utc>,
    expires_at: Option<DateTime<Utc>>,
    last_accessed: DateTime<Utc>,
    access_count: u64,
    size: usize,
}

struct CacheConfig {
    max_size: Option<usize>,
    max_items: Option<usize>,
    default_ttl: Option<Duration>,
    eviction_policy: EvictionPolicy,
    replication_factor: u32,
}

enum EvictionPolicy {
    LRU,
    LFU,
    FIFO,
    Random,
}

struct CacheStats {
    hit_count: AtomicU64,
    miss_count: AtomicU64,
    eviction_count: AtomicU64,
    total_size: AtomicUsize,
    item_count: AtomicUsize,
}

struct EvictionManager {
    running: AtomicBool,
    check_interval: Duration,
    eviction_thread: Option<JoinHandle<()>>,
}

struct CacheReplicationManager {
    replication_tasks: RwLock<Vec<CacheReplicationTask>>,
    running: AtomicBool,
    replication_thread: Option<JoinHandle<()>>,
}

struct CacheReplicationTask {
    cache_name: String,
    key: String,
    value: Vec<u8>,
    expires_at: Option<DateTime<Utc>>,
    target_nodes: Vec<String>,
    created_at: DateTime<Utc>,
}

struct CacheStatsManager {
    stats_snapshots: RwLock<HashMap<String, Vec<CacheStatsSnapshot>>>,
    running: AtomicBool,
    snapshot_interval: Duration,
    stats_thread: Option<JoinHandle<()>>,
}

struct CacheStatsSnapshot {
    cache_name: String,
    timestamp: DateTime<Utc>,
    hit_count: u64,
    miss_count: u64,
    eviction_count: u64,
    total_size: usize,
    item_count: usize,
}

impl DistributedCacheSystem {
    fn new(node_id: &str) -> Self {
        let cache_manager = CacheManager {
            caches: RwLock::new(HashMap::new()),
        };

        let eviction_manager = EvictionManager {
            running: AtomicBool::new(false),
            check_interval: Duration::from_secs(10),
            eviction_thread: None,
        };

        let replication_manager = CacheReplicationManager {
            replication_tasks: RwLock::new(Vec::new()),
            running: AtomicBool::new(false),
            replication_thread: None,
        };

        let stats_manager = CacheStatsManager {
            stats_snapshots: RwLock::new(HashMap::new()),
            running: AtomicBool::new(false),
            snapshot_interval: Duration::from_secs(60),
            stats_thread: None,
        };

        DistributedCacheSystem {
            node_id: node_id.to_string(),
            cache_manager,
            eviction_manager,
            replication_manager,
            stats_manager,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿ");

        // å¯åŠ¨é©±é€ç®¡ç†å™¨
        self.start_eviction_manager()?;

        // å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨
        self.start_replication_manager()?;

        // å¯åŠ¨ç»Ÿè®¡ç®¡ç†å™¨
        self.start_stats_manager()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿ");

        // åœæ­¢é©±é€ç®¡ç†å™¨
        self.eviction_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.eviction_manager.eviction_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("é©±é€çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢å¤åˆ¶ç®¡ç†å™¨
        self.replication_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.replication_manager.replication_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤åˆ¶çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢ç»Ÿè®¡ç®¡ç†å™¨
        self.stats_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.stats_manager.stats_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("ç»Ÿè®¡çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn start_eviction_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨é©±é€ç®¡ç†å™¨");

        let caches = self.cache_manager.caches.clone();
        let interval = self.eviction_manager.check_interval;

        self.eviction_manager.running.store(true, Ordering::SeqCst);

        let running = self.eviction_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥æ‰€æœ‰ç¼“å­˜çš„è¿‡æœŸé¡¹
                let cache_names: Vec<String> = {
                    let caches = caches.read().unwrap();
                    caches.keys().cloned().collect()
                };

                for cache_name in cache_names {
                    let caches = caches.read().unwrap();
                    if let Some(cache) = caches.get(&cache_name) {
                        // æ‰§è¡Œè¿‡æœŸé¡¹æ¸…ç†
                        let now = Utc::now();
                        let mut expired_keys = Vec::new();

                        {
                            let items = cache.items.read().unwrap();
                            for (key, item) in items.iter() {
                                if let Some(expires_at) = item.expires_at {
                                    if expires_at <= now {
                                        expired_keys.push(key.clone());
                                    }
                                }
                            }
                        }

                        if !expired_keys.is_empty() {
                            let mut items = cache.items.write().unwrap();
                            for key in &expired_keys {
                                if let Some(item) = items.remove(key) {
                                    // æ›´æ–°ç»Ÿè®¡
                                    cache.stats.eviction_count.fetch_add(1, Ordering::SeqCst);
                                    cache.stats.total_size.fetch_sub(item.size, Ordering::SeqCst);
                                    cache.stats.item_count.fetch_sub(1, Ordering::SeqCst);
                                }
                            }

                            println!("ä»ç¼“å­˜ {} ä¸­ç§»é™¤äº† {} ä¸ªè¿‡æœŸé¡¹", cache_name, expired_keys.len());
                        }

                        // æ£€æŸ¥å¤§å°é™åˆ¶
                        if let Some(max_size) = cache.config.max_size {
                            let current_size = cache.stats.total_size.load(Ordering::SeqCst);

                            if current_size > max_size {
                                // éœ€è¦é©±é€
                                let mut items = cache.items.write().unwrap();
                                let mut item_list: Vec<_> = items.iter().collect();

                                // æ ¹æ®é©±é€ç­–ç•¥æ’åº
                                match cache.config.eviction_policy {
                                    EvictionPolicy::LRU => {
                                        item_list.sort_by(|a, b| a.1.last_accessed.cmp(&b.1.last_accessed));
                                    },
                                    EvictionPolicy::LFU => {
                                        item_list.sort_by(|a, b| a.1.access_count.cmp(&b.1.access_count));
                                    },
                                    EvictionPolicy::FIFO => {
                                        item_list.sort_by(|a, b| a.1.created_at.cmp(&b.1.created_at));
                                    },
                                    EvictionPolicy::Random => {
                                        let mut rng = rand::thread_rng();
                                        item_list.shuffle(&mut rng);
                                    },
                                }

                                let mut size_to_remove = current_size - max_size + max_size / 10; // å¤šç§»é™¤ä¸€äº›ï¼Œé¿å…é¢‘ç¹é©±é€
                                let mut removed_count = 0;

                                while size_to_remove > 0 && !item_list.is_empty() {
                                    let (key, _) = item_list.remove(0);

                                    if let Some(item) = items.remove(key) {
                                        size_to_remove = size_to_remove.saturating_sub(item.size);

                                        // æ›´æ–°ç»Ÿè®¡
                                        cache.stats.eviction_count.fetch_add(1, Ordering::SeqCst);
                                        cache.stats.total_size.fetch_sub(item.size, Ordering::SeqCst);
                                        cache.stats.item_count.fetch_sub(1, Ordering::SeqCst);

                                        removed_count += 1;
                                    }
                                }

                                println!("ä»ç¼“å­˜ {} ä¸­é©±é€äº† {} ä¸ªé¡¹ç›®ä»¥æ»¡è¶³å¤§å°é™åˆ¶", cache_name, removed_count);
                            }
                        }

                        // æ£€æŸ¥é¡¹ç›®æ•°é‡é™åˆ¶
                        if let Some(max_items) = cache.config.max_items {
                            let current_items = cache.stats.item_count.load(Ordering::SeqCst);

                            if current_items > max_items {
                                // éœ€è¦é©±é€
                                let mut items = cache.items.write().unwrap();
                                let mut item_list: Vec<_> = items.iter().collect();

                                // æ ¹æ®é©±é€ç­–ç•¥æ’åº
                                match cache.config.eviction_policy {
                                    EvictionPolicy::LRU => {
                                        item_list.sort_by(|a, b| a.1.last_accessed.cmp(&b.1.last_accessed));
                                    },
                                    EvictionPolicy::LFU => {
                                        item_list.sort_by(|a, b| a.1.access_count.cmp(&b.1.access_count));
                                    },
                                    EvictionPolicy::FIFO => {
                                        item_list.sort_by(|a, b| a.1.created_at.cmp(&b.1.created_at));
                                    },
                                    EvictionPolicy::Random => {
                                        let mut rng = rand::thread_rng();
                                        item_list.shuffle(&mut rng);
                                    },
                                }

                                let items_to_remove = current_items - max_items + max_items / 10; // å¤šç§»é™¤ä¸€äº›ï¼Œé¿å…é¢‘ç¹é©±é€

                                for i in 0..std::cmp::min(items_to_remove, item_list.len()) {
                                    let (key, _) = item_list[i];

                                    if let Some(item) = items.remove(key) {
                                        // æ›´æ–°ç»Ÿè®¡
                                        cache.stats.eviction_count.fetch_add(1, Ordering::SeqCst);
                                        cache.stats.total_size.fetch_sub(item.size, Ordering::SeqCst);
                                        cache.stats.item_count.fetch_sub(1, Ordering::SeqCst);
                                    }
                                }

                                println!("ä»ç¼“å­˜ {} ä¸­é©±é€äº† {} ä¸ªé¡¹ç›®ä»¥æ»¡è¶³æ•°é‡é™åˆ¶", cache_name, items_to_remove);
                            }
                        }
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.eviction_manager.eviction_thread = Some(thread);

        Ok(())
    }

    fn start_replication_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨");

        let replication_tasks = self

```rust
        let replication_tasks = self.replication_manager.replication_tasks.clone();

        self.replication_manager.running.store(true, Ordering::SeqCst);

        let running = self.replication_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // å¤„ç†å¤åˆ¶ä»»åŠ¡
                let tasks_to_process = {
                    let mut tasks = replication_tasks.write().unwrap();
                    let result = tasks.clone();
                    tasks.clear();
                    result
                };

                for task in tasks_to_process {
                    println!("å¤„ç†ç¼“å­˜å¤åˆ¶ä»»åŠ¡: {} -> {:?}", task.key, task.target_nodes);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†ç¼“å­˜é¡¹å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹
                    // ç®€åŒ–ï¼šå‡è®¾å¤åˆ¶æˆåŠŸ

                    println!("ç¼“å­˜å¤åˆ¶å®Œæˆ: {}", task.key);
                }

                // ä¼‘çœ ä¸€ä¼šå„¿
                thread::sleep(Duration::from_millis(100));
            }
        });

        self.replication_manager.replication_thread = Some(thread);

        Ok(())
    }

    fn start_stats_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ç»Ÿè®¡ç®¡ç†å™¨");

        let caches = self.cache_manager.caches.clone();
        let stats_snapshots = self.stats_manager.stats_snapshots.clone();
        let interval = self.stats_manager.snapshot_interval;

        self.stats_manager.running.store(true, Ordering::SeqCst);

        let running = self.stats_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // ä¸ºæ‰€æœ‰ç¼“å­˜åˆ›å»ºç»Ÿè®¡å¿«ç…§
                let now = Utc::now();

                let cache_names: Vec<String> = {
                    let caches = caches.read().unwrap();
                    caches.keys().cloned().collect()
                };

                for cache_name in cache_names {
                    let caches = caches.read().unwrap();
                    if let Some(cache) = caches.get(&cache_name) {
                        let snapshot = CacheStatsSnapshot {
                            cache_name: cache_name.clone(),
                            timestamp: now,
                            hit_count: cache.stats.hit_count.load(Ordering::SeqCst),
                            miss_count: cache.stats.miss_count.load(Ordering::SeqCst),
                            eviction_count: cache.stats.eviction_count.load(Ordering::SeqCst),
                            total_size: cache.stats.total_size.load(Ordering::SeqCst),
                            item_count: cache.stats.item_count.load(Ordering::SeqCst),
                        };

                        let mut snapshots = stats_snapshots.write().unwrap();
                        let cache_snapshots = snapshots.entry(cache_name.clone())
                            .or_insert_with(Vec::new);

                        cache_snapshots.push(snapshot);

                        // é™åˆ¶å¿«ç…§æ•°é‡
                        if cache_snapshots.len() > 100 {
                            cache_snapshots.sort_by(|a, b| a.timestamp.cmp(&b.timestamp));
                            cache_snapshots.truncate(100);
                        }
                    }
                }

                // ä¼‘çœ åˆ°ä¸‹ä¸€ä¸ªå¿«ç…§æ—¶é—´
                thread::sleep(interval);
            }
        });

        self.stats_manager.stats_thread = Some(thread);

        Ok(())
    }

    fn create_cache(&self, name: &str, config: CacheConfig) -> Result<(), String> {
        println!("åˆ›å»ºç¼“å­˜: {}", name);

        let mut caches = self.cache_manager.caches.write().unwrap();

        if caches.contains_key(name) {
            return Err(format!("ç¼“å­˜å·²å­˜åœ¨: {}", name));
        }

        let stats = CacheStats {
            hit_count: AtomicU64::new(0),
            miss_count: AtomicU64::new(0),
            eviction_count: AtomicU64::new(0),
            total_size: AtomicUsize::new(0),
            item_count: AtomicUsize::new(0),
        };

        let cache = Cache {
            name: name.to_string(),
            items: RwLock::new(HashMap::new()),
            config,
            stats,
        };

        caches.insert(name.to_string(), cache);

        // åˆå§‹åŒ–ç»Ÿè®¡å¿«ç…§
        let mut snapshots = self.stats_manager.stats_snapshots.write().unwrap();
        snapshots.insert(name.to_string(), Vec::new());

        Ok(())
    }

    fn put(
        &self,
        cache_name: &str,
        key: &str,
        value: Vec<u8>,
        ttl: Option<Duration>
    ) -> Result<(), String> {
        println!("è®¾ç½®ç¼“å­˜: {}/{}", cache_name, key);

        let caches = self.cache_manager.caches.read().unwrap();

        let cache = caches.get(cache_name)
            .ok_or_else(|| format!("ç¼“å­˜ä¸å­˜åœ¨: {}", cache_name))?;

        let now = Utc::now();
        let expires_at = ttl.map(|duration| now + duration);

        // æ£€æŸ¥æ˜¯å¦æ›´æ–°ç°æœ‰é¡¹æˆ–åˆ›å»ºæ–°é¡¹
        let size = value.len();
        let mut items = cache.items.write().unwrap();

        if let Some(existing_item) = items.get_mut(key) {
            // æ›´æ–°ç°æœ‰é¡¹
            let old_size = existing_item.size;

            existing_item.value = value.clone();
            existing_item.last_accessed = now;
            existing_item.expires_at = expires_at;
            existing_item.size = size;

            // æ›´æ–°ç»Ÿè®¡
            if old_size != size {
                let size_diff = if size > old_size {
                    size - old_size
                } else {
                    0
                };

                let size_reduction = if old_size > size {
                    old_size - size
                } else {
                    0
                };

                if size_diff > 0 {
                    cache.stats.total_size.fetch_add(size_diff, Ordering::SeqCst);
                }

                if size_reduction > 0 {
                    cache.stats.total_size.fetch_sub(size_reduction, Ordering::SeqCst);
                }
            }
        } else {
            // åˆ›å»ºæ–°é¡¹
            let item = CacheItem {
                key: key.to_string(),
                value: value.clone(),
                created_at: now,
                expires_at,
                last_accessed: now,
                access_count: 0,
                size,
            };

            items.insert(key.to_string(), item);

            // æ›´æ–°ç»Ÿè®¡
            cache.stats.total_size.fetch_add(size, Ordering::SeqCst);
            cache.stats.item_count.fetch_add(1, Ordering::SeqCst);
        }

        // åˆ›å»ºå¤åˆ¶ä»»åŠ¡
        if cache.config.replication_factor > 1 {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€‰æ‹©ç›®æ ‡èŠ‚ç‚¹
            let target_nodes = vec![
                format!("node-{}", uuid::Uuid::new_v4().to_string()),
                format!("node-{}", uuid::Uuid::new_v4().to_string()),
            ];

            let replication_task = CacheReplicationTask {
                cache_name: cache_name.to_string(),
                key: key.to_string(),
                value,
                expires_at,
                target_nodes,
                created_at: now,
            };

            let mut tasks = self.replication_manager.replication_tasks.write().unwrap();
            tasks.push(replication_task);
        }

        Ok(())
    }

    fn get(&self, cache_name: &str, key: &str) -> Result<Option<Vec<u8>>, String> {
        let caches = self.cache_manager.caches.read().unwrap();

        let cache = caches.get(cache_name)
            .ok_or_else(|| format!("ç¼“å­˜ä¸å­˜åœ¨: {}", cache_name))?;

        let mut items = cache.items.write().unwrap();

        if let Some(item) = items.get_mut(key) {
            // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            let now = Utc::now();
            if let Some(expires_at) = item.expires_at {
                if expires_at <= now {
                    // é¡¹ç›®å·²è¿‡æœŸï¼Œç§»é™¤å®ƒ
                    let removed_item = items.remove(key).unwrap();

                    // æ›´æ–°ç»Ÿè®¡
                    cache.stats.miss_count.fetch_add(1, Ordering::SeqCst);
                    cache.stats.total_size.fetch_sub(removed_item.size, Ordering::SeqCst);
                    cache.stats.item_count.fetch_sub(1, Ordering::SeqCst);

                    return Ok(None);
                }
            }

            // æ›´æ–°è®¿é—®ç»Ÿè®¡
            item.last_accessed = now;
            item.access_count += 1;

            // æ›´æ–°ç¼“å­˜å‘½ä¸­ç»Ÿè®¡
            cache.stats.hit_count.fetch_add(1, Ordering::SeqCst);

            return Ok(Some(item.value.clone()));
        }

        // ç¼“å­˜æœªå‘½ä¸­
        cache.stats.miss_count.fetch_add(1, Ordering::SeqCst);

        Ok(None)
    }

    fn remove(&self, cache_name: &str, key: &str) -> Result<bool, String> {
        println!("ç§»é™¤ç¼“å­˜: {}/{}", cache_name, key);

        let caches = self.cache_manager.caches.read().unwrap();

        let cache = caches.get(cache_name)
            .ok_or_else(|| format!("ç¼“å­˜ä¸å­˜åœ¨: {}", cache_name))?;

        let mut items = cache.items.write().unwrap();

        if let Some(item) = items.remove(key) {
            // æ›´æ–°ç»Ÿè®¡
            cache.stats.total_size.fetch_sub(item.size, Ordering::SeqCst);
            cache.stats.item_count.fetch_sub(1, Ordering::SeqCst);

            return Ok(true);
        }

        Ok(false)
    }

    fn clear(&self, cache_name: &str) -> Result<usize, String> {
        println!("æ¸…ç©ºç¼“å­˜: {}", cache_name);

        let caches = self.cache_manager.caches.read().unwrap();

        let cache = caches.get(cache_name)
            .ok_or_else(|| format!("ç¼“å­˜ä¸å­˜åœ¨: {}", cache_name))?;

        let mut items = cache.items.write().unwrap();

        let item_count = items.len();

        if item_count > 0 {
            // æ›´æ–°ç»Ÿè®¡
            cache.stats.total_size.store(0, Ordering::SeqCst);
            cache.stats.item_count.store(0, Ordering::SeqCst);

            items.clear();
        }

        Ok(item_count)
    }

    fn get_stats(&self, cache_name: &str) -> Result<CacheStatsSnapshot, String> {
        let caches = self.cache_manager.caches.read().unwrap();

        let cache = caches.get(cache_name)
            .ok_or_else(|| format!("ç¼“å­˜ä¸å­˜åœ¨: {}", cache_name))?;

        let snapshot = CacheStatsSnapshot {
            cache_name: cache_name.to_string(),
            timestamp: Utc::now(),
            hit_count: cache.stats.hit_count.load(Ordering::SeqCst),
            miss_count: cache.stats.miss_count.load(Ordering::SeqCst),
            eviction_count: cache.stats.eviction_count.load(Ordering::SeqCst),
            total_size: cache.stats.total_size.load(Ordering::SeqCst),
            item_count: cache.stats.item_count.load(Ordering::SeqCst),
        };

        Ok(snapshot)
    }

    fn get_stats_history(&self, cache_name: &str) -> Result<Vec<CacheStatsSnapshot>, String> {
        let snapshots = self.stats_manager.stats_snapshots.read().unwrap();

        let cache_snapshots = snapshots.get(cache_name)
            .ok_or_else(|| format!("ç¼“å­˜ä¸å­˜åœ¨æˆ–æ²¡æœ‰ç»Ÿè®¡å†å²: {}", cache_name))?;

        Ok(cache_snapshots.clone())
    }
}

impl Clone for CacheReplicationTask {
    fn clone(&self) -> Self {
        CacheReplicationTask {
            cache_name: self.cache_name.clone(),
            key: self.key.clone(),
            value: self.value.clone(),
            expires_at: self.expires_at,
            target_nodes: self.target_nodes.clone(),
            created_at: self.created_at,
        }
    }
}

impl Clone for CacheStatsSnapshot {
    fn clone(&self) -> Self {
        CacheStatsSnapshot {
            cache_name: self.cache_name.clone(),
            timestamp: self.timestamp,
            hit_count: self.hit_count,
            miss_count: self.miss_count,
            eviction_count: self.eviction_count,
            total_size: self.total_size,
            item_count: self.item_count,
        }
    }
}

// æœåŠ¡æ³¨å†Œä¸å‘ç°
struct ServiceRegistry {
    node_id: String,
    services: RwLock<HashMap<String, ServiceInfo>>,
    instances: RwLock<HashMap<String, HashMap<String, ServiceInstance>>>,
    health_checker: HealthChecker,
    watchers: RwLock<HashMap<String, Vec<ServiceWatcher>>>,
}

struct ServiceInfo {
    id: String,
    name: String,
    description: Option<String>,
    metadata: HashMap<String, String>,
    created_at: DateTime<Utc>,
}

struct ServiceInstance {
    id: String,
    service_id: String,
    host: String,
    port: u16,
    address: String,
    status: ServiceStatus,
    metadata: HashMap<String, String>,
    health_check: Option<HealthCheck>,
    last_heartbeat: DateTime<Utc>,
    registered_at: DateTime<Utc>,
}

enum ServiceStatus {
    UP,
    DOWN,
    STARTING,
    OUT_OF_SERVICE,
    UNKNOWN,
}

struct HealthCheck {
    check_type: HealthCheckType,
    interval: Duration,
    timeout: Duration,
    path: Option<String>,
    port: Option<u16>,
}

enum HealthCheckType {
    HTTP,
    TCP,
    SCRIPT,
    TTL,
}

struct HealthChecker {
    running: AtomicBool,
    check_interval: Duration,
    checker_thread: Option<JoinHandle<()>>,
}

struct ServiceWatcher {
    id: String,
    service_id: String,
    callback: Box<dyn Fn(ServiceEvent) -> Result<(), String> + Send + Sync>,
    created_at: DateTime<Utc>,
}

enum ServiceEvent {
    InstanceRegistered { service_id: String, instance_id: String },
    InstanceDeregistered { service_id: String, instance_id: String },
    InstanceChanged { service_id: String, instance_id: String, old_status: ServiceStatus, new_status: ServiceStatus },
}

impl ServiceRegistry {
    fn new(node_id: &str) -> Self {
        let health_checker = HealthChecker {
            running: AtomicBool::new(false),
            check_interval: Duration::from_secs(10),
            checker_thread: None,
        };

        ServiceRegistry {
            node_id: node_id.to_string(),
            services: RwLock::new(HashMap::new()),
            instances: RwLock::new(HashMap::new()),
            health_checker,
            watchers: RwLock::new(HashMap::new()),
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æœåŠ¡æ³¨å†Œè¡¨");

        // å¯åŠ¨å¥åº·æ£€æŸ¥å™¨
        self.start_health_checker()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢æœåŠ¡æ³¨å†Œè¡¨");

        // åœæ­¢å¥åº·æ£€æŸ¥å™¨
        self.health_checker.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.health_checker.checker_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¥åº·æ£€æŸ¥çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn start_health_checker(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¥åº·æ£€æŸ¥å™¨");

        let instances = self.instances.clone();
        let watchers = self.watchers.clone();
        let interval = self.health_checker.check_interval;

        self.health_checker.running.store(true, Ordering::SeqCst);

        let running = self.health_checker.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥æ‰€æœ‰å®ä¾‹çš„å¥åº·çŠ¶æ€
                let now = Utc::now();

                // è·å–æ‰€æœ‰æœåŠ¡å’Œå®ä¾‹
                let services_instances: HashMap<String, HashMap<String, ServiceInstance>> = {
                    let instances = instances.read().unwrap();
                    instances.clone()
                };

                for (service_id, service_instances) in &services_instances {
                    for (instance_id, instance) in service_instances {
                        // æ£€æŸ¥TTLå¥åº·æ£€æŸ¥
                        if let Some(health_check) = &instance.health_check {
                            if matches!(health_check.check_type, HealthCheckType::TTL) {
                                let heartbeat_timeout = instance.last_heartbeat + health_check.interval + health_check.timeout;

                                if heartbeat_timeout < now && instance.status != ServiceStatus::DOWN {
                                    // å®ä¾‹è¶…æ—¶ï¼Œæ ‡è®°ä¸ºDOWN
                                    let mut instances_map = instances.write().unwrap();

                                    if let Some(instances) = instances_map.get_mut(service_id) {
                                        if let Some(instance) = instances.get_mut(instance_id) {
                                            let old_status = instance.status.clone();
                                            instance.status = ServiceStatus::DOWN;

                                            // è§¦å‘çŠ¶æ€å˜æ›´äº‹ä»¶
                                            ServiceRegistry::trigger_instance_changed_event(
                                                &watchers,
                                                service_id,
                                                instance_id,
                                                old_status,
                                                ServiceStatus::DOWN,
                                            );

                                            println!("å®ä¾‹å¥åº·æ£€æŸ¥å¤±è´¥: {}/{}", service_id, instance_id);
                                        }
                                    }
                                }
                            }
                        }

                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡ŒHTTPã€TCPã€è„šæœ¬ç­‰å¥åº·æ£€æŸ¥
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.health_checker.checker_thread = Some(thread);

        Ok(())
    }

    fn trigger_instance_changed_event(
        watchers: &RwLock<HashMap<String, Vec<ServiceWatcher>>>,
        service_id: &str,
        instance_id: &str,
        old_status: ServiceStatus,
        new_status: ServiceStatus,
    ) {
        let watchers_map = watchers.read().unwrap();

        if let Some(service_watchers) = watchers_map.get(service_id) {
            let event = ServiceEvent::InstanceChanged {
                service_id: service_id.to_string(),
                instance_id: instance_id.to_string(),
                old_status,
                new_status: new_status.clone(),
            };

            for watcher in service_watchers {
                match (watcher.callback)(event.clone()) {
                    Ok(_) => {},
                    Err(e) => println!("æœåŠ¡ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                }
            }
        }
    }

    fn register_service(&self, service: ServiceInfo) -> Result<(), String> {
        println!("æ³¨å†ŒæœåŠ¡: {}", service.name);

        let mut services = self.services.write().unwrap();

        if services.contains_key(&service.id) {
            return Err(format!("æœåŠ¡å·²å­˜åœ¨: {}", service.id));
        }

        services.insert(service.id.clone(), service);

        // åˆ›å»ºå®ä¾‹æ˜ å°„
        let mut instances = self.instances.write().unwrap();
        instances.insert(service.id.clone(), HashMap::new());

        // åˆ›å»ºç›‘è§†å™¨æ˜ å°„
        let mut watchers = self.watchers.write().unwrap();
        watchers.insert(service.id.clone(), Vec::new());

        Ok(())
    }

    fn deregister_service(&self, service_id: &str) -> Result<(), String> {
        println!("æ³¨é”€æœåŠ¡: {}", service_id);

        // æ£€æŸ¥æœåŠ¡æ˜¯å¦å­˜åœ¨
        let mut services = self.services.write().unwrap();

        if !services.contains_key(service_id) {
            return Err(format!("æœåŠ¡ä¸å­˜åœ¨: {}", service_id));
        }

        // ç§»é™¤æ‰€æœ‰å®ä¾‹
        let mut instances = self.instances.write().unwrap();
        instances.remove(service_id);

        // ç§»é™¤æ‰€æœ‰ç›‘è§†å™¨
        let mut watchers = self.watchers.write().unwrap();
        watchers.remove(service_id);

        // ç§»é™¤æœåŠ¡
        services.remove(service_id);

        Ok(())
    }

    fn register_instance(&self, instance: ServiceInstance) -> Result<(), String> {
        println!("æ³¨å†Œå®ä¾‹: {}/{}", instance.service_id, instance.id);

        // æ£€æŸ¥æœåŠ¡æ˜¯å¦å­˜åœ¨
        let services = self.services.read().unwrap();

        if !services.contains_key(&instance.service_id) {
            return Err(format!("æœåŠ¡ä¸å­˜åœ¨: {}", instance.service_id));
        }

        let mut instances = self.instances.write().unwrap();

        let service_instances = instances.get_mut(&instance.service_id)
            .ok_or_else(|| format!("æœåŠ¡å®ä¾‹æ˜ å°„ä¸å­˜åœ¨: {}", instance.service_id))?;

        // æ·»åŠ å®ä¾‹
        service_instances.insert(instance.id.clone(), instance.clone());

        // è§¦å‘å®ä¾‹æ³¨å†Œäº‹ä»¶
        let watchers = self.watchers.read().unwrap();

        if let Some(service_watchers) = watchers.get(&instance.service_id) {
            let event = ServiceEvent::InstanceRegistered {
                service_id: instance.service_id.clone(),
                instance_id: instance.id.clone(),
            };

            for watcher in service_watchers {
                match (watcher.callback)(event.clone()) {
                    Ok(_) => {},
                    Err(e) => println!("æœåŠ¡ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                }
            }
        }

        Ok(())
    }

    fn deregister_instance(&self, service_id: &str, instance_id: &str) -> Result<(), String> {
        println!("æ³¨é”€å®ä¾‹: {}/{}", service_id, instance_id);

        let mut instances = self.instances.write().unwrap();

        let service_instances = instances.get_mut(service_id)
            .ok_or_else(|| format!("æœåŠ¡ä¸å­˜åœ¨: {}", service_id))?;

        if !service_instances.contains_key(instance_id) {
            return Err(format!("å®ä¾‹ä¸å­˜åœ¨: {}/{}", service_id, instance_id));
        }

        // ç§»é™¤å®ä¾‹
        service_instances.remove(instance_id);

        // è§¦å‘å®ä¾‹æ³¨é”€äº‹ä»¶
        let watchers = self.watchers.read().unwrap();

        if let Some(service_watchers) = watchers.get(service_id) {
            let event = ServiceEvent::InstanceDeregistered {
                service_id: service_id.to_string(),
                instance_id: instance_id.to_string(),
            };

            for watcher in service_watchers {
                match (watcher.callback)(event.clone()) {
                    Ok(_) => {},
                    Err(e) => println!("æœåŠ¡ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                }
            }
        }

        Ok(())
    }

    fn heartbeat(&self, service_id: &str, instance_id: &str) -> Result<(), String> {
        let mut instances = self.instances.write().unwrap();

        let service_instances = instances.get_mut(service_id)
            .ok_or_else(|| format!("æœåŠ¡ä¸å­˜åœ¨: {}", service_id))?;

        let instance = service_instances.get_mut(instance_id)
            .ok_or_else(|| format!("å®ä¾‹ä¸å­˜åœ¨: {}/{}", service_id, instance_id))?;

        let now = Utc::now();
        instance.last_heartbeat = now;

        // å¦‚æœå®ä¾‹å½“å‰ä¸ºDOWNçŠ¶æ€ï¼Œæ›´æ–°ä¸ºUP
        if instance.status == ServiceStatus::DOWN {
            let old_status = instance.status.clone();
            instance.status = ServiceStatus::UP;

            // è§¦å‘çŠ¶æ€å˜æ›´äº‹ä»¶
            drop(instances);
            let watchers = self.watchers.read().unwrap();

            if let Some(service_watchers) = watchers.get(service_id) {
                let event = ServiceEvent::InstanceChanged {
                    service_id: service_id.to_string(),
                    instance_id: instance_id.to_string(),
                    old_status,
                    new_status: ServiceStatus::UP,
                };

                for watcher in service_watchers {
                    match (watcher.callback)(event.clone()) {
                        Ok(_) => {},
                        Err(e) => println!("æœåŠ¡ç›‘è§†å›è°ƒå¤±è´¥: {}", e),
                    }
                }
            }
        }

        Ok(())
    }

    fn get_services(&self) -> Vec<ServiceInfo> {
        let services = self.services.read().unwrap();
        services.values().cloned().collect()
    }

    fn get_service(&self, service_id: &str) -> Option<ServiceInfo> {
        let services = self.services.read().unwrap();
        services.get(service_id).cloned()
    }

    fn get_instances(&self, service_id: &str) -> Result<Vec<ServiceInstance>, String> {
        let instances = self.instances.read().unwrap();

        let service_instances = instances.get(service_id)
            .ok_or_else(|| format!("æœåŠ¡ä¸å­˜åœ¨: {}", service_id))?;

        Ok(service_instances.values().cloned().collect())
    }

    fn get_instance(&self, service_id: &str, instance_id: &str) -> Result<Option<ServiceInstance>, String> {
        let instances = self.instances.read().unwrap();

        let service_instances = instances.get(service_id)
            .ok_or_else(|| format!("æœåŠ¡ä¸å­˜åœ¨: {}", service_id))?;

        Ok(service_instances.get(instance_id).cloned())
    }

    fn watch_service(
        &self,
        service_id: &str,
        callback: Box<dyn Fn(ServiceEvent) -> Result<(), String> + Send + Sync>
    ) -> Result<String, String> {
        println!("ç›‘è§†æœåŠ¡: {}", service_id);

        // æ£€æŸ¥æœåŠ¡æ˜¯å¦å­˜åœ¨
        let services = self.services.read().unwrap();

        if !services.contains_key(service_id) {
            return Err(format!("æœåŠ¡ä¸å­˜åœ¨: {}", service_id));
        }

        let watcher_id = uuid::Uuid::new_v4().to_string();

        let watcher = ServiceWatcher {
            id: watcher_id.clone(),
            service_id: service_id.to_string(),
            callback,
            created_at: Utc::now(),
        };

        let mut watchers = self.watchers.write().unwrap();

        let service_watchers = watchers.get_mut(service_id)
            .ok_or_else(|| format!("æœåŠ¡ç›‘è§†å™¨æ˜ å°„ä¸å­˜åœ¨: {}", service_id))?;

        service_watchers.push(watcher);

        Ok(watcher_id)
    }

    fn unwatch_service(&self, service_id: &str, watcher_id: &str) -> Result<bool, String> {
        println!("å–æ¶ˆç›‘è§†æœåŠ¡: {}/{}", service_id, watcher_id);

        let mut watchers = self.watchers.write().unwrap();

        let service_watchers = watchers.get_mut(service_id)
            .ok_or_else(|| format!("æœåŠ¡ç›‘è§†å™¨æ˜ å°„ä¸å­˜åœ¨: {}", service_id))?;

        let len_before = service_watchers.len();
        service_watchers.retain(|w| w.id != watcher_id);

        Ok(len_before > service_watchers.len())
    }
}

impl Clone for ServiceInfo {
    fn clone(&self) -> Self {
        ServiceInfo {
            id: self.id.clone(),
            name: self.name.clone(),
            description: self.description.clone(),
            metadata: self.metadata.clone(),
            created_at: self.created_at,
        }
    }
}

impl Clone for ServiceInstance {
    fn clone(&self) -> Self {
        ServiceInstance {
            id: self.id.clone(),
            service_id: self.service_id.clone(),
            host: self.host.clone(),
            port: self.port,
            address: self.address.clone(),
            status: self.status.clone(),
            metadata: self.metadata.clone(),
            health_check: self.health_check.clone(),
            last_heartbeat: self.last_heartbeat,
            registered_at: self.registered_at,
        }
    }
}

impl Clone for ServiceStatus {
    fn clone(&self) -> Self {
        match self {
            ServiceStatus::UP => ServiceStatus::UP,
            ServiceStatus::DOWN => ServiceStatus::DOWN,
            ServiceStatus::STARTING => ServiceStatus::STARTING,
            ServiceStatus::OUT_OF_SERVICE => ServiceStatus::OUT_OF_SERVICE,
            ServiceStatus::UNKNOWN => ServiceStatus::UNKNOWN,
        }
    }
}

impl Clone for HealthCheck {
    fn clone(&self) -> Self {
        HealthCheck {
            check_type: self.check_type.clone(),
            interval: self.interval,
            timeout: self.timeout,
            path: self.path.clone(),
            port: self.port,
        }
    }
}

impl Clone for HealthCheckType {
    fn clone(&self) -> Self {
        match self {
            HealthCheckType::HTTP => HealthCheckType::HTTP,
            HealthCheckType::TCP => HealthCheckType::TCP,
            HealthCheckType::SCRIPT => HealthCheckType::SCRIPT,
            HealthCheckType::TTL => HealthCheckType::TTL,
        }
    }
}

impl Clone for ServiceEvent {
    fn clone(&self) -> Self {
        match self {
            ServiceEvent::InstanceRegistered { service_id, instance_id } => {
                ServiceEvent::InstanceRegistered {
                    service_id: service_id.clone(),
                    instance_id: instance_id.clone(),
                }
            },
            ServiceEvent::InstanceDeregistered { service_id, instance_id } => {
                ServiceEvent::InstanceDeregistered {
                    service_id: service_id.clone(),
                    instance_id: instance_id.clone(),
                }
            },
            ServiceEvent::InstanceChanged { service_id, instance_id, old_status, new_status } => {
                ServiceEvent::InstanceChanged {
                    service_id: service_id.clone(),
                    instance_id: instance_id.clone(),
                    old_status: old_status.clone(),
                    new_status: new_status.clone(),
                }
            },
        }
    }
}
```

### 1.11 ç»¼åˆåº”ç”¨11-åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—

```rust
// åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—
struct DistributedMessageQueue {
    node_id: String,
    topic_manager: TopicManager,
    message_store: MessageStore,
    consumer_manager: ConsumerManager,
    producer_manager: ProducerManager,
    replication_manager: MessageReplicationManager,
}

struct TopicManager {
    topics: RwLock<HashMap<String, Topic>>,
}

struct Topic {
    name: String,
    partitions: Vec<Partition>,
    config: TopicConfig,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct Partition {
    id: u32,
    topic: String,
    leader: Option<String>,
    replicas: Vec<String>,
    isr: Vec<String>, // In-Sync Replicas
    last_offset: AtomicI64,
}

struct TopicConfig {
    partitions: u32,
    replication_factor: u32,
    retention_hours: u32,
    max_message_bytes: usize,
    cleanup_policy: CleanupPolicy,
}

enum CleanupPolicy {
    Delete,
    Compact,
}

struct MessageStore {
    messages: RwLock<HashMap<String, HashMap<u32, HashMap<i64, Message>>>>, // topic -> partition -> offset -> message
    indices: RwLock<HashMap<String, HashMap<String, BTreeMap<String, Vec<MessageRef>>>>>, // topic -> index_name -> index_value -> message_refs
    segment_manager: SegmentManager,
}

struct Message {
    offset: i64,
    key: Option<Vec<u8>>,
    value: Vec<u8>,
    headers: HashMap<String, Vec<u8>>,
    timestamp: DateTime<Utc>,
    partition: u32,
    topic: String,
}

struct MessageRef {
    topic: String,
    partition: u32,
    offset: i64,
}

struct SegmentManager {
    active_segments: RwLock<HashMap<SegmentKey, Segment>>,
    segments_path: PathBuf,
    max_segment_size: usize,
    segment_roll_hours: u32,
}

struct SegmentKey {
    topic: String,
    partition: u32,
    base_offset: i64,
}

struct Segment {
    key: SegmentKey,
    start_time: DateTime<Utc>,
    current_size: AtomicUsize,
    min_offset: i64,
    max_offset: AtomicI64,
    message_count: AtomicUsize,
    file_handle: Option<Box<dyn Write + Send + Sync>>,
    index_handle: Option<Box<dyn Write + Send + Sync>>,
}

struct ConsumerManager {
    consumers: RwLock<HashMap<String, Consumer>>,
    groups: RwLock<HashMap<String, ConsumerGroup>>,
}

struct Consumer {
    id: String,
    client_id: String,
    subscriptions: Vec<TopicSubscription>,
    last_poll: DateTime<Utc>,
    metadata: HashMap<String, String>,
}

struct TopicSubscription {
    topic: String,
    assignment: Vec<PartitionAssignment>,
}

struct PartitionAssignment {
    partition: u32,
    current_offset: i64,
    committed_offset: i64,
    last_fetch_timestamp: DateTime<Utc>,
}

struct ConsumerGroup {
    name: String,
    members: HashMap<String, GroupMember>,
    generation: u32,
    leader: Option<String>,
    protocol: String,
    state: GroupState,
    partitions_assignment: HashMap<String, HashMap<u32, String>>, // topic -> partition -> consumer_id
}

struct GroupMember {
    id: String,
    client_id: String,
    subscriptions: Vec<String>,
    metadata: Vec<u8>,
    last_heartbeat: DateTime<Utc>,
}

enum GroupState {
    Empty,
    PreparingRebalance,
    CompletingRebalance,
    Stable,
    Dead,
}

struct ProducerManager {
    producers: RwLock<HashMap<String, Producer>>,
    transaction_coordinator: TransactionCoordinator,
}

struct Producer {
    id: String,
    client_id: String,
    transaction_id: Option<String>,
    last_activity: DateTime<Utc>,
    metadata: HashMap<String, String>,
}

struct TransactionCoordinator {
    transactions: RwLock<HashMap<String, Transaction>>,
}

struct Transaction {
    id: String,
    producer_id: String,
    state: TransactionState,
    timeout_ms: u64,
    start_time: DateTime<Utc>,
    last_update_time: DateTime<Utc>,
    topic_partitions: HashSet<(String, u32)>,
}

enum TransactionState {
    Ongoing,
    PrepareCommit,
    PrepareAbort,
    CompleteCommit,
    CompleteAbort,
}

struct MessageReplicationManager {
    replication_tasks: RwLock<Vec<MessageReplicationTask>>,
    running: AtomicBool,
    replication_thread: Option<JoinHandle<()>>,
}

struct MessageReplicationTask {
    topic: String,
    partition: u32,
    source_node: String,
    target_node: String,
    start_offset: i64,
    end_offset: i64,
    created_at: DateTime<Utc>,
}

impl DistributedMessageQueue {
    fn new(node_id: &str, data_path: &Path) -> Self {
        let topic_manager = TopicManager {
            topics: RwLock::new(HashMap::new()),
        };

        let segment_manager = SegmentManager {
            active_segments: RwLock::new(HashMap::new()),
            segments_path: data_path.join("segments"),
            max_segment_size: 1024 * 1024 * 1024, // 1GB
            segment_roll_hours: 24,
        };

        let message_store = MessageStore {
            messages: RwLock::new(HashMap::new()),
            indices: RwLock::new(HashMap::new()),
            segment_manager,
        };

        let consumer_manager = ConsumerManager {
            consumers: RwLock::new(HashMap::new()),
            groups: RwLock::new(HashMap::new()),
        };

        let transaction_coordinator = TransactionCoordinator {
            transactions: RwLock::new(HashMap::new()),
        };

        let producer_manager = ProducerManager {
            producers: RwLock::new(HashMap::new()),
            transaction_coordinator,
        };

        let replication_manager = MessageReplicationManager {
            replication_tasks: RwLock::new(Vec::new()),
            running: AtomicBool::new(false),
            replication_thread: None,
        };

        DistributedMessageQueue {
            node_id: node_id.to_string(),
            topic_manager,
            message_store,
            consumer_manager,
            producer_manager,
            replication_manager,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        let segments_path = &self.message_store.segment_manager.segments_path;
        if !segments_path.exists() {
            match std::fs::create_dir_all(segments_path) {
                Ok(_) => {},
                Err(e) => return Err(format!("åˆ›å»ºæ®µç›®å½•å¤±è´¥: {}", e)),
            }
        }

        // å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨
        self.start_replication_manager()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—");

        // åœæ­¢å¤åˆ¶ç®¡ç†å™¨
        self.replication_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.replication_manager.replication_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤åˆ¶çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // å…³é—­æ‰€æœ‰æ´»åŠ¨æ®µ
        let mut active_segments = self.message_store.segment_manager.active_segments.write().unwrap();
        for (_, segment) in active_segments.drain() {
            // å…³é—­æ–‡ä»¶å¥æŸ„
            if let Some(mut handle) = segment.file_handle {
                match handle.flush() {
                    Ok(_) => {},
                    Err(e) => println!("åˆ·æ–°æ®µæ–‡ä»¶å¤±è´¥: {}", e),
                }
            }

            if let Some(mut handle) = segment.index_handle {
                match handle.flush() {
                    Ok(_) => {},
                    Err(e) => println!("åˆ·æ–°æ®µç´¢å¼•æ–‡ä»¶å¤±è´¥: {}", e),
                }
            }
        }

        Ok(())
    }

    fn start_replication_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨");

        let replication_tasks = self.replication_manager.replication_tasks.clone();
        let message_store = &self.message_store;

        self.replication_manager.running.store(true, Ordering::SeqCst);

        let running = self.replication_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // å¤„ç†å¤åˆ¶ä»»åŠ¡
                let tasks_to_process = {
                    let mut tasks = replication_tasks.write().unwrap();
                    let result = tasks.clone();
                    tasks.clear();
                    result
                };

                for task in tasks_to_process {
                    println!("å¤„ç†æ¶ˆæ¯å¤åˆ¶ä»»åŠ¡: {}/{} -> {}", task.topic, task.partition, task.target_node);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ¶ˆæ¯å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹
                    // ç®€åŒ–ï¼šå‡è®¾å¤åˆ¶æˆåŠŸ

                    println!("æ¶ˆæ¯å¤åˆ¶å®Œæˆ: {}/{}", task.topic, task.partition);
                }

                // ä¼‘çœ ä¸€ä¼šå„¿
                thread::sleep(Duration::from_millis(100));
            }
        });

        self.replication_manager.replication_thread = Some(thread);

        Ok(())
    }

    fn create_topic(&self, name: &str, config: TopicConfig) -> Result<(), String> {
        println!("åˆ›å»ºä¸»é¢˜: {}", name);

        let mut topics = self.topic_manager.topics.write().unwrap();

        if topics.contains_key(name) {
            return Err(format!("ä¸»é¢˜å·²å­˜åœ¨: {}", name));
        }

        let now = Utc::now();

        // åˆ›å»ºåˆ†åŒº
        let mut partitions = Vec::new();
        for i in 0..config.partitions {
            let partition = Partition {
                id: i,
                topic: name.to_string(),
                leader: None,
                replicas: Vec::new(),
                isr: Vec::new(),
                last_offset: AtomicI64::new(0),
            };

            partitions.push(partition);
        }

        let topic = Topic {
            name: name.to_string(),
            partitions,
            config,
            created_at: now,
            updated_at: now,
        };

        topics.insert(name.to_string(), topic);

        // åˆå§‹åŒ–æ¶ˆæ¯å­˜å‚¨
        let mut messages = self.message_store.messages.write().unwrap();
        let mut indices = self.message_store.indices.write().unwrap();

        messages.insert(name.to_string(), HashMap::new());
        indices.insert(name.to_string(), HashMap::new());

        // åˆå§‹åŒ–åˆ†åŒºæ¶ˆæ¯å­˜å‚¨
        let topic_messages = messages.get_mut(name).unwrap();
        for i in 0..config.partitions {
            topic_messages.insert(i, HashMap::new());
        }

        Ok(())
    }

    fn delete_topic(&self, name: &str) -> Result<(), String> {
        println!("åˆ é™¤ä¸»é¢˜: {}", name);

        let mut topics = self.topic_manager.topics.write().unwrap();

        if !topics.contains_key(name) {
            return Err(format!("ä¸»é¢˜ä¸å­˜åœ¨: {}", name));
        }

        // ç§»é™¤æ¶ˆæ¯å­˜å‚¨
        let mut messages = self.message_store.messages.write().unwrap();
        let mut indices = self.message_store.indices.write().unwrap();

        messages.remove(name);
        indices.remove(name);

        // ç§»é™¤æ‰€æœ‰ç›¸å…³æ®µ
        let mut active_segments = self.message_store.segment_manager.active_segments.write().unwrap();
        active_segments.retain(|key, _| key.topic != name);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåˆ é™¤ç£ç›˜ä¸Šçš„æ®µæ–‡ä»¶

        // ç§»é™¤ä¸»é¢˜
        topics.remove(name);

        Ok(())
    }

    fn get_topic(&self, name: &str) -> Result<Topic, String> {
        let topics = self.topic_manager.topics.read().unwrap();

        let topic = topics.get(name)
            .ok_or_else(|| format!("ä¸»é¢˜ä¸å­˜åœ¨: {}", name))?;

        Ok(topic.clone())
    }

    fn list_topics(&self) -> Vec<String> {
        let topics = self.topic_manager.topics.read().unwrap();
        topics.keys().cloned().collect()
    }

    fn produce_message(
        &self,
        topic: &str,
        partition: Option<u32>,
        key: Option<Vec<u8>>,
        value: Vec<u8>,
        headers: HashMap<String, Vec<u8>>,
        transaction_id: Option<&str>
    ) -> Result<(u32, i64), String> {
        println!("ç”Ÿäº§æ¶ˆæ¯: {}", topic);

        // æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å­˜åœ¨
        let topics = self.topic_manager.topics.read().unwrap();

        let topic_info = topics.get(topic)
            .ok_or_else(|| format!("ä¸»é¢˜ä¸å­˜åœ¨: {}", topic))?;

        // é€‰æ‹©åˆ†åŒº
        let partition_id = match partition {
            Some(p) => {
                if p >= topic_info.config.partitions {
                    return Err(format!("åˆ†åŒºä¸å­˜åœ¨: {}/{}", topic, p));
                }
                p
            },
            None => {
                // ç®€å•çš„è½®è¯¢åˆ†åŒºé€‰æ‹©
                let key_hash = match &key {
                    Some(k) => {
                        let mut hasher = DefaultHasher::new();
                        k.hash(&mut hasher);
                        hasher.finish()
                    },
                    None => {
                        // å¦‚æœæ²¡æœ‰é”®ï¼Œä½¿ç”¨éšæœºæ•°
                        rand::random::<u64>()
                    },
                };

                (key_hash % topic_info.config.partitions as u64) as u32
            },
        };

        // æ£€æŸ¥äº‹åŠ¡
        if let Some(tx_id) = transaction_id {
            let producers = self.producer_manager.producers.read().unwrap();
            let mut found = false;

            for producer in producers.values() {
                if let Some(producer_tx_id) = &producer.transaction_id {
                    if producer_tx_id == tx_id {
                        found = true;
                        break;
                    }
                }
            }

            if !found {
                return Err(format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id));
            }

            let transactions = self.producer_manager.transaction_coordinator.transactions.read().unwrap();

            if let Some(tx) = transactions.get(tx_id) {
                match tx.state {
                    TransactionState::Ongoing => {
                        // OK, å¯ä»¥ç»§ç»­
                    },
                    _ => {
                        return Err(format!("äº‹åŠ¡çŠ¶æ€ä¸å…è®¸ç”Ÿäº§æ¶ˆæ¯: {:?}", tx.state));
                    },
                }
            } else {
                return Err(format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", tx_id));
            }
        }

        // è·å–ä¸‹ä¸€ä¸ªåç§»é‡
        let partition = &topic_info.partitions[partition_id as usize];
        let next_offset = partition.last_offset.fetch_add(1, Ordering::SeqCst) + 1;

        // åˆ›å»ºæ¶ˆæ¯
        let now = Utc::now();
        let message = Message {
            offset: next_offset,
            key,
            value,
            headers,
            timestamp: now,
            partition: partition_id,
            topic: topic.to_string(),
        };

        // å­˜å‚¨æ¶ˆæ¯
        let mut messages = self.message_store.messages.write().unwrap();

        let topic_messages = messages.get_mut(topic)
            .ok_or_else(|| format!("ä¸»é¢˜æ¶ˆæ¯å­˜å‚¨ä¸å­˜åœ¨: {}", topic))?;

        let partition_messages = topic_messages.get_mut(&partition_id)
            .ok_or_else(|| format!("åˆ†åŒºæ¶ˆæ¯å­˜å‚¨ä¸å­˜åœ¨: {}/{}", topic, partition_id))?;

        partition_messages.insert(next_offset, message.clone());

        // æ·»åŠ åˆ°æ´»åŠ¨æ®µ
        let result = self.add_message_to_segment(&message);

        // å¦‚æœæ˜¯äº‹åŠ¡æ€§æ¶ˆæ¯ï¼Œæ›´æ–°äº‹åŠ¡çŠ¶æ€
        if let Some(tx_id) = transaction_id {
            let mut transactions = self.producer_manager.transaction_coordinator.transactions.write().unwrap();

            if let Some(tx) = transactions.get_mut(tx_id) {
                tx.topic_partitions.insert((topic.to_string(), partition_id));
                tx.last_update_time = now;
            }
        }

        // è§¦å‘å¤åˆ¶
        if topic_info.config.replication_factor > 1 {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€‰æ‹©ç›®æ ‡èŠ‚ç‚¹
            let target_nodes = vec![
                format!("node-{}", uuid::Uuid::new_v4().to_string()),
                format!("node-{}", uuid::Uuid::new_v4().to_string()),
            ];

            for target_node in target_nodes {
                let replication_task = MessageReplicationTask {
                    topic: topic.to_string(),
                    partition: partition_id,
                    source_node: self.node_id.clone(),
                    target_node,
                    start_offset: next_offset,
                    end_offset: next_offset,
                    created_at: now,
                };

                let mut tasks = self.replication_manager.replication_tasks.write().unwrap();
                tasks.push(replication_task);
            }
        }

        match result {
            Ok(_) => Ok((partition_id, next_offset)),
            Err(e) => Err(e),
        }
    }

    fn add_message_to_segment(&self, message: &Message) -> Result<(), String> {
        let mut active_segments = self.message_store.segment_manager.active_segments.write().unwrap();

        let segment_key = SegmentKey {
            topic: message.topic.clone(),
            partition: message.partition,
            base_offset: message.offset / 1000 * 1000, // æŒ‰1000ä¸ªæ¶ˆæ¯åˆ†æ®µ
        };

        let segment = if let Some(segment) = active_segments.get_mut(&segment_key) {
            segment
        } else {
            // åˆ›å»ºæ–°æ®µ
            let segment = self.create_new_segment(&segment_key)?;
            active_segments.insert(segment_key.clone(), segment);
            active_segments.get_mut(&segment_key).unwrap()
        };

        // æ›´æ–°æ®µå…ƒæ•°æ®
        segment.max_offset.store(message.offset, Ordering::SeqCst);
        segment.message_count.fetch_add(1, Ordering::SeqCst);

        // åºåˆ—åŒ–æ¶ˆæ¯
        let serialized = self.serialize_message(message)?;

        // å†™å…¥æ–‡ä»¶
        if let Some(file_handle) = &mut segment.file_handle {
            match file_handle.write_all(&serialized) {
                Ok(_) => {
                    segment.current_size.fetch_add(serialized.len(), Ordering::SeqCst);
                },
                Err(e) => return Err(format!("å†™å…¥æ®µæ–‡ä»¶å¤±è´¥: {}", e)),
            }
        } else {
            return Err("æ®µæ–‡ä»¶å¥æŸ„æœªåˆå§‹åŒ–".to_string());
        }

        // å†™å…¥ç´¢å¼•ï¼ˆç®€åŒ–ï¼Œå®é™…å®ç°ä¼šæ›´å¤æ‚ï¼‰
        if let Some(index_handle) = &mut segment.index_handle {
            let index_entry = format!("{},{},{}\n", message.offset, serialized.len(), segment.current_size.load(Ordering::SeqCst));
            match index_handle.write_all(index_entry.as_bytes()) {
                Ok(_) => {},
                Err(e) => return Err(format!("å†™å…¥æ®µç´¢å¼•æ–‡ä»¶å¤±è´¥: {}", e)),
            }
        }

        Ok(())
    }

    fn create_new_segment(&self, key: &SegmentKey) -> Result<Segment, String> {
        let segments_path = &self.message_store.segment_manager.segments_path;

        // åˆ›å»ºä¸»é¢˜ç›®å½•
        let topic_dir = segments_path.join(&key.topic);
        if !topic_dir.exists() {
            match std::fs::create_dir_all(&topic_dir) {
                Ok(_) => {},
                Err(e) => return Err(format!("åˆ›å»ºä¸»é¢˜ç›®å½•å¤±è´¥: {}", e)),
            }
        }

        // åˆ›å»ºåˆ†åŒºç›®å½•
        let partition_dir = topic_dir.join(format!("p-{}", key.partition));
        if !partition_dir.exists() {
            match std::fs::create_dir_all(&partition_dir) {
                Ok(_) => {},
                Err(e) => return Err(format!("åˆ›å»ºåˆ†åŒºç›®å½•å¤±è´¥: {}", e)),
            }
        }

        // åˆ›å»ºæ®µæ–‡ä»¶
        let segment_file_path = partition_dir.join(format!("{}.log", key.base_offset));
        let segment_index_path = partition_dir.join(format!("{}.index", key.base_offset));

        let file_handle = match std::fs::OpenOptions::new()
            .create(true)
            .write(true)
            .append(true)
            .open(&segment_file_path) {
            Ok(file) => Some(Box::new(file) as Box<dyn Write + Send + Sync>),
            Err(e) => return Err(format!("åˆ›å»ºæ®µæ–‡ä»¶å¤±è´¥: {}", e)),
        };

        let index_handle = match std::fs::OpenOptions::new()
            .create(true)
            .write(true)
            .append(true)
            .open(&segment_index_path) {
            Ok(file) => Some(Box::new(file) as Box<dyn Write + Send + Sync>),
            Err(e) => return Err(format!("åˆ›å»ºæ®µç´¢å¼•æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        let segment = Segment {
            key: key.clone(),
            start_time: Utc::now(),
            current_size: AtomicUsize::new(0),
            min_offset: key.base_offset,
            max_offset: AtomicI64::new(key.base_offset - 1),
            message_count: AtomicUsize::new(0),
            file_handle,
            index_handle,
        };

        Ok(segment)
    }

    fn serialize_message(&self, message: &Message) -> Result<Vec<u8>, String> {
        // ç®€åŒ–çš„æ¶ˆæ¯åºåˆ—åŒ–
        let mut data = Vec::new();

        // å†™å…¥å…ƒæ•°æ®
        data.extend_from_slice(&message.offset.to_be_bytes());
        data.extend_from_slice(&message.partition.to_be_bytes());

        let timestamp_millis = message.timestamp.timestamp_millis();
        data.extend_from_slice(&timestamp_millis.to_be_bytes());

        // å†™å…¥é”®
        if let Some(key) = &message.key {
            data.extend_from_slice(&(key.len() as u32).to_be_bytes());
            data.extend_from_slice(key);
        } else {
            data.extend_from_slice(&(0_u32).to_be_bytes());
        }

        // å†™å…¥å€¼
        data.extend_from_slice(&(message.value.len() as u32).to_be_bytes());
        data.extend_from_slice(&message.value);

        // å†™å…¥å¤´éƒ¨æ•°é‡
        data.extend_from_slice(&(message.headers.len() as u32).to_be_bytes());

        // å†™å…¥æ¯ä¸ªå¤´éƒ¨
        for (name, value) in &message.headers {
            data.extend_from_slice(&(name.len() as u32).to_be_bytes());
            data.extend_from_slice(name.as_bytes());

            data.extend_from_slice(&(value.len() as u32).to_be_bytes());
            data.extend_from_slice(value);
        }

        Ok(data)
    }

    fn consume_messages(
        &self,
        topic: &str,
        partition: u32,
        offset: i64,
        max_bytes: usize,
        consumer_id: &str,
        group_id: Option<&str>
    ) -> Result<Vec<Message>, String> {
        println!("æ¶ˆè´¹æ¶ˆæ¯: {}/{} ä»åç§»é‡ {}", topic, partition, offset);

        // æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å­˜åœ¨
        let topics = self.topic_manager.topics.read().unwrap();

        let topic_info = topics.get(topic)
            .ok_or_else(|| format!("ä¸»é¢˜ä¸å­˜åœ¨: {}", topic))?;

        // æ£€æŸ¥åˆ†åŒºæ˜¯å¦å­˜åœ¨
        if partition >= topic_info.config.partitions {
            return Err(format!("åˆ†åŒºä¸å­˜åœ¨: {}/{}", topic, partition));
        }

        // æ£€æŸ¥æ¶ˆè´¹è€…
        let consumers = self.consumer_manager.consumers.read().unwrap();

        if !consumers.contains_key(consumer_id) {
            return Err(format!("æ¶ˆè´¹è€…ä¸å­˜åœ¨: {}", consumer_id));
        }

        // å¦‚æœæŒ‡å®šäº†æ¶ˆè´¹è€…ç»„ï¼Œæ£€æŸ¥åˆ†åŒºåˆ†é…
        if let Some(group_id) = group_id {
            let groups = self.consumer_manager.groups.read().unwrap();

            let group = groups.get(group_id)
                .ok_or_else(|| format!("æ¶ˆè´¹è€…ç»„ä¸å­˜åœ¨: {}", group_id))?;

            if let Some(topic_assignments) = group.partitions_assignment.get(topic) {
                if let Some(assigned_consumer) = topic_assignments.get(&partition) {
                    if assigned_consumer != consumer_id {
                        return Err(format!("åˆ†åŒºæœªåˆ†é…ç»™æ­¤æ¶ˆè´¹è€…: {}/{} -> {}", topic, partition, consumer_id));
                    }
                } else {
                    return Err(format!("åˆ†åŒºæœªåˆ†é…: {}/{}", topic, partition));
                }
            } else {
                return Err(format!("ä¸»é¢˜æœªåˆ†é…: {}", topic));
            }
        }

        // è·å–æ¶ˆæ¯
        let messages = self.message_store.messages.read().unwrap();

        let topic_messages = messages.get(topic)
            .ok_or_else(|| format!("ä¸»é¢˜æ¶ˆæ¯å­˜å‚¨ä¸å­˜åœ¨: {}", topic))?;

        let partition_messages = topic_messages.get(&partition)
            .ok_or_else(|| format!("åˆ†åŒºæ¶ˆæ¯å­˜å‚¨ä¸å­˜åœ¨: {}/{}", topic, partition))?;

        // æŸ¥æ‰¾ä»ç»™å®šåç§»é‡å¼€å§‹çš„æ¶ˆæ¯
        let mut result = Vec::new();
        let mut total_bytes = 0;

        for (msg_offset, message) in partition_messages.iter().filter(|(o, _)| **o >= offset).take(1000) {
            let msg_size = message.value.len() +
                           message.key.as_ref().map_or(0, |k| k.len()) +
                           message.headers.iter().map(|(k, v)| k.len() + v.len()).sum::<usize>() +
                           100; // ä¼°è®¡çš„é¢å¤–å¼€é”€

            if total_bytes + msg_size > max_bytes && !result.is_empty() {
                // å·²è¾¾åˆ°æœ€å¤§å­—èŠ‚æ•°ï¼Œåœæ­¢æ·»åŠ æ¶ˆæ¯
                break;
            }

            result.push(message.clone());
            total_bytes += msg_size;
        }

        // æ›´æ–°æ¶ˆè´¹è€…çš„å½“å‰åç§»é‡
        if let Some(group_id) = group_id {
            let mut groups = self.consumer_manager.groups.write().unwrap();

            if let Some(group) = groups.get_mut(group_id) {
                if let Some(member) = group.members.get_mut(consumer_id) {
                    member.last_heartbeat = Utc::now();
                }
            }
        }

        let mut consumers = self.consumer_manager.consumers.write().unwrap();
        if let Some(consumer) = consumers.get_mut(consumer_id) {
            for sub in &mut consumer.subscriptions {
                if sub.topic == topic {
                    for assignment in &mut sub.assignment {
                        if assignment.partition == partition && !result.is_empty() {
                            let last_offset = result.last().unwrap().offset;
                            assignment.current_offset = last_offset + 1;
                            assignment.last_fetch_timestamp = Utc::now();
                        }
                    }
                }
            }
        }

        Ok(result)
    }

    fn create_consumer(
        &self,
        id: &str,
        client_id: &str,
        group_id: Option<&str>
    ) -> Result<(), String> {
        println!("åˆ›å»ºæ¶ˆè´¹è€…: {}", id);

        let mut consumers = self.consumer_manager.consumers.write().unwrap();

        if consumers.contains_key(id) {
            return Err(format!("æ¶ˆè´¹è€…å·²å­˜åœ¨: {}", id));
        }

        let consumer = Consumer {
            id: id.to_string(),
            client_id: client_id.to_string(),
            subscriptions: Vec::new(),
            last_poll: Utc::now(),
            metadata: HashMap::new(),
        };

        consumers.insert(id.to_string(), consumer);

        // å¦‚æœæŒ‡å®šäº†æ¶ˆè´¹è€…ç»„ï¼Œå°†æ¶ˆè´¹è€…æ·»åŠ åˆ°ç»„
        if let Some(group_id) = group_id {
            let mut groups = self.consumer_manager.groups.write().unwrap();

            let group = groups.entry(group_id.to_string())
                .or_insert_with(|| ConsumerGroup {
                    name: group_id.to_string(),
                    members: HashMap::new(),
                    generation: 0,
                    leader: None,
                    protocol: "consumer".to_string(),
                    state: GroupState::Empty,
                    partitions_assignment: HashMap::new(),
                });

            let member = GroupMember {
                id: id.to_string(),
                client_id: client_id.to_string(),
                subscriptions: Vec::new(),
                metadata: Vec::new(),
                last_heartbeat: Utc::now(),
            };

            group.members.insert(id.to_string(), member);

            // å¦‚æœè¿™æ˜¯ç¬¬ä¸€ä¸ªæˆå‘˜ï¼Œå°†å…¶è®¾ä¸ºé¢†å¯¼è€…
            if group.leader.is_none() {
                group.leader = Some(id.to_string());
            }

            // æ›´æ–°ç»„çŠ¶æ€
            if matches!(group.state, GroupState::Empty) {
                group.state = GroupState::Stable;
            } else {
                group.state = GroupState::PreparingRebalance;
            }

            // å¢åŠ ä¸–ä»£
            group.generation += 1;
        }

        Ok(())
    }

    fn subscribe(
        &self,
        consumer_id: &str,
        topics: Vec<String>
    ) -> Result<(), String> {
        println!("è®¢é˜…ä¸»é¢˜: æ¶ˆè´¹è€… {} -> {:?}", consumer_id, topics);

        // æ£€æŸ¥ä¸»é¢˜æ˜¯å¦å­˜åœ¨
        let topic_manager = self.topic_manager.topics.read().unwrap();

        for topic in &topics {
            if !topic_manager.contains_key(topic) {
                return Err(format!("ä¸»é¢˜ä¸å­˜åœ¨: {}", topic));
            }
        }

        // æ›´æ–°æ¶ˆè´¹è€…è®¢é˜…
        let mut consumers = self.consumer_manager.consumers.write().unwrap();

        let consumer = consumers.get_mut(consumer_id)
            .ok_or_else(|| format!("æ¶ˆè´¹è€…ä¸å­˜åœ¨: {}", consumer_id))?;

        // æ¸…é™¤ç°æœ‰è®¢é˜…
        consumer.subscriptions.clear();

        // æ·»åŠ æ–°è®¢é˜…
        for topic_name in &topics {
            let topic = topic_manager.get(topic_name).unwrap();

            let mut assignments = Vec::new();
            for partition in &topic.partitions {
                let assignment = PartitionAssignment {
                    partition: partition.id,
                    current_offset: 0,
                    committed_offset: 0,
                    last_fetch_timestamp: Utc::now(),
                };

                assignments.push(assignment);
            }

            let subscription = TopicSubscription {
                topic: topic_name.clone(),
                assignment: assignments,
            };

            consumer.subscriptions.push(subscription);
        }

        // æŸ¥æ‰¾æ¶ˆè´¹è€…æ‰€å±çš„ç»„
        let mut groups = self.consumer_manager.groups.write().unwrap();

        for group in groups.values_mut() {
            if let Some(member) = group.members.get_mut(consumer_id) {
                member.subscriptions = topics.clone();

                // è§¦å‘é‡å¹³è¡¡
                group.state = GroupState::PreparingRebalance;

                // éœ€è¦åˆ†é…åˆ†åŒº
                self.assign_partitions(group)?;
            }
        }

        Ok(())
    }

    fn assign_partitions(&self, group: &mut ConsumerGroup) -> Result<(), String> {
        println!("åˆ†é…åˆ†åŒºç»™æ¶ˆè´¹è€…ç»„: {}", group.name);

        // æ”¶é›†æ‰€æœ‰ä¸»é¢˜å’Œåˆ†åŒº
        let topics = self.topic_manager.topics.read().unwrap();

        let mut all_topic_partitions = HashMap::new();
        let mut all_subscriptions = HashMap::new();

        for member in group.members.values() {
            all_subscriptions.insert(member.id.clone(), member.subscriptions.clone());

            for topic in &member.subscriptions {
                if let Some(topic_info) = topics.get(topic) {
                    let topic_partitions = all_topic_partitions.entry(topic.clone())
                        .or_insert_with(Vec::new);

                    for partition in &topic_info.partitions {
                        if !topic_partitions.contains(&partition.id) {
                            topic_partitions.push(partition.id);
                        }
                    }
                }
            }
        }

        // å®ç°ç®€å•çš„åˆ†åŒºåˆ†é…ç­–ç•¥ï¼ˆè½®è¯¢ï¼‰
        let mut assignments = HashMap::new();

        for (topic, partitions) in all_topic_partitions {
            let mut topic_assignments = HashMap::new();

            // æ‰¾å‡ºè®¢é˜…æ­¤ä¸»é¢˜çš„æˆå‘˜
            let members: Vec<_> = all_subscriptions.iter()
                .filter(|(_, topics)| topics.contains(&topic))
                .map(|(id, _)| id.clone())
                .collect();

            if members.is_empty() {
                continue;
            }

            // è½®è¯¢åˆ†é…
            for (i, partition) in partitions.iter().enumerate() {
                let member_index = i % members.len();
                let member_id = &members[member_index];

                topic_assignments.insert(*partition, member_id.clone());
            }

            assignments.insert(topic, topic_assignments);
        }

        // æ›´æ–°ç»„çš„åˆ†åŒºåˆ†é…
        group.partitions_assignment = assignments;
        group.state = GroupState::Stable;

        // æ›´æ–°æ¶ˆè´¹è€…çš„åˆ†åŒºåˆ†é…
        let mut consumers = self.consumer_manager.consumers.write().unwrap();

        for (topic, partitions) in &group.partitions_assignment {
            for (partition, consumer_id) in partitions {
                if let Some(consumer) = consumers.get_mut(consumer_id) {
                    for subscription in &mut consumer.subscriptions {
                        if &subscription.topic == topic {
                            // æ‰¾åˆ°æ˜¯å¦å·²æœ‰æ­¤åˆ†åŒºçš„åˆ†é…
                            let has_assignment = subscription.assignment.iter().any(|a| a.partition == *partition);

                            if !has_assignment {
                                // æ·»åŠ æ–°åˆ†é…
                                let assignment = PartitionAssignment {


```rust
                                let assignment = PartitionAssignment {
                                    partition: *partition,
                                    current_offset: 0,
                                    committed_offset: 0,
                                    last_fetch_timestamp: Utc::now(),
                                };

                                subscription.assignment.push(assignment);
                            }
                        }
                    }
                }
            }
        }

        Ok(())
    }

    fn create_producer(
        &self,
        id: &str,
        client_id: &str,
        transaction_id: Option<&str>
    ) -> Result<(), String> {
        println!("åˆ›å»ºç”Ÿäº§è€…: {}", id);

        let mut producers = self.producer_manager.producers.write().unwrap();

        if producers.contains_key(id) {
            return Err(format!("ç”Ÿäº§è€…å·²å­˜åœ¨: {}", id));
        }

        let producer = Producer {
            id: id.to_string(),
            client_id: client_id.to_string(),
            transaction_id: transaction_id.map(|s| s.to_string()),
            last_activity: Utc::now(),
            metadata: HashMap::new(),
        };

        producers.insert(id.to_string(), producer);

        // å¦‚æœæŒ‡å®šäº†äº‹åŠ¡IDï¼Œåˆå§‹åŒ–äº‹åŠ¡
        if let Some(tx_id) = transaction_id {
            let now = Utc::now();

            let transaction = Transaction {
                id: tx_id.to_string(),
                producer_id: id.to_string(),
                state: TransactionState::Ongoing,
                timeout_ms: 60000, // é»˜è®¤1åˆ†é’Ÿè¶…æ—¶
                start_time: now,
                last_update_time: now,
                topic_partitions: HashSet::new(),
            };

            let mut transactions = self.producer_manager.transaction_coordinator.transactions.write().unwrap();
            transactions.insert(tx_id.to_string(), transaction);
        }

        Ok(())
    }

    fn begin_transaction(&self, producer_id: &str) -> Result<String, String> {
        println!("å¼€å§‹äº‹åŠ¡: ç”Ÿäº§è€… {}", producer_id);

        let producers = self.producer_manager.producers.read().unwrap();

        let producer = producers.get(producer_id)
            .ok_or_else(|| format!("ç”Ÿäº§è€…ä¸å­˜åœ¨: {}", producer_id))?;

        if producer.transaction_id.is_some() {
            return Err("ç”Ÿäº§è€…å·²æœ‰ä¸€ä¸ªæ´»è·ƒçš„äº‹åŠ¡".to_string());
        }

        let tx_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let transaction = Transaction {
            id: tx_id.clone(),
            producer_id: producer_id.to_string(),
            state: TransactionState::Ongoing,
            timeout_ms: 60000, // é»˜è®¤1åˆ†é’Ÿè¶…æ—¶
            start_time: now,
            last_update_time: now,
            topic_partitions: HashSet::new(),
        };

        let mut transactions = self.producer_manager.transaction_coordinator.transactions.write().unwrap();
        transactions.insert(tx_id.clone(), transaction);

        // æ›´æ–°ç”Ÿäº§è€…
        drop(producers);
        let mut producers = self.producer_manager.producers.write().unwrap();

        if let Some(producer) = producers.get_mut(producer_id) {
            producer.transaction_id = Some(tx_id.clone());
            producer.last_activity = now;
        }

        Ok(tx_id)
    }

    fn commit_transaction(&self, producer_id: &str, transaction_id: &str) -> Result<(), String> {
        println!("æäº¤äº‹åŠ¡: {} ç”Ÿäº§è€… {}", transaction_id, producer_id);

        let producers = self.producer_manager.producers.read().unwrap();

        let producer = producers.get(producer_id)
            .ok_or_else(|| format!("ç”Ÿäº§è€…ä¸å­˜åœ¨: {}", producer_id))?;

        if producer.transaction_id.as_deref() != Some(transaction_id) {
            return Err("ç”Ÿäº§è€…æœªå…³è”æ­¤äº‹åŠ¡".to_string());
        }

        let mut transactions = self.producer_manager.transaction_coordinator.transactions.write().unwrap();

        let transaction = transactions.get_mut(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.producer_id != producer_id {
            return Err("äº‹åŠ¡ä¸å±äºæ­¤ç”Ÿäº§è€…".to_string());
        }

        if transaction.state != TransactionState::Ongoing {
            return Err(format!("äº‹åŠ¡çŠ¶æ€ä¸å…è®¸æäº¤: {:?}", transaction.state));
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.state = TransactionState::PrepareCommit;
        transaction.last_update_time = Utc::now();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåè°ƒæ‰€æœ‰å‚ä¸è€…æäº¤äº‹åŠ¡

        // å®Œæˆæäº¤
        transaction.state = TransactionState::CompleteCommit;

        // ç§»é™¤äº‹åŠ¡
        transactions.remove(transaction_id);

        // æ›´æ–°ç”Ÿäº§è€…
        drop(transactions);
        drop(producers);
        let mut producers = self.producer_manager.producers.write().unwrap();

        if let Some(producer) = producers.get_mut(producer_id) {
            producer.transaction_id = None;
            producer.last_activity = Utc::now();
        }

        Ok(())
    }

    fn abort_transaction(&self, producer_id: &str, transaction_id: &str) -> Result<(), String> {
        println!("ä¸­æ­¢äº‹åŠ¡: {} ç”Ÿäº§è€… {}", transaction_id, producer_id);

        let producers = self.producer_manager.producers.read().unwrap();

        let producer = producers.get(producer_id)
            .ok_or_else(|| format!("ç”Ÿäº§è€…ä¸å­˜åœ¨: {}", producer_id))?;

        if producer.transaction_id.as_deref() != Some(transaction_id) {
            return Err("ç”Ÿäº§è€…æœªå…³è”æ­¤äº‹åŠ¡".to_string());
        }

        let mut transactions = self.producer_manager.transaction_coordinator.transactions.write().unwrap();

        let transaction = transactions.get_mut(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.producer_id != producer_id {
            return Err("äº‹åŠ¡ä¸å±äºæ­¤ç”Ÿäº§è€…".to_string());
        }

        if transaction.state != TransactionState::Ongoing {
            return Err(format!("äº‹åŠ¡çŠ¶æ€ä¸å…è®¸ä¸­æ­¢: {:?}", transaction.state));
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.state = TransactionState::PrepareAbort;
        transaction.last_update_time = Utc::now();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ’¤é”€æ‰€æœ‰ç›¸å…³æ¶ˆæ¯

        // å®Œæˆä¸­æ­¢
        transaction.state = TransactionState::CompleteAbort;

        // ç§»é™¤äº‹åŠ¡
        transactions.remove(transaction_id);

        // æ›´æ–°ç”Ÿäº§è€…
        drop(transactions);
        drop(producers);
        let mut producers = self.producer_manager.producers.write().unwrap();

        if let Some(producer) = producers.get_mut(producer_id) {
            producer.transaction_id = None;
            producer.last_activity = Utc::now();
        }

        Ok(())
    }

    fn commit_offset(
        &self,
        consumer_id: &str,
        group_id: &str,
        topic: &str,
        partition: u32,
        offset: i64
    ) -> Result<(), String> {
        println!("æäº¤åç§»é‡: æ¶ˆè´¹è€… {} ç»„ {} ä¸»é¢˜ {} åˆ†åŒº {} åç§»é‡ {}",
                consumer_id, group_id, topic, partition, offset);

        let mut groups = self.consumer_manager.groups.write().unwrap();

        let group = groups.get_mut(group_id)
            .ok_or_else(|| format!("æ¶ˆè´¹è€…ç»„ä¸å­˜åœ¨: {}", group_id))?;

        let member = group.members.get_mut(consumer_id)
            .ok_or_else(|| format!("æˆå‘˜ä¸å­˜åœ¨äºæ¶ˆè´¹è€…ç»„: {} -> {}", consumer_id, group_id))?;

        // æ£€æŸ¥æ˜¯å¦åˆ†é…äº†æ­¤åˆ†åŒº
        if let Some(topic_assignments) = group.partitions_assignment.get(topic) {
            if let Some(assigned_consumer) = topic_assignments.get(&partition) {
                if assigned_consumer != consumer_id {
                    return Err(format!("åˆ†åŒºæœªåˆ†é…ç»™æ­¤æ¶ˆè´¹è€…: {}/{} -> {}", topic, partition, consumer_id));
                }
            } else {
                return Err(format!("åˆ†åŒºæœªåˆ†é…: {}/{}", topic, partition));
            }
        } else {
            return Err(format!("ä¸»é¢˜æœªåˆ†é…: {}", topic));
        }

        // æ›´æ–°æäº¤åç§»é‡
        let mut consumers = self.consumer_manager.consumers.write().unwrap();

        let consumer = consumers.get_mut(consumer_id)
            .ok_or_else(|| format!("æ¶ˆè´¹è€…ä¸å­˜åœ¨: {}", consumer_id))?;

        for subscription in &mut consumer.subscriptions {
            if subscription.topic == topic {
                for assignment in &mut subscription.assignment {
                    if assignment.partition == partition {
                        assignment.committed_offset = offset;
                        break;
                    }
                }
            }
        }

        // æ›´æ–°å¿ƒè·³
        member.last_heartbeat = Utc::now();

        Ok(())
    }

    fn consumer_heartbeat(
        &self,
        consumer_id: &str,
        group_id: Option<&str>
    ) -> Result<(), String> {
        let now = Utc::now();

        // æ›´æ–°æ¶ˆè´¹è€…æœ€åæ´»åŠ¨æ—¶é—´
        let mut consumers = self.consumer_manager.consumers.write().unwrap();

        let consumer = consumers.get_mut(consumer_id)
            .ok_or_else(|| format!("æ¶ˆè´¹è€…ä¸å­˜åœ¨: {}", consumer_id))?;

        consumer.last_poll = now;

        // å¦‚æœæŒ‡å®šäº†æ¶ˆè´¹è€…ç»„ï¼Œæ›´æ–°ç»„æˆå‘˜å¿ƒè·³
        if let Some(group_id) = group_id {
            let mut groups = self.consumer_manager.groups.write().unwrap();

            let group = groups.get_mut(group_id)
                .ok_or_else(|| format!("æ¶ˆè´¹è€…ç»„ä¸å­˜åœ¨: {}", group_id))?;

            if let Some(member) = group.members.get_mut(consumer_id) {
                member.last_heartbeat = now;
            } else {
                return Err(format!("æ¶ˆè´¹è€…ä¸æ˜¯æ­¤ç»„çš„æˆå‘˜: {} -> {}", consumer_id, group_id));
            }
        }

        Ok(())
    }
}

impl Clone for Topic {
    fn clone(&self) -> Self {
        Topic {
            name: self.name.clone(),
            partitions: self.partitions.clone(),
            config: self.config.clone(),
            created_at: self.created_at,
            updated_at: self.updated_at,
        }
    }
}

impl Clone for Partition {
    fn clone(&self) -> Self {
        Partition {
            id: self.id,
            topic: self.topic.clone(),
            leader: self.leader.clone(),
            replicas: self.replicas.clone(),
            isr: self.isr.clone(),
            last_offset: AtomicI64::new(self.last_offset.load(Ordering::SeqCst)),
        }
    }
}

impl Clone for TopicConfig {
    fn clone(&self) -> Self {
        TopicConfig {
            partitions: self.partitions,
            replication_factor: self.replication_factor,
            retention_hours: self.retention_hours,
            max_message_bytes: self.max_message_bytes,
            cleanup_policy: self.cleanup_policy.clone(),
        }
    }
}

impl Clone for CleanupPolicy {
    fn clone(&self) -> Self {
        match self {
            CleanupPolicy::Delete => CleanupPolicy::Delete,
            CleanupPolicy::Compact => CleanupPolicy::Compact,
        }
    }
}

impl Clone for Message {
    fn clone(&self) -> Self {
        Message {
            offset: self.offset,
            key: self.key.clone(),
            value: self.value.clone(),
            headers: self.headers.clone(),
            timestamp: self.timestamp,
            partition: self.partition,
            topic: self.topic.clone(),
        }
    }
}

impl Clone for MessageReplicationTask {
    fn clone(&self) -> Self {
        MessageReplicationTask {
            topic: self.topic.clone(),
            partition: self.partition,
            source_node: self.source_node.clone(),
            target_node: self.target_node.clone(),
            start_offset: self.start_offset,
            end_offset: self.end_offset,
            created_at: self.created_at,
        }
    }
}

impl Clone for SegmentKey {
    fn clone(&self) -> Self {
        SegmentKey {
            topic: self.topic.clone(),
            partition: self.partition,
            base_offset: self.base_offset,
        }
    }
}

impl PartialEq for SegmentKey {
    fn eq(&self, other: &Self) -> bool {
        self.topic == other.topic &&
        self.partition == other.partition &&
        self.base_offset == other.base_offset
    }
}

impl Eq for SegmentKey {}

impl Hash for SegmentKey {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.topic.hash(state);
        self.partition.hash(state);
        self.base_offset.hash(state);
    }
}

// åˆ†å¸ƒå¼ç›‘æ§æœåŠ¡
struct DistributedMonitoringSystem {
    node_id: String,
    metric_collector: MetricCollector,
    alert_manager: AlertManager,
    dashboard_manager: DashboardManager,
    notification_service: NotificationService,
}

struct MetricCollector {
    metrics: RwLock<HashMap<String, Metric>>,
    time_series: RwLock<HashMap<String, Vec<MetricSample>>>,
    collection_interval: Duration,
    retention_period: Duration,
    running: AtomicBool,
    collector_thread: Option<JoinHandle<()>>,
}

struct Metric {
    name: String,
    description: String,
    metric_type: MetricType,
    labels: HashMap<String, String>,
    created_at: DateTime<Utc>,
}

enum MetricType {
    Counter,
    Gauge,
    Histogram,
    Summary,
}

struct MetricSample {
    metric_name: String,
    timestamp: DateTime<Utc>,
    value: f64,
    labels: HashMap<String, String>,
}

struct AlertManager {
    alert_rules: RwLock<HashMap<String, AlertRule>>,
    active_alerts: RwLock<HashMap<String, Alert>>,
    checking_interval: Duration,
    running: AtomicBool,
    checker_thread: Option<JoinHandle<()>>,
}

struct AlertRule {
    id: String,
    name: String,
    description: String,
    metric_query: String,
    condition: AlertCondition,
    threshold: f64,
    duration: Duration,
    severity: AlertSeverity,
    notifications: Vec<String>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

enum AlertCondition {
    GreaterThan,
    LessThan,
    Equal,
    NotEqual,
}

enum AlertSeverity {
    Info,
    Warning,
    Error,
    Critical,
}

struct Alert {
    id: String,
    rule_id: String,
    value: f64,
    started_at: DateTime<Utc>,
    last_updated: DateTime<Utc>,
    status: AlertStatus,
    silenced: bool,
    acknowledged_by: Option<String>,
    annotations: HashMap<String, String>,
}

enum AlertStatus {
    Firing,
    Resolved,
}

struct DashboardManager {
    dashboards: RwLock<HashMap<String, Dashboard>>,
}

struct Dashboard {
    id: String,
    title: String,
    description: Option<String>,
    panels: Vec<Panel>,
    refresh_interval: Duration,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
    created_by: String,
}

struct Panel {
    id: String,
    title: String,
    panel_type: PanelType,
    position: PanelPosition,
    data_source: Option<String>,
    query: Option<String>,
    options: HashMap<String, String>,
}

enum PanelType {
    Graph,
    SingleStat,
    Table,
    Text,
    Heatmap,
}

struct PanelPosition {
    x: u32,
    y: u32,
    width: u32,
    height: u32,
}

struct NotificationService {
    channels: RwLock<HashMap<String, NotificationChannel>>,
}

struct NotificationChannel {
    id: String,
    name: String,
    channel_type: NotificationType,
    config: HashMap<String, String>,
    enabled: bool,
}

enum NotificationType {
    Email,
    Slack,
    Webhook,
    PagerDuty,
    SMS,
}

impl DistributedMonitoringSystem {
    fn new(node_id: &str) -> Self {
        let metric_collector = MetricCollector {
            metrics: RwLock::new(HashMap::new()),
            time_series: RwLock::new(HashMap::new()),
            collection_interval: Duration::from_secs(10),
            retention_period: Duration::from_secs(86400 * 7), // 7å¤©
            running: AtomicBool::new(false),
            collector_thread: None,
        };

        let alert_manager = AlertManager {
            alert_rules: RwLock::new(HashMap::new()),
            active_alerts: RwLock::new(HashMap::new()),
            checking_interval: Duration::from_secs(30),
            running: AtomicBool::new(false),
            checker_thread: None,
        };

        let dashboard_manager = DashboardManager {
            dashboards: RwLock::new(HashMap::new()),
        };

        let notification_service = NotificationService {
            channels: RwLock::new(HashMap::new()),
        };

        DistributedMonitoringSystem {
            node_id: node_id.to_string(),
            metric_collector,
            alert_manager,
            dashboard_manager,
            notification_service,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ");

        // å¯åŠ¨æŒ‡æ ‡æ”¶é›†å™¨
        self.start_metric_collector()?;

        // å¯åŠ¨å‘Šè­¦æ£€æŸ¥å™¨
        self.start_alert_checker()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ");

        // åœæ­¢æŒ‡æ ‡æ”¶é›†å™¨
        self.metric_collector.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.metric_collector.collector_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æŒ‡æ ‡æ”¶é›†çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢å‘Šè­¦æ£€æŸ¥å™¨
        self.alert_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.alert_manager.checker_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å‘Šè­¦æ£€æŸ¥çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn start_metric_collector(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æŒ‡æ ‡æ”¶é›†å™¨");

        let metrics = self.metric_collector.metrics.clone();
        let time_series = self.metric_collector.time_series.clone();
        let interval = self.metric_collector.collection_interval;
        let retention = self.metric_collector.retention_period;

        self.metric_collector.running.store(true, Ordering::SeqCst);

        let running = self.metric_collector.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ”¶é›†ç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºæŒ‡æ ‡
                let now = Utc::now();

                // æ¨¡æ‹Ÿæ”¶é›†ä¸€äº›éšæœºæŒ‡æ ‡
                {
                    let metrics_read = metrics.read().unwrap();
                    let mut time_series_write = time_series.write().unwrap();

                    for (name, metric) in metrics_read.iter() {
                        if let Some(samples) = time_series_write.get_mut(name) {
                            // ç”Ÿæˆéšæœºå€¼
                            let value = match metric.metric_type {
                                MetricType::Counter => {
                                    let last_value = samples.last().map_or(0.0, |s| s.value);
                                    last_value + rand::random::<f64>() * 10.0
                                },
                                MetricType::Gauge => {
                                    rand::random::<f64>() * 100.0
                                },
                                MetricType::Histogram | MetricType::Summary => {
                                    rand::random::<f64>() * 1000.0
                                },
                            };

                            let sample = MetricSample {
                                metric_name: name.clone(),
                                timestamp: now,
                                value,
                                labels: metric.labels.clone(),
                            };

                            samples.push(sample);

                            // åº”ç”¨ä¿ç•™ç­–ç•¥
                            let cutoff = now - retention;
                            while let Some(first) = samples.first() {
                                if first.timestamp < cutoff {
                                    samples.remove(0);
                                } else {
                                    break;
                                }
                            }
                        }
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.metric_collector.collector_thread = Some(thread);

        Ok(())
    }

    fn start_alert_checker(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å‘Šè­¦æ£€æŸ¥å™¨");

        let alert_rules = self.alert_manager.alert_rules.clone();
        let active_alerts = self.alert_manager.active_alerts.clone();
        let time_series = self.metric_collector.time_series.clone();
        let interval = self.alert_manager.checking_interval;

        self.alert_manager.running.store(true, Ordering::SeqCst);

        let running = self.alert_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥æ‰€æœ‰å‘Šè­¦è§„åˆ™
                let now = Utc::now();

                // è·å–æ‰€æœ‰è§„åˆ™
                let rules: Vec<AlertRule> = {
                    let rules = alert_rules.read().unwrap();
                    rules.values().cloned().collect()
                };

                for rule in rules {
                    // æ‰§è¡Œè§„åˆ™æŸ¥è¯¢
                    let query_result = {
                        // ç®€åŒ–ï¼šå‡è®¾æŸ¥è¯¢æ ¼å¼ä¸ºæŒ‡æ ‡åç§°
                        let query = rule.metric_query.trim();
                        let ts = time_series.read().unwrap();

                        if let Some(samples) = ts.get(query) {
                            if let Some(last_sample) = samples.last() {
                                Some(last_sample.value)
                            } else {
                                None
                            }
                        } else {
                            None
                        }
                    };

                    if let Some(value) = query_result {
                        // è¯„ä¼°æ¡ä»¶
                        let condition_met = match rule.condition {
                            AlertCondition::GreaterThan => value > rule.threshold,
                            AlertCondition::LessThan => value < rule.threshold,
                            AlertCondition::Equal => (value - rule.threshold).abs() < 0.0001,
                            AlertCondition::NotEqual => (value - rule.threshold).abs() >= 0.0001,
                        };

                        if condition_met {
                            // æ›´æ–°æˆ–åˆ›å»ºå‘Šè­¦
                            let mut alerts = active_alerts.write().unwrap();

                            if let Some(alert) = alerts.get_mut(&rule.id) {
                                // å¦‚æœå‘Šè­¦å·²ç»å­˜åœ¨ä½†å·²è§£å†³ï¼Œé‡æ–°æ¿€æ´»
                                if alert.status == AlertStatus::Resolved {
                                    alert.status = AlertStatus::Firing;
                                    alert.started_at = now;
                                }

                                alert.value = value;
                                alert.last_updated = now;
                            } else {
                                // åˆ›å»ºæ–°å‘Šè­¦
                                let alert = Alert {
                                    id: uuid::Uuid::new_v4().to_string(),
                                    rule_id: rule.id.clone(),
                                    value,
                                    started_at: now,
                                    last_updated: now,
                                    status: AlertStatus::Firing,
                                    silenced: false,
                                    acknowledged_by: None,
                                    annotations: HashMap::new(),
                                };

                                alerts.insert(rule.id.clone(), alert);

                                // é€šçŸ¥ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼‰
                                println!("å‘Šè­¦è§¦å‘: {}", rule.name);
                            }
                        } else {
                            // è§£å†³å‘Šè­¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            let mut alerts = active_alerts.write().unwrap();

                            if let Some(alert) = alerts.get_mut(&rule.id) {
                                if alert.status == AlertStatus::Firing {
                                    alert.status = AlertStatus::Resolved;
                                    alert.last_updated = now;

                                    // é€šçŸ¥ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼‰
                                    println!("å‘Šè­¦è§£å†³: {}", rule.name);
                                }
                            }
                        }
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.alert_manager.checker_thread = Some(thread);

        Ok(())
    }

    fn register_metric(
        &self,
        name: &str,
        description: &str,
        metric_type: MetricType,
        labels: HashMap<String, String>
    ) -> Result<(), String> {
        println!("æ³¨å†ŒæŒ‡æ ‡: {}", name);

        let mut metrics = self.metric_collector.metrics.write().unwrap();

        if metrics.contains_key(name) {
            return Err(format!("æŒ‡æ ‡å·²å­˜åœ¨: {}", name));
        }

        let metric = Metric {
            name: name.to_string(),
            description: description.to_string(),
            metric_type,
            labels,
            created_at: Utc::now(),
        };

        metrics.insert(name.to_string(), metric);

        // åˆå§‹åŒ–æ—¶é—´åºåˆ—
        let mut time_series = self.metric_collector.time_series.write().unwrap();
        time_series.insert(name.to_string(), Vec::new());

        Ok(())
    }

    fn update_metric(
        &self,
        name: &str,
        value: f64,
        labels: Option<HashMap<String, String>>
    ) -> Result<(), String> {
        let metrics = self.metric_collector.metrics.read().unwrap();

        let metric = metrics.get(name)
            .ok_or_else(|| format!("æŒ‡æ ‡ä¸å­˜åœ¨: {}", name))?;

        let sample = MetricSample {
            metric_name: name.to_string(),
            timestamp: Utc::now(),
            value,
            labels: labels.unwrap_or_else(|| metric.labels.clone()),
        };

        let mut time_series = self.metric_collector.time_series.write().unwrap();

        let samples = time_series.get_mut(name)
            .ok_or_else(|| format!("æŒ‡æ ‡æ—¶é—´åºåˆ—ä¸å­˜åœ¨: {}", name))?;

        samples.push(sample);

        Ok(())
    }

    fn query_metric(
        &self,
        name: &str,
        start_time: DateTime<Utc>,
        end_time: DateTime<Utc>
    ) -> Result<Vec<MetricSample>, String> {
        let time_series = self.metric_collector.time_series.read().unwrap();

        let samples = time_series.get(name)
            .ok_or_else(|| format!("æŒ‡æ ‡ä¸å­˜åœ¨: {}", name))?;

        let filtered: Vec<_> = samples.iter()
            .filter(|s| s.timestamp >= start_time && s.timestamp <= end_time)
            .cloned()
            .collect();

        Ok(filtered)
    }

    fn create_alert_rule(
        &self,
        name: &str,
        description: &str,
        metric_query: &str,
        condition: AlertCondition,
        threshold: f64,
        duration: Duration,
        severity: AlertSeverity,
        notifications: Vec<String>
    ) -> Result<String, String> {
        println!("åˆ›å»ºå‘Šè­¦è§„åˆ™: {}", name);

        let mut alert_rules = self.alert_manager.alert_rules.write().unwrap();

        // æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
        for rule in alert_rules.values() {
            if rule.name == name {
                return Err(format!("å‘Šè­¦è§„åˆ™åç§°å·²å­˜åœ¨: {}", name));
            }
        }

        let now = Utc::now();
        let id = uuid::Uuid::new_v4().to_string();

        let rule = AlertRule {
            id: id.clone(),
            name: name.to_string(),
            description: description.to_string(),
            metric_query: metric_query.to_string(),
            condition,
            threshold,
            duration,
            severity,
            notifications,
            created_at: now,
            updated_at: now,
        };

        alert_rules.insert(id.clone(), rule);

        Ok(id)
    }

    fn delete_alert_rule(&self, id: &str) -> Result<(), String> {
        println!("åˆ é™¤å‘Šè­¦è§„åˆ™: {}", id);

        let mut alert_rules = self.alert_manager.alert_rules.write().unwrap();

        if !alert_rules.contains_key(id) {
            return Err(format!("å‘Šè­¦è§„åˆ™ä¸å­˜åœ¨: {}", id));
        }

        alert_rules.remove(id);

        // è§£å†³ç›¸å…³å‘Šè­¦
        let mut active_alerts = self.alert_manager.active_alerts.write().unwrap();
        active_alerts.remove(id);

        Ok(())
    }

    fn get_active_alerts(&self) -> Vec<Alert> {
        let active_alerts = self.alert_manager.active_alerts.read().unwrap();
        active_alerts.values().cloned().collect()
    }

    fn silence_alert(&self, alert_id: &str, silenced: bool) -> Result<(), String> {
        let mut active_alerts = self.alert_manager.active_alerts.write().unwrap();

        let alert = active_alerts.values_mut()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| format!("å‘Šè­¦ä¸å­˜åœ¨: {}", alert_id))?;

        alert.silenced = silenced;
        alert.last_updated = Utc::now();

        Ok(())
    }

    fn acknowledge_alert(&self, alert_id: &str, user: &str) -> Result<(), String> {
        let mut active_alerts = self.alert_manager.active_alerts.write().unwrap();

        let alert = active_alerts.values_mut()
            .find(|a| a.id == alert_id)
            .ok_or_else(|| format!("å‘Šè­¦ä¸å­˜åœ¨: {}", alert_id))?;

        alert.acknowledged_by = Some(user.to_string());
        alert.last_updated = Utc::now();

        Ok(())
    }

    fn create_dashboard(
        &self,
        title: &str,
        description: Option<&str>,
        creator: &str
    ) -> Result<String, String> {
        println!("åˆ›å»ºä»ªè¡¨æ¿: {}", title);

        let mut dashboards = self.dashboard_manager.dashboards.write().unwrap();

        // æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
        for dashboard in dashboards.values() {
            if dashboard.title == title {
                return Err(format!("ä»ªè¡¨æ¿åç§°å·²å­˜åœ¨: {}", title));
            }
        }

        let now = Utc::now();
        let id = uuid::Uuid::new_v4().to_string();

        let dashboard = Dashboard {
            id: id.clone(),
            title: title.to_string(),
            description: description.map(|s| s.to_string()),
            panels: Vec::new(),
            refresh_interval: Duration::from_secs(60),
            created_at: now,
            updated_at: now,
            created_by: creator.to_string(),
        };

        dashboards.insert(id.clone(), dashboard);

        Ok(id)
    }

    fn add_panel(
        &self,
        dashboard_id: &str,
        title: &str,
        panel_type: PanelType,
        position: PanelPosition,
        data_source: Option<&str>,
        query: Option<&str>,
        options: HashMap<String, String>
    ) -> Result<String, String> {
        println!("æ·»åŠ é¢æ¿: {} -> {}", dashboard_id, title);

        let mut dashboards = self.dashboard_manager.dashboards.write().unwrap();

        let dashboard = dashboards.get_mut(dashboard_id)
            .ok_or_else(|| format!("ä»ªè¡¨æ¿ä¸å­˜åœ¨: {}", dashboard_id))?;

        let id = uuid::Uuid::new_v4().to_string();

        let panel = Panel {
            id: id.clone(),
            title: title.to_string(),
            panel_type,
            position,
            data_source: data_source.map(|s| s.to_string()),
            query: query.map(|s| s.to_string()),
            options,
        };

        dashboard.panels.push(panel);
        dashboard.updated_at = Utc::now();

        Ok(id)
    }

    fn get_dashboard(&self, id: &str) -> Result<Dashboard, String> {
        let dashboards = self.dashboard_manager.dashboards.read().unwrap();

        let dashboard = dashboards.get(id)
            .ok_or_else(|| format!("ä»ªè¡¨æ¿ä¸å­˜åœ¨: {}", id))?;

        Ok(dashboard.clone())
    }

    fn list_dashboards(&self) -> Vec<Dashboard> {
        let dashboards = self.dashboard_manager.dashboards.read().unwrap();
        dashboards.values().cloned().collect()
    }

    fn create_notification_channel(
        &self,
        name: &str,
        channel_type: NotificationType,
        config: HashMap<String, String>
    ) -> Result<String, String> {
        println!("åˆ›å»ºé€šçŸ¥æ¸ é“: {}", name);

        let mut channels = self.notification_service.channels.write().unwrap();

        // æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
        for channel in channels.values() {
            if channel.name == name {
                return Err(format!("é€šçŸ¥æ¸ é“åç§°å·²å­˜åœ¨: {}", name));
            }
        }

        let id = uuid::Uuid::new_v4().to_string();

        let channel = NotificationChannel {
            id: id.clone(),
            name: name.to_string(),
            channel_type,
            config,
            enabled: true,
        };

        channels.insert(id.clone(), channel);

        Ok(id)
    }

    fn send_notification(
        &self,
        channel_id: &str,
        subject: &str,
        message: &str
    ) -> Result<(), String> {
        println!("å‘é€é€šçŸ¥: {} -> {}", channel_id, subject);

        let channels = self.notification_service.channels.read().unwrap();

        let channel = channels.get(channel_id)
            .ok_or_else(|| format!("é€šçŸ¥æ¸ é“ä¸å­˜åœ¨: {}", channel_id))?;

        if !channel.enabled {
            return Err("é€šçŸ¥æ¸ é“å·²ç¦ç”¨".to_string());
        }

        // æ ¹æ®é€šçŸ¥æ¸ é“ç±»å‹å‘é€æ¶ˆæ¯
        match channel.channel_type {
            NotificationType::Email => {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€ç”µå­é‚®ä»¶
                println!("å‘é€ç”µå­é‚®ä»¶é€šçŸ¥: {}", subject);
            },
            NotificationType::Slack => {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€Slackæ¶ˆæ¯
                println!("å‘é€Slacké€šçŸ¥: {}", subject);
            },
            NotificationType::Webhook => {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨Webhook
                println!("è°ƒç”¨Webhooké€šçŸ¥: {}", subject);
            },
            NotificationType::PagerDuty => {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§¦å‘PagerDutyäº‹ä»¶
                println!("è§¦å‘PagerDutyé€šçŸ¥: {}", subject);
            },
            NotificationType::SMS => {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€SMSçŸ­ä¿¡
                println!("å‘é€SMSé€šçŸ¥: {}", subject);
            },
        }

        Ok(())
    }

    fn disable_notification_channel(&self, channel_id: &str) -> Result<(), String> {
        let mut channels = self.notification_service.channels.write().unwrap();

        let channel = channels.get_mut(channel_id)
            .ok_or_else(|| format!("é€šçŸ¥æ¸ é“ä¸å­˜åœ¨: {}", channel_id))?;

        channel.enabled = false;

        Ok(())
    }

    fn enable_notification_channel(&self, channel_id: &str) -> Result<(), String> {
        let mut channels = self.notification_service.channels.write().unwrap();

        let channel = channels.get_mut(channel_id)
            .ok_or_else(|| format!("é€šçŸ¥æ¸ é“ä¸å­˜åœ¨: {}", channel_id))?;

        channel.enabled = true;

        Ok(())
    }
}

impl Clone for Metric {
    fn clone(&self) -> Self {
        Metric {
            name: self.name.clone(),
            description: self.description.clone(),
            metric_type: self.metric_type.clone(),
            labels: self.labels.clone(),
            created_at: self.created_at,
        }
    }
}

impl Clone for MetricType {
    fn clone(&self) -> Self {
        match self {
            MetricType::Counter => MetricType::Counter,
            MetricType::Gauge => MetricType::Gauge,
            MetricType::Histogram => MetricType::Histogram,
            MetricType::Summary => MetricType::Summary,
        }
    }
}

impl Clone for MetricSample {
    fn clone(&self) -> Self {
        MetricSample {
            metric_name: self.metric_name.clone(),
            timestamp: self.timestamp,
            value: self.value,
            labels: self.labels.clone(),
        }
    }
}

impl Clone for AlertRule {
    fn clone(&self) -> Self {
        AlertRule {
            id: self.id.clone(),
            name: self.name.clone(),
            description: self.description.clone(),
            metric_query: self.metric_query.clone(),
            condition: self.condition.clone(),
            threshold: self.threshold,
            duration: self.duration,
            severity: self.severity.clone(),
            notifications: self.notifications.clone(),
            created_at: self.created_at,
            updated_at: self.updated_at,
        }
    }
}

impl Clone for AlertCondition {
    fn clone(&self) -> Self {
        match self {
            AlertCondition::GreaterThan => AlertCondition::GreaterThan,
            AlertCondition::LessThan => AlertCondition::LessThan,
            AlertCondition::Equal => AlertCondition::Equal,
            AlertCondition::NotEqual => AlertCondition::NotEqual,
        }
    }
}

impl Clone for AlertSeverity {
    fn clone(&self) -> Self {
        match self {
            AlertSeverity::Info => AlertSeverity::Info,
            AlertSeverity::Warning => AlertSeverity::Warning,
            AlertSeverity::Error => AlertSeverity::Error,
            AlertSeverity::Critical => AlertSeverity::Critical,
        }
    }
}

impl Clone for Alert {
    fn clone(&self) -> Self {
        Alert {
            id: self.id.clone(),
            rule_id: self.rule_id.clone(),
            value: self.value,
            started_at: self.started_at,
            last_updated: self.last_updated,
            status: self.status.clone(),
            silenced: self.silenced,
            acknowledged_by: self.acknowledged_by.clone(),
            annotations: self.annotations.clone(),
        }
    }
}

impl Clone for AlertStatus {
    fn clone(&self) -> Self {
        match self {
            AlertStatus::Firing => AlertStatus::Firing,
            AlertStatus::Resolved => AlertStatus::Resolved,
        }
    }
}

impl Clone for Dashboard {
    fn clone(&self) -> Self {
        Dashboard {
            id: self.id.clone(),
            title: self.title.clone(),
            description: self.description.clone(),
            panels: self.panels.clone(),
            refresh_interval: self.refresh_interval,
            created_at: self.created_at,
            updated_at: self.updated_at,
            created_by: self.created_by.clone(),
        }
    }
}

impl Clone for Panel {
    fn clone(&self) -> Self {
        Panel {
            id: self.id.clone(),
            title: self.title.clone(),
            panel_type: self.panel_type.clone(),
            position: self.position.clone(),
            data_source: self.data_source.clone(),
            query: self.query.clone(),
            options: self.options.clone(),
        }
    }
}

impl Clone for PanelType {
    fn clone(&self) -> Self {
        match self {
            PanelType::Graph => PanelType::Graph,
            PanelType::SingleStat => PanelType::SingleStat,
            PanelType::Table => PanelType::Table,
            PanelType::Text => PanelType::Text,
            PanelType::Heatmap => PanelType::Heatmap,
        }
    }
}

impl Clone for PanelPosition {
    fn clone(&self) -> Self {
        PanelPosition {
            x: self.x,
            y: self.y,
            width: self.width,
            height: self.height,
        }
    }
}

// åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ
struct DistributedTracingSystem {
    node_id: String,
    trace_collector: TraceCollector,
    trace_storage: TraceStorage,
    sampling_manager: SamplingManager,
    query_service: TraceQueryService,
}

struct TraceCollector {
    traces: RwLock<HashMap<String, Trace>>,
    spans: RwLock<HashMap<String, Span>>,
    flush_interval: Duration,
    batch_size: usize,
    running: AtomicBool,
    collector_thread: Option<JoinHandle<()>>,
}

struct Trace {
    trace_id: String,
    name: String,
    start_time: DateTime<Utc>,
    end_time: Option<DateTime<Utc>>,
    root_span_id: Option<String>,
    status: TraceStatus,
    service_name: String,
    tags: HashMap<String, String>,
}

struct Span {
    span_id: String,
    trace_id: String,
    parent_span_id: Option<String>,
    name: String,
    start_time: DateTime<Utc>,
    end_time: Option<DateTime<Utc>>,
    status: SpanStatus,
    service_name: String,
    operation_name: String,
    tags: HashMap<String, String>,
    logs: Vec<SpanLog>,
}

struct SpanLog {
    timestamp: DateTime<Utc>,
    fields: HashMap<String, String>,
}

enum TraceStatus {
    Active,
    Completed,
    Error,
}

enum SpanStatus {
    Success,
    Error,
    Canceled,
}

struct TraceStorage {
    storage_type: StorageType,
    retention_days: u32,
    index_fields: Vec<String>,
    connection_string: String,
}

enum StorageType {
    InMemory,
    ElasticSearch,
    Cassandra,
    Custom(String),
}

struct SamplingManager {
    strategies: RwLock<HashMap<String, SamplingStrategy>>,
    default_rate: f64,
}

enum SamplingStrategy {
    RateLimiting {
        traces_per_second: u32,
    },
    Probabilistic {
        sampling_rate: f64,
    },
    RulesBased {
        rules: Vec<SamplingRule>,
    },
}

struct SamplingRule {
    service_name: Option<String>,
    operation_name: Option<String>,
    tags: HashMap<String, String>,
    sampling_rate: f64,
}

struct TraceQueryService {
    max_query_size: usize,
    max_results: usize,
    query_timeout: Duration,
}

impl DistributedTracingSystem {
    fn new(node_id: &str, storage_type: StorageType, connection_string: &str) -> Self {
        let trace_collector = TraceCollector {
            traces: RwLock::new(HashMap::new()),
            spans: RwLock::new(HashMap::new()),
            flush_interval: Duration::from_secs(10),
            batch_size: 1000,
            running: AtomicBool::new(false),
            collector_thread: None,
        };

        let trace_storage = TraceStorage {
            storage_type,
            retention_days: 7,
            index_fields: vec![
                "service_name".to_string(),
                "operation_name".to_string(),
                "status".to_string(),
            ],
            connection_string: connection_string.to_string(),
        };

        let sampling_manager = SamplingManager {
            strategies: RwLock::new(HashMap::new()),
            default_rate: 0.1, // é»˜è®¤é‡‡æ ·10%
        };

        let query_service = TraceQueryService {
            max_query_size: 10000,
            max_results: 1000,
            query_timeout: Duration::from_secs(30),
        };

        DistributedTracingSystem {
            node_id: node_id.to_string(),
            trace_collector,
            trace_storage,
            sampling_manager,
            query_service,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ");

        // å¯åŠ¨è·Ÿè¸ªæ”¶é›†å™¨
        self.start_trace_collector()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ");

        // åœæ­¢è·Ÿè¸ªæ”¶é›†å™¨
        self.trace_collector.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.trace_collector.collector_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("è·Ÿè¸ªæ”¶é›†çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn start_trace_collector(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨è·Ÿè¸ªæ”¶é›†å™¨");

        let traces = self.trace_collector.traces.clone();
        let spans = self.trace_collector.spans.clone();
        let interval = self.trace_collector.flush_interval;
        let batch_size = self.trace_collector.batch_size;
        let storage_type = self.trace_storage.storage_type.clone();
        let connection_string = self.trace_storage.connection_string.clone();

        self.trace_collector.running.store(true, Ordering::SeqCst);

        let running = self.trace_collector.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æŸ¥æ‰¾å·²å®Œæˆçš„è·Ÿè¸ª
                let mut completed_traces = Vec::new();
                let mut completed_spans = Vec::new();

                {
                    let traces_read = traces.read().unwrap();
                    let spans_read = spans.read().unwrap();

                    for (trace_id, trace) in traces_read.iter() {
                        if trace.status == TraceStatus::Completed || trace.status == TraceStatus::Error {
                            // æ”¶é›†è·Ÿè¸ªåŠå…¶æ‰€æœ‰è·¨åº¦
                            completed_traces.push(trace.clone());

                            for (span_id, span) in spans_read.iter() {
                                if span.trace_id == *trace_id {
                                    completed_spans.push(span.clone());
                                }
                            }
                        }
                    }
                }

                if !completed_traces.is_empty() {
                    // åˆ†æ‰¹å¤„ç†
                    for chunk in completed_traces.chunks(batch_size) {
                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†è·Ÿè¸ªå‘é€åˆ°å­˜å‚¨åç«¯
                        println!("åˆ·æ–° {} ä¸ªè·Ÿè¸ªåˆ°å­˜å‚¨", chunk.len());

                        match storage_type {
                            StorageType::InMemory => {
                                // åœ¨å†…å­˜ä¸­ï¼Œå·²ç»å­˜å‚¨ï¼Œåªéœ€ç§»é™¤å·²å¤„ç†çš„
                            },
                            StorageType::ElasticSearch => {
                                // å‘é€åˆ° ElasticSearch
                                println!("å°†è·Ÿè¸ªå‘é€åˆ° ElasticSearch: {}", connection_string);
                            },
                            StorageType::Cassandra => {
                                // å‘é€åˆ° Cassandra
                                println!("å°†è·Ÿè¸ªå‘é€åˆ° Cassandra: {}", connection_string);
                            },
                            StorageType::Custom(ref name) => {
                                // å‘é€åˆ°è‡ªå®šä¹‰å­˜å‚¨
                                println!("å°†è·Ÿè¸ªå‘é€åˆ° {}: {}", name, connection_string);
                            },
                        }
                    }

                    // ç§»é™¤å·²å¤„ç†çš„è·Ÿè¸ªå’Œè·¨åº¦
                    let mut traces_write = traces.write().unwrap();
                    let mut spans_write = spans.write().unwrap();

                    for trace in &completed_traces {
                        traces_write.remove(&trace.trace_id);

                        // ç§»é™¤å…³è”çš„è·¨åº¦
                        spans_write.retain(|_, span| span.trace_id != trace.trace_id);
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.trace_collector.collector_thread = Some(thread);

        Ok(())
    }

    fn start_trace(&self, name: &str, service_name: &str, tags: HashMap<String, String>) -> Result<String, String> {
        println!("å¼€å§‹è·Ÿè¸ª: {}", name);

        // å†³å®šæ˜¯å¦é‡‡æ ·
        if !self.should_sample(service_name, name, &tags) {
            // å¦‚æœä¸é‡‡æ ·ï¼Œè¿”å›ä¸€ä¸ªç‰¹æ®Šçš„è·Ÿè¸ªIDè¡¨ç¤ºè·Ÿè¸ªè¢«å¿½ç•¥
            return Ok("00000000-0000-0000-0000-000000000000".to_string());
        }

        let trace_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let trace = Trace {
            trace_id: trace_id.clone(),
            name: name.to_string(),
            start_time: now,
            end_time: None,
            root_span_id: None,
            status: TraceStatus::Active,
            service_name: service_name.to_string(),
            tags,
        };

        let mut traces = self.trace_collector.traces.write().unwrap();
        traces.insert(trace_id.clone(), trace);

        Ok(trace_id)
    }

    fn should_sample(&self, service_name: &str, operation_name: &str, tags: &HashMap<String, String>) -> bool {
        let strategies = self.sampling_manager.strategies.read().unwrap();

        // é¦–å…ˆæ£€æŸ¥åŸºäºè§„åˆ™çš„ç­–ç•¥
        for (_, strategy) in strategies.iter() {
            if let SamplingStrategy::RulesBased { rules } = strategy {
                for rule in rules {
                    let service_match = rule.service_name.as_ref().map_or(true, |s| s == service_name);
                    let operation_match = rule.operation_name.as_ref().map_or(true, |o| o == operation_name);

                    let tags_match = rule.tags.iter().all(|(k, v)| {
                        tags.get(k).map_or(false, |tag_value| tag_value == v)
                    });

                    if service_match && operation_match && tags_match {
                        // åŒ¹é…è§„åˆ™ï¼Œåº”ç”¨é‡‡æ ·ç‡
                        return rand::random::<f64>() < rule.sampling_rate;
                    }
                }
            }
        }

        // æ£€æŸ¥æœåŠ¡ç‰¹å®šçš„ç­–ç•¥
        if let Some(strategy) = strategies.get(service_name) {
            match strategy {
                SamplingStrategy::Probabilistic { sampling_rate } => {
                    return rand::random::<f64>() < *sampling_rate;
                },
                SamplingStrategy::RateLimiting { traces_per_second } => {
                    // ç®€åŒ–å®ç°ï¼Œåœ¨å®é™…æƒ…å†µä¸‹éœ€è¦è¿½è¸ªå•ä½æ—¶é—´å†…çš„è¯·æ±‚æ•°
                    return rand::random::<u32>() % 100 < *traces_per_second;
                },
                _ => {}
            }
        }

        // åº”ç”¨é»˜è®¤é‡‡æ ·ç‡
        rand::random::<f64>() < self.sampling_manager.default_rate
    }

    fn end_trace(&self, trace_id: &str, status: TraceStatus) -> Result<(), String> {
        println!("ç»“æŸè·Ÿè¸ª: {}", trace_id);

        let mut traces = self.trace_collector.traces.write().unwrap();

        let trace = traces.get_mut(trace_id)
            .ok_or_else(|| format!("è·Ÿè¸ªä¸å­˜åœ¨: {}", trace_id))?;

        trace.end_time = Some(Utc::now());
        trace.status = status;

        Ok(())
    }

    fn start_span(
        &self,
        trace_id: &str,
        parent_span_id: Option<&str>,
        name: &str,
        service_name: &str,
        operation_name: &str,
        tags: HashMap<String, String>
    ) -> Result<String, String> {
        println!("å¼€å§‹è·¨åº¦: {} åœ¨è·Ÿè¸ª {}", name, trace_id);

        // æ£€æŸ¥è·Ÿè¸ªæ˜¯å¦å­˜åœ¨
        let mut traces = self.trace_collector.traces.write().unwrap();

        if !traces.contains_key(trace_id) {
            return Err(format!("è·Ÿè¸ªä¸å­˜åœ¨: {}", trace_id));
        }

        let span_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let span = Span {
            span_id: span_id.clone(),
            trace_id: trace_id.to_string(),
            parent_span_id: parent_span_id.map(|s| s.to_string()),
            name: name.to_string(),
            start_time: now,
            end_time: None,
            status: SpanStatus::Success,
            service_name: service_name.to_string(),
            operation_name: operation_name.to_string(),
            tags,
            logs: Vec::new(),
        };

        let mut spans = self.trace_collector.spans.write().unwrap();
        spans.insert(span_id.clone(), span);

        // å¦‚æœæ˜¯æ ¹è·¨åº¦ï¼Œæ›´æ–°è·Ÿè¸ª
        if parent_span_id.is_none() {
            if let Some(trace) = traces.get_mut(trace_id) {
                if trace.root_span_id.is_none() {
                    trace.root_span_id = Some(span_id.clone());
                }
            }
        }

        Ok(span_id)
    }

    fn end_span(&self, span_id: &str, status: SpanStatus) -> Result<(), String> {
        println!("ç»“æŸè·¨åº¦: {}", span_id);

        let mut spans = self.trace_collector.spans.write().unwrap();

        let span = spans.get_mut(span_id)
            .ok_or_else(|| format!("è·¨åº¦ä¸å­˜åœ¨: {}", span_id))?;

        span.end_time = Some(Utc::now());
        span.status = status;

        Ok(())
    }

    fn add_span_log(
        &self,
        span_id: &str,
        fields: HashMap<String, String>
    ) -> Result<(), String> {
        let mut spans = self.trace_collector.spans.write().unwrap();

        let span = spans.get_mut(span_id)
            .ok_or_else(|| format!("è·¨åº¦ä¸å­˜åœ¨: {}", span_id))?;

        let log = SpanLog {
            timestamp: Utc::now(),
            fields,
        };

        span.logs.push(log);

        Ok(())
    }

    fn add_span_tag(
        &self,
        span_id: &str,
        key: &str,
        value: &str
    ) -> Result<(), String> {
        let mut spans = self.trace_collector.spans.write().unwrap();

        let span = spans.get_mut(span_id)
            .ok_or_else(|| format!("è·¨åº¦ä¸å­˜åœ¨: {}", span_id))?;

        span.tags.insert(key.to_string(), value.to_string());

        Ok(())
    }

    fn query_traces(
        &self,
        services: Option<Vec<String>>,
        operations: Option<Vec<String>>,
        tags: Option<HashMap<String, String>>,
        start_time: Option<DateTime<Utc>>,
        end_time: Option<DateTime<Utc>>,
        limit: Option<usize>
    ) -> Result<Vec<Trace>, String> {
        println!("æŸ¥è¯¢è·Ÿè¸ª");

        let traces = self.trace_collector.traces.read().unwrap();

        let mut result: Vec<_> = traces.values().cloned().collect();

        // åº”ç”¨ç­›é€‰æ¡ä»¶
        if let Some(svcs) = &services {
            result.retain(|t| svcs.contains(&t.service_name));
        }

        if let Some(start) = start_time {
            result.retain(|t| t.start_time >= start);
        }

        if let Some(end) = end_time {
            result.retain(|t| t.start_time <= end);
        }

        if let Some(tgs) = &tags {
            result.retain(|t| {
                tgs.iter().all(|(k, v)| {
                    t.tags.get(k).map_or(false, |val| val == v)
                })
            });
        }

        // æ£€æŸ¥è·¨åº¦çš„æ“ä½œ
        if let Some(ops) = &operations {
            let spans = self.trace_collector.spans.read().unwrap();

            result.retain(|t| {
                spans.values()
                    .any(|s| s.trace_id == t.trace_id && ops.contains(&s.operation_name))
            });
        }

        // åº”ç”¨é™åˆ¶
        let limit = limit.unwrap_or_else(|| self.query_service.max_results);
        if result.len() > limit {
            result.truncate(limit);
        }

        Ok(result)
    }

    fn get_trace_details(&self, trace_id: &str) -> Result<(Trace, Vec<Span>), String> {
        println!("è·å–è·Ÿè¸ªè¯¦æƒ…: {}", trace_id);

        let traces = self.trace_collector.traces.read().unwrap();

        let trace = traces.get(trace_id)
            .ok_or_else(|| format!("è·Ÿè¸ªä¸å­˜åœ¨: {}", trace_id))?
            .clone();

        let spans = self.trace_collector.spans.read().unwrap();

        let trace_spans: Vec<_> = spans.values()
            .filter(|s| s.trace_id == trace_id)
            .cloned()
            .collect();

        Ok((trace, trace_spans))
    }

    fn set_sampling_strategy(
        &self,
        service_name: &str,
        strategy: SamplingStrategy
    ) -> Result<(), String> {
        println!("è®¾ç½®é‡‡æ ·ç­–ç•¥: {}", service_name);

        let mut strategies = self.sampling_manager.strategies.write().unwrap();
        strategies.insert(service_name.to_string(), strategy);

        Ok(())
    }

    fn set_default_sampling_rate(&mut self, rate: f64) -> Result<(), String> {
        if rate < 0.0 || rate > 1.0 {
            return Err("é‡‡æ ·ç‡å¿…é¡»åœ¨0.0å’Œ1.0ä¹‹é—´".to_string());
        }

        self.sampling_manager.default_rate = rate;

        Ok(())
    }
}

impl Clone for Trace {
    fn clone(&self) -> Self {
        Trace {
            trace_id: self.trace_id.clone(),
            name: self.name.clone(),
            start_time: self.start_time,
            end_time: self.end_time,
            root_span_id: self.root_span_id.clone(),
            status: self.status.clone(),
            service_name: self.service_name.clone(),
            tags: self.tags.clone(),
        }
    }
}

impl Clone for TraceStatus {
    fn clone(&self) -> Self {
        match self {
            TraceStatus::Active => TraceStatus::Active,
            TraceStatus::Completed => TraceStatus::Completed,
            TraceStatus::Error => TraceStatus::Error,
        }
    }
}

impl Clone for SpanStatus {
    fn clone(&self) -> Self {
        match self {
            SpanStatus::Success => SpanStatus::Success,
            SpanStatus::Error => SpanStatus::Error,
            SpanStatus::Canceled => SpanStatus::Canceled,
        }
    }
}

impl Clone for Span {
    fn clone(&self) -> Self {
        Span {
            span_id: self.span_id.clone(),
            trace_id: self.trace_id.clone(),
            parent_span_id: self.parent_span_id.clone(),
            name: self.name.clone(),
            start_time: self.start_time,
            end_time: self.end_time,
            status: self.status.clone(),
            service_name: self.service_name.clone(),
            operation_name: self.operation_name.clone(),
            tags: self.tags.clone(),
            logs: self.logs.clone(),
        }
    }
}

impl Clone for SpanLog {
    fn clone(&self) -> Self {
        SpanLog {
            timestamp: self.timestamp,
            fields: self.fields.clone(),
        }
    }
}

impl Clone for StorageType {
    fn clone(&self) -> Self {
        match self {
            StorageType::InMemory => StorageType::InMemory,
            StorageType::ElasticSearch => StorageType::ElasticSearch,
            StorageType::Cassandra => StorageType::Cassandra,
            StorageType::Custom(name) => StorageType::Custom(name.clone()),
        }
    }
}
```

### 1.12 ç»¼åˆåº”ç”¨12-åˆ†å¸ƒå¼å…±è¯†å¼•æ“

```rust
// åˆ†å¸ƒå¼å…±è¯†å¼•æ“ï¼ˆRaft å®ç°ï¼‰
struct RaftConsensusEngine {
    node_id: String,
    cluster_config: ClusterConfig,
    state_machine: StateMachine,
    log_manager: LogManager,
    state_manager: StateManager,
    network_manager: NetworkManager,
    election_timer: ElectionTimer,
    heartbeat_timer: HeartbeatTimer,
}

struct ClusterConfig {
    nodes: HashMap<String, NodeInfo>,
    election_timeout_min: Duration,
    election_timeout_max: Duration,
    heartbeat_interval: Duration,
    snapshot_threshold: u64,
    max_log_batch_size: usize,
}

struct NodeInfo {
    id: String,
    address: String,
    voter: bool,
}

struct StateMachine {
    state: RwLock<HashMap<String, Vec<u8>>>,
    applied_index: AtomicU64,
    snapshot_index: AtomicU64,
}

struct LogManager {
    log_entries: RwLock<BTreeMap<u64, LogEntry>>,
    last_log_index: AtomicU64,
    last_log_term: AtomicU64,
    commit_index: AtomicU64,
    log_dir: PathBuf,
}

struct LogEntry {
    index: u64,
    term: u64,
    command: Command,
    timestamp: DateTime<Utc>,
}

enum Command {
    Put { key: String, value: Vec<u8> },
    Delete { key: String },
    Snapshot { index: u64, term: u64, data: Vec<u8> },
    Configuration { nodes: HashMap<String, NodeInfo> },
    Noop,
}

struct StateManager {
    current_term: AtomicU64,
    voted_for: RwLock<Option<String>>,
    node_state: RwLock<NodeState>,
    leader_id: RwLock<Option<String>>,
    state_dir: PathBuf,
}

enum NodeState {
    Follower,
    Candidate,
    Leader,
}

struct NetworkManager {
    server: Option<JoinHandle<()>>,
    clients: RwLock<HashMap<String, RaftClient>>,
    running: AtomicBool,
    bind_address: String,
}

struct RaftClient {
    node_id: String,
    address: String,
    last_success: DateTime<Utc>,
    next_index: AtomicU64,
    match_index: AtomicU64,
}

struct ElectionTimer {
    timeout: RwLock<Duration>,
    last_reset: RwLock<DateTime<Utc>>,
    running: AtomicBool,
    timer_thread: Option<JoinHandle<()>>,
}

struct HeartbeatTimer {
    interval: Duration,
    running: AtomicBool,
    timer_thread: Option<JoinHandle<()>>,
}

impl RaftConsensusEngine {
    fn new(node_id: &str, cluster_config: ClusterConfig, data_dir: &Path) -> Self {
        let state_machine = StateMachine {
            state: RwLock::new(HashMap::new()),
            applied_index: AtomicU64::new(0),
            snapshot_index: AtomicU64::new(0),
        };

        let log_manager = LogManager {
            log_entries: RwLock::new(BTreeMap::new()),
            last_log_index: AtomicU64::new(0),
            last_log_term: AtomicU64::new(0),
            commit_index: AtomicU64::new(0),
            log_dir: data_dir.join("logs"),
        };

        let state_manager = StateManager {
            current_term: AtomicU64::new(0),
            voted_for: RwLock::new(None),
            node_state: RwLock::new(NodeState::Follower),
            leader_id: RwLock::new(None),
            state_dir: data_dir.join("state"),
        };

        let node_info = cluster_config.nodes.get(node_id)
            .expect("å½“å‰èŠ‚ç‚¹IDå¿…é¡»åœ¨é›†ç¾¤é…ç½®ä¸­");

        let network_manager = NetworkManager {
            server: None,
            clients: RwLock::new(HashMap::new()),
            running: AtomicBool::new(false),
            bind_address: node_info.address.clone(),
        };

        // éšæœºé€‰æ‹©é€‰ä¸¾è¶…æ—¶
        let mut rng = rand::thread_rng();
        let range = cluster_config.election_timeout_max - cluster_config.election_timeout_min;
        let random_timeout = cluster_config.election_timeout_min +
                             Duration::from_millis(rng.gen_range(0..range.as_millis() as u64));

        let election_timer = ElectionTimer {
            timeout: RwLock::new(random_timeout),
            last_reset: RwLock::new(Utc::now()),
            running: AtomicBool::new(false),
            timer_thread: None,
        };

        let heartbeat_timer = HeartbeatTimer {
            interval: cluster_config.heartbeat_interval,
            running: AtomicBool::new(false),
            timer_thread: None,
        };

        RaftConsensusEngine {
            node_id: node_id.to_string(),
            cluster_config,
            state_machine,
            log_manager,
            state_manager,
            network_manager,
            election_timer,
            heartbeat_timer,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨Raftå…±è¯†å¼•æ“");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.log_manager.log_dir) {
            return Err(format!("åˆ›å»ºæ—¥å¿—ç›®å½•å¤±è´¥: {}", e));
        }

        if let Err(e) = std::fs::create_dir_all(&self.state_manager.state_dir) {
            return Err(format!("åˆ›å»ºçŠ¶æ€ç›®å½•å¤±è´¥: {}", e));
        }

        // æ¢å¤æŒä¹…åŒ–çŠ¶æ€
        self.recover_state()?;

        // å¯åŠ¨ç½‘ç»œæœåŠ¡å™¨
        self.start_network_server()?;

        // åˆå§‹åŒ–ä¸å…¶ä»–èŠ‚ç‚¹çš„è¿æ¥
        self.initialize_clients()?;

        // å¯åŠ¨é€‰ä¸¾å®šæ—¶å™¨
        self.start_election_timer()?;

        // å¯åŠ¨å¿ƒè·³å®šæ—¶å™¨ï¼ˆä»…å½“æˆä¸ºé¢†å¯¼è€…æ—¶ä½¿ç”¨ï¼‰
        self.start_heartbeat_timer()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢Raftå…±è¯†å¼•æ“");

        // åœæ­¢å¿ƒè·³å®šæ—¶å™¨
        self.heartbeat_timer.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.heartbeat_timer.timer_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¿ƒè·³å®šæ—¶å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢é€‰ä¸¾å®šæ—¶å™¨
        self.election_timer.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.election_timer.timer_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("é€‰ä¸¾å®šæ—¶å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢ç½‘ç»œæœåŠ¡å™¨
        self.network_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.network_manager.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("ç½‘ç»œæœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // æŒä¹…åŒ–çŠ¶æ€
        self.persist_state()?;

        Ok(())
    }

    fn recover_state(&self) -> Result<(), String> {
        println!("æ¢å¤æŒä¹…åŒ–çŠ¶æ€");

        // æ¢å¤å…ƒæ•°æ®
        let metadata_path = self.state_manager.state_dir.join("metadata");
        if metadata_path.exists() {
            let metadata = match std::fs::read_to_string(&metadata_path) {
                Ok(content) => content,
                Err(e) => return Err(format!("è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {}", e)),
            };

            // è§£æå…ƒæ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            for line in metadata.lines() {
                let parts: Vec<&str> = line.split('=').collect();
                if parts.len() == 2 {
                    match parts[0] {
                        "current_term" => {
                            if let Ok(term) = parts[1].parse::<u64>() {
                                self.state_manager.current_term.store(term, Ordering::SeqCst);
                            }
                        },
                        "voted_for" => {
                            if parts[1] != "null" {
                                let mut voted_for = self.state_manager.voted_for.write().unwrap();
                                *voted_for = Some(parts[1].to_string());
                            }
                        },
                        _ => {},
                    }
                }
            }
        }

        // æ¢å¤æ—¥å¿—æ¡ç›®
        let log_path = self.log_manager.log_dir.join("log");
        if log_path.exists() {
            let log_content = match std::fs::read_to_string(&log_path) {
                Ok(content) => content,
                Err(e) => return Err(format!("è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {}", e)),
            };

            let mut log_entries = self.log_manager.log_entries.write().unwrap();

            // è§£ææ—¥å¿—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            for line in log_content.lines() {
                let parts: Vec<&str> = line.split(',').collect();
                if parts.len() >= 3 {
                    if let (Ok(index), Ok(term)) = (parts[0].parse::<u64>(), parts[1].parse::<u64>()) {
                        // ç®€åŒ–çš„å‘½ä»¤è§£æ
                        let command = match parts[2] {
                            "PUT" => {
                                if parts.len() >= 5 {
                                    Command::Put {
                                        key: parts[3].to_string(),
                                        value: parts[4].as_bytes().to_vec(),
                                    }
                                } else {
                                    continue;
                                }
                            },
                            "DELETE" => {
                                if parts.len() >= 4 {
                                    Command::Delete {
                                        key: parts[3].to_string(),
                                    }
                                } else {
                                    continue;
                                }
                            },
                            "NOOP" => Command::Noop,
                            _ => continue,
                        };

                        let entry = LogEntry {
                            index,
                            term,
                            command,
                            timestamp: Utc::now(), // æ— æ³•æ¢å¤åŸå§‹æ—¶é—´æˆ³
                        };

                        log_entries.insert(index, entry);

                        // æ›´æ–°æœ€åæ—¥å¿—ç´¢å¼•å’Œä»»æœŸ
                        if index > self.log_manager.last_log_index.load(Ordering::SeqCst) {
                            self.log_manager.last_log_index.store(index, Ordering::SeqCst);
                            self.log_manager.last_log_term.store(term, Ordering::SeqCst);
                        }
                    }
                }
            }
        }

        Ok(())
    }

    fn persist_state(&self) -> Result<(), String> {
        println!("æŒä¹…åŒ–çŠ¶æ€");

        // æŒä¹…åŒ–å…ƒæ•°æ®
        let metadata_path = self.state_manager.state_dir.join("metadata");
        let current_term = self.state_manager.current_term.load(Ordering::SeqCst);
        let voted_for = self.state_manager.voted_for.read().unwrap();

        let voted_for_str = match &*voted_for {
            Some(node_id) => node_id.clone(),
            None => "null".to_string(),
        };

        let metadata = format!("current_term={}\nvoted_for={}", current_term, voted_for_str);

        if let Err(e) = std::fs::write(&metadata_path, metadata) {
            return Err(format!("å†™å…¥å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {}", e));
        }

        // æŒä¹…åŒ–æ—¥å¿—æ¡ç›®
        let log_path = self.log_manager.log_dir.join("log");
        let log_entries = self.log_manager.log_entries.read().unwrap();

        let mut log_content = String::new();

        for entry in log_entries.values() {
            let command_str = match &entry.command {
                Command::Put { key, value } => {
                    format!("PUT,{},{}", key, String::from_utf8_lossy(value))
                },
                Command::Delete { key } => {
                    format!("DELETE,{}", key)
                },
                Command::Noop => "NOOP".to_string(),
                _ => continue, // è·³è¿‡å…¶ä»–å‘½ä»¤ç±»å‹
            };

            log_content.push_str(&format!("{},{},{}\n", entry.index, entry.term, command_str));
        }

        if let Err(e) = std::fs::write(&log_path, log_content) {
            return Err(format!("å†™å…¥æ—¥å¿—æ–‡ä»¶å¤±è´¥: {}", e));
        }

        Ok(())
    }

    fn start_network_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ç½‘ç»œæœåŠ¡å™¨");

        let bind_address = self.network_manager.bind_address.clone();
        let node_id = self.node_id.clone();
        let state_manager = Arc::new(self.state_manager);
        let log_manager = Arc::new(self.log_manager);
        let state_machine = Arc::new(self.state_machine);
        let election_timer = Arc::new(self.election_timer);

        self.network_manager.running.store(true, Ordering::SeqCst);

        let running = self.network_manager.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªTCPæˆ–HTTPæœåŠ¡å™¨
            println!("ç½‘ç»œæœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†æ¥è‡ªå…¶ä»–èŠ‚ç‚¹çš„RPCè¯·æ±‚
            }
        });

        self.network_manager.server = Some(thread);

        Ok(())
    }

    fn initialize_clients(&self) -> Result<(), String> {
        println!("åˆå§‹åŒ–ä¸å…¶ä»–èŠ‚ç‚¹çš„è¿æ¥");

        let mut clients = self.network_manager.clients.write().unwrap();

        for (id, info) in &self.cluster_config.nodes {
            if id != &self.node_id {
                let client = RaftClient {
                    node_id: id.clone(),
                    address: info.address.clone(),
                    last_success: Utc::now(),
                    next_index: AtomicU64::new(self.log_manager.last_log_index.load(Ordering::SeqCst) + 1),
                    match_index: AtomicU64::new(0),
                };

                clients.insert(id.clone(), client);
            }
        }

        Ok(())
    }

    fn start_election_timer(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨é€‰ä¸¾å®šæ—¶å™¨");

        let timeout = *self.election_timer.timeout.read().unwrap();
        let node_id = self.node_id.clone();
        let state_manager = Arc::new(self.state_manager);
        let log_manager = Arc::new(self.log_manager);
        let network_manager = Arc::new(self.network_manager);
        let cluster_config = Arc::new(self.cluster_config);

        self.election_timer.running.store(true, Ordering::SeqCst);

        let running = self.election_timer.running.clone();
        let last_reset = self.election_timer.last_reset.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                let now = Utc::now();
                let last = *last_reset.read().unwrap();
                let elapsed = now.signed_duration_since(last);

                if elapsed > timeout.to_std().unwrap().into() {
                    // é€‰ä¸¾è¶…æ—¶ï¼Œå¯åŠ¨æ–°çš„é€‰ä¸¾
                    println!("é€‰ä¸¾è¶…æ—¶ï¼Œå¼€å§‹æ–°çš„é€‰ä¸¾");

                    // è½¬æ¢ä¸ºå€™é€‰äºº
                    {
                        let mut state = state_manager.node_state.write().unwrap();
                        *state = NodeState::Candidate;
                    }

                    // å¢åŠ å½“å‰ä»»æœŸ
                    let new_term = state_manager.current_term.fetch_add(1, Ordering::SeqCst) + 1;

                    // ä¸ºè‡ªå·±æŠ•ç¥¨
                    {
                        let mut voted_for = state_manager.voted_for.write().unwrap();
                        *voted_for = Some(node_id.clone());
                    }

                    // é‡ç½®é€‰ä¸¾å®šæ—¶å™¨
                    *last_reset.write().unwrap() = now;

                    // å‘é€è¯·æ±‚æŠ•ç¥¨RPC
                    let last_log_index = log_manager.last_log_index.load(Ordering::SeqCst);
                    let last_log_term = log_manager.last_log_term.load(Ordering::SeqCst);

                    // æ”¶é›†æŠ•ç¥¨
                    let mut votes = 1; // å·²ç»ä¸ºè‡ªå·±æŠ•äº†ä¸€ç¥¨

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€å¹¶è¡ŒRPCè¯·æ±‚
                    let clients = network_manager.clients.read().unwrap();

                    for client in clients.values() {
                        // æ¨¡æ‹Ÿå‘é€RequestVote RPC
                        println!("å‘èŠ‚ç‚¹ {} å‘é€RequestVote: term={}, lastLogIndex={}, lastLogTerm={}",
                                client.node_id, new_term, last_log_index, last_log_term);

                        // æ¨¡æ‹Ÿæ¥æ”¶å“åº”
                        let vote_granted = true; // å‡è®¾æ€»æ˜¯è·å¾—æŠ•ç¥¨

                        if vote_granted {
                            votes += 1;
                        }
                    }

                    // æ£€æŸ¥æ˜¯å¦è·å¾—å¤šæ•°ç¥¨
                    let majority = (cluster_config.nodes.len() / 2) + 1;

                    if votes >= majority {
                        println!("è·å¾—å¤šæ•°ç¥¨ ({}/{}), æˆä¸ºé¢†å¯¼è€…", votes, cluster_config.nodes.len());

                        // æˆä¸ºé¢†å¯¼è€…
                        {
                            let mut state = state_manager.node_state.write().unwrap();
                            *state = NodeState::Leader;
                        }

                        {
                            let mut leader = state_manager.leader_id.write().unwrap();
                            *leader = Some(node_id.clone());
                        }

                        // é‡ç½®ä¸‹ä¸€ä¸ªç´¢å¼•å’ŒåŒ¹é…ç´¢å¼•
                        let next_index = log_manager.last_log_index.load(Ordering::SeqCst) + 1;

                        for client in clients.values() {
                            client.next_index.store(next_index, Ordering::SeqCst);
                            client.match_index.store(0, Ordering::SeqCst);
                        }

                        // æ·»åŠ ä¸€ä¸ªç©ºæ“ä½œæ—¥å¿—æ¡ç›®
                        // åœ¨å®é™…å®ç°ä¸­ä¼šæ‰§è¡Œæ­¤æ“ä½œ
                    }
                }

                // æ£€æŸ¥é—´éš”ï¼ˆå°äºè¶…æ—¶æ—¶é—´ï¼‰
                thread::sleep(Duration::from_millis(50));
            }
        });

        self.election_timer.timer_thread = Some(thread);

        Ok(())
    }

    fn start_heartbeat_timer(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¿ƒè·³å®šæ—¶å™¨");

        let interval = self.heartbeat_timer.interval;
        let node_id = self.node_id.clone();
        let state_manager = Arc::new(self.state_manager);
        let log_manager = Arc::new(self.log_manager);
        let network_manager = Arc::new(self.network_manager);

        self.heartbeat_timer.running.store(true, Ordering::SeqCst);

        let running = self.heartbeat_timer.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // åªæœ‰é¢†å¯¼è€…å‘é€å¿ƒè·³
                let is_leader = match *state_manager.node_state.read().unwrap() {
                    NodeState::Leader => true,
                    _ => false,
                };

                if is_leader {
                    let current_term = state_manager.current_term.load(Ordering::SeqCst);
                    let commit_index = log_manager.commit_index.load(Ordering::SeqCst);

                    // å‘é€AppendEntries RPCï¼ˆå¿ƒè·³ï¼‰
                    let clients = network_manager.clients.read().unwrap();

                    for client in clients.values() {
                        // è·å–è¦å‘é€çš„æ—¥å¿—æ¡ç›®
                        let next_index = client.next_index.load(Ordering::SeqCst);
                        let log_entries = log_manager.log_entries.read().unwrap();

                        let entries: Vec<_> = log_entries.range(next_index..)
                            .map(|(_, entry)| entry.clone())
                            .take(20) // é™åˆ¶æ‰¹æ¬¡å¤§å°
                            .collect();

                        let prev_log_index = next_index - 1;
                        let prev_log_term = if prev_log_index > 0 {
                            log_entries.get(&prev_log_index).map_or(0, |e| e.term)
                        } else {
                            0
                        };

                        // æ¨¡æ‹Ÿå‘é€AppendEntries RPC
                        println!("å‘èŠ‚ç‚¹ {} å‘é€AppendEntries: term={}, prevLogIndex={}, prevLogTerm={}, entries.len={}, commitIndex={}",
                                client.node_id, current_term, prev_log_index, prev_log_term, entries.len(), commit_index);

                        // æ¨¡æ‹Ÿæ¥æ”¶å“åº”
                        let success = true; // å‡è®¾æ€»æ˜¯æˆåŠŸ

                        if success {
                            // æ›´æ–°åŒ¹é…ç´¢å¼•å’Œä¸‹ä¸€ä¸ªç´¢å¼•
                            if !entries.is_empty() {
                                let last_entry = entries.last().unwrap();
                                let match_index = last_entry.index;
                                client.match_index.store(match_index, Ordering::SeqCst);
                                client.next_index.store(match_index + 1, Ordering::SeqCst);
                            }
                        } else {
                            // å¦‚æœå¤±è´¥ï¼Œå‡å°‘ä¸‹ä¸€ä¸ªç´¢å¼•
                            let current_next = client.next_index.load(Ordering::SeqCst);
                            if current_next > 1 {
                                client.next_index.store(current_next - 1, Ordering::SeqCst);
                            }
                        }
                    }

                    // æ›´æ–°æäº¤ç´¢å¼•
                    self.update_commit_index(log_manager.clone(), state_manager.clone(), network_manager.clone());
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.heartbeat_timer.timer_thread = Some(thread);

        Ok(())
    }

    fn update_commit_index(
        &self,
        log_manager: Arc<LogManager>,
        state_manager: Arc<StateManager>,
        network_manager: Arc<NetworkManager>
    ) {
        // ä»…é€‚ç”¨äºé¢†å¯¼è€…
        if let NodeState::Leader = *state_manager.node_state.read().unwrap() {
            let current_term = state_manager.current_term.load(Ordering::SeqCst);
            let last_log_index = log_manager.last_log_index.load(Ordering::SeqCst);

            // å½“å‰æäº¤ç´¢å¼•
            let current_commit = log_manager.commit_index.load(Ordering::SeqCst);

            // æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„åŒ¹é…ç´¢å¼•
            let clients = network_manager.clients.read().unwrap();
            let mut match_indices = Vec::new();

            // æ·»åŠ é¢†å¯¼è€…è‡ªå·±çš„åŒ¹é…ç´¢å¼•
            match_indices.push(last_log_index);

            // æ·»åŠ æ‰€æœ‰è·Ÿéšè€…çš„åŒ¹é…ç´¢å¼•
            for client in clients.values() {
                match_indices.push(client.match_index.load(Ordering::SeqCst));
            }

            // æ’åºåŒ¹é…ç´¢å¼•
            match_indices.sort();

            // æ‰¾åˆ°è¿‡åŠæ•°çš„åŒ¹é…ç´¢å¼•
            let majority_index = match_indices.len() / 2;
            let new_commit_index = match_indices[majority_index];

            // åªæäº¤å½“å‰ä»»æœŸçš„æ—¥å¿—æ¡ç›®
            if new_commit_index > current_commit {
                let log_entries = log_manager.log_entries.read().unwrap();

                if let Some(entry) = log_entries.get(&new_commit_index) {
                    if entry.term == current_term {
                        log_manager.commit_index.store(new_commit_index, Ordering::SeqCst);

                        // åº”ç”¨åˆ°çŠ¶æ€æœº
                        self.apply_logs_to_state_machine();
                    }
                }
            }
        }
    }

    fn apply_logs_to_state_machine(&self) {
        let applied_index = self.state_machine.applied_index.load(Ordering::SeqCst);
        let commit_index = self.log_manager.commit_index.load(Ordering::SeqCst);

        if commit_index > applied_index {
            let mut state = self.state_machine.state.write().unwrap();
            let log_entries = self.log_manager.log_entries.read().unwrap();

            for i in (applied_index + 1)..=commit_index {
                if let Some(entry) = log_entries.get(&i) {
                    // åº”ç”¨å‘½ä»¤åˆ°çŠ¶æ€æœº
                    match &entry.command {
                        Command::Put { key, value } => {
                            state.insert(key.clone(), value.clone());
                        },
                        Command::Delete { key } => {
                            state.remove(key);
                        },
                        _ => {}, // å…¶ä»–å‘½ä»¤ç±»å‹ä¸éœ€è¦åº”ç”¨åˆ°çŠ¶æ€æœº
                    }
                }
            }

            // æ›´æ–°åº”ç”¨çš„ç´¢å¼•
            self.state_machine.applied_index.store(commit_index, Ordering::SeqCst);
        }
    }

    fn propose_command(&self, command: Command) -> Result<(u64, u64), String> {
        // æ£€æŸ¥æ˜¯å¦æ˜¯é¢†å¯¼è€…
        if let NodeState::Leader = *self.state_manager.node_state.read().unwrap() {
            let current_term = self.state_manager.current_term.load(Ordering::SeqCst);
            let last_index = self.log_manager.last_log_index.load(Ordering::SeqCst);
            let new_index = last_index + 1;

            // åˆ›å»ºæ—¥å¿—æ¡ç›®
            let entry = LogEntry {
                index: new_index,
                term: current_term,
                command,
                timestamp: Utc::now(),
            };

            // æ·»åŠ åˆ°æ—¥å¿—
            let mut log_entries = self.log_manager.log_entries.write().unwrap();
            log_entries.insert(new_index, entry);

            // æ›´æ–°æœ€åæ—¥å¿—ç´¢å¼•å’Œä»»æœŸ
            self.log_manager.last_log_index.store(new_index, Ordering::SeqCst);
            self.log_manager.last_log_term.store(current_term, Ordering::SeqCst);

            // æŒä¹…åŒ–æ—¥å¿—
            drop(log_entries);
            if let Err(e) = self.persist_state() {
                return Err(e);
            }

            // è¿”å›ç´¢å¼•å’Œä»»æœŸ
            Ok((new_index, current_term))
        } else {
            // ä¸æ˜¯é¢†å¯¼è€…ï¼Œè¿”å›é”™è¯¯
            let leader = self.state_manager.leader_id.read().unwrap();
            Err(format!("èŠ‚ç‚¹ä¸æ˜¯é¢†å¯¼è€…ï¼Œå½“å‰é¢†å¯¼è€…: {:?}", *leader))
        }
    }

    fn read_state(&self, key: &str) -> Option<Vec<u8>> {
        let state = self.state_machine.state.read().unwrap();
        state.get(key).cloned()
    }

    fn get_status(&self) -> RaftStatus {
        let current_term = self.state_manager.current_term.load(Ordering::SeqCst);
        let node_state = self.state_manager.node_state.read().unwrap().clone();
        let leader_id = self.state_manager.leader_id.read().unwrap().clone();
        let voted_for = self.state_manager.voted_for.read().unwrap().clone();
        let commit_index = self.log_manager.commit_index.load(Ordering::SeqCst);
        let last_log_index = self.log_manager.last_log_index.load(Ordering::SeqCst);
        let applied_index = self.state_machine.applied_index.load(Ordering::SeqCst);

        RaftStatus {
            node_id: self.node_id.clone(),
            current_term,
            node_state,
            leader_id,
            voted_for,
            commit_index,
            last_log_index,
            applied_index,
        }
    }
}

struct RaftStatus {
    node_id: String,
    current_term: u64,
    node_state: NodeState,
    leader_id: Option<String>,
    voted_for: Option<String>,
    commit_index: u64,
    last_log_index: u64,
    applied_index: u64,
}

impl Clone for LogEntry {
    fn clone(&self) -> Self {
        LogEntry {
            index: self.index,
            term: self.term,
            command: self.command.clone(),
            timestamp: self.timestamp,
        }
    }
}

impl Clone for Command {
    fn clone(&self) -> Self {
        match self {
            Command::Put { key, value } => Command::Put {
                key: key.clone(),
                value: value.clone(),
            },
            Command::Delete { key } => Command::Delete {
                key: key.clone(),
            },
            Command::Snapshot { index, term, data } => Command::Snapshot {
                index: *index,
                term: *term,
                data: data.clone(),
            },
            Command::Configuration { nodes } => Command::Configuration {
                nodes: nodes.clone(),
            },
            Command::Noop => Command::Noop,
        }
    }
}

impl Clone for NodeState {
    fn clone(&self) -> Self {
        match self {
            NodeState::Follower => NodeState::Follower,
            NodeState::Candidate => NodeState::Candidate,
            NodeState::Leader => NodeState::Leader,
        }
    }
}

// åˆ†å¸ƒå¼KVå­˜å‚¨ç³»ç»Ÿ
struct DistributedKVStore {
    node_id: String,
    raft_engine: RaftConsensusEngine,
    api_server: ApiServer,
    gossip_manager: GossipManager,
    rebalancer: Rebalancer,
}

struct ApiServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

struct GossipManager {
    members: RwLock<HashMap<String, MemberInfo>>,
    seed_nodes: Vec<String>,
    gossip_interval: Duration,
    running: AtomicBool,
    gossip_thread: Option<JoinHandle<()>>,
}

struct MemberInfo {
    id: String,
    address: String,
    status: MemberStatus,
    last_heartbeat: DateTime<Utc>,
    tags: HashMap<String, String>,
}

enum MemberStatus {
    Alive,
    Suspect,
    Dead,
}

struct Rebalancer {
    running: AtomicBool,
    check_interval: Duration,
    rebalance_thread: Option<JoinHandle<()>>,
}

impl DistributedKVStore {
    fn new(node_id: &str, raft_engine: RaftConsensusEngine, bind_address: &str, seed_nodes: Vec<String>) -> Self {
        let api_server = ApiServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
        };

        let gossip_manager = GossipManager {
            members: RwLock::new(HashMap::new()),
            seed_nodes,
            gossip_interval: Duration::from_secs(1),
            running: AtomicBool::new(false),
            gossip_thread: None,
        };

        let rebalancer = Rebalancer {
            running: AtomicBool::new(false),
            check_interval: Duration::from_secs(60),
            rebalance_thread: None,
        };

        DistributedKVStore {
            node_id: node_id.to_string(),
            raft_engine,
            api_server,
            gossip_manager,
            rebalancer,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼KVå­˜å‚¨ç³»ç»Ÿ");

        // å¯åŠ¨Raftå…±è¯†å¼•æ“
        self.raft_engine.start()?;

        // å¯åŠ¨APIæœåŠ¡å™¨
        self.start_api_server()?;

        // å¯åŠ¨Gossipç®¡ç†å™¨
        self.start_gossip_manager()?;

        // å¯åŠ¨é‡å¹³è¡¡å™¨
        self.start_rebalancer()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼KVå­˜å‚¨ç³»ç»Ÿ");

        // åœæ­¢é‡å¹³è¡¡å™¨
        self.rebalancer.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.rebalancer.rebalance_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("é‡å¹³è¡¡çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢Gossipç®¡ç†å™¨
        self.gossip_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.gossip_manager.gossip_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("Gossipçº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢APIæœåŠ¡å™¨
        self.api_server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.api_server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("APIæœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢Raftå…±è¯†å¼•æ“
        self.raft_engine.stop()?;

        Ok(())
    }

    fn start_api_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨APIæœåŠ¡å™¨");

        let bind_address = self.api_server.bind_address.clone();
        let node_id = self.node_id.clone();

        self.api_server.running.store(true, Ordering::SeqCst);

        let running = self.api_server.running.clone();

        let raft_engine = Arc::new(Mutex::new(self.raft_engine));

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTPæœåŠ¡å™¨
            println!("APIæœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {

                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
            }
        });

        self.api_server.server = Some(thread);

        Ok(())
    }

    fn start_gossip_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨Gossipç®¡ç†å™¨");

        let node_id = self.node_id.clone();
        let seed_nodes = self.gossip_manager.seed_nodes.clone();
        let interval = self.gossip_manager.gossip_interval;
        let members = self.gossip_manager.members.clone();

        // æ·»åŠ è‡ªå·±ä½œä¸ºæˆå‘˜
        {
            let mut members_map = members.write().unwrap();

            let self_info = MemberInfo {
                id: node_id.clone(),
                address: self.api_server.bind_address.clone(),
                status: MemberStatus::Alive,
                last_heartbeat: Utc::now(),
                tags: HashMap::new(),
            };

            members_map.insert(node_id.clone(), self_info);
        }

        self.gossip_manager.running.store(true, Ordering::SeqCst);

        let running = self.gossip_manager.running.clone();

        let thread = thread::spawn(move || {
            // è¿æ¥åˆ°ç§å­èŠ‚ç‚¹
            for seed in &seed_nodes {
                if seed != &node_id {
                    println!("è¿æ¥åˆ°ç§å­èŠ‚ç‚¹: {}", seed);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°è¯•è¿æ¥åˆ°ç§å­èŠ‚ç‚¹å¹¶è·å–æˆå‘˜åˆ—è¡¨

                    // æ¨¡æ‹Ÿæ·»åŠ ç§å­èŠ‚ç‚¹åˆ°æˆå‘˜åˆ—è¡¨
                    let mut members_map = members.write().unwrap();

                    let seed_info = MemberInfo {
                        id: seed.clone(),
                        address: format!("10.0.0.{}:7000", rand::random::<u8>()),
                        status: MemberStatus::Alive,
                        last_heartbeat: Utc::now(),
                        tags: HashMap::new(),
                    };

                    members_map.insert(seed.clone(), seed_info);
                }
            }

            // Gossipå¾ªç¯
            while running.load(Ordering::SeqCst) {
                // é€‰æ‹©éšæœºçš„gossipç›®æ ‡
                let target = {
                    let members_map = members.read().unwrap();

                    let alive_members: Vec<_> = members_map.values()
                        .filter(|m| m.id != node_id && matches!(m.status, MemberStatus::Alive))
                        .collect();

                    if alive_members.is_empty() {
                        None
                    } else {
                        let idx = rand::random::<usize>() % alive_members.len();
                        Some(alive_members[idx].clone())
                    }
                };

                if let Some(target_member) = target {
                    println!("å‘èŠ‚ç‚¹ {} å‘é€gossipæ¶ˆæ¯", target_member.id);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€gossipæ¶ˆæ¯å¹¶æ¥æ”¶å“åº”

                    // æ¨¡æ‹Ÿæ¥æ”¶åˆ°æ–°çš„æˆå‘˜ä¿¡æ¯
                    let mut new_members = Vec::new();

                    for i in 0..3 {
                        let random_id = format!("node-{}", uuid::Uuid::new_v4());

                        let member_info = MemberInfo {
                            id: random_id.clone(),
                            address: format!("10.0.0.{}:7000", rand::random::<u8>()),
                            status: MemberStatus::Alive,
                            last_heartbeat: Utc::now(),
                            tags: HashMap::new(),
                        };

                        new_members.push(member_info);
                    }

                    // æ›´æ–°æˆå‘˜åˆ—è¡¨
                    let mut members_map = members.write().unwrap();

                    for member in new_members {
                        if !members_map.contains_key(&member.id) {
                            members_map.insert(member.id.clone(), member);
                        }
                    }

                    // æ£€æµ‹å¯ç–‘èŠ‚ç‚¹
                    let now = Utc::now();
                    let timeout = Duration::from_secs(10);

                    for (_, member) in members_map.iter_mut() {
                        let elapsed = now.signed_duration_since(member.last_heartbeat);

                        if elapsed > timeout.into() && matches!(member.status, MemberStatus::Alive) {
                            member.status = MemberStatus::Suspect;
                            println!("èŠ‚ç‚¹ {} è¢«æ ‡è®°ä¸ºå¯ç–‘", member.id);
                        } else if elapsed > (timeout * 2).into() && matches!(member.status, MemberStatus::Suspect) {
                            member.status = MemberStatus::Dead;
                            println!("èŠ‚ç‚¹ {} è¢«æ ‡è®°ä¸ºæ­»äº¡", member.id);
                        }
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.gossip_manager.gossip_thread = Some(thread);

        Ok(())
    }

    fn start_rebalancer(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨é‡å¹³è¡¡å™¨");

        let node_id = self.node_id.clone();
        let interval = self.rebalancer.check_interval;
        let members = self.gossip_manager.members.clone();

        self.rebalancer.running.store(true, Ordering::SeqCst);

        let running = self.rebalancer.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¹³è¡¡
                let need_rebalance = {
                    let members_map = members.read().unwrap();

                    // è®¡ç®—æ´»è·ƒèŠ‚ç‚¹æ•°
                    let alive_count = members_map.values()
                        .filter(|m| matches!(m.status, MemberStatus::Alive))
                        .count();

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥æ•°æ®åˆ†å¸ƒæƒ…å†µ
                    // ç®€åŒ–ï¼šå½“æ´»è·ƒèŠ‚ç‚¹æ•°å˜åŒ–æ—¶è§¦å‘é‡å¹³è¡¡
                    alive_count > 0 && alive_count % 2 == 0 // ç®€å•ç¤ºä¾‹
                };

                if need_rebalance {
                    println!("è§¦å‘æ•°æ®é‡å¹³è¡¡");

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡Œé‡å¹³è¡¡ç®—æ³•
                    // è®¡ç®—æ–°çš„æ•°æ®åˆ†å¸ƒå¹¶è¿ç§»æ•°æ®

                    println!("æ•°æ®é‡å¹³è¡¡å®Œæˆ");
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(interval);
            }
        });

        self.rebalancer.rebalance_thread = Some(thread);

        Ok(())
    }

    fn put(&self, key: &str, value: &[u8]) -> Result<(), String> {
        println!("è®¾ç½®é”®å€¼å¯¹: {} -> {} bytes", key, value.len());

        // åˆ›å»ºå‘½ä»¤
        let command = Command::Put {
            key: key.to_string(),
            value: value.to_vec(),
        };

        // æäº¤åˆ°Raft
        match self.raft_engine.propose_command(command) {
            Ok((index, term)) => {
                println!("å‘½ä»¤å·²æäº¤: index={}, term={}", index, term);

                // ç­‰å¾…åº”ç”¨åˆ°çŠ¶æ€æœº
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šç­‰å¾…å‘½ä»¤è¢«åº”ç”¨

                Ok(())
            },
            Err(e) => Err(e),
        }
    }

    fn get(&self, key: &str) -> Result<Option<Vec<u8>>, String> {
        println!("è·å–é”®å€¼: {}", key);

        // ä»çŠ¶æ€æœºä¸­è¯»å–
        let value = self.raft_engine.read_state(key);

        Ok(value)
    }

    fn delete(&self, key: &str) -> Result<(), String> {
        println!("åˆ é™¤é”®å€¼: {}", key);

        // åˆ›å»ºå‘½ä»¤
        let command = Command::Delete {
            key: key.to_string(),
        };

        // æäº¤åˆ°Raft
        match self.raft_engine.propose_command(command) {
            Ok((index, term)) => {
                println!("å‘½ä»¤å·²æäº¤: index={}, term={}", index, term);

                // ç­‰å¾…åº”ç”¨åˆ°çŠ¶æ€æœº
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šç­‰å¾…å‘½ä»¤è¢«åº”ç”¨

                Ok(())
            },
            Err(e) => Err(e),
        }
    }

    fn get_cluster_status(&self) -> ClusterStatus {
        // è·å–RaftçŠ¶æ€
        let raft_status = self.raft_engine.get_status();

        // è·å–é›†ç¾¤æˆå‘˜
        let members = self.gossip_manager.members.read().unwrap();

        // æ„å»ºé›†ç¾¤çŠ¶æ€
        let mut alive_nodes = 0;
        let mut suspect_nodes = 0;
        let mut dead_nodes = 0;

        for member in members.values() {
            match member.status {
                MemberStatus::Alive => alive_nodes += 1,
                MemberStatus::Suspect => suspect_nodes += 1,
                MemberStatus::Dead => dead_nodes += 1,
            }
        }

        ClusterStatus {
            node_id: self.node_id.clone(),
            leader_id: raft_status.leader_id,
            term: raft_status.current_term,
            alive_nodes,
            suspect_nodes,
            dead_nodes,
            total_nodes: members.len(),
        }
    }
}

struct ClusterStatus {
    node_id: String,
    leader_id: Option<String>,
    term: u64,
    alive_nodes: usize,
    suspect_nodes: usize,
    dead_nodes: usize,
    total_nodes: usize,
}

impl Clone for MemberInfo {
    fn clone(&self) -> Self {
        MemberInfo {
            id: self.id.clone(),
            address: self.address.clone(),
            status: self.status.clone(),
            last_heartbeat: self.last_heartbeat,
            tags: self.tags.clone(),
        }
    }
}

impl Clone for MemberStatus {
    fn clone(&self) -> Self {
        match self {
            MemberStatus::Alive => MemberStatus::Alive,
            MemberStatus::Suspect => MemberStatus::Suspect,
            MemberStatus::Dead => MemberStatus::Dead,
        }
    }
}

// åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨ç³»ç»Ÿ
struct DistributedObjectStore {
    node_id: String,
    data_manager: DataManager,
    metadata_manager: ObjectMetadataManager,
    placement_manager: PlacementManager,
    replication_manager: ObjectReplicationManager,
    server: ObjectStoreServer,
}

struct DataManager {
    objects: RwLock<HashMap<String, ObjectData>>,
    data_dir: PathBuf,
    storage_policy: StoragePolicy,
}

struct ObjectData {
    key: String,
    data: Vec<u8>,
    checksum: String,
    storage_class: StorageClass,
    created_at: DateTime<Utc>,
    last_accessed: DateTime<Utc>,
}

enum StorageClass {
    Standard,
    ReducedRedundancy,
    Glacier,
    DeepArchive,
}

struct StoragePolicy {
    min_size: usize,
    max_size: usize,
    default_class: StorageClass,
    retention_policy: RetentionPolicy,
}

enum RetentionPolicy {
    None,
    Days(u32),
    Compliance {
        days: u32,
        legal_hold: bool,
    },
}

struct ObjectMetadataManager {
    metadata: RwLock<HashMap<String, ObjectMetadata>>,
    indexes: RwLock<HashMap<String, BTreeMap<String, Vec<String>>>>,
}

struct ObjectMetadata {
    key: String,
    size: usize,
    content_type: String,
    etag: String,
    version_id: Option<String>,
    is_latest: bool,
    storage_class: StorageClass,
    owner: String,
    created_at: DateTime<Utc>,
    last_modified: DateTime<Utc>,
    checksum: String,
    encryption: Option<EncryptionInfo>,
    tags: HashMap<String, String>,
    user_metadata: HashMap<String, String>,
}

struct EncryptionInfo {
    algorithm: String,
    key_id: String,
    iv: Vec<u8>,
}

struct PlacementManager {
    placement_strategy: PlacementStrategy,
    nodes: RwLock<HashMap<String, NodeStorage>>,
    partition_map: RwLock<HashMap<String, Vec<String>>>,
}

enum PlacementStrategy {
    Random,
    LeastUsed,
    Consistent {
        virtual_nodes: usize,
    },
    Zone {
        zone_info: HashMap<String, String>,
    },
}

struct NodeStorage {
    node_id: String,
    zone: String,
    capacity: u64,
    used: u64,
    status: NodeStorageStatus,
    last_heartbeat: DateTime<Utc>,
}

enum NodeStorageStatus {
    Available,
    Full,
    Maintenance,
    Down,
}

struct ObjectReplicationManager {
    replication_tasks: RwLock<Vec<ObjectReplicationTask>>,
    running: AtomicBool,
    replication_thread: Option<JoinHandle<()>>,
}

struct ObjectReplicationTask {
    object_key: String,
    source_node: String,
    target_node: String,
    priority: u32,
    created_at: DateTime<Utc>,
}

struct ObjectStoreServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

impl DistributedObjectStore {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str) -> Self {
        let storage_policy = StoragePolicy {
            min_size: 0,
            max_size: 5 * 1024 * 1024 * 1024, // 5GB
            default_class: StorageClass::Standard,
            retention_policy: RetentionPolicy::None,
        };

        let data_manager = DataManager {
            objects: RwLock::new(HashMap::new()),
            data_dir: data_dir.to_path_buf(),
            storage_policy,
        };

        let metadata_manager = ObjectMetadataManager {
            metadata: RwLock::new(HashMap::new()),
            indexes: RwLock::new(HashMap::new()),
        };

        let placement_manager = PlacementManager {
            placement_strategy: PlacementStrategy::Consistent {
                virtual_nodes: 256,
            },
            nodes: RwLock::new(HashMap::new()),
            partition_map: RwLock::new(HashMap::new()),
        };

        let replication_manager = ObjectReplicationManager {
            replication_tasks: RwLock::new(Vec::new()),
            running: AtomicBool::new(false),
            replication_thread: None,
        };

        let server = ObjectStoreServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
        };

        DistributedObjectStore {
            node_id: node_id.to_string(),
            data_manager,
            metadata_manager,
            placement_manager,
            replication_manager,
            server,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨ç³»ç»Ÿ");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.data_manager.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        // æ³¨å†Œå½“å‰èŠ‚ç‚¹
        self.register_node()?;

        // å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨
        self.start_replication_manager()?;

        // å¯åŠ¨æœåŠ¡å™¨
        self.start_server()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨ç³»ç»Ÿ");

        // åœæ­¢æœåŠ¡å™¨
        self.server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢å¤åˆ¶ç®¡ç†å™¨
        self.replication_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.replication_manager.replication_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤åˆ¶çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }

    fn register_node(&self) -> Result<(), String> {
        println!("æ³¨å†Œå­˜å‚¨èŠ‚ç‚¹");

        let mut nodes = self.placement_manager.nodes.write().unwrap();

        let node = NodeStorage {
            node_id: self.node_id.clone(),
            zone: "default".to_string(),
            capacity: 1024 * 1024 * 1024 * 1024, // 1TB
            used: 0,
            status: NodeStorageStatus::Available,
            last_heartbeat: Utc::now(),
        };

        nodes.insert(self.node_id.clone(), node);

        // æ›´æ–°åˆ†åŒºæ˜ å°„
        self.update_partition_map()?;

        Ok(())
    }

    fn update_partition_map(&self) -> Result<(), String> {
        // ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•é‡æ–°è®¡ç®—åˆ†åŒºæ˜ å°„
        match &self.placement_manager.placement_strategy {
            PlacementStrategy::Consistent { virtual_nodes } => {
                let nodes = self.placement_manager.nodes.read().unwrap();
                let available_nodes: Vec<_> = nodes.keys()
                    .filter(|id| {
                        if let Some(node) = nodes.get(*id) {
                            matches!(node.status, NodeStorageStatus::Available)
                        } else {
                            false
                        }
                    })
                    .cloned()
                    .collect();

                if available_nodes.is_empty() {
                    return Err("æ²¡æœ‰å¯ç”¨çš„å­˜å‚¨èŠ‚ç‚¹".to_string());
                }

                // åˆ›å»ºè™šæ‹ŸèŠ‚ç‚¹ç¯
                let mut ring = Vec::new();

                for node_id in &available_nodes {
                    for i in 0..*virtual_nodes {
                        let key = format!("{}:{}", node_id, i);
                        let mut hasher = DefaultHasher::new();
                        key.hash(&mut hasher);
                        let hash = hasher.finish();

                        ring.push((hash, node_id.clone()));
                    }
                }

                // æ’åºç¯
                ring.sort_by(|a, b| a.0.cmp(&b.0));

                // æ›´æ–°åˆ†åŒºæ˜ å°„
                let mut partition_map = self.placement_manager.partition_map.write().unwrap();
                partition_map.clear();

                // ä¸ºæ¯ä¸ªåˆ†åŒºåˆ†é…èŠ‚ç‚¹
                for i in 0..1024 { // 1024ä¸ªåˆ†åŒº
                    let partition_key = format!("partition:{}", i);
                    let mut hasher = DefaultHasher::new();
                    partition_key.hash(&mut hasher);
                    let hash = hasher.finish();

                    // æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¤§äºç­‰äºè¯¥å“ˆå¸Œå€¼çš„èŠ‚ç‚¹
                    let pos = ring.binary_search_by(|probe| probe.0.cmp(&hash))
                        .unwrap_or_else(|pos| pos % ring.len());

                    let assigned_node = ring[pos].1.clone();

                    // åˆ†é…å¤‡ä»½èŠ‚ç‚¹
                    let mut backup_nodes = Vec::new();
                    let backup_count = 2; // 2ä¸ªå¤‡ä»½

                    for i in 1..=backup_count {
                        let backup_pos = (pos + i) % ring.len();
                        let backup_node = ring[backup_pos].1.clone();

                        if backup_node != assigned_node && !backup_nodes.contains(&backup_node) {
                            backup_nodes.push(backup_node);
                        }
                    }

                    // å­˜å‚¨ä¸»èŠ‚ç‚¹å’Œå¤‡ä»½èŠ‚ç‚¹
                    let mut nodes = vec![assigned_node];
                    nodes.extend(backup_nodes);

                    partition_map.insert(partition_key, nodes);
                }
            },
            _ => {
                // å…¶ä»–ç­–ç•¥çš„å®ç°
                return Err("ä¸æ”¯æŒçš„æ”¾ç½®ç­–ç•¥".to_string());
            }
        }

        Ok(())
    }

    fn start_replication_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨");

        let replication_tasks = self.replication_manager.replication_tasks.clone();
        let node_id = self.node_id.clone();
        let data_dir = self.data_manager.data_dir.clone();

        self.replication_manager.running.store(true, Ordering::SeqCst);

        let running = self.replication_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // å¤„ç†å¤åˆ¶ä»»åŠ¡
                let tasks_to_process = {
                    let mut tasks = replication_tasks.write().unwrap();
                    let result = tasks.clone();
                    tasks.clear();
                    result
                };

                for task in tasks_to_process {
                    println!("å¤„ç†å¯¹è±¡å¤åˆ¶ä»»åŠ¡: {} -> {}", task.object_key, task.target_node);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»ç£ç›˜è¯»å–å¯¹è±¡å¹¶å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹
                    // æˆ–è€…ä½¿ç”¨æµå¼ä¼ è¾“ä¼˜åŒ–å¤§å¯¹è±¡çš„å¤åˆ¶

                    println!("å¯¹è±¡å¤åˆ¶å®Œæˆ: {}", task.object_key);
                }

                // ä¼‘çœ ä¸€ä¼šå„¿
                thread::sleep(Duration::from_millis(100));
            }
        });

        self.replication_manager.replication_thread = Some(thread);

        Ok(())
    }

    fn start_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¯¹è±¡å­˜å‚¨æœåŠ¡å™¨");

        let bind_address = self.server.bind_address.clone();
        let node_id = self.node_id.clone();

        self.server.running.store(true, Ordering::SeqCst);

        let running = self.server.running.clone();

        let data_manager = Arc::new(self.data_manager);
        let metadata_manager = Arc::new(self.metadata_manager);
        let placement_manager = Arc::new(self.placement_manager);

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTPæœåŠ¡å™¨
            println!("å¯¹è±¡å­˜å‚¨æœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
            }
        });

        self.server.server = Some(thread);

        Ok(())
    }

    fn put_object(
        &self,
        key: &str,
        data: Vec<u8>,
        content_type: &str,
        storage_class: Option<StorageClass>,
        metadata: Option<HashMap<String, String>>,
        tags: Option<HashMap<String, String>>
    ) -> Result<ObjectMetadata, String> {
        println!("å­˜å‚¨å¯¹è±¡: {}, å¤§å°: {} bytes", key, data.len());

        // éªŒè¯å¯¹è±¡å¤§å°
        if data.len() < self.data_manager.storage_policy.min_size {
            return Err(format!("å¯¹è±¡å¤ªå°: {} bytesï¼Œæœ€å°è¦æ±‚: {} bytes",
                             data.len(), self.data_manager.storage_policy.min_size));
        }

        if data.len() > self.data_manager.storage_policy.max_size {
            return Err(format!("å¯¹è±¡å¤ªå¤§: {} bytesï¼Œæœ€å¤§å…è®¸: {} bytes",
                             data.len(), self.data_manager.storage_policy.max_size));
        }

        // è®¡ç®—æ ¡éªŒå’Œ
        let mut hasher = sha2::Sha256::new();
        hasher.update(&data);
        let checksum = format!("{:x}", hasher.finalize());

        // ç¡®å®šå­˜å‚¨èŠ‚ç‚¹
        let placement = self.find_placement_nodes(key)?;

        if placement.is_empty() {
            return Err("æ— æ³•æ‰¾åˆ°åˆé€‚çš„å­˜å‚¨èŠ‚ç‚¹".to_string());
        }

        // åˆ›å»ºå¯¹è±¡å…ƒæ•°æ®
        let now = Utc::now();
        let storage_class = storage_class.unwrap_or_else(|| self.data_manager.storage_policy.default_class.clone());

        let metadata = ObjectMetadata {
            key: key.to_string(),
            size: data.len(),
            content_type: content_type.to_string(),
            etag: checksum.clone(),
            version_id: Some(uuid::Uuid::new_v4().to_string()),
            is_latest: true,
            storage_class: storage_class.clone(),
            owner: "system".to_string(), // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯å®é™…ç”¨æˆ·
            created_at: now,
            last_modified: now,
            checksum: checksum.clone(),
            encryption: None, // åœ¨å®é™…å®ç°ä¸­ï¼Œå¯èƒ½ä¼šåŠ å¯†
            tags: tags.unwrap_or_default(),
            user_metadata: metadata.unwrap_or_default(),
        };

        // å­˜å‚¨å¯¹è±¡æ•°æ®ï¼ˆå½“å‰èŠ‚ç‚¹æ˜¯ä¸»èŠ‚ç‚¹æˆ–å¤‡ä»½èŠ‚ç‚¹ï¼‰
        if placement.contains(&self.node_id) {
            let object_data = ObjectData {
                key: key.to_string(),
                data: data.clone(),
                checksum: checksum.clone(),
                storage_class: storage_class.clone(),
                created_at: now,
                last_accessed: now,
            };

            // å­˜å‚¨åˆ°å†…å­˜
            let mut objects = self.data_manager.objects.write().unwrap();
            objects.insert(key.to_string(), object_data);

            // å†™å…¥ç£ç›˜
            let object_path = self.data_manager.data_dir.join(key);
            if let Some(parent) = object_path.parent() {
                if !parent.exists() {
                    if let Err(e) = std::fs::create_dir_all(parent) {
                        return Err(format!("åˆ›å»ºå¯¹è±¡ç›®å½•å¤±è´¥: {}", e));
                    }
                }
            }

            if let Err(e) = std::fs::write(&object_path, &data) {
                return Err(format!("å†™å…¥å¯¹è±¡æ–‡ä»¶å¤±è´¥: {}", e));
            }
        }

        // å­˜å‚¨å…ƒæ•°æ®
        let mut metadata_map = self.metadata_manager.metadata.write().unwrap();
        metadata_map.insert(key.to_string(), metadata.clone());

        // æ›´æ–°ç´¢å¼•
        for (tag_key, tag_value) in &metadata.tags {
            let mut indexes = self.metadata_manager.indexes.write().unwrap();

            let tag_index = indexes.entry(format!("tag:{}", tag_key))
                .or_insert_with(BTreeMap::new);

            let value_keys = tag_index.entry(tag_value.clone())
                .or_insert_with(Vec::new);

            if !value_keys.contains(&key.to_string()) {
                value_keys.push(key.to_string());
            }
        }

        // åˆ›å»ºå¤åˆ¶ä»»åŠ¡ï¼ˆå¦‚æœå½“å‰èŠ‚ç‚¹æ˜¯ä¸»èŠ‚ç‚¹ï¼‰
        if placement[0] == self.node_id {
            for target_node in &placement[1..] {
                let replication_task = ObjectReplicationTask {
                    object_key: key.to_string(),
                    source_node: self.node_id.clone(),
                    target_node: target_node.clone(),
                    priority: 0,
                    created_at: now,
                };

                let mut tasks = self.replication_manager.replication_tasks.write().unwrap();
                tasks.push(replication_task);
            }
        }

        Ok(metadata)
    }

    fn get_object(&self, key: &str) -> Result<(Vec<u8>, ObjectMetadata), String> {
        println!("è·å–å¯¹è±¡: {}", key);

        // æ£€æŸ¥å…ƒæ•°æ®
        let metadata_map = self.metadata_manager.metadata.read().unwrap();

        let metadata = metadata_map.get(key)
            .ok_or_else(|| format!("å¯¹è±¡ä¸å­˜åœ¨: {}", key))?
            .clone();

        // æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        let mut objects = self.data_manager.objects.write().unwrap();

        if let Some(object) = objects.get_mut(key) {
            // æ›´æ–°è®¿é—®æ—¶é—´
            object.last_accessed = Utc::now();

            return Ok((object.data.clone(), metadata));
        }

        // æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
        let object_path = self.data_manager.data_dir.join(key);

        if object_path.exists() {
            match std::fs::read(&object_path) {
                Ok(data) => {
                    // éªŒè¯æ ¡éªŒå’Œ
                    let mut hasher = sha2::Sha256::new();
                    hasher.update(&data);
                    let checksum = format!("{:x}", hasher.finalize());

                    if checksum != metadata.checksum {
                        return Err(format!("å¯¹è±¡æ ¡éªŒå’Œä¸åŒ¹é…: {}", key));
                    }

                    // æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
                    let now = Utc::now();
                    let object_data = ObjectData {
                        key: key.to_string(),
                        data: data.clone(),
                        checksum,
                        storage_class: metadata.storage_class.clone(),
                        created_at: metadata.created_at,
                        last_accessed: now,
                    };

                    objects.insert(key.to_string(), object_data);

                    return Ok((data, metadata));
                },
                Err(e) => {
                    return Err(format!("è¯»å–å¯¹è±¡æ–‡ä»¶å¤±è´¥: {}", e));
                }
            }
        }

        // å¦‚æœä¸åœ¨æœ¬åœ°ï¼Œå°è¯•ä»å…¶ä»–èŠ‚ç‚¹è·å–
        let placement = self.find_placement_nodes(key)?;

        for node_id in &placement {
            if node_id != &self.node_id {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»å…¶ä»–èŠ‚ç‚¹è·å–å¯¹è±¡
                println!("ä»èŠ‚ç‚¹ {} è·å–å¯¹è±¡ {}", node_id, key);

                // æ¨¡æ‹Ÿä»å…¶ä»–èŠ‚ç‚¹è·å–æ•°æ®
                let data = vec![0; metadata.size]; // å‡æ•°æ®

                return Ok((data, metadata));
            }
        }

        Err(format!("æ— æ³•è·å–å¯¹è±¡: {}", key))
    }

    fn delete_object(&self, key: &str) -> Result<(), String> {
        println!("åˆ é™¤å¯¹è±¡: {}", key);

        // æ£€æŸ¥å…ƒæ•°æ®
        let mut metadata_map = self.metadata_manager.metadata.write().unwrap();

        if !metadata_map.contains_key(key) {
            return Err(format!("å¯¹è±¡ä¸å­˜åœ¨: {}", key));
        }

        // ä»å†…å­˜ç¼“å­˜ä¸­ç§»é™¤
        let mut objects = self.data_manager.objects.write().unwrap();
        objects.remove(key);

        // ä»ç£ç›˜åˆ é™¤
        let object_path = self.data_manager.data_dir.join(key);

        if object_path.exists() {
            if let Err(e) = std::fs::remove_file(&object_path) {
                return Err(format!("åˆ é™¤å¯¹è±¡æ–‡ä»¶å¤±è´¥: {}", e));
            }
        }

        // ä»å…ƒæ•°æ®ä¸­ç§»é™¤
        if let Some(metadata) = metadata_map.remove(key) {
            // ä»ç´¢å¼•ä¸­ç§»é™¤
            for (tag_key, tag_value) in &metadata.tags {
                let mut indexes = self.metadata_manager.indexes.write().unwrap();

                if let Some(tag_index) = indexes.get_mut(&format!("tag:{}", tag_key)) {
                    if let Some(value_keys) = tag_index.get_mut(tag_value) {
                        value_keys.retain(|k| k != key);

                        if value_keys.is_empty() {
                            tag_index.remove(tag_value);
                        }
                    }

                    if tag_index.is_empty() {
                        indexes.remove(&format!("tag:{}", tag_key));
                    }
                }
            }
        }

        Ok(())
    }

    fn list_objects(
        &self,
        prefix: Option<&str>,
        delimiter: Option<&str>,
        max_keys: Option<usize>
    ) -> Result<Vec<ObjectMetadata>, String> {
        let metadata_map = self.metadata_manager.metadata.read().unwrap();

        let mut result = Vec::new();

        for (key, metadata) in metadata_map.iter() {
            // åº”ç”¨å‰ç¼€è¿‡æ»¤
            if let Some(prefix_str) = prefix {
                if !key.starts_with(prefix_str) {
                    continue;
                }
            }

            // åº”ç”¨åˆ†éš”ç¬¦é€»è¾‘ï¼ˆç®€åŒ–ï¼‰
            if let Some(delim) = delimiter {
                if let Some(suffix) = key.strip_prefix(prefix.unwrap_or("")) {
                    if suffix.contains(delim) {
                        // è¿™é‡Œç®€åŒ–äº†å¯¹äºå…¬å…±å‰ç¼€çš„å¤„ç†
                        continue;
                    }
                }
            }

            result.push(metadata.clone());
        }

        // åº”ç”¨é™åˆ¶
        if let Some(limit) = max_keys {
            if result.len() > limit {
                result.truncate(limit);
            }
        }

        Ok(result)
    }

    fn find_placement_nodes(&self, key: &str) -> Result<Vec<String>, String> {
        // è®¡ç®—åˆ†åŒºé”®
        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        let hash = hasher.finish();

        let partition_key = format!("partition:{}", hash % 1024);

        // æŸ¥æ‰¾åˆ†åŒºæ˜ å°„
        let partition_map = self.placement_manager.partition_map.read().unwrap();

        if let Some(nodes) = partition_map.get(&partition_key) {
            Ok(nodes.clone())
        } else {
            Err(format!("æœªæ‰¾åˆ°åˆ†åŒºæ˜ å°„: {}", partition_key))
        }
    }
}

impl Clone for StorageClass {
    fn clone(&self) -> Self {
        match self {
            StorageClass::Standard => StorageClass::Standard,
            StorageClass::ReducedRedundancy => StorageClass::ReducedRedundancy,
            StorageClass::Glacier => StorageClass::Glacier,
            StorageClass::DeepArchive => StorageClass::DeepArchive,
        }
    }
}

impl Clone for ObjectReplicationTask {
    fn clone(&self) -> Self {
        ObjectReplicationTask {
            object_key: self.object_key.clone(),
            source_node: self.source_node.clone(),
            target_node: self.target_node.clone(),
            priority: self.priority,
            created_at: self.created_at,
        }
    }
}
```

### 1.13 ç»¼åˆåº”ç”¨13-åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ

```rust
// åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ
struct DistributedDatabase {
    node_id: String,
    config: DatabaseConfig,
    storage_engine: StorageEngine,
    query_processor: QueryProcessor,
    transaction_manager: DBTransactionManager,
    replication_manager: ReplicationManager,
    sharding_manager: ShardingManager,
    schema_manager: SchemaManager,
    server: DatabaseServer,
}

struct DatabaseConfig {
    data_dir: PathBuf,
    log_dir: PathBuf,
    bind_address: String,
    cluster_nodes: Vec<String>,
    replication_factor: u32,
    shard_count: u32,
    isolation_level: IsolationLevel,
    max_connections: u32,
    max_query_time: Duration,
}

enum IsolationLevel {
    ReadUncommitted,
    ReadCommitted,
    RepeatableRead,
    Serializable,
}

struct StorageEngine {
    tables: RwLock<HashMap<String, Table>>,
    indexes: RwLock<HashMap<String, Index>>,
    data_dir: PathBuf,
}

struct Table {
    name: String,
    schema: TableSchema,
    data: RwLock<Vec<Row>>,
    primary_key: Option<String>,
    indexes: Vec<String>,
    shard_key: Option<String>,
    created_at: DateTime<Utc>,
    last_modified: DateTime<Utc>,
}

struct TableSchema {
    columns: Vec<Column>,
    constraints: Vec<Constraint>,
}

struct Column {
    name: String,
    data_type: DataType,
    nullable: bool,
    default_value: Option<Value>,
}

enum DataType {
    Integer,
    Float,
    Text,
    Boolean,
    DateTime,
    Blob,
    Uuid,
}

enum Constraint {
    PrimaryKey(String),
    ForeignKey {
        column: String,
        ref_table: String,
        ref_column: String,
        on_delete: ReferentialAction,
        on_update: ReferentialAction,
    },
    Unique(String),
    Check {
        expression: String,
    },
    NotNull(String),
}

enum ReferentialAction {
    Restrict,
    Cascade,
    SetNull,
    SetDefault,
    NoAction,
}

struct Row {
    id: u64,
    values: HashMap<String, Value>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

enum Value {
    Integer(i64),
    Float(f64),
    Text(String),
    Boolean(bool),
    DateTime(DateTime<Utc>),
    Blob(Vec<u8>),
    Uuid(Uuid),
    Null,
}

struct Index {
    name: String,
    table_name: String,
    columns: Vec<String>,
    index_type: IndexType,
    data: RwLock<BTreeMap<IndexKey, Vec<u64>>>,
    created_at: DateTime<Utc>,
}

enum IndexType {
    BTree,
    Hash,
    Unique,
}

enum IndexKey {
    Integer(i64),
    Float(f64),
    Text(String),
    Boolean(bool),
    DateTime(DateTime<Utc>),
    Composite(Vec<IndexKey>),
}

struct QueryProcessor {
    parsers: HashMap<String, Box<dyn QueryParser>>,
    optimizers: Vec<Box<dyn QueryOptimizer>>,
    executors: HashMap<String, Box<dyn QueryExecutor>>,
}

trait QueryParser: Send + Sync {
    fn parse(&self, query: &str) -> Result<QueryPlan, String>;
}

trait QueryOptimizer: Send + Sync {
    fn optimize(&self, plan: QueryPlan) -> QueryPlan;
}

trait QueryExecutor: Send + Sync {
    fn execute(&self, plan: QueryPlan, transaction: &Transaction) -> Result<QueryResult, String>;
}

struct QueryPlan {
    operation_type: OperationType,
    table: String,
    projection: Option<Vec<String>>,
    filter: Option<Expression>,
    join: Option<JoinPlan>,
    group_by: Option<Vec<String>>,
    having: Option<Expression>,
    order_by: Option<Vec<OrderByClause>>,
    limit: Option<usize>,
    offset: Option<usize>,
}

enum OperationType {
    Select,
    Insert,
    Update,
    Delete,
    CreateTable,
    DropTable,
    CreateIndex,
    DropIndex,
}

enum Expression {
    Column(String),
    Literal(Value),
    BinaryOp {
        op: BinaryOperator,
        left: Box<Expression>,
        right: Box<Expression>,
    },
    Function {
        name: String,
        args: Vec<Expression>,
    },
}

enum BinaryOperator {
    Eq,
    Neq,
    Lt,
    Lte,
    Gt,
    Gte,
    And,
    Or,
    Add,
    Subtract,
    Multiply,
    Divide,
    Like,
    In,
}

struct JoinPlan {
    join_type: JoinType,
    table: String,
    condition: Expression,
}

enum JoinType {
    Inner,
    Left,
    Right,
    Full,
}

struct OrderByClause {
    column: String,
    direction: SortDirection,
}

enum SortDirection {
    Ascending,
    Descending,
}

struct QueryResult {
    columns: Vec<String>,
    rows: Vec<HashMap<String, Value>>,
    affected_rows: usize,
    last_insert_id: Option<u64>,
    execution_time: Duration,
}

struct DBTransactionManager {
    transactions: RwLock<HashMap<String, Transaction>>,
    isolation_level: IsolationLevel,
    lock_manager: LockManager,
}

struct Transaction {
    id: String,
    start_time: DateTime<Utc>,
    isolation_level: IsolationLevel,
    operations: Vec<Operation>,
    status: TransactionStatus,
}

enum TransactionStatus {
    Active,
    Committed,
    Aborted,
}

struct Operation {
    table: String,
    operation_type: OperationType,
    rows_affected: Vec<u64>,
    before_image: Option<HashMap<u64, Row>>,
    after_image: Option<HashMap<u64, Row>>,
}

struct LockManager {
    locks: RwLock<HashMap<LockKey, LockEntry>>,
}

struct LockKey {
    resource_type: ResourceType,
    resource_id: String,
}

enum ResourceType {
    Table,
    Row,
    Index,
}

struct LockEntry {
    lock_type: LockType,
    holder: String,
    granted_at: DateTime<Utc>,
    waiting: Vec<LockWaiter>,
}

enum LockType {
    Shared,
    Exclusive,
    Intent,
}

struct LockWaiter {
    transaction_id: String,
    lock_type: LockType,
    waiting_since: DateTime<Utc>,
}

struct ReplicationManager {
    replication_mode: ReplicationMode,
    nodes: RwLock<HashMap<String, ReplicationNode>>,
    log_dir: PathBuf,
    running: AtomicBool,
    replication_thread: Option<JoinHandle<()>>,
}

enum ReplicationMode {
    Sync,
    Async,
    SemiSync,
}

struct ReplicationNode {
    node_id: String,
    address: String,
    role: NodeRole,
    status: NodeStatus,
    last_heartbeat: DateTime<Utc>,
    replication_lag: Duration,
}

enum NodeRole {
    Primary,
    Secondary,
    Arbiter,
}

enum NodeStatus {
    Healthy,
    Degraded,
    Disconnected,
}

struct ShardingManager {
    strategy: ShardingStrategy,
    shards: RwLock<HashMap<u32, Shard>>,
    shard_count: u32,
}

enum ShardingStrategy {
    Hash,
    Range,
    List,
}

struct Shard {
    id: u32,
    node_id: String,
    tables: HashSet<String>,
    key_range: Option<(Value, Value)>,
    key_list: Option<HashSet<Value>>,
}

struct SchemaManager {
    schemas: RwLock<HashMap<String, DatabaseSchema>>,
    version: AtomicU64,
}

struct DatabaseSchema {
    name: String,
    tables: HashMap<String, TableSchema>,
    version: u64,
    created_at: DateTime<Utc>,
    last_modified: DateTime<Utc>,
}

struct DatabaseServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
    connections: RwLock<HashMap<String, Connection>>,
    max_connections: u32,
}

struct Connection {
    id: String,
    client_address: String,
    user: String,
    connected_at: DateTime<Utc>,
    last_activity: DateTime<Utc>,
    transaction_id: Option<String>,
}

impl DistributedDatabase {
    fn new(node_id: &str, config: DatabaseConfig) -> Self {
        let storage_engine = StorageEngine {
            tables: RwLock::new(HashMap::new()),
            indexes: RwLock::new(HashMap::new()),
            data_dir: config.data_dir.clone(),
        };

        let query_processor = QueryProcessor {
            parsers: HashMap::new(),
            optimizers: Vec::new(),
            executors: HashMap::new(),
        };

        let transaction_manager = DBTransactionManager {
            transactions: RwLock::new(HashMap::new()),
            isolation_level: config.isolation_level.clone(),
            lock_manager: LockManager {
                locks: RwLock::new(HashMap::new()),
            },
        };

        let replication_manager = ReplicationManager {
            replication_mode: ReplicationMode::Async,
            nodes: RwLock::new(HashMap::new()),
            log_dir: config.log_dir.clone(),
            running: AtomicBool::new(false),
            replication_thread: None,
        };

        let sharding_manager = ShardingManager {
            strategy: ShardingStrategy::Hash,
            shards: RwLock::new(HashMap::new()),
            shard_count: config.shard_count,
        };

        let schema_manager = SchemaManager {
            schemas: RwLock::new(HashMap::new()),
            version: AtomicU64::new(0),
        };

        let server = DatabaseServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: config.bind_address.clone(),
            connections: RwLock::new(HashMap::new()),
            max_connections: config.max_connections,
        };

        DistributedDatabase {
            node_id: node_id.to_string(),
            config,
            storage_engine,
            query_processor,
            transaction_manager,
            replication_manager,
            sharding_manager,
            schema_manager,
            server,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.config.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        if let Err(e) = std::fs::create_dir_all(&self.config.log_dir) {
            return Err(format!("åˆ›å»ºæ—¥å¿—ç›®å½•å¤±è´¥: {}", e));
        }

        // åŠ è½½æ•°æ®å’Œå…ƒæ•°æ®
        self.load_data()?;

        // åˆå§‹åŒ–åˆ†ç‰‡
        self.initialize_shards()?;

        // å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨
        self.start_replication_manager()?;

        // å¯åŠ¨æœåŠ¡å™¨
        self.start_server()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼æ•°æ®åº“ç³»ç»Ÿ");

        // åœæ­¢æœåŠ¡å™¨
        self.server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢å¤åˆ¶ç®¡ç†å™¨
        self.replication_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.replication_manager.replication_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤åˆ¶çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // æäº¤æˆ–å›æ»šæ‰€æœ‰æ´»è·ƒäº‹åŠ¡
        let transactions = self.transaction_manager.transactions.read().unwrap();
        for (id, transaction) in transactions.iter() {
            if transaction.status == TransactionStatus::Active {
                println!("å›æ»šæœªå®Œæˆäº‹åŠ¡: {}", id);
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå›æ»šäº‹åŠ¡
            }
        }

        // ä¿å­˜æ•°æ®å’Œå…ƒæ•°æ®
        self.save_data()?;

        Ok(())
    }

    fn load_data(&self) -> Result<(), String> {
        println!("åŠ è½½æ•°æ®å’Œå…ƒæ•°æ®");

        // åŠ è½½æ¶æ„
        let schema_path = self.config.data_dir.join("schema.json");
        if schema_path.exists() {
            match std::fs::read_to_string(&schema_path) {
                Ok(content) => {
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSONå¹¶åŠ è½½æ¶æ„
                    println!("åŠ è½½æ¶æ„æ–‡ä»¶");
                },
                Err(e) => return Err(format!("è¯»å–æ¶æ„æ–‡ä»¶å¤±è´¥: {}", e)),
            }
        }

        // åŠ è½½è¡¨æ•°æ®
        let tables_dir = self.config.data_dir.join("tables");
        if tables_dir.exists() {
            match std::fs::read_dir(&tables_dir) {
                Ok(entries) => {
                    for entry in entries {
                        if let Ok(entry) = entry {
                            let file_name = entry.file_name();
                            if let Some(name) = file_name.to_str() {
                                if name.ends_with(".dat") {
                                    let table_name = name.trim_end_matches(".dat");
                                    println!("åŠ è½½è¡¨æ•°æ®: {}", table_name);

                                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½è¡¨æ•°æ®
                                }
                            }
                        }
                    }
                },
                Err(e) => return Err(format!("è¯»å–è¡¨ç›®å½•å¤±è´¥: {}", e)),
            }
        }

        // åŠ è½½ç´¢å¼•
        let indexes_dir = self.config.data_dir.join("indexes");
        if indexes_dir.exists() {
            match std::fs::read_dir(&indexes_dir) {
                Ok(entries) => {
                    for entry in entries {
                        if let Ok(entry) = entry {
                            let file_name = entry.file_name();
                            if let Some(name) = file_name.to_str() {
                                if name.ends_with(".idx") {
                                    let index_name = name.trim_end_matches(".idx");
                                    println!("åŠ è½½ç´¢å¼•: {}", index_name);

                                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½ç´¢å¼•æ•°æ®
                                }
                            }
                        }
                    }
                },
                Err(e) => return Err(format!("è¯»å–ç´¢å¼•ç›®å½•å¤±è´¥: {}", e)),
            }
        }

        Ok(())
    }

    fn save_data(&self) -> Result<(), String> {
        println!("ä¿å­˜æ•°æ®å’Œå…ƒæ•°æ®");

        // ä¿å­˜æ¶æ„
        let schema_path = self.config.data_dir.join("schema.json");
        let schemas = self.schema_manager.schemas.read().unwrap();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ¶æ„åºåˆ—åŒ–ä¸ºJSONå¹¶ä¿å­˜
        println!("ä¿å­˜æ¶æ„æ–‡ä»¶");

        // åˆ›å»ºè¡¨ç›®å½•
        let tables_dir = self.config.data_dir.join("tables");
        if !tables_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&tables_dir) {
                return Err(format!("åˆ›å»ºè¡¨ç›®å½•å¤±è´¥: {}", e));
            }
        }

        // ä¿å­˜è¡¨æ•°æ®
        let tables = self.storage_engine.tables.read().unwrap();
        for (name, table) in tables.iter() {
            let table_path = tables_dir.join(format!("{}.dat", name));
            println!("ä¿å­˜è¡¨æ•°æ®: {}", name);

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†è¡¨æ•°æ®åºåˆ—åŒ–å¹¶ä¿å­˜
        }

        // åˆ›å»ºç´¢å¼•ç›®å½•
        let indexes_dir = self.config.data_dir.join("indexes");
        if !indexes_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&indexes_dir) {
                return Err(format!("åˆ›å»ºç´¢å¼•ç›®å½•å¤±è´¥: {}", e));
            }
        }

        // ä¿å­˜ç´¢å¼•
        let indexes = self.storage_engine.indexes.read().unwrap();
        for (name, index) in indexes.iter() {
            let index_path = indexes_dir.join(format!("{}.idx", name));
            println!("ä¿å­˜ç´¢å¼•: {}", name);

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†ç´¢å¼•æ•°æ®åºåˆ—åŒ–å¹¶ä¿å­˜
        }

        Ok(())
    }

    fn initialize_shards(&self) -> Result<(), String> {
        println!("åˆå§‹åŒ–åˆ†ç‰‡");

        let mut shards = self.sharding_manager.shards.write().unwrap();

        // åˆ›å»ºåˆ†ç‰‡
        for i in 0..self.sharding_manager.shard_count {
            let shard = Shard {
                id: i,
                node_id: self.node_id.clone(),
                tables: HashSet::new(),
                key_range: None,
                key_list: None,
            };

            shards.insert(i, shard);
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®é›†ç¾¤é…ç½®åˆ†é…åˆ†ç‰‡
        // å¯¹äºå•èŠ‚ç‚¹ï¼Œæ‰€æœ‰åˆ†ç‰‡éƒ½åœ¨å½“å‰èŠ‚ç‚¹

        Ok(())
    }

    fn start_replication_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨");

        // æ·»åŠ å½“å‰èŠ‚ç‚¹
        let mut nodes = self.replication_manager.nodes.write().unwrap();

        let self_node = ReplicationNode {
            node_id: self.node_id.clone(),
            address: self.config.bind_address.clone(),
            role: NodeRole::Primary, // å‡è®¾å½“å‰èŠ‚ç‚¹æ˜¯ä¸»èŠ‚ç‚¹
            status: NodeStatus::Healthy,
            last_heartbeat: Utc::now(),
            replication_lag: Duration::from_secs(0),
        };

        nodes.insert(self.node_id.clone(), self_node);

        // æ·»åŠ å…¶ä»–é›†ç¾¤èŠ‚ç‚¹
        for node_addr in &self.config.cluster_nodes {
            if node_addr != &self.config.bind_address {
                let node_id = format!("node-{}", uuid::Uuid::new_v4());

                let node = ReplicationNode {
                    node_id: node_id.clone(),
                    address: node_addr.clone(),
                    role: NodeRole::Secondary,
                    status: NodeStatus::Disconnected,
                    last_heartbeat: Utc::now(),
                    replication_lag: Duration::from_secs(0),
                };

                nodes.insert(node_id, node);
            }
        }

        drop(nodes);

        // å¯åŠ¨å¤åˆ¶çº¿ç¨‹
        let node_id = self.node_id.clone();
        let nodes = self.replication_manager.nodes.clone();
        let log_dir = self.replication_manager.log_dir.clone();

        self.replication_manager.running.store(true, Ordering::SeqCst);

        let running = self.replication_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
                let mut nodes_map = nodes.write().unwrap();
                let now = Utc::now();

                for (id, node) in nodes_map.iter_mut() {
                    if id != &node_id {
                        // æ¨¡æ‹Ÿå‘è¾…åŠ©èŠ‚ç‚¹å‘é€å¿ƒè·³
                        println!("å‘èŠ‚ç‚¹ {} å‘é€å¿ƒè·³", id);

                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€å¿ƒè·³å¹¶æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
                        node.last_heartbeat = now;
                        node.status = NodeStatus::Healthy;
                        node.replication_lag = Duration::from_millis(rand::random::<u64>() % 1000);
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(Duration::from_secs(5));
            }
        });

        self.replication_manager.replication_thread = Some(thread);

        Ok(())
    }

    fn start_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æ•°æ®åº“æœåŠ¡å™¨");

        let bind_address = self.server.bind_address.clone();
        let node_id = self.node_id.clone();
        let max_connections = self.server.max_connections;
        let connections = self.server.connections.clone();

        self.server.running.store(true, Ordering::SeqCst);

        let running = self.server.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªTCPæœåŠ¡å™¨
            println!("æ•°æ®åº“æœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹Ÿæ¥å—æ–°è¿æ¥
                let conn_count = connections.read().unwrap().len();

                if conn_count < max_connections as usize && rand::random::<u32>() % 10 == 0 {
                    let conn_id = uuid::Uuid::new_v4().to_string();
                    let client_addr = format!("192.168.1.{}:{}", rand::random::<u8>(), 10000 + rand::random::<u16>() % 10000);

                    let connection = Connection {
                        id: conn_id.clone(),
                        client_address: client_addr.clone(),
                        user: "test_user".to_string(),
                        connected_at: Utc::now(),
                        last_activity: Utc::now(),
                        transaction_id: None,
                    };

                    let mut conns = connections.write().unwrap();
                    conns.insert(conn_id.clone(), connection);

                    println!("æ¥å—æ–°è¿æ¥: {} ä» {}", conn_id, client_addr);
                }

                // æ¨¡æ‹Ÿè¿æ¥æ´»åŠ¨
                let mut conns = connections.write().unwrap();
                let now = Utc::now();

                for (_, conn) in conns.iter_mut() {
                    // éšæœºæ›´æ–°æ´»åŠ¨æ—¶é—´
                    if rand::random::<u32>() % 5 == 0 {
                        conn.last_activity = now;
                    }
                }

                // ç§»é™¤ç©ºé—²è¿æ¥
                let timeout = Duration::from_secs(300); // 5åˆ†é’Ÿè¶…æ—¶
                let mut to_remove = Vec::new();

                for (id, conn) in conns.iter() {
                    let idle_time = now.signed_duration_since(conn.last_activity);
                    if idle_time > timeout.into() {
                        to_remove.push(id.clone());
                    }
                }

                for id in to_remove {
                    if let Some(conn) = conns.remove(&id) {
                        println!("å…³é—­ç©ºé—²è¿æ¥: {} ä» {}", id, conn.client_address);
                    }
                }

                // ä¼‘çœ ä¸€ä¼šå„¿
                thread::sleep(Duration::from_secs(1));
            }
        });

        self.server.server = Some(thread);

        Ok(())
    }

    fn execute_query(&self, query: &str, connection_id: &str) -> Result<QueryResult, String> {
        println!("æ‰§è¡ŒæŸ¥è¯¢: {}", query);

        // æ£€æŸ¥è¿æ¥
        let mut connections = self.server.connections.write().unwrap();

        let connection = connections.get_mut(connection_id)
            .ok_or_else(|| format!("è¿æ¥ä¸å­˜åœ¨: {}", connection_id))?;

        // æ›´æ–°æ´»åŠ¨æ—¶é—´
        connection.last_activity = Utc::now();

        // è§£ææŸ¥è¯¢
        let parser = self.query_processor.parsers.get("sql")
            .ok_or_else(|| "SQLè§£æå™¨ä¸å¯ç”¨".to_string())?;

        let mut plan = parser.parse(query)?;

        // ä¼˜åŒ–æŸ¥è¯¢
        for optimizer in &self.query_processor.optimizers {
            plan = optimizer.optimize(plan);
        }

        // è·å–æˆ–åˆ›å»ºäº‹åŠ¡
        let transaction_id = match &connection.transaction_id {
            Some(id) => id.clone(),
            None => {
                // è‡ªåŠ¨åˆ›å»ºäº‹åŠ¡
                let tx_id = self.begin_transaction(connection_id)?;
                connection.transaction_id = Some(tx_id.clone());
                tx_id
            }
        };

        let transactions = self.transaction_manager.transactions.read().unwrap();

        let transaction = transactions.get(&transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        // æ‰§è¡ŒæŸ¥è¯¢
        let executor = self.query_processor.executors.get(match plan.operation_type {
            OperationType::Select => "select",
            OperationType::Insert => "insert",
            OperationType::Update => "update",
            OperationType::Delete => "delete",
            OperationType::CreateTable => "create_table",
            OperationType::DropTable => "drop_table",
            OperationType::CreateIndex => "create_index",
            OperationType::DropIndex => "drop_index",
        }).ok_or_else(|| "æŸ¥è¯¢æ‰§è¡Œå™¨ä¸å¯ç”¨".to_string())?;

        let start_time = Utc::now();
        let result = executor.execute(plan, transaction)?;
        let end_time = Utc::now();

        let execution_time = end_time.signed_duration_since(start_time).to_std().unwrap();

        // è‡ªåŠ¨æäº¤äº‹åŠ¡
        if matches!(plan.operation_type, OperationType::Select) {
            // å¯¹äºSELECTæŸ¥è¯¢ï¼Œä¸è‡ªåŠ¨æäº¤
        } else {
            // å¯¹äºä¿®æ”¹æ“ä½œï¼Œå¦‚æœä¸åœ¨æ˜¾å¼äº‹åŠ¡ä¸­ï¼Œè‡ªåŠ¨æäº¤
            if transaction.operations.len() == 1 {
                drop(transactions);
                drop(connections);
                self.commit_transaction(&transaction_id)?;

                // é‡æ–°è·å–è¿æ¥å¹¶æ¸…é™¤äº‹åŠ¡ID
                let mut connections = self.server.connections.write().unwrap();
                if let Some(conn) = connections.get_mut(connection_id) {
                    conn.transaction_id = None;
                }
            }
        }

        Ok(result)
    }

    fn begin_transaction(&self, connection_id: &str) -> Result<String, String> {
        println!("å¼€å§‹äº‹åŠ¡: è¿æ¥ {}", connection_id);

        // æ£€æŸ¥è¿æ¥
        let connections = self.server.connections.read().unwrap();

        if !connections.contains_key(connection_id) {
            return Err(format!("è¿æ¥ä¸å­˜åœ¨: {}", connection_id));
        }

        // åˆ›å»ºäº‹åŠ¡
        let transaction_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let transaction = Transaction {
            id: transaction_id.clone(),
            start_time: now,
            isolation_level: self.transaction_manager.isolation_level.clone(),
            operations: Vec::new(),
            status: TransactionStatus::Active,
        };

        // ä¿å­˜äº‹åŠ¡
        let mut transactions = self.transaction_manager.transactions.write().unwrap();
        transactions.insert(transaction_id.clone(), transaction);

        // æ›´æ–°è¿æ¥
        drop(connections);
        let mut connections = self.server.connections.write().unwrap();

        if let Some(connection) = connections.get_mut(connection_id) {
            connection.transaction_id = Some(transaction_id.clone());
            connection.last_activity = now;
        }

        Ok(transaction_id)
    }

    fn commit_transaction(&self, transaction_id: &str) -> Result<(), String> {
        println!("æäº¤äº‹åŠ¡: {}", transaction_id);

        // è·å–äº‹åŠ¡
        let mut transactions = self.transaction_manager.transactions.write().unwrap();

        let transaction = transactions.get_mut(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸æ˜¯æ´»åŠ¨çŠ¶æ€: {:?}", transaction.status));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæäº¤æ‰€æœ‰æ“ä½œ
        for operation in &transaction.operations {
            println!("æäº¤æ“ä½œ: è¡¨ {}, ç±»å‹ {:?}", operation.table, operation.operation_type);
            // å®é™…çš„æäº¤é€»è¾‘
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.status = TransactionStatus::Committed;

        // é‡Šæ”¾é”
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé‡Šæ”¾äº‹åŠ¡æŒæœ‰çš„æ‰€æœ‰é”

        Ok(())
    }

    fn rollback_transaction(&self, transaction_id: &str) -> Result<(), String> {
        println!("å›æ»šäº‹åŠ¡: {}", transaction_id);

        // è·å–äº‹åŠ¡
        let mut transactions = self.transaction_manager.transactions.write().unwrap();

        let transaction = transactions.get_mut(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸æ˜¯æ´»åŠ¨çŠ¶æ€: {:?}", transaction.status));
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå›æ»šæ‰€æœ‰æ“ä½œ
        for operation in transaction.operations.iter().rev() {
            println!("å›æ»šæ“ä½œ: è¡¨ {}, ç±»å‹ {:?}", operation.table, operation.operation_type);

            // è¿˜åŸå‰é•œåƒ
            if let Some(before_image) = &operation.before_image {
                // å®é™…çš„å›æ»šé€»è¾‘
            }
        }

        // æ›´æ–°äº‹åŠ¡çŠ¶æ€
        transaction.status = TransactionStatus::Aborted;

        // é‡Šæ”¾é”
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé‡Šæ”¾äº‹åŠ¡æŒæœ‰çš„æ‰€æœ‰é”

        Ok(())
    }

    fn create_table(&self, schema: TableSchema, name: &str) -> Result<(), String> {
        println!("åˆ›å»ºè¡¨: {}", name);

        // æ£€æŸ¥è¡¨å
        if name.is_empty() || !name.chars().all(|c| c.is_alphanumeric() || c == '_') {
            return Err("æ— æ•ˆçš„è¡¨å".to_string());
        }

        // æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();

        if tables.contains_key(name) {
            return Err(format!("è¡¨å·²å­˜åœ¨: {}", name));
        }

        // éªŒè¯æ¶æ„
        if schema.columns.is_empty() {
            return Err("è¡¨å¿…é¡»è‡³å°‘æœ‰ä¸€åˆ—".to_string());
        }

        // æ£€æŸ¥ä¸»é”®
        let mut primary_key = None;

        for constraint in &schema.constraints {
            if let Constraint::PrimaryKey(column) = constraint {
                primary_key = Some(column.clone());

                // æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                if !schema.columns.iter().any(|c| c.name == *column) {
                    return Err(format!("ä¸»é”®åˆ—ä¸å­˜åœ¨: {}", column));
                }
            }
        }

        drop(tables);

        // åˆ›å»ºè¡¨
        let now = Utc::now();

        let table = Table {
            name: name.to_string(),
            schema,
            data: RwLock::new(Vec::new()),
            primary_key,
            indexes: Vec::new(),
            shard_key: None, // å°†åœ¨åé¢è®¾ç½®
            created_at: now,
            last_modified: now,
        };

        // æ›´æ–°å­˜å‚¨å¼•æ“
        let mut tables = self.storage_engine.tables.write().unwrap();
        tables.insert(name.to_string(), table);

        // æ›´æ–°æ¶æ„ç‰ˆæœ¬
        self.schema_manager.version.fetch_add(1, Ordering::SeqCst);

        // æ›´æ–°åˆ†ç‰‡åˆ†é…
        self.assign_table_to_shard(name)?;

        Ok(())
    }

    fn assign_table_to_shard(&self, table_name: &str) -> Result<(), String> {
        // æ ¹æ®åˆ†ç‰‡ç­–ç•¥åˆ†é…è¡¨
        match self.sharding_manager.strategy {
            ShardingStrategy::Hash => {
                // ä½¿ç”¨å“ˆå¸Œåˆ†é…
                let mut hasher = DefaultHasher::new();
                table_name.hash(&mut hasher);
                let hash = hasher.finish();

                let shard_id = hash % self.sharding_manager.shard_count as u64;

                // æ›´æ–°åˆ†ç‰‡è¡¨é›†åˆ
                let mut shards = self.sharding_manager.shards.write().unwrap();

                if let Some(shard) = shards.get_mut(&(shard_id as u32)) {
                    shard.tables.insert(table_name.to_string());

                    // è®¾ç½®è¡¨çš„åˆ†ç‰‡é”®
                    let mut tables = self.storage_engine.tables.write().unwrap();

                    if let Some(table) = tables.get_mut(table_name) {
                        // é€‰æ‹©ä¸»é”®æˆ–ç¬¬ä¸€åˆ—ä½œä¸ºåˆ†ç‰‡é”®
                        table.shard_key = table.primary_key.clone()
                            .or_else(|| table.schema.columns.first().map(|c| c.name.clone()));
                    }
                }
            },
            _ => {
                // å…¶ä»–ç­–ç•¥çš„å®ç°
                return Err("ä¸æ”¯æŒçš„åˆ†ç‰‡ç­–ç•¥".to_string());
            }
        }

        Ok(())
    }

    fn create_index(&self, table_name: &str, index_name: &str, columns: Vec<String>, index_type: IndexType) -> Result<(), String> {
        println!("åˆ›å»ºç´¢å¼•: {} åœ¨è¡¨ {}", index_name, table_name);

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();

        let table = tables.get(table_name)
            .ok_or_else(|| format!("è¡¨ä¸å­˜åœ¨: {}", table_name))?;

        // æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        for column in &columns {
            if !table.schema.columns.iter().any(|c| c.name == *column) {
                return Err(format!("åˆ—ä¸å­˜åœ¨: {}", column));
            }
        }

        // æ£€æŸ¥ç´¢å¼•åæ˜¯å¦å·²å­˜åœ¨
        let indexes = self.storage_engine.indexes.read().unwrap();

        if indexes.contains_key(index_name) {
            return Err(format!("ç´¢å¼•å·²å­˜åœ¨: {}", index_name));
        }

        drop(indexes);

        // åˆ›å»ºç´¢å¼•
        let now = Utc::now();

        let index = Index {
            name: index_name.to_string(),
            table_name: table_name.to_string(),
            columns: columns.clone(),
            index_type,
            data: RwLock::new(BTreeMap::new()),
            created_at: now,
        };

        // æ›´æ–°å­˜å‚¨å¼•æ“
        let mut indexes = self.storage_engine.indexes.write().unwrap();
        indexes.insert(index_name.to_string(), index);

        // æ›´æ–°è¡¨çš„ç´¢å¼•åˆ—è¡¨
        drop(tables);
        let mut tables = self.storage_engine.tables.write().unwrap();

        if let Some(table) = tables.get_mut(table_name) {
            table.indexes.push(index_name.to_string());
            table.last_modified = now;
        }

        // å¡«å……ç´¢å¼•ï¼ˆéå†è¡¨ä¸­çš„æ‰€æœ‰è¡Œï¼‰
        if let Some(table) = tables.get(table_name) {
            let rows = table.data.read().unwrap();
            let mut index_data = indexes.get_mut(index_name).unwrap().data.write().unwrap();

            for row in rows.iter() {
                // æ„å»ºç´¢å¼•é”®
                let index_key = self.build_index_key(row, &columns)?;

                // æ·»åŠ åˆ°ç´¢å¼•
                let entry = index_data.entry(index_key).or_insert_with(Vec::new);
                entry.push(row.id);
            }
        }

        // æ›´æ–°æ¶æ„ç‰ˆæœ¬
        self.schema_manager.version.fetch_add(1, Ordering::SeqCst);

        Ok(())
    }

    fn build_index_key(&self, row: &Row, columns: &[String]) -> Result<IndexKey, String> {
        if columns.len() == 1 {
            // å•åˆ—ç´¢å¼•
            let column = &columns[0];

            if let Some(value) = row.values.get(column) {
                match value {
                    Value::Integer(i) => Ok(IndexKey::Integer(*i)),
                    Value::Float(f) => Ok(IndexKey::Float(*f)),
                    Value::Text(s) => Ok(IndexKey::Text(s.clone())),
                    Value::Boolean(b) => Ok(IndexKey::Boolean(*b)),
                    Value::DateTime(dt) => Ok(IndexKey::DateTime(*dt)),
                    Value::Blob(_) => Err("ä¸æ”¯æŒå°†BLOBåˆ—ä½œä¸ºç´¢å¼•é”®".to_string()),
                    Value::Uuid(_) => Err("ä¸æ”¯æŒå°†UUIDåˆ—ä½œä¸ºç´¢å¼•é”®".to_string()),
                    Value::Null => Err("ä¸èƒ½å°†NULLå€¼ä½œä¸ºç´¢å¼•é”®".to_string()),
                }
            } else {
                Err(format!("åˆ—ä¸å­˜åœ¨äºè¡Œä¸­: {}", column))
            }
        } else {
            // å¤åˆç´¢å¼•
            let mut keys = Vec::new();

            for column in columns {
                if let Some(value) = row.values.get(column) {
                    let key = match value {
                        Value::Integer(i) => IndexKey::Integer(*i),
                        Value::Float(f) => IndexKey::Float(*f),
                        Value::Text(s) => IndexKey::Text(s.clone()),
                        Value::Boolean(b) => IndexKey::Boolean(*b),
                        Value::DateTime(dt) => IndexKey::DateTime(*dt),
                        Value::Blob(_) => return Err("ä¸æ”¯æŒå°†BLOBåˆ—ä½œä¸ºç´¢å¼•é”®".to_string()),
                        Value::Uuid(_) => return Err("ä¸æ”¯æŒå°†UUIDåˆ—ä½œä¸ºç´¢å¼•é”®".to_string()),
                        Value::Null => return Err("ä¸èƒ½å°†NULLå€¼ä½œä¸ºç´¢å¼•é”®".to_string()),
                    };

                    keys.push(key);
                } else {
                    return Err(format!("åˆ—ä¸å­˜åœ¨äºè¡Œä¸­: {}", column));
                }
            }

            Ok(IndexKey::Composite(keys))
        }
    }

    fn drop_table(&self, table_name: &str) -> Result<(), String> {
        println!("åˆ é™¤è¡¨: {}", table_name);

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();

        if !tables.contains_key(table_name) {
            return Err(format!("è¡¨ä¸å­˜åœ¨: {}", table_name));
        }

        // è·å–è¡¨çš„ç´¢å¼•
        let indexes_to_drop = if let Some(table) = tables.get(table_name) {
            table.indexes.clone()
        } else {
            Vec::new()
        };

        drop(tables);

        // åˆ é™¤ç›¸å…³çš„ç´¢å¼•
        let mut indexes = self.storage_engine.indexes.write().unwrap();

        for index_name in &indexes_to_drop {
            indexes.remove(index_name);
        }

        // ç§»é™¤è¡¨
        let mut tables = self.storage_engine.tables.write().unwrap();
        tables.remove(table_name);

        // æ›´æ–°åˆ†ç‰‡åˆ†é…
        let mut shards = self.sharding_manager.shards.write().unwrap();

        for shard in shards.values_mut() {
            shard.tables.remove(table_name);
        }

        // æ›´æ–°æ¶æ„ç‰ˆæœ¬
        self.schema_manager.version.fetch_add(1, Ordering::SeqCst);

        Ok(())
    }

    fn drop_index(&self, index_name: &str) -> Result<(), String> {
        println!("åˆ é™¤ç´¢å¼•: {}", index_name);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indexes = self.storage_engine.indexes.read().unwrap();

        let table_name = if let Some(index) = indexes.get(index_name) {
            index.table_name.clone()
        } else {
            return Err(format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name));
        };

        drop(indexes);

        // ç§»é™¤ç´¢å¼•
        let mut indexes = self.storage_engine.indexes.write().unwrap();
        indexes.remove(index_name);

        // æ›´æ–°è¡¨çš„ç´¢å¼•åˆ—è¡¨
        let mut tables = self.storage_engine.tables.write().unwrap();

        if let Some(table) = tables.get_mut(&table_name) {
            table.indexes.retain(|i| i != index_name);
            table.last_modified = Utc::now();
        }

        // æ›´æ–°æ¶æ„ç‰ˆæœ¬
        self.schema_manager.version.fetch_add(1, Ordering::SeqCst);

        Ok(())
    }

    fn insert_row(&self, table_name: &str, values: HashMap<String, Value>, transaction_id: &str) -> Result<u64, String> {
        println!("æ’å…¥è¡Œåˆ°è¡¨: {}", table_name);

        // æ£€æŸ¥äº‹åŠ¡
        let transactions = self.transaction_manager.transactions.read().unwrap();

        let transaction = transactions.get(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸æ˜¯æ´»åŠ¨çŠ¶æ€: {:?}", transaction.status));
        }

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();

        let table = tables.get(table_name)
            .ok_or_else(|| format!("è¡¨ä¸å­˜åœ¨: {}", table_name))?;

        // éªŒè¯åˆ—
        for column in &table.schema.columns {
            if !values.contains_key(&column.name) && column.default_value.is_none() && !column.nullable {
                return Err(format!("ç¼ºå°‘ä¸å¯ä¸ºç©ºçš„åˆ—: {}", column.name));
            }
        }

        // éªŒè¯ä¸»é”®
        if let Some(primary_key) = &table.primary_key {
            if !values.contains_key(primary_key) {
                return Err(format!("ç¼ºå°‘ä¸»é”®: {}", primary_key));
            }

            // æ£€æŸ¥ä¸»é”®å”¯ä¸€æ€§
            let rows = table.data.read().unwrap();
            let pk_value = values.get(primary_key).unwrap();

            for row in rows.iter() {
                if let Some(row_pk) = row.values.get(primary_key) {
                    if row_pk == pk_value {
                        return Err(format!("ä¸»é”®å†²çª: {}", primary_key));
                    }
                }
            }
        }

        drop(tables);

        // è·å–è¡¨çš„å†™é”
        let tables = self.storage_engine.tables.read().unwrap();
        let table = tables.get(table_name).unwrap();
        let mut rows = table.data.write().unwrap();

        // ç”Ÿæˆè¡ŒID
        let row_id = if rows.is_empty() {
            1
        } else {
            rows.last().unwrap().id + 1
        };

        // åˆ›å»ºè¡Œ
        let now = Utc::now();

        let mut row_values = HashMap::new();

        // å¡«å……å€¼
        for column in &table.schema.columns {
            if let Some(value) = values.get(&column.name) {
                row_values.insert(column.name.clone(), value.clone());
            } else if let Some(default) = &column.default_value {
                row_values.insert(column.name.clone(), default.clone());
            } else if column.nullable {
                row_values.insert(column.name.clone(), Value::Null);
            }
        }

        let row = Row {
            id: row_id,
            values: row_values,
            created_at: now,
            updated_at: now,
        };

        // æ›´æ–°ç´¢å¼•
        for index_name in &table.indexes {
            let indexes = self.storage_engine.indexes.read().unwrap();

            if let Some(index) = indexes.get(index_name) {
                let index_key = self.build_index_key(&row, &index.columns)?;

                // æ£€æŸ¥å”¯ä¸€æ€§ï¼ˆå¯¹äºå”¯ä¸€ç´¢å¼•ï¼‰
                if matches!(index.index_type, IndexType::Unique) {
                    let index_data = index.data.read().unwrap();

                    if index_data.contains_key(&index_key) {
                        return Err(format!("å”¯ä¸€ç´¢å¼•å†²çª: {}", index_name));
                    }
                }

                drop(indexes);

                // æ·»åŠ åˆ°ç´¢å¼•
                let indexes = self.storage_engine.indexes.read().unwrap();
                let index = indexes.get(index_name).unwrap();
                let mut index_data = index.data.write().unwrap();

                let entry = index_data.entry(index_key).or_insert_with(Vec::new);
                entry.push(row_id);
            }
        }

        // æ·»åŠ è¡Œ
        rows.push(row);

        // è®°å½•äº‹åŠ¡æ“ä½œ
        drop(transactions);
        let mut transactions = self.transaction_manager.transactions.write().unwrap();

        if let Some(transaction) = transactions.get_mut(transaction_id) {
            let operation = Operation {
                table: table_name.to_string(),
                operation_type: OperationType::Insert,
                rows_affected: vec![row_id],
                before_image: None,
                after_image: Some([(row_id, rows.last().unwrap().clone())].iter().cloned().collect()),
            };

            transaction.operations.push(operation);
        }

        // å¤åˆ¶åˆ°è¾…åŠ©èŠ‚ç‚¹ï¼ˆå¦‚æœæ˜¯ä¸»èŠ‚ç‚¹ï¼‰
        if self.is_primary_node() {
            self.replicate_operation(OperationType::Insert, table_name, vec![row_id])?;
        }

        Ok(row_id)
    }

    fn is_primary_node(&self) -> bool {
        let nodes = self.replication_manager.nodes.read().unwrap();

        if let Some(node) = nodes.get(&self.node_id) {
            matches!(node.role, NodeRole::Primary)
        } else {
            false
        }
    }

    fn replicate_operation(&self, op_type: OperationType, table_name: &str, row_ids: Vec<u64>) -> Result<(), String> {
        // è·å–è¾…åŠ©èŠ‚ç‚¹
        let nodes = self.replication_manager.nodes.read().unwrap();

        let secondaries: Vec<_> = nodes.values()
            .filter(|n| matches!(n.role, NodeRole::Secondary) && matches!(n.status, NodeStatus::Healthy))
            .cloned()
            .collect();

        if secondaries.is_empty() {
            return Ok(());
        }

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¼‚æ­¥å°†æ“ä½œå¤åˆ¶åˆ°è¾…åŠ©èŠ‚ç‚¹
        // æ ¹æ®å¤åˆ¶æ¨¡å¼å†³å®šæ˜¯å¦ç­‰å¾…å“åº”

        match self.replication_manager.replication_mode {
            ReplicationMode::Sync => {
                // åŒæ­¥å¤åˆ¶ï¼Œç­‰å¾…æ‰€æœ‰è¾…åŠ©èŠ‚ç‚¹ç¡®è®¤
                for node in &secondaries {
                    println!("åŒæ­¥å¤åˆ¶æ“ä½œåˆ°èŠ‚ç‚¹ {}: {:?} {} {:?}", node.node_id, op_type, table_name, row_ids);
                }
            },
            ReplicationMode::SemiSync => {
                // åŠåŒæ­¥å¤åˆ¶ï¼Œç­‰å¾…éƒ¨åˆ†è¾…åŠ©èŠ‚ç‚¹ç¡®è®¤
                let min_ack = (secondaries.len() + 1) / 2; // è‡³å°‘ä¸€åŠèŠ‚ç‚¹ç¡®è®¤

                println!("åŠåŒæ­¥å¤åˆ¶æ“ä½œï¼Œç­‰å¾… {} ä¸ªèŠ‚ç‚¹ç¡®è®¤", min_ack);

                for node in secondaries.iter().take(min_ack) {
                    println!("å¤åˆ¶æ“ä½œåˆ°èŠ‚ç‚¹ {}: {:?} {} {:?}", node.node_id, op_type, table_name, row_ids);
                }
            },
            ReplicationMode::Async => {
                // å¼‚æ­¥å¤åˆ¶ï¼Œä¸ç­‰å¾…ç¡®è®¤
                for node in &secondaries {
                    println!("å¼‚æ­¥å¤åˆ¶æ“ä½œåˆ°èŠ‚ç‚¹ {}: {:?} {} {:?}", node.node_id, op_type, table_name, row_ids);
                }
            },
        }

        Ok(())
    }

    fn update_row(&self, table_name: &str, row_id: u64, values: HashMap<String, Value>, transaction_id: &str) -> Result<(), String> {
        println!("æ›´æ–°è¡¨ {} ä¸­çš„è¡Œ {}", table_name, row_id);

        // æ£€æŸ¥äº‹åŠ¡
        let transactions = self.transaction_manager.transactions.read().unwrap();

        let transaction = transactions.get(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸æ˜¯æ´»åŠ¨çŠ¶æ€: {:?}", transaction.status));
        }

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();

        let table = tables.get(table_name)
            .ok_or_else(|| format!("è¡¨ä¸å­˜åœ¨: {}", table_name))?;

        // éªŒè¯åˆ—
        for column_name in values.keys() {
            if !table.schema.columns.iter().any(|c| c.name == *column_name) {
                return Err(format!("åˆ—ä¸å­˜åœ¨: {}", column_name));
            }
        }

        // è·å–è¡Œ
        let mut rows = table.data.write().unwrap();

        let row_index = rows.iter().position(|r| r.id == row_id)
            .ok_or_else(|| format!("è¡Œä¸å­˜åœ¨: {}", row_id))?;

        // ä¿å­˜åŸå§‹è¡Œï¼ˆå‰é•œåƒï¼‰
        let before_image = rows[row_index].clone();

        // æ›´æ–°è¡Œ
        let now = Utc::now();

        // å…‹éš†è¡Œ
        let mut updated_row = before_image.clone();
        updated_row.updated_at = now;

        // æ›´æ–°å€¼
        for (column, value) in values.iter() {
            updated_row.values.insert(column.clone(), value.clone());
        }

        // éªŒè¯ä¸»é”®
        if let Some(primary_key) = &table.primary_key {
            if values.contains_key(primary_key) {
                // æ£€æŸ¥ä¸»é”®å”¯ä¸€æ€§
                let pk_value = updated_row.values.get(primary_key).unwrap();

                for (i, row) in rows.iter().enumerate() {
                    if i != row_index && row.values.get(primary_key) == Some(pk_value) {
                        return Err(format!("ä¸»é”®å†²çª: {}", primary_key));
                    }
                }
            }
        }

        // ä»ç´¢å¼•ä¸­ç§»é™¤æ—§å€¼
        for index_name in &table.indexes {
            let indexes = self.storage_engine.indexes.read().unwrap();

            if let Some(index) = indexes.get(index_name) {
                let old_key = self.build_index_key(&before_image, &index.columns)?;

                let mut index_data = index.data.write().unwrap();

                if let Some(ids) = index_data.get_mut(&old_key) {
                    ids.retain(|&id| id != row_id);

                    if ids.is_empty() {
                        index_data.remove(&old_key);
                    }
                }

                // è®¡ç®—æ–°é”®
                let new_key = self.build_index_key(&updated_row, &index.columns)?;

                // æ£€æŸ¥å”¯ä¸€æ€§ï¼ˆå¯¹äºå”¯ä¸€ç´¢å¼•ï¼‰
                if matches!(index.index_type, IndexType::Unique) {
                    if let Some(ids) = index_data.get(&new_key) {
                        if !ids.is_empty() && ids[0] != row_id {
                            return Err(format!("å”¯ä¸€ç´¢å¼•å†²çª: {}", index_name));
                        }
                    }
                }

                // æ·»åŠ æ–°é”®
                let entry = index_data.entry(new_key).or_insert_with(Vec::new);

                if !entry.contains(&row_id) {
                    entry.push(row_id);
                }
            }
        }

        // æ›´æ–°è¡Œ
        rows[row_index] = updated_row.clone();

        // è®°å½•äº‹åŠ¡æ“ä½œ
        drop(transactions);
        let mut transactions = self.transaction_manager.transactions.write().unwrap();

        if let Some(transaction) = transactions.get_mut(transaction_id) {
            let operation = Operation {
                table: table_name.to_string(),
                operation_type: OperationType::Update,
                rows_affected: vec![row_id],
                before_image: Some([(row_id, before_image)].iter().cloned().collect()),
                after_image: Some([(row_id, updated_row)].iter().cloned().collect()),
            };

            transaction.operations.push(operation);
        }

        // å¤åˆ¶åˆ°è¾…åŠ©èŠ‚ç‚¹ï¼ˆå¦‚æœæ˜¯ä¸»èŠ‚ç‚¹ï¼‰
        if self.is_primary_node() {
            self.replicate_operation(OperationType::Update, table_name, vec![row_id])?;
        }

        Ok(())
    }

    fn delete_row(&self, table_name: &str, row_id: u64, transaction_id: &str) -> Result<(), String> {
        println!("åˆ é™¤è¡¨ {} ä¸­çš„è¡Œ {}", table_name, row_id);

        // æ£€æŸ¥äº‹åŠ¡
        let transactions = self.transaction_manager.transactions.read().unwrap();

        let transaction = transactions.get(transaction_id)
            .ok_or_else(|| format!("äº‹åŠ¡ä¸å­˜åœ¨: {}", transaction_id))?;

        if transaction.status != TransactionStatus::Active {
            return Err(format!("äº‹åŠ¡ä¸æ˜¯æ´»åŠ¨çŠ¶æ€: {:?}", transaction.status));
        }

        // æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        let tables = self.storage_engine.tables.read().unwrap();

        let table = tables.get(table_name)
            .ok_or_else(|| format!("è¡¨ä¸å­˜åœ¨: {}", table_name))?;

        // è·å–è¡Œ
        let mut rows = table.data.write().unwrap();

        let row_index = rows.iter().position(|r| r.id == row_id)
            .ok_or_else(|| format!("è¡Œä¸å­˜åœ¨: {}", row_id))?;

        // ä¿å­˜åŸå§‹è¡Œï¼ˆå‰é•œåƒï¼‰
        let before_image = rows[row_index].clone();

        // ä»ç´¢å¼•ä¸­ç§»é™¤
        for index_name in &table.indexes {
            let indexes = self.storage_engine.indexes.read().unwrap();

            if let Some(index) = indexes.get(index_name) {
                let key = self.build_index_key(&before_image, &index.columns)?;

                let mut index_data = index.data.write().unwrap();

                if let Some(ids) = index_data.get_mut(&key) {
                    ids.retain(|&id| id != row_id);

                    if ids.is_empty() {
                        index_data.remove(&key);
                    }
                }
            }
        }

        // åˆ é™¤è¡Œ
        rows.remove(row_index);

        // è®°å½•äº‹åŠ¡æ“ä½œ
        drop(transactions);
        let mut transactions = self.transaction_manager.transactions.write().unwrap();

        if let Some(transaction) = transactions.get_mut(transaction_id) {
            let operation = Operation {
                table: table_name.to_string(),
                operation_type: OperationType::Delete,
                rows_affected: vec![row_id],
                before_image: Some([(row_id, before_image)].iter().cloned().collect()),
                after_image: None,
            };

            transaction.operations.push(operation);
        }

        // å¤åˆ¶åˆ°è¾…åŠ©èŠ‚ç‚¹ï¼ˆå¦‚æœæ˜¯ä¸»èŠ‚ç‚¹ï¼‰
        if self.is_primary_node() {
            self.replicate_operation(OperationType::Delete, table_name, vec![row_id])?;
        }

        Ok(())
    }

    fn get_database_status(&self) -> DatabaseStatus {
        // è·å–èŠ‚ç‚¹çŠ¶æ€
        let nodes = self.replication_manager.nodes.read().unwrap();

        let mut primary_count = 0;
        let mut secondary_count = 0;
        let mut healthy_count = 0;

        for node in nodes.values() {
            match node.role {
                NodeRole::Primary => primary_count += 1,
                NodeRole::Secondary => secondary_count += 1,
                _ => {},
            }

            if matches!(node.status, NodeStatus::Healthy) {
                healthy_count += 1;
            }
        }

        // è·å–è¡¨å’Œç´¢å¼•ä¿¡æ¯
        let tables = self.storage_engine.tables.read().unwrap();
        let indexes = self.storage_engine.indexes.read().unwrap();

        let mut row_count = 0;

        for table in tables.values() {
            row_count += table.data.read().unwrap().len();
        }

        // è·å–äº‹åŠ¡ä¿¡æ¯
        let transactions = self.transaction_manager.transactions.read().unwrap();

        let active_transactions = transactions.values()
            .filter(|tx| tx.status == TransactionStatus::Active)
            .count();

        // è·å–è¿æ¥ä¿¡æ¯
        let connections = self.server.connections.read().unwrap();

        DatabaseStatus {
            node_id: self.node_id.clone(),
            role: self.get_node_role(),
            cluster_size: nodes.len(),
            primary_count,
            secondary_count,
            healthy_nodes: healthy_count,
            table_count: tables.len(),
            index_count: indexes.len(),
            row_count,
            active_transactions,
            connection_count: connections.len(),
            uptime: Duration::from_secs(3600), // å‡è®¾è¿è¡Œäº†1å°æ—¶
            schema_version: self.schema_manager.version.load(Ordering::SeqCst),
        }
    }

    fn get_node_role(&self) -> NodeRole {
        let nodes = self.replication_manager.nodes.read().unwrap();

        if let Some(node) = nodes.get(&self.node_id) {
            node.role.clone()
        } else {
            NodeRole::Secondary // é»˜è®¤
        }
    }
}

struct DatabaseStatus {
    node_id: String,
    role: NodeRole,
    cluster_size: usize,
    primary_count: usize,
    secondary_count: usize,
    healthy_nodes: usize,
    table_count: usize,
    index_count: usize,
    row_count: usize,
    active_transactions: usize,
    connection_count: usize,
    uptime: Duration,
    schema_version: u64,
}

impl Clone for Row {
    fn clone(&self) -> Self {
        Row {
            id: self.id,
            values: self.values.clone(),
            created_at: self.created_at,
            updated_at: self.updated_at,
        }
    }
}

impl Clone for Value {
    fn clone(&self) -> Self {
        match self {
            Value::Integer(i) => Value::Integer(*i),
            Value::Float(f) => Value::Float(*f),
            Value::Text(s) => Value::Text(s.clone()),
            Value::Boolean(b) => Value::Boolean(*b),
            Value::DateTime(dt) => Value::DateTime(*dt),
            Value::Blob(b) => Value::Blob(b.clone()),
            Value::Uuid(u) => Value::Uuid(*u),
            Value::Null => Value::Null,
        }
    }
}

impl Clone for ReplicationNode {
    fn clone(&self) -> Self {
        ReplicationNode {
            node_id: self.node_id.clone(),
            address: self.address.clone(),
            role: self.role.clone(),
            status: self.status.clone(),
            last_heartbeat: self.last_heartbeat,
            replication_lag: self.replication_lag,
        }
    }
}

impl Clone for NodeRole {
    fn clone(&self) -> Self {
        match self {
            NodeRole::Primary => NodeRole::Primary,
            NodeRole::Secondary => NodeRole::Secondary,
            NodeRole::Arbiter => NodeRole::Arbiter,
        }
    }
}

impl Clone for NodeStatus {
    fn clone(&self) -> Self {
        match self {
            NodeStatus::Healthy => NodeStatus::Healthy,
            NodeStatus::Degraded => NodeStatus::Degraded,
            NodeStatus::Disconnected => NodeStatus::Disconnected,
        }
    }
}

impl Clone for IsolationLevel {
    fn clone(&self) -> Self {
        match self {
            IsolationLevel::ReadUncommitted => IsolationLevel::ReadUncommitted,
            IsolationLevel::ReadCommitted => IsolationLevel::ReadCommitted,
            IsolationLevel::RepeatableRead => IsolationLevel::RepeatableRead,
            IsolationLevel::Serializable => IsolationLevel::Serializable,
        }
    }
}

impl PartialEq for IndexKey {
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (IndexKey::Integer(a), IndexKey::Integer(b)) => a == b,
            (IndexKey::Float(a), IndexKey::Float(b)) => a == b,
            (IndexKey::Text(a), IndexKey::Text(b)) => a == b,
            (IndexKey::Boolean(a), IndexKey::Boolean(b)) => a == b,
            (IndexKey::DateTime(a), IndexKey::DateTime(b)) => a == b,
            (IndexKey::Composite(a), IndexKey::Composite(b)) => a == b,
            _ => false,
        }
    }
}

impl Eq for IndexKey {}

impl Ord for IndexKey {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        match (self, other) {
            (IndexKey::Integer(a), IndexKey::Integer(b)) => a.cmp(b),
            (IndexKey::Float(a), IndexKey::Float(b)) => a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal),
            (IndexKey::Text(a), IndexKey::Text(b)) => a.cmp(b),
            (IndexKey::Boolean(a), IndexKey::Boolean(b)) => a.cmp(b),
            (IndexKey::DateTime(a), IndexKey::DateTime(b)) => a.cmp(b),
            (IndexKey::Composite(a), IndexKey::Composite(b)) => {
                let len_cmp = a.len().cmp(&b.len());
                if len_cmp != std::cmp::Ordering::Equal {
                    return len_cmp;
                }

                for (a_key, b_key) in a.iter().zip(b.iter()) {
                    let key_cmp = a_key.cmp(b_key);
                    if key_cmp != std::cmp::Ordering::Equal {
                        return key_cmp;
                    }
                }

                std::cmp::Ordering::Equal
            },
            // ç±»å‹ä¸åŒæ—¶çš„æ¯”è¾ƒè§„åˆ™
            (IndexKey::Integer(_), _) => std::cmp::Ordering::Less,
            (_, IndexKey::Integer(_)) => std::cmp::Ordering::Greater,
            (IndexKey::Float(_), _) => std::cmp::Ordering::Less,
            (_, IndexKey::Float(_)) => std::cmp::Ordering::Greater,
            (IndexKey::Text(_), _) => std::cmp::Ordering::Less,
            (_, IndexKey::Text(_)) => std::cmp::Ordering::Greater,
            (IndexKey::Boolean(_), _) => std::cmp::Ordering::Less,
            (_, IndexKey::Boolean(_)) => std::cmp::Ordering::Greater,
        }
    }
}

impl PartialOrd for IndexKey {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Hash for IndexKey {
    fn hash<H: Hasher>(&self, state: &mut H) {
        match self {
            IndexKey::Integer(i) => {
                0.hash(state);
                i.hash(state);
            },
            IndexKey::Float(f) => {
                1.hash(state);
                f.to_bits().hash(state);
            },
            IndexKey::Text(s) => {
                2.hash(state);
                s.hash(state);
            },
            IndexKey::Boolean(b) => {
                3.hash(state);
                b.hash(state);
            },
            IndexKey::DateTime(dt) => {
                4.hash(state);
                dt.timestamp().hash(state);
                dt.timestamp_subsec_nanos().hash(state);
            },
            IndexKey::Composite(keys) => {
                5.hash(state);
                for key in keys {
                    key.hash(state);
                }
            },
        }
    }
}

impl PartialEq for LockKey {
    fn eq(&self, other: &Self) -> bool {
        self.resource_type == other.resource_type && self.resource_id == other.resource_id
    }
}

impl Eq for LockKey {}

impl Hash for LockKey {
    fn hash<H: Hasher>(&self, state: &mut H) {
        self.resource_type.hash(state);
        self.resource_id.hash(state);
    }
}

impl Hash for ResourceType {
    fn hash<H: Hasher>(&self, state: &mut H) {
        match self {
            ResourceType::Table => 0.hash(state),
            ResourceType::Row => 1.hash(state),
            ResourceType::Index => 2.hash(state),
        }
    }
}

impl PartialEq for ResourceType {
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (ResourceType::Table, ResourceType::Table) => true,
            (ResourceType::Row, ResourceType::Row) => true,
            (ResourceType::Index, ResourceType::Index) => true,
            _ => false,
        }
    }
}

impl Eq for ResourceType {}
```

### 1.14 ç»¼åˆåº”ç”¨14-åˆ†å¸ƒå¼æœç´¢å¼•æ“

```rust
// åˆ†å¸ƒå¼æœç´¢å¼•æ“
struct DistributedSearchEngine {
    node_id: String,
    index_manager: IndexManager,
    document_store: DocumentStore,
    query_processor: SearchQueryProcessor,
    node_manager: SearchNodeManager,
    replication_manager: SearchReplicationManager,
    server: SearchServer,
}

struct IndexManager {
    indexes: RwLock<HashMap<String, SearchIndex>>,
    index_dir: PathBuf,
}

struct SearchIndex {
    name: String,
    schema: IndexSchema,
    segments: Vec<IndexSegment>,
    metadata: IndexMetadata,
    writer: RwLock<Option<IndexWriter>>,
}

struct IndexSchema {
    fields: Vec<FieldDefinition>,
    analyzers: HashMap<String, Analyzer>,
    index_options: IndexOptions,
}

struct FieldDefinition {
    name: String,
    field_type: FieldType,
    indexed: bool,
    stored: bool,
    tokenized: bool,
    vector_dimensions: Option<usize>,
}

enum FieldType {
    Text,
    Numeric,
    Boolean,
    Date,
    Vector,
    Geo,
}

struct Analyzer {
    name: String,
    tokenizer: Tokenizer,
    filters: Vec<TokenFilter>,
}

enum Tokenizer {
    Standard,
    Whitespace,
    NGram { min_size: usize, max_size: usize },
    EdgeNGram { min_size: usize, max_size: usize },
    Keyword,
}

enum TokenFilter {
    Lowercase,
    Uppercase,
    Stop { words: HashSet<String> },
    Stem { language: String },
    Synonym { mappings: HashMap<String, Vec<String>> },
}

struct IndexOptions {
    primary_key: Option<String>,
    index_merge_policy: MergePolicy,
    similarity: Similarity,
}

enum MergePolicy {
    Tiered { max_merged_segment_size: usize },
    LogByteSized { merge_factor: usize },
    NoMerge,
}

enum Similarity {
    BM25 { k1: f32, b: f32 },
    TF_IDF,
    Boolean,
}

struct IndexSegment {
    id: String,
    doc_count: usize,
    max_doc: usize,
    deleted_docs: HashSet<u32>,
    live_docs: BitVec,
    field_data: HashMap<String, FieldData>,
    created_at: DateTime<Utc>,
    size_bytes: u64,
}

enum FieldData {
    Terms(BTreeMap<String, Vec<PostingList>>),
    Numeric(BTreeMap<f64, Vec<u32>>),
    Vectors(Vec<(Vec<f32>, u32)>),
    Geo(Vec<(f64, f64, u32)>),
}

struct PostingList {
    doc_id: u32,
    positions: Vec<u32>,
    term_freq: u32,
    payload: Option<Vec<u8>>,
}

struct IndexMetadata {
    doc_count: AtomicUsize,
    created_at: DateTime<Utc>,
    last_modified: DateTime<Utc>,
    primary_node: String,
    replica_nodes: Vec<String>,
    settings: HashMap<String, String>,
}

struct IndexWriter {
    buffer: Vec<Document>,
    max_buffer_size: usize,
    auto_commit: bool,
    auto_commit_interval: Duration,
    last_commit: DateTime<Utc>,
}

struct Document {
    id: String,
    fields: HashMap<String, FieldValue>,
    boost: f32,
}

enum FieldValue {
    Text(String),
    Numeric(f64),
    Boolean(bool),
    Date(DateTime<Utc>),
    Vector(Vec<f32>),
    Geo { lat: f64, lon: f64 },
}

struct DocumentStore {
    documents: RwLock<HashMap<String, StoredDocument>>,
    doc_dir: PathBuf,
}

struct StoredDocument {
    id: String,
    fields: HashMap<String, Vec<u8>>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

struct SearchQueryProcessor {
    parsers: HashMap<String, Box<dyn QueryParser>>,
    searchers: HashMap<String, Box<dyn Searcher>>,
}

trait QueryParser: Send + Sync {
    fn parse(&self, query_string: &str, default_field: &str) -> Result<Query, String>;
}

trait Searcher: Send + Sync {
    fn search(&self, query: &Query, index: &SearchIndex, limit: usize) -> Result<SearchResults, String>;
}

enum Query {
    Term { field: String, term: String, boost: f32 },
    Phrase { field: String, terms: Vec<String>, slop: usize, boost: f32 },
    Boolean { clauses: Vec<BooleanClause> },
    Range { field: String, lower: Option<String>, upper: Option<String>, include_lower: bool, include_upper: bool },
    Prefix { field: String, prefix: String },
    Wildcard { field: String, pattern: String },
    Fuzzy { field: String, term: String, max_edits: usize },
    VectorSimilarity { field: String, vector: Vec<f32>, k: usize },
    GeoDistance { field: String, lat: f64, lon: f64, distance: f64 },
    All,
    None,
}

struct BooleanClause {
    query: Box<Query>,
    occur: Occur,
}

enum Occur {
    Must,
    Should,
    MustNot,
}

struct SearchResults {
    total_hits: usize,
    max_score: f32,
    hits: Vec<SearchHit>,
    took_ms: u64,
}

struct SearchHit {
    doc_id: String,
    score: f32,
    fields: HashMap<String, FieldValue>,
    highlights: Option<HashMap<String, Vec<String>>>,
}

struct SearchNodeManager {
    nodes: RwLock<HashMap<String, SearchNode>>,
    cluster_state: RwLock<ClusterState>,
    node_listener: Option<JoinHandle<()>>,
    running: AtomicBool,
}

struct SearchNode {
    id: String,
    address: String,
    status: NodeStatus,
    roles: HashSet<NodeRole>,
    last_heartbeat: DateTime<Utc>,
    resources: NodeResources,
}

enum NodeStatus {
    Green,
    Yellow,
    Red,
    Offline,
}

enum NodeRole {
    Master,
    DataNode,
    CoordinatorNode,
}

struct NodeResources {
    cpu_cores: usize,
    memory_total: u64,
    memory_used: u64,
    disk_total: u64,
    disk_used: u64,
}

struct ClusterState {
    master_node: Option<String>,
    version: u64,
    index_allocations: HashMap<String, Vec<String>>,
    settings: HashMap<String, String>,
}

struct SearchReplicationManager {
    replication_tasks: RwLock<Vec<SearchReplicationTask>>,
    running: AtomicBool,
    replication_thread: Option<JoinHandle<()>>,
}

struct SearchReplicationTask {
    index_name: String,
    segment_id: String,
    source_node: String,
    target_node: String,
    priority: u32,
    created_at: DateTime<Utc>,
}

struct SearchServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

impl DistributedSearchEngine {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str) -> Self {
        let index_dir = data_dir.join("indexes");
        let doc_dir = data_dir.join("documents");

        let index_manager = IndexManager {
            indexes: RwLock::new(HashMap::new()),
            index_dir,
        };

        let document_store = DocumentStore {
            documents: RwLock::new(HashMap::new()),
            doc_dir,
        };

        let query_processor = SearchQueryProcessor {
            parsers: HashMap::new(),
            searchers: HashMap::new(),
        };

        let node_manager = SearchNodeManager {
            nodes: RwLock::new(HashMap::new()),
            cluster_state: RwLock::new(ClusterState {
                master_node: None,
                version: 0,
                index_allocations: HashMap::new(),
                settings: HashMap::new(),
            }),
            node_listener: None,
            running: AtomicBool::new(false),
        };

        let replication_manager = SearchReplicationManager {
            replication_tasks: RwLock::new(Vec::new()),
            running: AtomicBool::new(false),
            replication_thread: None,
        };

        let server = SearchServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
        };

        DistributedSearchEngine {
            node_id: node_id.to_string(),
            index_manager,
            document_store,
            query_processor,
            node_manager,
            replication_manager,
            server,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æœç´¢å¼•æ“");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.index_manager.index_dir) {
            return Err(format!("åˆ›å»ºç´¢å¼•ç›®å½•å¤±è´¥: {}", e));
        }

        if let Err(e) = std::fs::create_dir_all(&self.document_store.doc_dir) {
            return Err(format!("åˆ›å»ºæ–‡æ¡£ç›®å½•å¤±è´¥: {}", e));
        }

        // åŠ è½½ç´¢å¼•
        self.load_indexes()?;

        // æ³¨å†Œå½“å‰èŠ‚ç‚¹
        self.register_node()?;

        // å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨
        self.start_replication_manager()?;

        // å¯åŠ¨æœåŠ¡å™¨
        self.start_server()?;

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼æœç´¢å¼•æ“");

        // åœæ­¢æœåŠ¡å™¨
        self.server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢å¤åˆ¶ç®¡ç†å™¨
        self.replication_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.replication_manager.replication_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å¤åˆ¶çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢èŠ‚ç‚¹ç›‘å¬å™¨
        self.node_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.node_manager.node_listener.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("èŠ‚ç‚¹ç›‘å¬å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // æäº¤æ‰€æœ‰å¾…å¤„ç†çš„å†™å…¥
        let indexes = self.index_manager.indexes.read().unwrap();
        for (name, index) in indexes.iter() {
            match self.commit_index(name) {
                Ok(_) => {},
                Err(e) => println!("æäº¤ç´¢å¼• {} å¤±è´¥: {}", name, e),
            }
        }

        Ok(())
    }

    fn load_indexes(&self) -> Result<(), String> {
        println!("åŠ è½½ç´¢å¼•");

        let index_dir = &self.index_manager.index_dir;

        if !index_dir.exists() {
            return Ok(());
        }

        match std::fs::read_dir(index_dir) {
            Ok(entries) => {
                for entry in entries {
                    if let Ok(entry) = entry {
                        let path = entry.path();

                        if path.is_dir() {
                            let index_name = path.file_name()
                                .and_then(|os| os.to_str())
                                .ok_or("æ— æ•ˆçš„ç´¢å¼•ç›®å½•å".to_string())?;

                            println!("åŠ è½½ç´¢å¼•: {}", index_name);

                            match self.load_index(index_name, &path) {
                                Ok(_) => {},
                                Err(e) => println!("åŠ è½½ç´¢å¼• {} å¤±è´¥: {}", index_name, e),
                            }
                        }
                    }
                }
            },
            Err(e) => return Err(format!("è¯»å–ç´¢å¼•ç›®å½•å¤±è´¥: {}", e)),
        }

        Ok(())
    }

    fn load_index(&self, name: &str, path: &Path) -> Result<(), String> {
        // åŠ è½½ç´¢å¼•å…ƒæ•°æ®
        let metadata_path = path.join("metadata.json");

        if !metadata_path.exists() {
            return Err(format!("ç´¢å¼•å…ƒæ•°æ®ä¸å­˜åœ¨: {}", name));
        }

        let metadata_json = match std::fs::read_to_string(&metadata_path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–ç´¢å¼•å…ƒæ•°æ®å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£æç´¢å¼•å…ƒæ•°æ®: {}", name);

        // åˆ›å»ºç´¢å¼•ç»“æ„
        let schema = IndexSchema {
            fields: Vec::new(),
            analyzers: HashMap::new(),
            index_options: IndexOptions {
                primary_key: Some("id".to_string()),
                index_merge_policy: MergePolicy::Tiered { max_merged_segment_size: 1024 * 1024 * 100 },
                similarity: Similarity::BM25 { k1: 1.2, b: 0.75 },
            },
        };

        let metadata = IndexMetadata {
            doc_count: AtomicUsize::new(0),
            created_at: Utc::now(),
            last_modified: Utc::now(),
            primary_node: self.node_id.clone(),
            replica_nodes: Vec::new(),
            settings: HashMap::new(),
        };

        let index = SearchIndex {
            name: name.to_string(),
            schema,
            segments: Vec::new(),
            metadata,
            writer: RwLock::new(None),
        };

        // ä¿å­˜ç´¢å¼•
        let mut indexes = self.index_manager.indexes.write().unwrap();
        indexes.insert(name.to_string(), index);

        Ok(())
    }

    fn register_node(&self) -> Result<(), String> {
        println!("æ³¨å†Œæœç´¢èŠ‚ç‚¹");

        let mut nodes = self.node_manager.nodes.write().unwrap();

        // åˆ›å»ºå½“å‰èŠ‚ç‚¹ä¿¡æ¯
        let node = SearchNode {
            id: self.node_id.clone(),
            address: self.server.bind_address.clone(),
            status: NodeStatus::Green,
            roles: [NodeRole::DataNode, NodeRole::CoordinatorNode].iter().cloned().collect(),
            last_heartbeat: Utc::now(),
            resources: NodeResources {
                cpu_cores: num_cpus::get(),
                memory_total: 16 * 1024 * 1024 * 1024, // 16GBï¼ˆç¤ºä¾‹å€¼ï¼‰
                memory_used: 1 * 1024 * 1024 * 1024,   // 1GBï¼ˆç¤ºä¾‹å€¼ï¼‰
                disk_total: 1000 * 1024 * 1024 * 1024, // 1TBï¼ˆç¤ºä¾‹å€¼ï¼‰
                disk_used: 100 * 1024 * 1024 * 1024,   // 100GBï¼ˆç¤ºä¾‹å€¼ï¼‰
            },
        };

        nodes.insert(self.node_id.clone(), node);

        // å¯åŠ¨èŠ‚ç‚¹å‘ç°æœåŠ¡
        self.start_node_listener()?;

        // å°è¯•åŠ å…¥é›†ç¾¤
        self.join_cluster()?;

        Ok(())
    }

    fn start_node_listener(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨èŠ‚ç‚¹ç›‘å¬å™¨");

        let nodes = self.node_manager.nodes.clone();
        let cluster_state = self.node_manager.cluster_state.clone();
        let node_id = self.node_id.clone();

        self.node_manager.running.store(true, Ordering::SeqCst);

        let running = self.node_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ›´æ–°èŠ‚ç‚¹å¿ƒè·³
                let mut nodes_map = nodes.write().unwrap();

                if let Some(node) = nodes_map.get_mut(&node_id) {
                    node.last_heartbeat = Utc::now();
                    node.status = NodeStatus::Green;

                    // æ›´æ–°èµ„æºä½¿ç”¨æƒ…å†µ
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ”¶é›†ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
                }

                // æ£€æµ‹ç¦»çº¿èŠ‚ç‚¹
                let now = Utc::now();
                let timeout = Duration::from_secs(30);

                for (id, node) in nodes_map.iter_mut() {
                    if id != &node_id {
                        let elapsed = now.signed_duration_since(node.last_heartbeat);

                        if elapsed > timeout.into() && node.status != NodeStatus::Offline {
                            println!("èŠ‚ç‚¹ç¦»çº¿: {}", id);
                            node.status = NodeStatus::Offline;

                            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§¦å‘é‡æ–°åˆ†é…
                        }
                    }
                }

                // ä¸»èŠ‚ç‚¹é€‰ä¸¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                let mut cluster = cluster_state.write().unwrap();

                if cluster.master_node.is_none() ||
                   cluster.master_node.as_ref().map_or(false, |id|
                        nodes_map.get(id).map_or(true, |n| n.status == NodeStatus::Offline)) {
                    // éœ€è¦é€‰ä¸¾æ–°çš„ä¸»èŠ‚ç‚¹
                    let candidates: Vec<_> = nodes_map.iter()
                        .filter(|(_, n)| n.status == NodeStatus::Green && n.roles.contains(&NodeRole::Master))
                        .map(|(id, _)| id.clone())
                        .collect();

                    if !candidates.is_empty() {
                        // ç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ªå¥åº·èŠ‚ç‚¹ä½œä¸ºä¸»èŠ‚ç‚¹
                        let new_master = candidates[0].clone();
                        cluster.master_node = Some(new_master.clone());
                        cluster.version += 1;

                        println!("é€‰ä¸¾æ–°çš„ä¸»èŠ‚ç‚¹: {}", new_master);
                    }
                }

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(Duration::from_secs(5));
            }
        });

        self.node_manager.node_listener = Some(thread);

        Ok(())
    }

    fn join_cluster(&self) -> Result<(), String> {
        println!("åŠ å…¥æœç´¢é›†ç¾¤");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°è¯•è¿æ¥å…¶ä»–å·²çŸ¥èŠ‚ç‚¹
        // å¹¶è·å–é›†ç¾¤çŠ¶æ€

        // ç®€åŒ–ï¼šå‡è®¾è¿™æ˜¯ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå°†å…¶è®¾ä¸ºä¸»èŠ‚ç‚¹
        let mut cluster_state = self.node_manager.cluster_state.write().unwrap();

        if cluster_state.master_node.is_none() {
            let nodes = self.node_manager.nodes.read().unwrap();

            if let Some(node) = nodes.get(&self.node_id) {
                if node.roles.contains(&NodeRole::Master) {
                    cluster_state.master_node = Some(self.node_id.clone());
                    cluster_state.version = 1;

                    println!("æˆä¸ºä¸»èŠ‚ç‚¹: {}", self.node_id);
                }
            }
        }

        Ok(())
    }

    fn start_replication_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å¤åˆ¶ç®¡ç†å™¨");

        let replication_tasks = self.replication_manager.replication_tasks.clone();
        let node_id = self.node_id.clone();
        let index_dir = self.index_manager.index_dir.clone();

        self.replication_manager.running.store(true, Ordering::SeqCst);

        let running = self.replication_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // å¤„ç†å¤åˆ¶ä»»åŠ¡
                let tasks_to_process = {
                    let mut tasks = replication_tasks.write().unwrap();
                    let result = tasks.clone();
                    tasks.clear();
                    result
                };

                for task in tasks_to_process {
                    println!("å¤„ç†ç´¢å¼•å¤åˆ¶ä»»åŠ¡: {} -> {}", task.index_name, task.target_node);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»ç£ç›˜è¯»å–æ®µæ•°æ®å¹¶å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹

                    println!("ç´¢å¼•æ®µå¤åˆ¶å®Œæˆ: {}/{}", task.index_name, task.segment_id);
                }

                // ä¼‘çœ ä¸€ä¼šå„¿
                thread::sleep(Duration::from_millis(100));
            }
        });

        self.replication_manager.replication_thread = Some(thread);

        Ok(())
    }

    fn start_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æœç´¢æœåŠ¡å™¨");

        let bind_address = self.server.bind_address.clone();
        let node_id = self.node_id.clone();

        self.server.running.store(true, Ordering::SeqCst);

        let running = self.server.running.clone();

        let index_manager = Arc::new(self.index_manager);
        let document_store = Arc::new(self.document_store);
        let query_processor = Arc::new(self.query_processor);

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTPæœåŠ¡å™¨
            println!("æœç´¢æœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
            }
        });

        self.server.server = Some(thread);

        Ok(())
    }

    fn create_index(&self, name: &str, schema: IndexSchema) -> Result<(), String> {
        println!("åˆ›å»ºç´¢å¼•: {}", name);

        // éªŒè¯ç´¢å¼•åç§°
        if name.is_empty() || !name.chars().all(|c| c.is_alphanumeric() || c == '_' || c == '-') {
            return Err("æ— æ•ˆçš„ç´¢å¼•åç§°".to_string());
        }

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
        let indexes = self.index_manager.indexes.read().unwrap();

        if indexes.contains_key(name) {
            return Err(format!("ç´¢å¼•å·²å­˜åœ¨: {}", name));
        }

        drop(indexes);

        // éªŒè¯æ¶æ„
        if schema.fields.is_empty() {
            return Err("ç´¢å¼•å¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªå­—æ®µ".to_string());
        }

        // æ£€æŸ¥ä¸»é”®
        if let Some(pk) = &schema.index_options.primary_key {
            if !schema.fields.iter().any(|f| f.name == *pk) {
                return Err(format!("ä¸»é”®å­—æ®µä¸å­˜åœ¨: {}", pk));
            }
        }

        // åˆ›å»ºç´¢å¼•ç›®å½•
        let index_path = self.index_manager.index_dir.join(name);

        if let Err(e) = std::fs::create_dir_all(&index_path) {
            return Err(format!("åˆ›å»ºç´¢å¼•ç›®å½•å¤±è´¥: {}", e));
        }

        // åˆ›å»ºç´¢å¼•å†™å…¥å™¨
        let now = Utc::now();

        let writer = IndexWriter {
            buffer: Vec::new(),
            max_buffer_size: 1000,
            auto_commit: true,
            auto_commit_interval: Duration::from_secs(30),
            last_commit: now,
        };

        // åˆ›å»ºç´¢å¼•å…ƒæ•°æ®
        let metadata = IndexMetadata {
            doc_count: AtomicUsize::new(0),
            created_at: now,
            last_modified: now,
            primary_node: self.node_id.clone(),
            replica_nodes: Vec::new(),
            settings: HashMap::new(),
        };

        // åˆ›å»ºç´¢å¼•
        let index = SearchIndex {
            name: name.to_string(),
            schema,
            segments: Vec::new(),
            metadata,
            writer: RwLock::new(Some(writer)),
        };

        // ä¿å­˜ç´¢å¼•
        let mut indexes = self.index_manager.indexes.write().unwrap();
        indexes.insert(name.to_string(), index);

        // ä¿å­˜å…ƒæ•°æ®åˆ°ç£ç›˜
        self.save_index_metadata(name)?;

        // æ›´æ–°é›†ç¾¤çŠ¶æ€
        self.update_index_allocation(name)?;

        Ok(())
    }

    fn save_index_metadata(&self, index_name: &str) -> Result<(), String> {
        let indexes = self.index_manager.indexes.read().unwrap();

        let index = indexes.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†å…ƒæ•°æ®åºåˆ—åŒ–ä¸ºJSON
        let metadata_json = format!("{{\"name\":\"{}\",\"doc_count\":{},\"created_at\":\"{}\",\"primary_node\":\"{}\"}}",
                                   index_name,
                                   index.metadata.doc_count.load(Ordering::SeqCst),
                                   index.metadata.created_at,
                                   index.metadata.primary_node);

        let metadata_path = self.index_manager.index_dir.join(index_name).join("metadata.json");

        if let Err(e) = std::fs::write(&metadata_path, metadata_json) {
            return Err(format!("å†™å…¥ç´¢å¼•å…ƒæ•°æ®å¤±è´¥: {}", e));
        }

        Ok(())
    }

    fn update_index_allocation(&self, index_name: &str) -> Result<(), String> {
        // æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»èŠ‚ç‚¹
        let cluster_state = self.node_manager.cluster_state.read().unwrap();

        if cluster_state.master_node.as_ref() != Some(&self.node_id) {
            // åªæœ‰ä¸»èŠ‚ç‚¹å¯ä»¥æ›´æ–°åˆ†é…
            return Ok(());
        }

        drop(cluster_state);

        // è·å–å¯ç”¨èŠ‚ç‚¹
        let nodes = self.node_manager.nodes.read().unwrap();

        let available_nodes: Vec<_> = nodes.iter()
            .filter(|(_, node)| node.status == NodeStatus::Green && node.roles.contains(&NodeRole::DataNode))
            .map(|(id, _)| id.clone())
            .collect();

        if available_nodes.is_empty() {
            return Err("æ²¡æœ‰å¯ç”¨çš„æ•°æ®èŠ‚ç‚¹".to_string());
        }

        // æ›´æ–°é›†ç¾¤çŠ¶æ€
        let mut cluster_state = self.node_manager.cluster_state.write().unwrap();

        // åˆ†é…ç´¢å¼•å‰¯æœ¬ï¼ˆç®€åŒ–ï¼šå°†ç´¢å¼•åˆ†é…ç»™æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹ï¼‰
        cluster_state.index_allocations.insert(index_name.to_string(), available_nodes);
        cluster_state.version += 1;

        // æ›´æ–°ç´¢å¼•å…ƒæ•°æ®
        drop(cluster_state);
        drop(nodes);

        let indexes = self.index_manager.indexes.read().unwrap();

        if let Some(index) = indexes.get(index_name) {
            // è·å–å‰¯æœ¬èŠ‚ç‚¹
            let mut replica_nodes = Vec::new();
            let cluster_state = self.node_manager.cluster_state.read().unwrap();

            if let Some(allocated_nodes) = cluster_state.index_allocations.get(index_name) {
                for node_id in allocated_nodes {
                    if node_id != &self.node_id {
                        replica_nodes.push(node_id.clone());
                    }
                }
            }

            // æ›´æ–°ç´¢å¼•å…ƒæ•°æ®
            drop(cluster_state);
            drop(indexes);

            let indexes = self.index_manager.indexes.read().unwrap();

            if let Some(index) = indexes.get(index_name) {
                let mut index_meta = index.metadata;
                index_meta.replica_nodes = replica_nodes;

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ›´æ–°ç´¢å¼•å…ƒæ•°æ®
            }
        }

        Ok(())
    }

    fn index_document(&self, index_name: &str, document: Document) -> Result<(), String> {
        println!("ç´¢å¼•æ–‡æ¡£: {} -> {}", index_name, document.id);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indexes = self.index_manager.indexes.read().unwrap();

        let index = indexes.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        // éªŒè¯æ–‡æ¡£å­—æ®µ
        for field in &index.schema.fields {
            if field.name == "id" && !document.fields.contains_key(&field.name) {
                return Err("ç¼ºå°‘IDå­—æ®µ".to_string());
            }

            if field.name != "id" && !field.nullable && !document.fields.contains_key(&field.name) {
                return Err(format!("ç¼ºå°‘å¿…å¡«å­—æ®µ: {}", field.name));
            }
        }

        // æ£€æŸ¥æ–‡æ¡£IDå”¯ä¸€æ€§
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥IDæ˜¯å¦å·²å­˜åœ¨

        // å°†æ–‡æ¡£æ·»åŠ åˆ°å†™å…¥ç¼“å†²åŒº
        let mut writer = index.writer.write().unwrap();

        if let Some(w) = writer.as_mut() {
            w.buffer.push(document.clone());

            // è‡ªåŠ¨æäº¤
            if w.auto_commit {
                let now = Utc::now();
                let elapsed = now.signed_duration_since(w.last_commit);

                if elapsed > w.auto_commit_interval.into() || w.buffer.len() >= w.max_buffer_size {
                    drop(writer);
                    self.commit_index(index_name)?;
                }
            }
        } else {
            return Err(format!("ç´¢å¼• {} ä¸å¯å†™", index_name));
        }

        // æ·»åŠ åˆ°æ–‡æ¡£å­˜å‚¨
        self.store_document(index_name, &document)?;

        Ok(())
    }

    fn store_document(&self, index_name: &str, document: &Document) -> Result<(), String> {
        // åˆ›å»ºå­˜å‚¨æ–‡æ¡£
        let now = Utc::now();

        let mut stored_fields = HashMap::new();

        // åªå­˜å‚¨æ ‡è®°ä¸ºstoredçš„å­—æ®µ
        let indexes = self.index_manager.indexes.read().unwrap();

        if let Some(index) = indexes.get(index_name) {
            for field in &index.schema.fields {
                if field.stored {
                    if let Some(value) = document.fields.get(&field.name) {
                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®å­—æ®µç±»å‹è¿›è¡Œåºåˆ—åŒ–
                        let serialized = match value {
                            FieldValue::Text(text) => text.as_bytes().to_vec(),
                            FieldValue::Numeric(num) => num.to_string().as_bytes().to_vec(),
                            FieldValue::Boolean(b) => b.to_string().as_bytes().to_vec(),
                            FieldValue::Date(date) => date.to_rfc3339().as_bytes().to_vec(),
                            FieldValue::Vector(vec) => format!("{:?}", vec).as_bytes().to_vec(),
                            FieldValue::Geo { lat, lon } => format!("{},{}", lat, lon).as_bytes().to_vec(),
                        };

                        stored_fields.insert(field.name.clone(), serialized);
                    }
                }
            }
        }

        let stored_doc = StoredDocument {
            id: document.id.clone(),
            fields: stored_fields,
            created_at: now,
            updated_at: now,
        };

        // ä¿å­˜åˆ°æ–‡æ¡£å­˜å‚¨
        let mut documents = self.document_store.documents.write().unwrap();
        documents.insert(document.id.clone(), stored_doc);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ–‡æ¡£å†™å…¥ç£ç›˜

        Ok(())
    }

    fn commit_index(&self, index_name: &str) -> Result<(), String> {
        println!("æäº¤ç´¢å¼•: {}", index_name);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indexes = self.index_manager.indexes.read().unwrap();

        let index = indexes.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        // è·å–å†™å…¥å™¨
        let mut writer = index.writer.write().unwrap();

        if let Some(w) = writer.as_mut() {
            if w.buffer.is_empty() {
                return Ok(());
            }

            // åˆ›å»ºæ–°æ®µ
            let segment_id = uuid::Uuid::new_v4().to_string();
            let now = Utc::now();

            let doc_count = w.buffer.len();

            // å¤„ç†æ–‡æ¡£å¹¶æ„å»ºå€’æ’ç´¢å¼•
            let mut field_data = HashMap::new();

            for field in &index.schema.fields {
                if field.indexed {
                    match field.field_type {
                        FieldType::Text => {
                            let mut terms = BTreeMap::new();

                            for (doc_id, doc) in w.buffer.iter().enumerate() {
                                if let Some(FieldValue::Text(text)) = doc.fields.get(&field.name) {
                                    if field.tokenized {
                                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨åˆ†æå™¨è¿›è¡Œåˆ†è¯
                                        let tokens = text.split_whitespace().collect::<Vec<_>>();

                                        for (pos, token) in tokens.iter().enumerate() {
                                            let term = token.to_lowercase();

                                            let entry = terms.entry(term).or_insert_with(Vec::new);

                                            let posting = if let Some(idx) = entry.iter().position(|p: &PostingList| p.doc_id == doc_id as u32) {
                                                &mut entry[idx]
                                            } else {
                                                entry.push(PostingList {
                                                    doc_id: doc_id as u32,
                                                    positions: Vec::new(),
                                                    term_freq: 0,
                                                    payload: None,
                                                });
                                                entry.last_mut().unwrap()
                                            };

                                            posting.positions.push(pos as u32);
                                            posting.term_freq += 1;
                                        }
                                    } else {
                                        // ä¸åˆ†è¯ï¼Œæ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªè¯é¡¹
                                        let term = text.to_lowercase();

                                        let entry = terms.entry(term).or_insert_with(Vec::new);

                                        entry.push(PostingList {
                                            doc_id: doc_id as u32,
                                            positions: vec![0],
                                            term_freq: 1,
                                            payload: None,
                                        });
                                    }
                                }
                            }

                            field_data.insert(field.name.clone(), FieldData::Terms(terms));
                        },
                        FieldType::Numeric => {
                            let mut values = BTreeMap::new();

                            for (doc_id, doc) in w.buffer.iter().enumerate() {
                                if let Some(FieldValue::Numeric(num)) = doc.fields.get(&field.name) {
                                    let entry = values.entry(*num).or_insert_with(Vec::new);
                                    entry.push(doc_id as u32);
                                }
                            }

                            field_data.insert(field.name.clone(), FieldData::Numeric(values));
                        },
                        FieldType::Vector => {
                            let mut vectors = Vec::new();

                            for (doc_id, doc) in w.buffer.iter().enumerate() {
                                if let Some(FieldValue::Vector(vec)) = doc.fields.get(&field.name) {
                                    vectors.push((vec.clone(), doc_id as u32));
                                }
                            }

                            field_data.insert(field.name.clone(), FieldData::Vectors(vectors));
                        },
                        FieldType::Geo => {
                            let mut points = Vec::new();

                            for (doc_id, doc) in w.buffer.iter().enumerate() {
                                if let Some(FieldValue::Geo { lat, lon }) = doc.fields.get(&field.name) {
                                    points.push((*lat, *lon, doc_id as u32));
                                }
                            }

                            field_data.insert(field.name.clone(), FieldData::Geo(points));
                        },
                        _ => {
                            // å…¶ä»–å­—æ®µç±»å‹æš‚ä¸å¤„ç†
                        }
                    }
                }
            }

            // åˆ›å»ºæ–°æ®µ
            let segment = IndexSegment {
                id: segment_id.clone(),
                doc_count,
                max_doc: doc_count,
                deleted_docs: HashSet::new(),
                live_docs: bitvec::bitvec![1; doc_count],
                field_data,
                created_at: now,
                size_bytes: 0, // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè®¡ç®—å®é™…å¤§å°
            };

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ®µå†™å…¥ç£ç›˜
            let segment_path = self.index_manager.index_dir.join(index_name).join("segments").join(&segment_id);

            if let Err(e) = std::fs::create_dir_all(&segment_path) {
                return Err(format!("åˆ›å»ºæ®µç›®å½•å¤±è´¥: {}", e));
            }

            println!("æ–°æ®µåˆ›å»º: {}/{}", index_name, segment_id);

            // æ›´æ–°ç´¢å¼•å…ƒæ•°æ®
            index.metadata.doc_count.fetch_add(doc_count, Ordering::SeqCst);
            index.metadata.last_modified = now;

            // æ·»åŠ æ®µåˆ°ç´¢å¼•
            let segments = &mut index.segments;
            segments.push(segment);

            // æ¸…ç©ºç¼“å†²åŒº
            w.buffer.clear();
            w.last_commit = now;

            // åˆ›å»ºå¤åˆ¶ä»»åŠ¡
            let replica_nodes = index.metadata.replica_nodes.clone();

            for target_node in replica_nodes {
                let task = SearchReplicationTask {
                    index_name: index_name.to_string(),
                    segment_id: segment_id.clone(),
                    source_node: self.node_id.clone(),
                    target_node,
                    priority: 1,
                    created_at: now,
                };

                self.replication_manager.replication_tasks.write().unwrap().push(task);
            }

            // ä¿å­˜å…ƒæ•°æ®
            self.save_index_metadata(index_name)?;

            // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶æ®µ
            drop(writer);

            if segments.len() > 10 {
                // è§¦å‘æ®µåˆå¹¶
                self.merge_segments(index_name)?;
            }
        }

        Ok(())
    }

    fn merge_segments(&self, index_name: &str) -> Result<(), String> {
        println!("åˆå¹¶ç´¢å¼•æ®µ: {}", index_name);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®åˆå¹¶ç­–ç•¥é€‰æ‹©è¦åˆå¹¶çš„æ®µ
        // ç„¶ååˆå¹¶å®ƒä»¬å¹¶åˆ›å»ºæ–°æ®µ

        Ok(())
    }

    fn get_document(&self, index_name: &str, doc_id: &str) -> Result<Option<Document>, String> {
        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indexes = self.index_manager.indexes.read().unwrap();

        if !indexes.contains_key(index_name) {
            return Err(format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name));
        }

        // ä»æ–‡æ¡£å­˜å‚¨ä¸­è·å–æ–‡æ¡£
        let documents = self.document_store.documents.read().unwrap();

        if let Some(stored_doc) = documents.get(doc_id) {
            // æ„å»ºæ–‡æ¡£å¯¹è±¡
            let mut fields = HashMap::new();

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®å­—æ®µç±»å‹ååºåˆ—åŒ–
            for (name, value) in &stored_doc.fields {
                // ç®€åŒ–ï¼šå‡è®¾æ‰€æœ‰å­—æ®µéƒ½æ˜¯æ–‡æœ¬
                let text = String::from_utf8_lossy(value).to_string();
                fields.insert(name.clone(), FieldValue::Text(text));
            }

            let doc = Document {
                id: doc_id.to_string(),
                fields,
                boost: 1.0,
            };

            return Ok(Some(doc));
        }

        Ok(None)
    }

    fn delete_document(&self, index_name: &str, doc_id: &str) -> Result<bool, String> {
        println!("åˆ é™¤æ–‡æ¡£: {} -> {}", index_name, doc_id);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indexes = self.index_manager.indexes.read().unwrap();

        if !indexes.contains_key(index_name) {
            return Err(format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name));
        }

        // ä»æ–‡æ¡£å­˜å‚¨ä¸­åˆ é™¤æ–‡æ¡£
        let mut documents = self.document_store.documents.write().unwrap();
        let removed = documents.remove(doc_id).is_some();

        if removed {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ‡è®°æ–‡æ¡£ä¸ºå·²åˆ é™¤
            // å¹¶åœ¨ä¸‹ä¸€æ¬¡åˆå¹¶æ—¶ç‰©ç†åˆ é™¤

            // æ ‡è®°æ–‡æ¡£ä¸ºå·²åˆ é™¤
            // æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå®é™…ä¸Šéœ€è¦æ ¹æ®æ–‡æ¡£IDæŸ¥æ‰¾å¯¹åº”çš„æ®µå’Œå†…éƒ¨doc_id
        }

        Ok(removed)
    }

    fn search(&self, index_name: &str, query: Query, limit: usize) -> Result<SearchResults, String> {
        println!("æœç´¢: {} -> {:?}", index_name, query);

        // æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        let indexes = self.index_manager.indexes.read().unwrap();

        let index = indexes.get(index_name)
            .ok_or_else(|| format!("ç´¢å¼•ä¸å­˜åœ¨: {}", index_name))?;

        // æäº¤ä»»ä½•æœªæäº¤çš„æ›´æ”¹
        match self.commit_index(index_name) {
            Ok(_) => {},
            Err(e) => println!("æäº¤ç´¢å¼•å¤±è´¥: {}", e),
        }

        // æ‰§è¡Œæœç´¢æŸ¥è¯¢
        let start_time = std::time::Instant::now();

        let searcher = SimpleSearcher {};

        let results = searcher.search(&query, index, limit)?;

        // è®¡ç®—æœç´¢è€—æ—¶
        let elapsed = start_time.elapsed().as_millis() as u64;

        let mut search_results = SearchResults {
            total_hits: results.total_hits,
            max_score: results.max_score,
            hits: Vec::new(),
            took_ms: elapsed,
        };

        // åŠ è½½æ–‡æ¡£æ•°æ®
        for hit in &results.hits {
            match self.get_document(index_name, &hit.doc_id) {
                Ok(Some(doc)) => {
                    let search_hit = SearchHit {
                        doc_id: hit.doc_id.clone(),
                        score: hit.score,
                        fields: doc.fields,
                        highlights: None, // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ·»åŠ é«˜äº®ä¿¡æ¯
                    };

                    search_results.hits.push(search_hit);
                },
                Ok(None) => {
                    println!("è­¦å‘Šï¼šæ‰¾ä¸åˆ°åŒ¹é…çš„æ–‡æ¡£: {}", hit.doc_id);
                },
                Err(e) => {
                    println!("è·å–æ–‡æ¡£å¤±è´¥: {}", e);
                }
            }
        }

        Ok(search_results)
    }
}

struct SimpleSearcher {}

impl Searcher for SimpleSearcher {
    fn search(&self, query: &Query, index: &SearchIndex, limit: usize) -> Result<SearchResults, String> {
        println!("æ‰§è¡Œæœç´¢æŸ¥è¯¢");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®æŸ¥è¯¢ç±»å‹æ‰§è¡Œä¸åŒçš„æœç´¢ç®—æ³•
        // å¹¶è®¡ç®—æ–‡æ¡£ç›¸å…³æ€§å¾—åˆ†

        // ç®€åŒ–ï¼šè¿”å›ä¸€äº›æ¨¡æ‹Ÿç»“æœ
        let results = SearchResults {
            total_hits: 0,
            max_score: 0.0,
            hits: Vec::new(),
            took_ms: 0,
        };

        Ok(results)
    }
}

// åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“
struct DistributedTimeSeriesDB {
    node_id: String,
    storage_engine: TimeSeriesStorageEngine,
    time_series_schema: TimeSeriesSchema,
    partition_manager: TimeSeriesPartitionManager,
    compression_manager: TimeSeriesCompressionManager,
    aggregation_engine: TimeSeriesAggregationEngine,
    retention_manager: TimeSeriesRetentionManager,
    server: TimeSeriesServer,
}

struct TimeSeriesStorageEngine {
    data_dir: PathBuf,
    metrics: RwLock<HashMap<String, MetricSeries>>,
    shard_map: RwLock<HashMap<String, String>>, // metric -> node_id
}

struct MetricSeries {
    metric_name: String,
    labels: HashMap<String, String>,
    data_points: RwLock<Vec<DataPoint>>,
    chunks: Vec<TimeSeriesChunk>,
    current_chunk: RwLock<Option<TimeSeriesChunk>>,
}

struct DataPoint {
    timestamp: i64,
    value: f64,
}

struct TimeSeriesChunk {
    id: String,
    metric_name: String,
    labels: HashMap<String, String>,
    min_time: i64,
    max_time: i64,
    points: Vec<DataPoint>,
    compression_type: CompressionType,
    compressed_data: Option<Vec<u8>>,
    size_bytes: u64,
}

enum CompressionType {
    None,
    Gorilla,
    XOR,
    Delta,
    Dictionary,
    Custom(String),
}

struct TimeSeriesSchema {
    metrics: RwLock<HashMap<String, MetricDefinition>>,
    labels: RwLock<HashSet<String>>,
}

struct MetricDefinition {
    name: String,
    data_type: MetricType,
    description: String,
    default_retention: Duration,
    aggregation_rules: Vec<AggregationRule>,
    labels: HashSet<String>,
}

enum MetricType {
    Counter,
    Gauge,
    Histogram,
    Summary,
}

struct AggregationRule {
    interval: Duration,
    function: AggregationFunction,
    retention: Duration,
}

enum AggregationFunction {
    Sum,
    Avg,
    Min,
    Max,
    Count,
    P50,
    P90,
    P95,
    P99,
}

struct TimeSeriesPartitionManager {
    partitions: RwLock<HashMap<String, TimeSeriesPartition>>,
    partition_strategy: PartitionStrategy,
}

struct TimeSeriesPartition {
    id: String,
    time_range: (i64, i64),
    metrics: HashSet<String>,
    node_id: String,
    status: PartitionStatus,
}

enum PartitionStatus {
    Active,
    ReadOnly,
    Archived,
}

enum PartitionStrategy {
    TimeRange { interval: Duration },
    Hash { num_partitions: usize },
    LabelBased { label: String },
}

struct TimeSeriesCompressionManager {
    strategies: HashMap<String, CompressionStrategy>,
}

struct CompressionStrategy {
    compression_type: CompressionType,
    threshold_bytes: u64,
    settings: HashMap<String, String>,
}

struct TimeSeriesAggregationEngine {
    rules: HashMap<String, Vec<AggregationRule>>,
    aggregation_queue: Vec<AggregationTask>,
    thread_pool: ThreadPool,
    downsampled_metrics: HashMap<String, HashMap<Duration, MetricSeries>>,
}

struct AggregationTask {
    metric_name: String,
    labels: HashMap<String, String>,
    rule: AggregationRule,
    time_range: (i64, i64),
}

struct TimeSeriesRetentionManager {
    policies: HashMap<String, RetentionPolicy>,
    retention_thread: Option<JoinHandle<()>>,
    running: AtomicBool,
}

struct RetentionPolicy {
    metric_name: String,
    retention_period: Duration,
    downsampling: Vec<DownsamplingConfig>,
}

struct DownsamplingConfig {
    period: Duration,
    function: AggregationFunction,
    retention: Duration,
}

struct TimeSeriesServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

impl DistributedTimeSeriesDB {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str) -> Self {
        let storage_engine = TimeSeriesStorageEngine {
            data_dir: data_dir.join("data"),
            metrics: RwLock::new(HashMap::new()),
            shard_map: RwLock::new(HashMap::new()),
        };

        let time_series_schema = TimeSeriesSchema {
            metrics: RwLock::new(HashMap::new()),
            labels: RwLock::new(HashSet::new()),
        };

        let partition_manager = TimeSeriesPartitionManager {
            partitions: RwLock::new(HashMap::new()),
            partition_strategy: PartitionStrategy::TimeRange {
                interval: Duration::from_secs(24 * 60 * 60) // 1å¤©
            },
        };

        let compression_manager = TimeSeriesCompressionManager {
            strategies: [
                (
                    "default".to_string(),
                    CompressionStrategy {
                        compression_type: CompressionType::Gorilla,
                        threshold_bytes: 1024 * 1024, // 1MB
                        settings: HashMap::new(),
                    }
                )
            ].iter().cloned().collect(),
        };

        let aggregation_engine = TimeSeriesAggregationEngine {
            rules: HashMap::new(),
            aggregation_queue: Vec::new(),
            thread_pool: ThreadPool::new(4),
            downsampled_metrics: HashMap::new(),
        };

        let retention_manager = TimeSeriesRetentionManager {
            policies: HashMap::new(),
            retention_thread: None,
            running: AtomicBool::new(false),
        };

        let server = TimeSeriesServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
        };

        DistributedTimeSeriesDB {
            node_id: node_id.to_string(),
            storage_engine,
            time_series_schema,
            partition_manager,
            compression_manager,
            aggregation_engine,
            retention_manager,
            server,
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.storage_engine.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        // åŠ è½½æŒ‡æ ‡å®šä¹‰
        self.load_metric_definitions()?;

        // åŠ è½½åˆ†åŒºä¿¡æ¯
        self.load_partitions()?;

        // åŠ è½½æ•°æ®
        self.load_data()?;

        // å¯åŠ¨ä¿ç•™ç­–ç•¥ç®¡ç†å™¨
        self.start_retention_manager()?;

        // å¯åŠ¨æœåŠ¡å™¨
        self.start_server()?;

        Ok(())
    }
}

// å·¥ä½œçº¿ç¨‹æ± 
struct ThreadPool {
    workers: Vec<Worker>,
    sender: Option<mpsc::Sender<Job>>,
}

type Job = Box<dyn FnOnce() + Send + 'static>;

impl ThreadPool {
    fn new(size: usize) -> ThreadPool {
        assert!(size > 0);

        let (sender, receiver) = mpsc::channel();
        let receiver = Arc::new(Mutex::new(receiver));

        let mut workers = Vec::with_capacity(size);

        for id in 0..size {
            workers.push(Worker::new(id, Arc::clone(&receiver)));
        }

        ThreadPool {
            workers,
            sender: Some(sender),
        }
    }

    fn execute<F>(&self, f: F)
    where
        F: FnOnce() + Send + 'static,
    {
        let job = Box::new(f);

        if let Some(sender) = &self.sender {
            sender.send(job).unwrap();
        }
    }
}

impl Drop for ThreadPool {
    fn drop(&mut self) {
        drop(self.sender.take());

        for worker in &mut self.workers {
            if let Some(thread) = worker.thread.take() {
                thread.join().unwrap();
            }
        }
    }
}

struct Worker {
    id: usize,
    thread: Option<JoinHandle<()>>,
}

impl Worker {
    fn new(id: usize, receiver: Arc<Mutex<mpsc::Receiver<Job>>>) -> Worker {
        let thread = thread::spawn(move || loop {
            let message = receiver.lock().unwrap().recv();

            match message {
                Ok(job) => {
                    job();
                }
                Err(_) => {
                    break;
                }
            }
        });

        Worker {
            id,
            thread: Some(thread),
        }
    }
}


```rust
impl DistributedTimeSeriesDB {
    // å·²å®ç°çš„ new å’Œ start æ–¹æ³•...

    fn load_metric_definitions(&self) -> Result<(), String> {
        println!("åŠ è½½æŒ‡æ ‡å®šä¹‰");

        let path = self.storage_engine.data_dir.join("metrics.json");

        if !path.exists() {
            return Ok(());
        }

        let content = match std::fs::read_to_string(&path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–æŒ‡æ ‡å®šä¹‰æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£ææŒ‡æ ‡å®šä¹‰");

        // æ¨¡æ‹Ÿä¸€äº›é¢„å®šä¹‰çš„æŒ‡æ ‡
        let mut metrics = self.time_series_schema.metrics.write().unwrap();

        let cpu_metric = MetricDefinition {
            name: "cpu_usage".to_string(),
            data_type: MetricType::Gauge,
            description: "CPU usage percentage".to_string(),
            default_retention: Duration::from_secs(30 * 24 * 60 * 60), // 30å¤©
            aggregation_rules: vec![
                AggregationRule {
                    interval: Duration::from_secs(60), // 1åˆ†é’Ÿ
                    function: AggregationFunction::Avg,
                    retention: Duration::from_secs(7 * 24 * 60 * 60), // 7å¤©
                },
                AggregationRule {
                    interval: Duration::from_secs(300), // 5åˆ†é’Ÿ
                    function: AggregationFunction::Avg,
                    retention: Duration::from_secs(30 * 24 * 60 * 60), // 30å¤©
                },
            ],
            labels: ["host", "service"].iter().map(|s| s.to_string()).collect(),
        };

        let memory_metric = MetricDefinition {
            name: "memory_usage".to_string(),
            data_type: MetricType::Gauge,
            description: "Memory usage in bytes".to_string(),
            default_retention: Duration::from_secs(30 * 24 * 60 * 60), // 30å¤©
            aggregation_rules: vec![
                AggregationRule {
                    interval: Duration::from_secs(60), // 1åˆ†é’Ÿ
                    function: AggregationFunction::Avg,
                    retention: Duration::from_secs(7 * 24 * 60 * 60), // 7å¤©
                },
                AggregationRule {
                    interval: Duration::from_secs(300), // 5åˆ†é’Ÿ
                    function: AggregationFunction::Avg,
                    retention: Duration::from_secs(30 * 24 * 60 * 60), // 30å¤©
                },
            ],
            labels: ["host", "service"].iter().map(|s| s.to_string()).collect(),
        };

        metrics.insert("cpu_usage".to_string(), cpu_metric);
        metrics.insert("memory_usage".to_string(), memory_metric);

        // æ·»åŠ æ ‡ç­¾
        let mut labels = self.time_series_schema.labels.write().unwrap();
        labels.insert("host".to_string());
        labels.insert("service".to_string());
        labels.insert("env".to_string());
        labels.insert("region".to_string());

        Ok(())
    }

    fn load_partitions(&self) -> Result<(), String> {
        println!("åŠ è½½åˆ†åŒºä¿¡æ¯");

        let path = self.storage_engine.data_dir.join("partitions.json");

        if !path.exists() {
            // åˆ›å»ºé»˜è®¤åˆ†åŒº
            self.create_default_partitions()?;
            return Ok(());
        }

        let content = match std::fs::read_to_string(&path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–åˆ†åŒºæ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£æåˆ†åŒºä¿¡æ¯");

        Ok(())
    }

    fn create_default_partitions(&self) -> Result<(), String> {
        println!("åˆ›å»ºé»˜è®¤åˆ†åŒº");

        let now = Utc::now().timestamp();
        let day_seconds = 24 * 60 * 60;

        let mut partitions = self.partition_manager.partitions.write().unwrap();

        // åˆ›å»ºè¿‡å»7å¤©çš„åˆ†åŒº
        for i in 0..7 {
            let start_time = now - (i + 1) * day_seconds;
            let end_time = now - i * day_seconds;

            let partition = TimeSeriesPartition {
                id: format!("p_{}", start_time),
                time_range: (start_time, end_time),
                metrics: HashSet::new(),
                node_id: self.node_id.clone(),
                status: if i == 0 { PartitionStatus::Active } else { PartitionStatus::ReadOnly },
            };

            partitions.insert(partition.id.clone(), partition);
        }

        // åˆ›å»ºæœªæ¥çš„åˆ†åŒº
        let partition = TimeSeriesPartition {
            id: format!("p_{}", now),
            time_range: (now, now + day_seconds),
            metrics: HashSet::new(),
            node_id: self.node_id.clone(),
            status: PartitionStatus::Active,
        };

        partitions.insert(partition.id.clone(), partition);

        // ä¿å­˜åˆ†åŒºä¿¡æ¯
        self.save_partitions()?;

        Ok(())
    }

    fn save_partitions(&self) -> Result<(), String> {
        println!("ä¿å­˜åˆ†åŒºä¿¡æ¯");

        let partitions = self.partition_manager.partitions.read().unwrap();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–ä¸ºJSON
        let json = serde_json::to_string(&*partitions).map_err(|e| format!("åºåˆ—åŒ–åˆ†åŒºä¿¡æ¯å¤±è´¥: {}", e))?;

        let path = self.storage_engine.data_dir.join("partitions.json");

        if let Err(e) = std::fs::write(&path, json) {
            return Err(format!("å†™å…¥åˆ†åŒºä¿¡æ¯å¤±è´¥: {}", e));
        }

        Ok(())
    }

    fn load_data(&self) -> Result<(), String> {
        println!("åŠ è½½æ—¶åºæ•°æ®");

        let data_dir = &self.storage_engine.data_dir;
        let metrics_dir = data_dir.join("metrics");

        if !metrics_dir.exists() {
            return Ok(());
        }

        match std::fs::read_dir(&metrics_dir) {
            Ok(entries) => {
                for entry in entries {
                    if let Ok(entry) = entry {
                        let path = entry.path();

                        if path.is_dir() {
                            let metric_name = path.file_name()
                                .and_then(|os| os.to_str())
                                .ok_or("æ— æ•ˆçš„æŒ‡æ ‡ç›®å½•å".to_string())?;

                            println!("åŠ è½½æŒ‡æ ‡æ•°æ®: {}", metric_name);

                            match self.load_metric_data(metric_name, &path) {
                                Ok(_) => {},
                                Err(e) => println!("åŠ è½½æŒ‡æ ‡ {} æ•°æ®å¤±è´¥: {}", metric_name, e),
                            }
                        }
                    }
                }
            },
            Err(e) => return Err(format!("è¯»å–æŒ‡æ ‡ç›®å½•å¤±è´¥: {}", e)),
        }

        Ok(())
    }

    fn load_metric_data(&self, metric_name: &str, path: &Path) -> Result<(), String> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½æŒ‡æ ‡æ•°æ®æ–‡ä»¶
        println!("åŠ è½½æŒ‡æ ‡ {} çš„æ•°æ®æ–‡ä»¶", metric_name);

        Ok(())
    }

    fn start_retention_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ä¿ç•™ç­–ç•¥ç®¡ç†å™¨");

        // åˆ›å»ºä¿ç•™ç­–ç•¥
        let metrics = self.time_series_schema.metrics.read().unwrap();

        for (name, def) in metrics.iter() {
            let mut downsampling = Vec::new();

            for rule in &def.aggregation_rules {
                downsampling.push(DownsamplingConfig {
                    period: rule.interval,
                    function: match rule.function {
                        AggregationFunction::Sum => AggregationFunction::Sum,
                        AggregationFunction::Avg => AggregationFunction::Avg,
                        AggregationFunction::Min => AggregationFunction::Min,
                        AggregationFunction::Max => AggregationFunction::Max,
                        _ => AggregationFunction::Avg,
                    },
                    retention: rule.retention,
                });
            }

            let policy = RetentionPolicy {
                metric_name: name.clone(),
                retention_period: def.default_retention,
                downsampling,
            };

            self.retention_manager.policies.insert(name.clone(), policy);
        }

        // å¯åŠ¨ä¿ç•™ç­–ç•¥çº¿ç¨‹
        let policies = self.retention_manager.policies.clone();
        let data_dir = self.storage_engine.data_dir.clone();
        let node_id = self.node_id.clone();

        self.retention_manager.running.store(true, Ordering::SeqCst);

        let running = self.retention_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ‰§è¡Œä¿ç•™ç­–ç•¥
                for (name, policy) in &policies {
                    println!("æ‰§è¡ŒæŒ‡æ ‡ {} çš„ä¿ç•™ç­–ç•¥", name);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥å’Œåˆ é™¤è¿‡æœŸæ•°æ®

                    // æ‰§è¡Œä¸‹é‡‡æ ·
                    for config in &policy.downsampling {
                        println!("æ‰§è¡Œä¸‹é‡‡æ ·: {}@{:?}", name, config.period);

                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡Œä¸‹é‡‡æ ·å¤„ç†
                    }
                }

                // æ¯å¤©æ‰§è¡Œä¸€æ¬¡
                thread::sleep(Duration::from_secs(24 * 60 * 60));
            }
        });

        self.retention_manager.retention_thread = Some(thread);

        Ok(())
    }

    fn start_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æ—¶åºæ•°æ®åº“æœåŠ¡å™¨");

        let bind_address = self.server.bind_address.clone();

        self.server.running.store(true, Ordering::SeqCst);

        let running = self.server.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTPæœåŠ¡å™¨
            println!("æ—¶åºæ•°æ®åº“æœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
            }
        });

        self.server.server = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“");

        // åœæ­¢æœåŠ¡å™¨
        self.server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢ä¿ç•™ç­–ç•¥ç®¡ç†å™¨
        self.retention_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.retention_manager.retention_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("ä¿ç•™ç­–ç•¥çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // ä¿å­˜åˆ†åŒºä¿¡æ¯
        self.save_partitions()?;

        Ok(())
    }

    fn write_data_point(&self, metric_name: &str, labels: HashMap<String, String>, point: DataPoint) -> Result<(), String> {
        // æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å­˜åœ¨
        let metrics_schema = self.time_series_schema.metrics.read().unwrap();

        if !metrics_schema.contains_key(metric_name) {
            return Err(format!("æŒ‡æ ‡ä¸å­˜åœ¨: {}", metric_name));
        }

        // æŸ¥æ‰¾æˆ–åˆ›å»ºæŒ‡æ ‡ç³»åˆ—
        let mut metrics_data = self.storage_engine.metrics.write().unwrap();

        let series_key = self.build_series_key(metric_name, &labels);

        if !metrics_data.contains_key(&series_key) {
            // åˆ›å»ºæ–°çš„æŒ‡æ ‡ç³»åˆ—
            let series = MetricSeries {
                metric_name: metric_name.to_string(),
                labels: labels.clone(),
                data_points: RwLock::new(Vec::new()),
                chunks: Vec::new(),
                current_chunk: RwLock::new(None),
            };

            metrics_data.insert(series_key.clone(), series);
        }

        // æ‰¾åˆ°é€‚åˆçš„åˆ†åŒº
        let partition_id = self.find_partition_for_timestamp(point.timestamp)?;

        // æ·»åŠ æ•°æ®ç‚¹
        if let Some(series) = metrics_data.get(&series_key) {
            let mut data_points = series.data_points.write().unwrap();
            data_points.push(point);

            // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„å—
            let mut current_chunk = series.current_chunk.write().unwrap();

            if current_chunk.is_none() || current_chunk.as_ref().unwrap().points.len() >= 1000 {
                // åˆ›å»ºæ–°å—
                let chunk_id = uuid::Uuid::new_v4().to_string();

                let chunk = TimeSeriesChunk {
                    id: chunk_id,
                    metric_name: metric_name.to_string(),
                    labels: labels.clone(),
                    min_time: point.timestamp,
                    max_time: point.timestamp,
                    points: vec![point],
                    compression_type: CompressionType::None,
                    compressed_data: None,
                    size_bytes: 0,
                };

                *current_chunk = Some(chunk);
            } else {
                // æ·»åŠ åˆ°ç°æœ‰å—
                let chunk = current_chunk.as_mut().unwrap();
                chunk.points.push(point);
                chunk.min_time = chunk.min_time.min(point.timestamp);
                chunk.max_time = chunk.max_time.max(point.timestamp);
            }

            // æ›´æ–°åˆ†åŒºçš„æŒ‡æ ‡é›†åˆ
            let mut partitions = self.partition_manager.partitions.write().unwrap();

            if let Some(partition) = partitions.get_mut(&partition_id) {
                partition.metrics.insert(metric_name.to_string());
            }

            // æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å‹ç¼©
            if data_points.len() >= 10000 {
                self.compress_metric_data(&series_key)?;
            }
        }

        Ok(())
    }

    fn build_series_key(&self, metric_name: &str, labels: &HashMap<String, String>) -> String {
        // æ„å»ºç³»åˆ—é”®ï¼šæŒ‡æ ‡å + æ’åºçš„æ ‡ç­¾
        let mut label_pairs: Vec<_> = labels.iter().collect();
        label_pairs.sort_by(|a, b| a.0.cmp(b.0));

        let labels_str: Vec<String> = label_pairs.iter()
            .map(|(k, v)| format!("{}={}", k, v))
            .collect();

        if labels_str.is_empty() {
            return metric_name.to_string();
        } else {
            return format!("{}:{}", metric_name, labels_str.join(","));
        }
    }

    fn find_partition_for_timestamp(&self, timestamp: i64) -> Result<String, String> {
        let partitions = self.partition_manager.partitions.read().unwrap();

        for (id, partition) in partitions.iter() {
            if timestamp >= partition.time_range.0 && timestamp < partition.time_range.1 {
                return Ok(id.clone());
            }
        }

        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åˆ†åŒºï¼Œåˆ›å»ºä¸€ä¸ªæ–°åˆ†åŒº
        drop(partitions);

        self.create_partition_for_timestamp(timestamp)
    }

    fn create_partition_for_timestamp(&self, timestamp: i64) -> Result<String, String> {
        println!("ä¸ºæ—¶é—´æˆ³ {} åˆ›å»ºæ–°åˆ†åŒº", timestamp);

        let day_seconds = 24 * 60 * 60;
        let start_time = (timestamp / day_seconds) * day_seconds;
        let end_time = start_time + day_seconds;

        let partition_id = format!("p_{}", start_time);

        let mut partitions = self.partition_manager.partitions.write().unwrap();

        let partition = TimeSeriesPartition {
            id: partition_id.clone(),
            time_range: (start_time, end_time),
            metrics: HashSet::new(),
            node_id: self.node_id.clone(),
            status: PartitionStatus::Active,
        };

        partitions.insert(partition_id.clone(), partition);

        // ä¿å­˜åˆ†åŒºä¿¡æ¯
        drop(partitions);
        self.save_partitions()?;

        Ok(partition_id)
    }

    fn compress_metric_data(&self, series_key: &str) -> Result<(), String> {
        println!("å‹ç¼©æŒ‡æ ‡æ•°æ®: {}", series_key);

        let metrics_data = self.storage_engine.metrics.read().unwrap();

        if let Some(series) = metrics_data.get(series_key) {
            let mut current_chunk = series.current_chunk.write().unwrap();

            if let Some(chunk) = current_chunk.take() {
                let compression_strategy = self.compression_manager.strategies.get("default")
                    .ok_or("é»˜è®¤å‹ç¼©ç­–ç•¥ä¸å­˜åœ¨".to_string())?;

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®å‹ç¼©ç­–ç•¥æ‰§è¡Œä¸åŒçš„å‹ç¼©ç®—æ³•
                let compressed_chunk = self.compress_chunk(chunk, &compression_strategy)?;

                // æ·»åŠ åˆ°å—åˆ—è¡¨
                let mut series_chunks = series.chunks;
                series_chunks.push(compressed_chunk);

                // æ¸…ç©ºæ•°æ®ç‚¹ç¼“å†²åŒº
                let mut data_points = series.data_points.write().unwrap();
                data_points.clear();
            }
        }

        Ok(())
    }

    fn compress_chunk(&self, chunk: TimeSeriesChunk, strategy: &CompressionStrategy) -> Result<TimeSeriesChunk, String> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡Œå®é™…çš„å‹ç¼©ç®—æ³•
        println!("å‹ç¼©å—: {} ({} ä¸ªæ•°æ®ç‚¹)", chunk.id, chunk.points.len());

        let mut compressed_chunk = chunk;
        compressed_chunk.compression_type = strategy.compression_type.clone();

        // ç®€åŒ–ï¼šä»…è®¾ç½®ä¸€ä¸ªæ¨¡æ‹Ÿçš„å‹ç¼©æ•°æ®
        compressed_chunk.compressed_data = Some(Vec::new());
        compressed_chunk.size_bytes = (chunk.points.len() * 16) as u64; // å‡è®¾æ¯ä¸ªç‚¹16å­—èŠ‚

        Ok(compressed_chunk)
    }

    fn query_range(&self, query: TimeSeriesQuery) -> Result<TimeSeriesQueryResult, String> {
        println!("æŸ¥è¯¢æ—¶é—´èŒƒå›´: {:?} -> {:?}", query.start_time, query.end_time);

        // æŸ¥æ‰¾ç›¸å…³åˆ†åŒº
        let matching_partitions = self.find_partitions_for_range(query.start_time, query.end_time)?;

        // æŸ¥æ‰¾åŒ¹é…çš„æŒ‡æ ‡ç³»åˆ—
        let matching_series = self.find_matching_series(&query.selector)?;

        if matching_series.is_empty() {
            return Ok(TimeSeriesQueryResult {
                series: Vec::new(),
            });
        }

        // ä»æ¯ä¸ªç³»åˆ—ä¸­æŸ¥è¯¢æ•°æ®ç‚¹
        let mut result_series = Vec::new();

        for series_key in matching_series {
            let metrics_data = self.storage_engine.metrics.read().unwrap();

            if let Some(series) = metrics_data.get(&series_key) {
                // æ”¶é›†æ•°æ®ç‚¹
                let mut all_points = Vec::new();

                // ä»å†…å­˜ç¼“å†²åŒºæ”¶é›†
                let data_points = series.data_points.read().unwrap();

                for point in data_points.iter() {
                    if point.timestamp >= query.start_time && point.timestamp <= query.end_time {
                        all_points.push(point.clone());
                    }
                }

                // ä»å—ä¸­æ”¶é›†
                for chunk in &series.chunks {
                    if chunk.min_time <= query.end_time && chunk.max_time >= query.start_time {
                        // å—ä¸æŸ¥è¯¢æ—¶é—´èŒƒå›´æœ‰é‡å 

                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£å‹ç¼©å—æ•°æ®
                        println!("è§£å‹ç¼©å—: {}", chunk.id);

                        for point in &chunk.points {
                            if point.timestamp >= query.start_time && point.timestamp <= query.end_time {
                                all_points.push(point.clone());
                            }
                        }
                    }
                }

                // æŒ‰æ—¶é—´æˆ³æ’åº
                all_points.sort_by_key(|p| p.timestamp);

                // åº”ç”¨èšåˆå‡½æ•°
                if let Some(func) = &query.aggregation {
                    let step = query.step.unwrap_or(60); // é»˜è®¤æ­¥é•¿1åˆ†é’Ÿ
                    all_points = self.aggregate_points(all_points, *func, step, query.start_time, query.end_time);
                }

                // æ·»åŠ åˆ°ç»“æœ
                if !all_points.is_empty() {
                    let result_series_data = TimeSeriesResult {
                        metric_name: series.metric_name.clone(),
                        labels: series.labels.clone(),
                        data_points: all_points,
                    };

                    result_series.push(result_series_data);
                }
            }
        }

        Ok(TimeSeriesQueryResult {
            series: result_series,
        })
    }
}

struct TimeSeriesQuery {
    selector: MetricSelector,
    start_time: i64,
    end_time: i64,
    step: Option<i64>,
    aggregation: Option<AggregationFunction>,
}

struct MetricSelector {
    metric_name: String,
    label_matchers: Vec<LabelMatcher>,
}

struct LabelMatcher {
    name: String,
    operator: MatchOperator,
    value: String,
}

enum MatchOperator {
    Equal,
    NotEqual,
    RegexMatch,
    RegexNotMatch,
}

struct TimeSeriesQueryResult {
    series: Vec<TimeSeriesResult>,
}

struct TimeSeriesResult {
    metric_name: String,
    labels: HashMap<String, String>,
    data_points: Vec<DataPoint>,
}

// åˆ†å¸ƒå¼ç³»ç»Ÿç›‘æ§å¹³å°
struct DistributedMonitoringSystem {
    node_id: String,
    data_dir: PathBuf,
    collectors: HashMap<String, Box<dyn MetricCollector>>,
    storage_backend: Box<dyn MetricStorage>,
    alert_manager: AlertManager,
    dashboard_service: DashboardService,
    api_server: ApiServer,
}

trait MetricCollector: Send + Sync {
    fn name(&self) -> &str;
    fn description(&self) -> &str;
    fn collect(&self) -> Vec<Metric>;
    fn start(&mut self) -> Result<(), String>;
    fn stop(&mut self) -> Result<(), String>;
}

trait MetricStorage: Send + Sync {
    fn store_metrics(&self, metrics: Vec<Metric>) -> Result<(), String>;
    fn query_metrics(&self, query: MetricQuery) -> Result<Vec<MetricSeries>, String>;
    fn start(&mut self) -> Result<(), String>;
    fn stop(&mut self) -> Result<(), String>;
}

struct Metric {
    name: String,
    value: f64,
    timestamp: i64,
    labels: HashMap<String, String>,
    metric_type: MetricType,
}

struct MetricQuery {
    selector: String,
    start_time: i64,
    end_time: i64,
    step: Option<i64>,
}

struct AlertManager {
    alerts: RwLock<HashMap<String, Alert>>,
    rules: RwLock<HashMap<String, AlertRule>>,
    notifiers: HashMap<String, Box<dyn AlertNotifier>>,
    check_interval: Duration,
    alert_thread: Option<JoinHandle<()>>,
    running: AtomicBool,
}

struct Alert {
    id: String,
    name: String,
    severity: AlertSeverity,
    status: AlertStatus,
    condition: String,
    value: f64,
    labels: HashMap<String, String>,
    annotations: HashMap<String, String>,
    started_at: DateTime<Utc>,
    last_updated: DateTime<Utc>,
    ends_at: Option<DateTime<Utc>>,
}

enum AlertStatus {
    Firing,
    Resolved,
    Acknowledged,
}

enum AlertSeverity {
    Critical,
    High,
    Medium,
    Low,
    Info,
}

struct AlertRule {
    id: String,
    name: String,
    query: String,
    threshold: f64,
    comparison: ComparisonOperator,
    duration: Duration,
    severity: AlertSeverity,
    labels: HashMap<String, String>,
    annotations: HashMap<String, String>,
    silenced_until: Option<DateTime<Utc>>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
}

enum ComparisonOperator {
    GreaterThan,
    GreaterThanOrEqual,
    LessThan,
    LessThanOrEqual,
    Equal,
    NotEqual,
}

trait AlertNotifier: Send + Sync {
    fn name(&self) -> &str;
    fn notify(&self, alert: &Alert) -> Result<(), String>;
    fn test(&self) -> Result<(), String>;
}

struct DashboardService {
    dashboards: RwLock<HashMap<String, Dashboard>>,
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

struct Dashboard {
    id: String,
    title: String,
    description: String,
    panels: Vec<Panel>,
    tags: Vec<String>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
    creator: String,
    variables: HashMap<String, DashboardVariable>,
}

struct Panel {
    id: String,
    title: String,
    panel_type: PanelType,
    queries: Vec<String>,
    position: PanelPosition,
    options: HashMap<String, String>,
}

enum PanelType {
    Graph,
    Stat,
    Table,
    Heatmap,
    Gauge,
    BarChart,
    PieChart,
    Text,
}

struct PanelPosition {
    x: u32,
    y: u32,
    width: u32,
    height: u32,
}

struct DashboardVariable {
    name: String,
    label: String,
    variable_type: VariableType,
    query: Option<String>,
    options: Vec<String>,
    current: String,
}

enum VariableType {
    Query,
    Interval,
    Constant,
    Custom,
}

struct ApiServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

impl DistributedMonitoringSystem {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str) -> Self {
        let alert_manager = AlertManager {
            alerts: RwLock::new(HashMap::new()),
            rules: RwLock::new(HashMap::new()),
            notifiers: HashMap::new(),
            check_interval: Duration::from_secs(30),
            alert_thread: None,
            running: AtomicBool::new(false),
        };

        let dashboard_service = DashboardService {
            dashboards: RwLock::new(HashMap::new()),
            server: None,
            running: AtomicBool::new(false),
            bind_address: format!("{}:3000", bind_address),
        };

        let api_server = ApiServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: format!("{}:8080", bind_address),
        };

        DistributedMonitoringSystem {
            node_id: node_id.to_string(),
            data_dir: data_dir.to_path_buf(),
            collectors: HashMap::new(),
            storage_backend: Box::new(InMemoryMetricStorage::new()),
            alert_manager,
            dashboard_service,
            api_server,
        }
    }
}

struct InMemoryMetricStorage {
    metrics: RwLock<HashMap<String, Vec<Metric>>>,
}

impl InMemoryMetricStorage {
    fn new() -> Self {
        InMemoryMetricStorage {
            metrics: RwLock::new(HashMap::new()),
        }
    }
}

impl MetricStorage for InMemoryMetricStorage {
    fn store_metrics(&self, metrics: Vec<Metric>) -> Result<(), String> {
        let mut storage = self.metrics.write().unwrap();

        for metric in metrics {
            let key = format!("{}:{}", metric.name, self.labels_to_string(&metric.labels));

            let entry = storage.entry(key).or_insert_with(Vec::new);
            entry.push(metric);
        }

        Ok(())
    }

    fn query_metrics(&self, query: MetricQuery) -> Result<Vec<MetricSeries>, String> {
        // å®é™…å®ç°ä¼šè§£ææŸ¥è¯¢é€‰æ‹©å™¨å¹¶åŒ¹é…æŒ‡æ ‡
        let storage = self.metrics.read().unwrap();
        let mut result = Vec::new();

        for (key, metrics) in storage.iter() {
            // ç®€å•åŒ¹é…ï¼šæ£€æŸ¥é”®æ˜¯å¦åŒ…å«é€‰æ‹©å™¨
            if key.contains(&query.selector) {
                let filtered_metrics: Vec<Metric> = metrics.iter()
                    .filter(|m| m.timestamp >= query.start_time && m.timestamp <= query.end_time)
                    .cloned()
                    .collect();

                if !filtered_metrics.is_empty() {
                    // æå–ä¸€ä¸ªç¤ºä¾‹æŒ‡æ ‡ä»¥è·å–æ ‡ç­¾
                    let sample = &filtered_metrics[0];

                    let series = MetricSeries {
                        metric_name: sample.name.clone(),
                        labels: sample.labels.clone(),
                        data_points: RwLock::new(filtered_metrics.iter().map(|m| DataPoint {
                            timestamp: m.timestamp,
                            value: m.value,
                        }).collect()),
                        chunks: Vec::new(),
                        current_chunk: RwLock::new(None),
                    };

                    result.push(series);
                }
            }
        }

        Ok(result)
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å†…å­˜æŒ‡æ ‡å­˜å‚¨");
        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢å†…å­˜æŒ‡æ ‡å­˜å‚¨");
        Ok(())
    }
}

impl InMemoryMetricStorage {
    fn labels_to_string(&self, labels: &HashMap<String, String>) -> String {
        let mut pairs: Vec<_> = labels.iter().collect();
        pairs.sort_by(|a, b| a.0.cmp(b.0));

        pairs.iter()
            .map(|(k, v)| format!("{}={}", k, v))
            .collect::<Vec<_>>()
            .join(",")
    }
}
```

### 1.15 ç»¼åˆåº”ç”¨15-åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ

```rust
impl DistributedMonitoringSystem {
    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        // å¯åŠ¨å­˜å‚¨åç«¯
        self.storage_backend.start()?;

        // æ³¨å†Œé»˜è®¤æ”¶é›†å™¨
        self.register_default_collectors()?;

        // å¯åŠ¨æ‰€æœ‰æ”¶é›†å™¨
        for (name, collector) in &mut self.collectors {
            if let Err(e) = collector.start() {
                println!("å¯åŠ¨æ”¶é›†å™¨ {} å¤±è´¥: {}", name, e);
            }
        }

        // åŠ è½½å‘Šè­¦è§„åˆ™
        self.load_alert_rules()?;

        // å¯åŠ¨å‘Šè­¦ç®¡ç†å™¨
        self.start_alert_manager()?;

        // åŠ è½½ä»ªè¡¨ç›˜
        self.load_dashboards()?;

        // å¯åŠ¨ä»ªè¡¨ç›˜æœåŠ¡
        self.start_dashboard_service()?;

        // å¯åŠ¨APIæœåŠ¡å™¨
        self.start_api_server()?;

        Ok(())
    }

    fn register_default_collectors(&mut self) -> Result<(), String> {
        println!("æ³¨å†Œé»˜è®¤æŒ‡æ ‡æ”¶é›†å™¨");

        // ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å™¨
        let system_collector = SystemMetricCollector::new();
        self.collectors.insert(system_collector.name().to_string(), Box::new(system_collector));

        // JVMæŒ‡æ ‡æ”¶é›†å™¨
        let jvm_collector = JvmMetricCollector::new();
        self.collectors.insert(jvm_collector.name().to_string(), Box::new(jvm_collector));

        // ç½‘ç»œæŒ‡æ ‡æ”¶é›†å™¨
        let network_collector = NetworkMetricCollector::new();
        self.collectors.insert(network_collector.name().to_string(), Box::new(network_collector));

        // è‡ªå®šä¹‰æ”¶é›†å™¨
        // ä»é…ç½®æ–‡ä»¶åŠ è½½

        Ok(())
    }

    fn load_alert_rules(&self) -> Result<(), String> {
        println!("åŠ è½½å‘Šè­¦è§„åˆ™");

        let rules_path = self.data_dir.join("alerts").join("rules.json");

        if !rules_path.exists() {
            return Ok(());
        }

        let content = match std::fs::read_to_string(&rules_path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–å‘Šè­¦è§„åˆ™æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£æå‘Šè­¦è§„åˆ™");

        // åˆ›å»ºä¸€äº›ç¤ºä¾‹è§„åˆ™
        let mut rules = self.alert_manager.rules.write().unwrap();

        let high_cpu_rule = AlertRule {
            id: "cpu-high".to_string(),
            name: "CPUä½¿ç”¨ç‡è¿‡é«˜".to_string(),
            query: "avg(cpu_usage) by (host) > 80".to_string(),
            threshold: 80.0,
            comparison: ComparisonOperator::GreaterThan,
            duration: Duration::from_secs(300), // 5åˆ†é’Ÿ
            severity: AlertSeverity::High,
            labels: [("service".to_string(), "system".to_string())].iter().cloned().collect(),
            annotations: [
                ("summary".to_string(), "ä¸»æœº {{host}} CPUä½¿ç”¨ç‡è¿‡é«˜".to_string()),
                ("description".to_string(), "ä¸»æœº {{host}} çš„CPUä½¿ç”¨ç‡å·²ç»è¶…è¿‡80%æŒç»­5åˆ†é’Ÿ".to_string()),
            ].iter().cloned().collect(),
            silenced_until: None,
            created_at: Utc::now(),
            updated_at: Utc::now(),
        };

        let memory_critical_rule = AlertRule {
            id: "memory-critical".to_string(),
            name: "å†…å­˜ä½¿ç”¨ç‡ä¸¥é‡".to_string(),
            query: "avg(memory_usage_percent) by (host) > 95".to_string(),
            threshold: 95.0,
            comparison: ComparisonOperator::GreaterThan,
            duration: Duration::from_secs(180), // 3åˆ†é’Ÿ
            severity: AlertSeverity::Critical,
            labels: [("service".to_string(), "system".to_string())].iter().cloned().collect(),
            annotations: [
                ("summary".to_string(), "ä¸»æœº {{host}} å†…å­˜ä½¿ç”¨ç‡ä¸¥é‡".to_string()),
                ("description".to_string(), "ä¸»æœº {{host}} çš„å†…å­˜ä½¿ç”¨ç‡å·²ç»è¶…è¿‡95%æŒç»­3åˆ†é’Ÿ".to_string()),
            ].iter().cloned().collect(),
            silenced_until: None,
            created_at: Utc::now(),
            updated_at: Utc::now(),
        };

        rules.insert(high_cpu_rule.id.clone(), high_cpu_rule);
        rules.insert(memory_critical_rule.id.clone(), memory_critical_rule);

        Ok(())
    }

    fn start_alert_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨å‘Šè­¦ç®¡ç†å™¨");

        // æ³¨å†Œé€šçŸ¥å™¨
        let email_notifier = EmailNotifier::new(
            "smtp.example.com",
            587,
            "alerts@example.com",
            "password123",
            vec!["admin@example.com".to_string()],
        );

        let slack_notifier = SlackNotifier::new(
            "https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX",
            "#alerts",
        );

        self.alert_manager.notifiers.insert("email".to_string(), Box::new(email_notifier));
        self.alert_manager.notifiers.insert("slack".to_string(), Box::new(slack_notifier));

        // å¯åŠ¨å‘Šè­¦æ£€æŸ¥çº¿ç¨‹
        let rules = self.alert_manager.rules.clone();
        let alerts = self.alert_manager.alerts.clone();
        let notifiers = self.alert_manager.notifiers.iter()
            .map(|(k, v)| (k.clone(), v.clone()))
            .collect::<HashMap<String, Box<dyn AlertNotifier>>>();

        let check_interval = self.alert_manager.check_interval;
        let storage = self.storage_backend;

        self.alert_manager.running.store(true, Ordering::SeqCst);

        let running = self.alert_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥æ‰€æœ‰å‘Šè­¦è§„åˆ™
                let rules_copy = rules.read().unwrap().clone();

                for (id, rule) in rules_copy.iter() {
                    // è§£ææŸ¥è¯¢å¹¶æ‰§è¡Œ
                    println!("æ£€æŸ¥å‘Šè­¦è§„åˆ™: {}", rule.name);

                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æè§„åˆ™æŸ¥è¯¢å¹¶æ‰§è¡Œ
                    // ç®€åŒ–ï¼šå‡è®¾æŸ¥è¯¢è¿”å›äº†ä¸€ä¸ªå€¼
                    let query_result = 85.0; // ä¾‹å¦‚ï¼Œå‡è®¾CPUä½¿ç”¨ç‡ä¸º85%

                    // æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                    let alert_triggered = match rule.comparison {
                        ComparisonOperator::GreaterThan => query_result > rule.threshold,
                        ComparisonOperator::GreaterThanOrEqual => query_result >= rule.threshold,
                        ComparisonOperator::LessThan => query_result < rule.threshold,
                        ComparisonOperator::LessThanOrEqual => query_result <= rule.threshold,
                        ComparisonOperator::Equal => (query_result - rule.threshold).abs() < 0.0001,
                        ComparisonOperator::NotEqual => (query_result - rule.threshold).abs() >= 0.0001,
                    };

                    if alert_triggered {
                        // æ£€æŸ¥æ˜¯å¦å¤„äºé™é»˜æœŸ
                        if let Some(silenced_until) = rule.silenced_until {
                            if Utc::now() < silenced_until {
                                continue;
                            }
                        }

                        // åˆ›å»ºæˆ–æ›´æ–°å‘Šè­¦
                        let mut alerts_map = alerts.write().unwrap();

                        let now = Utc::now();

                        if let Some(alert) = alerts_map.get_mut(id) {
                            // æ›´æ–°ç°æœ‰å‘Šè­¦
                            alert.value = query_result;
                            alert.last_updated = now;

                            // å¦‚æœå·²è§£å†³ï¼Œå°†å…¶é‡æ–°è®¾ç½®ä¸ºè§¦å‘çŠ¶æ€
                            if alert.status == AlertStatus::Resolved {
                                alert.status = AlertStatus::Firing;
                                alert.started_at = now;
                                alert.ends_at = None;

                                // å‘é€é€šçŸ¥
                                for (_, notifier) in &notifiers {
                                    if let Err(e) = notifier.notify(alert) {
                                        println!("å‘é€å‘Šè­¦é€šçŸ¥å¤±è´¥: {}", e);
                                    }
                                }
                            }
                        } else {
                            // åˆ›å»ºæ–°å‘Šè­¦
                            let alert = Alert {
                                id: id.clone(),
                                name: rule.name.clone(),
                                severity: rule.severity.clone(),
                                status: AlertStatus::Firing,
                                condition: rule.query.clone(),
                                value: query_result,
                                labels: rule.labels.clone(),
                                annotations: rule.annotations.clone(),
                                started_at: now,
                                last_updated: now,
                                ends_at: None,
                            };

                            // å‘é€é€šçŸ¥
                            for (_, notifier) in &notifiers {
                                if let Err(e) = notifier.notify(&alert) {
                                    println!("å‘é€å‘Šè­¦é€šçŸ¥å¤±è´¥: {}", e);
                                }
                            }

                            // ä¿å­˜å‘Šè­¦
                            alerts_map.insert(id.clone(), alert);
                        }
                    } else {
                        // æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è§¦å‘çš„å‘Šè­¦éœ€è¦è§£å†³
                        let mut alerts_map = alerts.write().unwrap();

                        if let Some(alert) = alerts_map.get_mut(id) {
                            if alert.status == AlertStatus::Firing {
                                // å°†å‘Šè­¦è®¾ç½®ä¸ºå·²è§£å†³
                                alert.status = AlertStatus::Resolved;
                                alert.ends_at = Some(Utc::now());
                                alert.last_updated = Utc::now();

                                // å‘é€è§£å†³é€šçŸ¥
                                for (_, notifier) in &notifiers {
                                    if let Err(e) = notifier.notify(alert) {
                                        println!("å‘é€å‘Šè­¦è§£å†³é€šçŸ¥å¤±è´¥: {}", e);
                                    }
                                }
                            }
                        }
                    }
                }

                // ç­‰å¾…ä¸‹ä¸€ä¸ªæ£€æŸ¥é—´éš”
                thread::sleep(check_interval);
            }
        });

        self.alert_manager.alert_thread = Some(thread);

        Ok(())
    }

    fn load_dashboards(&self) -> Result<(), String> {
        println!("åŠ è½½ä»ªè¡¨ç›˜");

        let dashboards_dir = self.data_dir.join("dashboards");

        if !dashboards_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&dashboards_dir) {
                return Err(format!("åˆ›å»ºä»ªè¡¨ç›˜ç›®å½•å¤±è´¥: {}", e));
            }

            // åˆ›å»ºé»˜è®¤ä»ªè¡¨ç›˜
            self.create_default_dashboards()?;
            return Ok(());
        }

        match std::fs::read_dir(&dashboards_dir) {
            Ok(entries) => {
                for entry in entries {
                    if let Ok(entry) = entry {
                        let path = entry.path();

                        if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                            let dashboard_id = path.file_stem()
                                .and_then(|os| os.to_str())
                                .ok_or("æ— æ•ˆçš„ä»ªè¡¨ç›˜æ–‡ä»¶å".to_string())?;

                            println!("åŠ è½½ä»ªè¡¨ç›˜: {}", dashboard_id);

                            match self.load_dashboard(dashboard_id, &path) {
                                Ok(_) => {},
                                Err(e) => println!("åŠ è½½ä»ªè¡¨ç›˜ {} å¤±è´¥: {}", dashboard_id, e),
                            }
                        }
                    }
                }
            },
            Err(e) => return Err(format!("è¯»å–ä»ªè¡¨ç›˜ç›®å½•å¤±è´¥: {}", e)),
        }

        Ok(())
    }

    fn load_dashboard(&self, id: &str, path: &Path) -> Result<(), String> {
        let content = match std::fs::read_to_string(path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–ä»ªè¡¨ç›˜æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£æä»ªè¡¨ç›˜JSON: {}", id);

        // ç®€åŒ–ï¼šåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿä»ªè¡¨ç›˜
        let dashboard = Dashboard {
            id: id.to_string(),
            title: format!("ä»ªè¡¨ç›˜ {}", id),
            description: "è¿™æ˜¯ä¸€ä¸ªåŠ è½½çš„ä»ªè¡¨ç›˜".to_string(),
            panels: Vec::new(),
            tags: Vec::new(),
            created_at: Utc::now(),
            updated_at: Utc::now(),
            creator: "admin".to_string(),
            variables: HashMap::new(),
        };

        let mut dashboards = self.dashboard_service.dashboards.write().unwrap();
        dashboards.insert(id.to_string(), dashboard);

        Ok(())
    }

    fn create_default_dashboards(&self) -> Result<(), String> {
        println!("åˆ›å»ºé»˜è®¤ä»ªè¡¨ç›˜");

        // ç³»ç»Ÿç›‘æ§ä»ªè¡¨ç›˜
        let system_dashboard = Dashboard {
            id: "system-overview".to_string(),
            title: "ç³»ç»Ÿæ¦‚è§ˆ".to_string(),
            description: "æ˜¾ç¤ºç³»ç»Ÿå…³é”®æŒ‡æ ‡çš„ä»ªè¡¨ç›˜".to_string(),
            panels: vec![
                // CPUä½¿ç”¨ç‡å›¾è¡¨
                Panel {
                    id: "cpu-usage".to_string(),
                    title: "CPUä½¿ç”¨ç‡".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec!["avg(cpu_usage) by (host)".to_string()],
                    position: PanelPosition {
                        x: 0,
                        y: 0,
                        width: 12,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
                // å†…å­˜ä½¿ç”¨ç‡å›¾è¡¨
                Panel {
                    id: "memory-usage".to_string(),
                    title: "å†…å­˜ä½¿ç”¨ç‡".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec!["avg(memory_usage_percent) by (host)".to_string()],
                    position: PanelPosition {
                        x: 12,
                        y: 0,
                        width: 12,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
                // ç£ç›˜ä½¿ç”¨ç‡å›¾è¡¨
                Panel {
                    id: "disk-usage".to_string(),
                    title: "ç£ç›˜ä½¿ç”¨ç‡".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec!["avg(disk_usage_percent) by (host, mount)".to_string()],
                    position: PanelPosition {
                        x: 0,
                        y: 8,
                        width: 12,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
                // ç½‘ç»œæµé‡å›¾è¡¨
                Panel {
                    id: "network-traffic".to_string(),
                    title: "ç½‘ç»œæµé‡".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec![
                        "sum(rate(network_received_bytes[5m])) by (host, interface)".to_string(),
                        "sum(rate(network_transmitted_bytes[5m])) by (host, interface)".to_string(),
                    ],
                    position: PanelPosition {
                        x: 12,
                        y: 8,
                        width: 12,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
            ],
            tags: vec!["system".to_string(), "monitoring".to_string()],
            created_at: Utc::now(),
            updated_at: Utc::now(),
            creator: "system".to_string(),
            variables: HashMap::new(),
        };

        // åº”ç”¨æ€§èƒ½ä»ªè¡¨ç›˜
        let app_dashboard = Dashboard {
            id: "application-performance".to_string(),
            title: "åº”ç”¨æ€§èƒ½".to_string(),
            description: "æ˜¾ç¤ºåº”ç”¨æ€§èƒ½æŒ‡æ ‡çš„ä»ªè¡¨ç›˜".to_string(),
            panels: vec![
                // è¯·æ±‚ç‡å›¾è¡¨
                Panel {
                    id: "request-rate".to_string(),
                    title: "è¯·æ±‚ç‡".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec!["sum(rate(http_requests_total[5m])) by (service, method)".to_string()],
                    position: PanelPosition {
                        x: 0,
                        y: 0,
                        width: 12,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
                // é”™è¯¯ç‡å›¾è¡¨
                Panel {
                    id: "error-rate".to_string(),
                    title: "é”™è¯¯ç‡".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec!["sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)".to_string()],
                    position: PanelPosition {
                        x: 12,
                        y: 0,
                        width: 12,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
                // å»¶è¿Ÿå›¾è¡¨
                Panel {
                    id: "latency".to_string(),
                    title: "è¯·æ±‚å»¶è¿Ÿ".to_string(),
                    panel_type: PanelType::Graph,
                    queries: vec![
                        "histogram_quantile(0.5, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le))".to_string(),
                        "histogram_quantile(0.9, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le))".to_string(),
                        "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le))".to_string(),
                    ],
                    position: PanelPosition {
                        x: 0,
                        y: 8,
                        width: 24,
                        height: 8,
                    },
                    options: HashMap::new(),
                },
            ],
            tags: vec!["application".to_string(), "performance".to_string()],
            created_at: Utc::now(),
            updated_at: Utc::now(),
            creator: "system".to_string(),
            variables: HashMap::new(),
        };

        let mut dashboards = self.dashboard_service.dashboards.write().unwrap();
        dashboards.insert(system_dashboard.id.clone(), system_dashboard);
        dashboards.insert(app_dashboard.id.clone(), app_dashboard);

        // ä¿å­˜ä»ªè¡¨ç›˜åˆ°ç£ç›˜
        self.save_dashboard("system-overview")?;
        self.save_dashboard("application-performance")?;

        Ok(())
    }

    fn save_dashboard(&self, id: &str) -> Result<(), String> {
        let dashboards = self.dashboard_service.dashboards.read().unwrap();

        let dashboard = dashboards.get(id)
            .ok_or_else(|| format!("ä»ªè¡¨ç›˜ä¸å­˜åœ¨: {}", id))?;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–ä¸ºJSON
        let json = serde_json::to_string_pretty(dashboard)
            .map_err(|e| format!("åºåˆ—åŒ–ä»ªè¡¨ç›˜å¤±è´¥: {}", e))?;

        let path = self.data_dir.join("dashboards").join(format!("{}.json", id));

        if let Err(e) = std::fs::write(&path, json) {
            return Err(format!("å†™å…¥ä»ªè¡¨ç›˜æ–‡ä»¶å¤±è´¥: {}", e));
        }

        Ok(())
    }

    fn start_dashboard_service(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ä»ªè¡¨ç›˜æœåŠ¡");

        let bind_address = self.dashboard_service.bind_address.clone();
        let dashboards = self.dashboard_service.dashboards.clone();

        self.dashboard_service.running.store(true, Ordering::SeqCst);

        let running = self.dashboard_service.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTPæœåŠ¡å™¨
            println!("ä»ªè¡¨ç›˜æœåŠ¡ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
            }
        });

        self.dashboard_service.server = Some(thread);

        Ok(())
    }

    fn start_api_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨APIæœåŠ¡å™¨");

        let bind_address = self.api_server.bind_address.clone();

        self.api_server.running.store(true, Ordering::SeqCst);

        let running = self.api_server.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTP APIæœåŠ¡å™¨
            println!("APIæœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†APIè¯·æ±‚
            }
        });

        self.api_server.server = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ");

        // åœæ­¢APIæœåŠ¡å™¨
        self.api_server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.api_server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("APIæœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢ä»ªè¡¨ç›˜æœåŠ¡
        self.dashboard_service.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.dashboard_service.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("ä»ªè¡¨ç›˜æœåŠ¡çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢å‘Šè­¦ç®¡ç†å™¨
        self.alert_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.alert_manager.alert_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("å‘Šè­¦ç®¡ç†å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢æ‰€æœ‰æ”¶é›†å™¨
        for (name, collector) in &mut self.collectors {
            if let Err(e) = collector.stop() {
                println!("åœæ­¢æ”¶é›†å™¨ {} å¤±è´¥: {}", name, e);
            }
        }

        // åœæ­¢å­˜å‚¨åç«¯
        self.storage_backend.stop()?;

        Ok(())
    }
}

struct SystemMetricCollector {
    running: AtomicBool,
    collection_thread: Option<JoinHandle<()>>,
    collection_interval: Duration,
}

impl SystemMetricCollector {
    fn new() -> Self {
        SystemMetricCollector {
            running: AtomicBool::new(false),
            collection_thread: None,
            collection_interval: Duration::from_secs(10),
        }
    }
}

impl MetricCollector for SystemMetricCollector {
    fn name(&self) -> &str {
        "system"
    }

    fn description(&self) -> &str {
        "æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œç­‰ï¼‰"
    }

    fn collect(&self) -> Vec<Metric> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ”¶é›†å®é™…çš„ç³»ç»ŸæŒ‡æ ‡
        let now = Utc::now().timestamp();

        vec![
            Metric {
                name: "cpu_usage".to_string(),
                value: 45.2,
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                    ("cpu".to_string(), "total".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Gauge,
            },
            Metric {
                name: "memory_usage_percent".to_string(),
                value: 62.5,
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Gauge,
            },
            Metric {
                name: "disk_usage_percent".to_string(),
                value: 78.3,
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                    ("mount".to_string(), "/".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Gauge,
            },
        ]
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å™¨");

        let interval = self.collection_interval;

        self.running.store(true, Ordering::SeqCst);

        let running = self.running.clone();
        let collector = SystemMetricCollector::new();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ”¶é›†æŒ‡æ ‡
                let metrics = collector.collect();

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æŒ‡æ ‡å‘é€åˆ°å­˜å‚¨åç«¯
                println!("æ”¶é›†åˆ° {} ä¸ªç³»ç»ŸæŒ‡æ ‡", metrics.len());

                // ç­‰å¾…ä¸‹ä¸€ä¸ªæ”¶é›†å‘¨æœŸ
                thread::sleep(interval);
            }
        });

        self.collection_thread = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å™¨");

        self.running.store(false, Ordering::SeqCst);

        if let Some(thread) = self.collection_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }
}

struct JvmMetricCollector {
    running: AtomicBool,
    collection_thread: Option<JoinHandle<()>>,
    collection_interval: Duration,
}

impl JvmMetricCollector {
    fn new() -> Self {
        JvmMetricCollector {
            running: AtomicBool::new(false),
            collection_thread: None,
            collection_interval: Duration::from_secs(10),
        }
    }
}

impl MetricCollector for JvmMetricCollector {
    fn name(&self) -> &str {
        "jvm"
    }

    fn description(&self) -> &str {
        "æ”¶é›†JVMæŒ‡æ ‡ï¼ˆå †å†…å­˜ã€GCç­‰ï¼‰"
    }

    fn collect(&self) -> Vec<Metric> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ”¶é›†å®é™…çš„JVMæŒ‡æ ‡
        let now = Utc::now().timestamp();

        vec![
            Metric {
                name: "jvm_heap_used".to_string(),
                value: 512.0 * 1024.0 * 1024.0, // 512MB
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                    ("service".to_string(), "app1".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Gauge,
            },
            Metric {
                name: "jvm_gc_collection_seconds_sum".to_string(),
                value: 0.254,
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                    ("service".to_string(), "app1".to_string()),
                    ("gc".to_string(), "G1 Young Generation".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Counter,
            },
        ]
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨JVMæŒ‡æ ‡æ”¶é›†å™¨");

        let interval = self.collection_interval;

        self.running.store(true, Ordering::SeqCst);

        let running = self.running.clone();
        let collector = JvmMetricCollector::new();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ”¶é›†æŒ‡æ ‡
                let metrics = collector.collect();

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æŒ‡æ ‡å‘é€åˆ°å­˜å‚¨åç«¯
                println!("æ”¶é›†åˆ° {} ä¸ªJVMæŒ‡æ ‡", metrics.len());

                // ç­‰å¾…ä¸‹ä¸€ä¸ªæ”¶é›†å‘¨æœŸ
                thread::sleep(interval);
            }
        });

        self.collection_thread = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢JVMæŒ‡æ ‡æ”¶é›†å™¨");

        self.running.store(false, Ordering::SeqCst);

        if let Some(thread) = self.collection_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("JVMæŒ‡æ ‡æ”¶é›†å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }
}

struct NetworkMetricCollector {
    running: AtomicBool,
    collection_thread: Option<JoinHandle<()>>,
    collection_interval: Duration,
}

impl NetworkMetricCollector {
    fn new() -> Self {
        NetworkMetricCollector {
            running: AtomicBool::new(false),
            collection_thread: None,
            collection_interval: Duration::from_secs(10),
        }
    }
}

impl MetricCollector for NetworkMetricCollector {
    fn name(&self) -> &str {
        "network"
    }

    fn description(&self) -> &str {
        "æ”¶é›†ç½‘ç»œæŒ‡æ ‡ï¼ˆæµé‡ã€è¿æ¥ç­‰ï¼‰"
    }

    fn collect(&self) -> Vec<Metric> {
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ”¶é›†å®é™…çš„ç½‘ç»œæŒ‡æ ‡
        let now = Utc::now().timestamp();

        vec![
            Metric {
                name: "network_received_bytes".to_string(),
                value: 1024.0 * 1024.0 * 5.7, // 5.7MB
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                    ("interface".to_string(), "eth0".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Counter,
            },
            Metric {
                name: "network_transmitted_bytes".to_string(),
                value: 1024.0 * 1024.0 * 3.2, // 3.2MB
                timestamp: now,
                labels: [
                    ("host".to_string(), "server1".to_string()),
                    ("interface".to_string(), "eth0".to_string()),
                ].iter().cloned().collect(),
                metric_type: MetricType::Counter,
            },
        ]
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ç½‘ç»œæŒ‡æ ‡æ”¶é›†å™¨");

        let interval = self.collection_interval;

        self.running.store(true, Ordering::SeqCst);

        let running = self.running.clone();
        let collector = NetworkMetricCollector::new();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ”¶é›†æŒ‡æ ‡
                let metrics = collector.collect();

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æŒ‡æ ‡å‘é€åˆ°å­˜å‚¨åç«¯
                println!("æ”¶é›†åˆ° {} ä¸ªç½‘ç»œæŒ‡æ ‡", metrics.len());

                // ç­‰å¾…ä¸‹ä¸€ä¸ªæ”¶é›†å‘¨æœŸ
                thread::sleep(interval);
            }
        });

        self.collection_thread = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢ç½‘ç»œæŒ‡æ ‡æ”¶é›†å™¨");

        self.running.store(false, Ordering::SeqCst);

        if let Some(thread) = self.collection_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("ç½‘ç»œæŒ‡æ ‡æ”¶é›†å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        Ok(())
    }
}

struct EmailNotifier {
    smtp_server: String,
    smtp_port: u16,
    smtp_username: String,
    smtp_password: String,
    recipients: Vec<String>,
}

impl EmailNotifier {
    fn new(server: &str, port: u16, username: &str, password: &str, recipients: Vec<String>) -> Self {
        EmailNotifier {
            smtp_server: server.to_string(),
            smtp_port: port,
            smtp_username: username.to_string(),
            smtp_password: password.to_string(),
            recipients,
        }
    }
}

impl AlertNotifier for EmailNotifier {
    fn name(&self) -> &str {
        "email"
    }

    fn notify(&self, alert: &Alert) -> Result<(), String> {
        println!("å‘é€é‚®ä»¶å‘Šè­¦é€šçŸ¥: {}", alert.name);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€å®é™…çš„ç”µå­é‚®ä»¶
        let status_text = match alert.status {
            AlertStatus::Firing => "è§¦å‘",
            AlertStatus::Resolved => "å·²è§£å†³",
            AlertStatus::Acknowledged => "å·²ç¡®è®¤",
        };

        let subject = format!("[{}] {} å‘Šè­¦: {}",
                             status_text,
                             match alert.severity {
                                 AlertSeverity::Critical => "ä¸¥é‡",
                                 AlertSeverity::High => "é«˜",
                                 AlertSeverity::Medium => "ä¸­",
                                 AlertSeverity::Low => "ä½",
                                 AlertSeverity::Info => "ä¿¡æ¯",
                             },
                             alert.name);

        let body = match alert.annotations.get("description") {
            Some(desc) => desc.clone(),
            None => format!("å‘Šè­¦ {} å·² {}", alert.name, status_text),
        };

        println!("é‚®ä»¶æ ‡é¢˜: {}", subject);
        println!("é‚®ä»¶å†…å®¹: {}", body);
        println!("æ”¶ä»¶äºº: {}", self.recipients.join(", "));

        Ok(())
    }

    fn test(&self) -> Result<(), String> {
        println!("æµ‹è¯•é‚®ä»¶é€šçŸ¥");

        // å‘é€æµ‹è¯•é‚®ä»¶
        let now = Utc::now();

        let test_alert = Alert {
            id: "test".to_string(),
            name: "æµ‹è¯•å‘Šè­¦".to_string(),
            severity: AlertSeverity::Info,
            status: AlertStatus::Firing,
            condition: "test = 1".to_string(),
            value: 1.0,
            labels: HashMap::new(),
            annotations: [
                ("summary".to_string(), "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å‘Šè­¦".to_string()),
                ("description".to_string(), "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é€šçŸ¥ç³»ç»Ÿçš„å‘Šè­¦".to_string()),
            ].iter().cloned().collect(),
            started_at: now,
            last_updated: now,
            ends_at: None,
        };

        self.notify(&test_alert)
    }
}

struct SlackNotifier {
    webhook_url: String,
    channel: String,
}

impl SlackNotifier {
    fn new(webhook_url: &str, channel: &str) -> Self {
        SlackNotifier {
            webhook_url: webhook_url.to_string(),
            channel: channel.to_string(),
        }
    }
}

impl AlertNotifier for SlackNotifier {
    fn name(&self) -> &str {
        "slack"
    }

    fn notify(&self, alert: &Alert) -> Result<(), String> {
        println!("å‘é€Slackå‘Šè­¦é€šçŸ¥: {}", alert.name);

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå‘é€å®é™…çš„Slackæ¶ˆæ¯
        let status_text = match alert.status {
            AlertStatus::Firing => "è§¦å‘",
            AlertStatus::Resolved => "å·²è§£å†³",
            AlertStatus::Acknowledged => "å·²ç¡®è®¤",
        };

        let color = match alert.severity {
            AlertSeverity::Critical => "#FF0000", // çº¢è‰²
            AlertSeverity::High => "#FFA500",     // æ©™è‰²
            AlertSeverity::Medium => "#FFFF00",   // é»„è‰²
            AlertSeverity::Low => "#0000FF",      // è“è‰²
            AlertSeverity::Info => "#808080",     // ç°è‰²
        };

        let title = format!("[{}] {}",
                           match alert.severity {
                               AlertSeverity::Critical => "ä¸¥é‡",
                               AlertSeverity::High => "é«˜",
                               AlertSeverity::Medium => "ä¸­",
                               AlertSeverity::Low => "ä½",
                               AlertSeverity::Info => "ä¿¡æ¯",
                           },
                           alert.name);

        let text = match alert.annotations.get("description") {
            Some(desc) => desc.clone(),
            None => format!("å‘Šè­¦ {} å·² {}", alert.name, status_text),
        };

        println!("Slackæ¶ˆæ¯æ ‡é¢˜: {}", title);
        println!("Slackæ¶ˆæ¯å†…å®¹: {}", text);
        println!("Slacké¢‘é“: {}", self.channel);

        Ok(())
    }

    fn test(&self) -> Result<(), String> {
        println!("æµ‹è¯•Slacké€šçŸ¥");

        // å‘é€æµ‹è¯•Slackæ¶ˆæ¯
        let now = Utc::now();

        let test_alert = Alert {
            id: "test".to_string(),
            name: "æµ‹è¯•å‘Šè­¦".to_string(),
            severity: AlertSeverity::Info,
            status: AlertStatus::Firing,
            condition: "test = 1".to_string(),
            value: 1.0,
            labels: HashMap::new(),
            annotations: [
                ("summary".to_string(), "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å‘Šè­¦".to_string()),
                ("description".to_string(), "è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•é€šçŸ¥ç³»ç»Ÿçš„å‘Šè­¦".to_string()),
            ].iter().cloned().collect(),
            started_at: now,
            last_updated: now,
            ends_at: None,
        };

        self.notify(&test_alert)
    }
}

// åˆ†å¸ƒå¼åè°ƒæœåŠ¡
struct DistributedCoordinationService {
    node_id: String,
    data_dir: PathBuf,
    zk_client: Option<ZooKeeperClient>,
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
    peers: HashMap<String, String>,
    state: RwLock<CoordinationState>,
}

struct ZooKeeperClient {
    connection: Option<()>, // è¿™é‡Œç®€åŒ–ï¼Œå®é™…ä¼šæ˜¯ä¸€ä¸ªZooKeeperå®¢æˆ·ç«¯è¿æ¥
    session_timeout: Duration,
    root_path: String,
}

struct CoordinationState {
    leader: Option<String>,
    term: u64,
    epoch: u64,
    sessions: HashMap<String, SessionInfo>,
    locks: HashMap<String, LockInfo>,
    watches: HashMap<String, Vec<WatchInfo>>,
    configuration: HashMap<String, String>,
}

struct SessionInfo {
    id: String,
    client_id: String,
    created_at: DateTime<Utc>,
    last_heartbeat: DateTime<Utc>,
    timeout: Duration,
}

struct LockInfo {
    path: String,
    owner: Option<String>,
    created_at: DateTime<Utc>,
    expires_at: Option<DateTime<Utc>>,
}

struct WatchInfo {
    path: String,
    session_id: String,
    created_at: DateTime<Utc>,
    watch_type: WatchType,
}

enum WatchType {
    Exists,
    Data,
    Children,
}

impl DistributedCoordinationService {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str, peers: HashMap<String, String>) -> Self {
        DistributedCoordinationService {
            node_id: node_id.to_string(),
            data_dir: data_dir.to_path_buf(),
            zk_client: None,
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
            peers,
            state: RwLock::new(CoordinationState {
                leader: None,
                term: 0,
                epoch: 0,
                sessions: HashMap::new(),
                locks: HashMap::new(),
                watches: HashMap::new(),
                configuration: HashMap::new(),
            }),
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼åè°ƒæœåŠ¡");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        // åŠ è½½çŠ¶æ€
        self.load_state()?;

        // åˆå§‹åŒ–ZooKeeperå®¢æˆ·ç«¯
        let zk_client = ZooKeeperClient {
            connection: None,
            session_timeout: Duration::from_secs(30),
            root_path: "/coordination".to_string(),
        };

        self.zk_client = Some(zk_client);

        // è¿æ¥åˆ°å…¶ä»–èŠ‚ç‚¹
        self.connect_to_peers()?;

        // å¼€å§‹é¢†å¯¼è€…é€‰ä¸¾
        self.start_leader_election()?;

        // å¯åŠ¨æœåŠ¡å™¨
        self.start_server()?;

        Ok(())
    }

    fn load_state(&self) -> Result<(), String> {
        println!("åŠ è½½åè°ƒæœåŠ¡çŠ¶æ€");

        let state_path = self.data_dir.join("state.json");

        if !state_path.exists() {
            return Ok(());
        }

        let content = match std::fs::read_to_string(&state_path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£æçŠ¶æ€æ–‡ä»¶");

        // ç®€åŒ–ï¼šåˆ›å»ºä¸€äº›åˆå§‹çŠ¶æ€
        let mut state = self.state.write().unwrap();

        // æ·»åŠ ä¸€äº›é»˜è®¤é…ç½®
        state.configuration.insert("max_session_timeout".to_string(), "60000".to_string());
        state.configuration.insert("min_session_timeout".to_string(), "5000".to_string());
        state.configuration.insert("tick_time".to_string(), "2000".to_string());

        Ok(())
    }

    fn connect_to_peers(&self) -> Result<(), String> {
        println!("è¿æ¥åˆ°å¯¹ç­‰èŠ‚ç‚¹");

        for (id, address) in &self.peers {
            println!("è¿æ¥åˆ°èŠ‚ç‚¹ {} at {}", id, address);

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå»ºç«‹åˆ°å¯¹ç­‰èŠ‚ç‚¹çš„è¿æ¥
        }

        Ok(())
    }

    fn start_leader_election(&self) -> Result<(), String> {
        println!("å¼€å§‹é¢†å¯¼è€…é€‰ä¸¾");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå®ç°ä¸€ä¸ªç±»ä¼¼Raftæˆ–ZABçš„é€‰ä¸¾ç®—æ³•

        // ç®€åŒ–ï¼šå¦‚æœæ²¡æœ‰è®¾ç½®é¢†å¯¼è€…ï¼Œæˆ‘ä»¬è®¤ä¸ºè‡ªå·±æ˜¯é¢†å¯¼è€…
        let mut state = self.state.write().unwrap();

        if state.leader.is_none() {
            state.leader = Some(self.node_id.clone());
            state.term += 1;
            state.epoch += 1;

            println!("æˆä¸ºé¢†å¯¼è€…: {}, ä»»æœŸ: {}, æ—¶ä»£: {}",
                    self.node_id, state.term, state.epoch);
        }

        Ok(())
    }

    fn start_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åè°ƒæœåŠ¡å™¨");

        let bind_address = self.bind_address.clone();
        let node_id = self.node_id.clone();
        let state = self.state.clone();

        self.running.store(true, Ordering::SeqCst);

        let running = self.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªç½‘ç»œæœåŠ¡å™¨
            println!("åè°ƒæœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // æ¸…ç†è¿‡æœŸçš„ä¼šè¯å’Œé”
                {
                    let mut state_guard = state.write().unwrap();
                    let now = Utc::now();

                    // æ¸…ç†è¿‡æœŸä¼šè¯
                    let expired_sessions: Vec<String> = state_guard.sessions.iter()
                        .filter(|(_, s)| now.signed_duration_since(s.last_heartbeat) > s.timeout.into())
                        .map(|(id, _)| id.clone())
                        .collect();

                    for session_id in &expired_sessions {
                        println!("ä¼šè¯è¿‡æœŸ: {}", session_id);
                        state_guard.sessions.remove(session_id);

                        // åˆ é™¤ä¼šè¯æ‹¥æœ‰çš„é”
                        let expired_locks: Vec<String> = state_guard.locks.iter()
                            .filter(|(_, l)| l.owner.as_ref() == Some(session_id))
                            .map(|(path, _)| path.clone())
                            .collect();

                        for lock_path in expired_locks {
                            println!("é‡Šæ”¾é”: {}", lock_path);

                            if let Some(lock) = state_guard.locks.get_mut(&lock_path) {
                                lock.owner = None;

                                // è§¦å‘é”é‡Šæ”¾çš„ç›‘è§†å™¨
                                Self::trigger_watches(&mut state_guard, &lock_path, WatchType::Data);
                            }
                        }

                        // åˆ é™¤ä¼šè¯çš„ç›‘è§†å™¨
                        for (path, watches) in state_guard.watches.iter_mut() {
                            watches.retain(|w| w.session_id != *session_id);
                        }
                    }

                    // æ¸…ç†ç©ºçš„ç›‘è§†å™¨åˆ—è¡¨
                    state_guard.watches.retain(|_, watches| !watches.is_empty());

                    // æ¸…ç†è¿‡æœŸçš„é”
                    let now = Utc::now();
                    let expired_locks: Vec<String> = state_guard.locks.iter()
                        .filter(|(_, l)| l.expires_at.map_or(false, |exp| now > exp))
                        .map(|(path, _)| path.clone())
                        .collect();

                    for lock_path in expired_locks {
                        println!("é”è¿‡æœŸ: {}", lock_path);

                        if let Some(lock) = state_guard.locks.get_mut(&lock_path) {
                            lock.owner = None;

                            // è§¦å‘é”é‡Šæ”¾çš„ç›‘è§†å™¨
                            Self::trigger_watches(&mut state_guard, &lock_path, WatchType::Data);
                        }
                    }
                }

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
            }
        });

        self.server = Some(thread);

        Ok(())
    }

    fn trigger_watches(state: &mut CoordinationState, path: &str, watch_type: WatchType) {
        if let Some(watches) = state.watches.get(path) {
            let matching_watches: Vec<_> = watches.iter()
                .filter(|w| std::mem::discriminant(&w.watch_type) == std::mem::discriminant(&watch_type))
                .collect();

            for watch in matching_watches {
                println!("è§¦å‘ç›‘è§†å™¨: {} -> {}", path, watch.session_id);

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šé€šçŸ¥å®¢æˆ·ç«¯
            }
        }
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼åè°ƒæœåŠ¡");

        // åœæ­¢æœåŠ¡å™¨
        self.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // ä¿å­˜çŠ¶æ€
        self.save_state()?;

        Ok(())
    }

    fn save_state(&self) -> Result<(), String> {
        println!("ä¿å­˜åè°ƒæœåŠ¡çŠ¶æ€");

        let state = self.state.read().unwrap();

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–ä¸ºJSON
        let json = serde_json::to_string_pretty(&*state)
            .map_err(|e| format!("åºåˆ—åŒ–çŠ¶æ€å¤±è´¥: {}", e))?;

        let state_path = self.data_dir.join("state.json");

        if let Err(e) = std::fs::write(&state_path, json) {
            return Err(format!("å†™å…¥çŠ¶æ€æ–‡ä»¶å¤±è´¥: {}", e));
        }

        Ok(())
    }

    fn create_session(&self, client_id: &str, timeout: Duration) -> Result<String, String> {
        println!("åˆ›å»ºä¼šè¯: {}", client_id);

        let session_id = uuid::Uuid::new_v4().to_string();
        let now = Utc::now();

        let session = SessionInfo {
            id: session_id.clone(),
            client_id: client_id.to_string(),
            created_at: now,
            last_heartbeat: now,
            timeout,
        };

        let mut state = self.state.write().unwrap();
        state.sessions.insert(session_id.clone(), session);

        Ok(session_id)
    }

    fn heartbeat(&self, session_id: &str) -> Result<(), String> {
        let mut state = self.state.write().unwrap();

        if let Some(session) = state.sessions.get_mut(session_id) {
            session.last_heartbeat = Utc::now();
            Ok(())
        } else {
            Err(format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id))
        }
    }

    fn acquire_lock(&self, session_id: &str, path: &str, wait: bool, timeout: Option<Duration>) -> Result<bool, String> {
        println!("è·å–é”: {} -> {}", session_id, path);

        // æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
        let mut state = self.state.write().unwrap();

        if !state.sessions.contains_key(session_id) {
            return Err(format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id));
        }

        let now = Utc::now();

        // æ£€æŸ¥é”æ˜¯å¦å­˜åœ¨
        if let Some(lock) = state.locks.get_mut(path) {
            if let Some(owner) = &lock.owner {
                if owner == session_id {
                    // å·²ç»æ‹¥æœ‰é”
                    return Ok(true);
                }

                if !wait {
                    // ä¸ç­‰å¾…ï¼Œç›´æ¥è¿”å›å¤±è´¥
                    return Ok(false);
                }

                // æ·»åŠ ç›‘è§†å™¨
                let watch = WatchInfo {
                    path: path.to_string(),
                    session_id: session_id.to_string(),
                    created_at: now,
                    watch_type: WatchType::Data,
                };

                let watches = state.watches.entry(path.to_string()).or_insert_with(Vec::new);
                watches.push(watch);

                // æ— æ³•ç«‹å³è·å–é”
                return Ok(false);
            } else {
                // é”æœªè¢«æŒæœ‰ï¼Œè·å–å®ƒ
                lock.owner = Some(session_id.to_string());
                lock.created_at = now;

                if let Some(timeout_duration) = timeout {
                    lock.expires_at = Some(now + timeout_duration.into());
                } else {
                    lock.expires_at = None;
                }

                // è§¦å‘ç›‘è§†å™¨
                Self::trigger_watches(&mut state, path, WatchType::Data);

                return Ok(true);
            }
        } else {
            // åˆ›å»ºæ–°é”
            let lock = LockInfo {
                path: path.to_string(),
                owner: Some(session_id.to_string()),
                created_at: now,
                expires_at: timeout.map(|t| now + t.into()),
            };

            state.locks.insert(path.to_string(), lock);

            // è§¦å‘ç›‘è§†å™¨
            Self::trigger_watches(&mut state, path, WatchType::Data);

            return Ok(true);
        }
    }

    fn release_lock(&self, session_id: &str, path: &str) -> Result<bool, String> {
        println!("é‡Šæ”¾é”: {} -> {}", session_id, path);

        // æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
        let mut state = self.state.write().unwrap();

        if !state.sessions.contains_key(session_id) {
            return Err(format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id));
        }

        // æ£€æŸ¥é”æ˜¯å¦å­˜åœ¨
        if let Some(lock) = state.locks.get_mut(path) {
            if let Some(owner) = &lock.owner {
                if owner == session_id {
                    // é‡Šæ”¾é”
                    lock.owner = None;

                    // è§¦å‘ç›‘è§†å™¨
                    Self::trigger_watches(&mut state, path, WatchType::Data);

                    return Ok(true);
                } else {
                    // ä¸æ˜¯é”çš„æ‹¥æœ‰è€…
                    return Err(format!("ä¸æ˜¯é”çš„æ‹¥æœ‰è€…: {}", path));
                }
            } else {
                // é”æœªè¢«æŒæœ‰
                return Err(format!("é”æœªè¢«æŒæœ‰: {}", path));
            }
        } else {
            // é”ä¸å­˜åœ¨
            return Err(format!("é”ä¸å­˜åœ¨: {}", path));
        }
    }

    fn watch(&self, session_id: &str, path: &str, watch_type: WatchType) -> Result<(), String> {
        println!("æ·»åŠ ç›‘è§†å™¨: {} -> {}", session_id, path);

        // æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
        let mut state = self.state.write().unwrap();

        if !state.sessions.contains_key(session_id) {
            return Err(format!("ä¼šè¯ä¸å­˜åœ¨: {}", session_id));
        }

        // æ·»åŠ ç›‘è§†å™¨
        let now = Utc::now();

        let watch = WatchInfo {
            path: path.to_string(),
            session_id: session_id.to_string(),
            created_at: now,
            watch_type,
        };

        let watches = state.watches.entry(path.to_string()).or_insert_with(Vec::new);
        watches.push(watch);

        Ok(())
    }
}

// åœ¨mainå‡½æ•°ä¸­æµ‹è¯•æ‰€æœ‰åˆ†å¸ƒå¼ç³»ç»Ÿ
fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("å¯åŠ¨åˆ†å¸ƒå¼ç³»ç»Ÿæµ‹è¯•");

    let temp_dir = std::env::temp_dir().join("distributed_systems_demo");

    if temp_dir.exists() {
        std::fs::remove_dir_all(&temp_dir)?;
    }
    std::fs::create_dir_all(&temp_dir)?;

    println!("ä½¿ç”¨ä¸´æ—¶ç›®å½•: {:?}", temp_dir);

    // æµ‹è¯•åˆ†å¸ƒå¼æœç´¢å¼•æ“
    {
        println!("\n===== æµ‹è¯•åˆ†å¸ƒå¼æœç´¢å¼•æ“ =====");

        let data_dir = temp_dir.join("search_engine");
        std::fs::create_dir_all(&data_dir)?;

        let mut search_engine = DistributedSearchEngine::new("node1", &data_dir, "127.0.0.1:9200");

        search_engine.start()?;

        // åˆ›å»ºç´¢å¼•
        let schema = IndexSchema {
            fields: vec![
                FieldDefinition {
                    name: "id".to_string(),
                    field_type: FieldType::Text,
                    indexed: true,
                    stored: true,
                    tokenized: false,
                    vector_dimensions: None,
                },
                FieldDefinition {
                    name: "title".to_string(),
                    field_type: FieldType::Text,
                    indexed: true,
                    stored: true,
                    tokenized: true,
                    vector_dimensions: None,
                },
                FieldDefinition {
                    name: "content".to_string(),
                    field_type: FieldType::Text,
                    indexed: true,
                    stored: true,
                    tokenized: true,
                    vector_dimensions: None,
                },
                FieldDefinition {
                    name: "embedding".to_string(),
                    field_type: FieldType::Vector,
                    indexed: true,
                    stored: true,
                    tokenized: false,
                    vector_dimensions: Some(128),
                },
            ],
            analyzers: HashMap::new(),
            index_options: IndexOptions {
                primary_key: Some("id".to_string()),
                index_merge_policy: MergePolicy::Tiered { max_merged_segment_size: 1024 * 1024 * 100 },
                similarity: Similarity::BM25 { k1: 1.2, b: 0.75 },
            },
        };

        search_engine.create_index("documents", schema)?;

        // ç´¢å¼•æ–‡æ¡£
        let doc1 = Document {
            id: "1".to_string(),
            fields: [
                ("id".to_string(), FieldValue::Text("1".to_string())),
                ("title".to_string(), FieldValue::Text("åˆ†å¸ƒå¼ç³»ç»Ÿç®€ä»‹".to_string())),
                ("content".to_string(), FieldValue::Text("åˆ†å¸ƒå¼ç³»ç»Ÿæ˜¯ç”±å¤šä¸ªé€šè¿‡ç½‘ç»œè¿æ¥çš„è®¡ç®—èŠ‚ç‚¹ç»„æˆçš„ç³»ç»Ÿã€‚".to_string())),
                ("embedding".to_string(), FieldValue::Vector(vec![0.1; 128])),
            ].iter().cloned().collect(),
            boost: 1.0,
        };

        search_engine.index_document("documents", doc1)?;

        // æœç´¢
        let query = Query::Term {
            field: "content".to_string(),
            term: "åˆ†å¸ƒå¼".to_string(),
            boost: 1.0,
        };

        let results = search_engine.search("documents", query, 10)?;

        println!("æœç´¢ç»“æœ: {} ä¸ªåŒ¹é…", results.total_hits);

        // åœæ­¢æœç´¢å¼•æ“
        search_engine.stop()?;
    }

    // æµ‹è¯•åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“
    {
        println!("\n===== æµ‹è¯•åˆ†å¸ƒå¼æ—¶åºæ•°æ®åº“ =====");

        let data_dir = temp_dir.join("time_series_db");
        std::fs::create_dir_all(&data_dir)?;

        let mut time_series_db = DistributedTimeSeriesDB::new("node1", &data_dir, "127.0.0.1:8086");

        time_series_db.start()?;

        // å†™å…¥ä¸€äº›æ•°æ®ç‚¹
        let now = Utc::now().timestamp();

        for i in 0..10 {
            let point = DataPoint {
                timestamp: now + i * 60,
                value: 42.0 + (i as f64),
            };

            let labels = [
                ("host".to_string(), "server1".to_string()),
                ("service".to_string(), "app1".to_string()),
            ].iter().cloned().collect();

            time_series_db.write_data_point("cpu_usage", labels.clone(), point)?;
        }

        // æŸ¥è¯¢æ•°æ®
        let query = TimeSeriesQuery {
            selector: MetricSelector {
                metric_name: "cpu_usage".to_string(),
                label_matchers: vec![
                    LabelMatcher {
                        name: "host".to_string(),
                        operator: MatchOperator::Equal,
                        value: "server1".to_string(),
                    },
                ],
            },
            start_time: now,
            end_time: now + 600,
            step: Some(60),
            aggregation: Some(AggregationFunction::Avg),
        };

        let result = time_series_db.query_range(query)?;

        println!("æŸ¥è¯¢ç»“æœ: {} ä¸ªåºåˆ—", result.series.len());

        // åœæ­¢æ—¶åºæ•°æ®åº“
        time_series_db.stop()?;
    }

    // æµ‹è¯•åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ
    {
        println!("\n===== æµ‹è¯•åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿ =====");

        let data_dir = temp_dir.join("monitoring_system");
        std::fs::create_dir_all(&data_dir)?;

        let mut monitoring_system = DistributedMonitoringSystem::new("node1", &data_dir, "127.0.0.1");

        monitoring_system.start()?;

        // æ”¶é›†ä¸€äº›æŒ‡æ ‡
        let system_collector = SystemMetricCollector::new();
        let metrics = system_collector.collect();

        println!("æ”¶é›†åˆ° {} ä¸ªç³»ç»ŸæŒ‡æ ‡", metrics.len());

        // åˆ›å»ºä»ªè¡¨ç›˜
        monitoring_system.create_default_dashboards()?;

        // åœæ­¢ç›‘æ§ç³»ç»Ÿ
        monitoring_system.stop()?;
    }

    // æµ‹è¯•åˆ†å¸ƒå¼åè°ƒæœåŠ¡
    {
        println!("\n===== æµ‹è¯•åˆ†å¸ƒå¼åè°ƒæœåŠ¡ =====");

        let data_dir = temp_dir.join("coordination_service");
        std::fs::create_dir_all(&data_dir)?;

        let peers = [
            ("node2".to_string(), "127.0.0.1:2182".to_string()),
            ("node3".to_string(), "127.0.0.1:2183".to_string()),
        ].iter().cloned().collect();

        let mut coordination_service = DistributedCoordinationService::new("node1", &data_dir, "127.0.0.1:2181", peers);

        coordination_service.start()?;

        // åˆ›å»ºä¼šè¯
        let session_id = coordination_service.create_session("client1", Duration::from_secs(60))?;

        println!("åˆ›å»ºä¼šè¯: {}", session_id);

        // è·å–é”
        let lock_acquired = coordination_service.acquire_lock(&session_id, "/locks/resource1", true, None)?;

        println!("è·å–é”ç»“æœ: {}", lock_acquired);

        // é‡Šæ”¾é”
        let lock_released = coordination_service.release_lock(&session_id, "/locks/resource1")?;

        println!("é‡Šæ”¾é”ç»“æœ: {}", lock_released);

        // æ·»åŠ ç›‘è§†å™¨
        coordination_service.watch(&session_id, "/config/app1", WatchType::Data)?;

        // åœæ­¢åè°ƒæœåŠ¡
        coordination_service.stop()?;
    }

    println!("\næ‰€æœ‰åˆ†å¸ƒå¼ç³»ç»Ÿæµ‹è¯•å®Œæˆ");

    // æ¸…ç†ä¸´æ—¶ç›®å½•
    std::fs::remove_dir_all(&temp_dir)?;

    Ok(())
}
```

### 1.16 ç»¼åˆåº”ç”¨16-åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ

```rust
// åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ
struct DistributedTaskScheduler {
    node_id: String,
    data_dir: PathBuf,
    task_executor: TaskExecutor,
    task_queue: TaskQueue,
    scheduler: Scheduler,
    lock_service: LockService,
    api_server: TaskApiServer,
    running: AtomicBool,
}

struct TaskExecutor {
    workers: RwLock<HashMap<String, Worker>>,
    worker_pool: ThreadPool,
    max_concurrent_tasks: usize,
    running_tasks: RwLock<HashMap<String, RunningTask>>,
}

struct Worker {
    id: String,
    capabilities: HashSet<String>,
    status: WorkerStatus,
    current_task: Option<String>,
    last_heartbeat: DateTime<Utc>,
    stats: WorkerStats,
}

struct WorkerStats {
    tasks_completed: u64,
    tasks_failed: u64,
    total_execution_time: Duration,
    avg_execution_time: Duration,
}

enum WorkerStatus {
    Idle,
    Busy,
    Offline,
    Draining,
}

struct RunningTask {
    task_id: String,
    worker_id: String,
    started_at: DateTime<Utc>,
    progress: f32,
    last_update: DateTime<Utc>,
}

struct TaskQueue {
    pending_tasks: RwLock<BTreeMap<TaskPriority, VecDeque<Task>>>,
    failed_tasks: RwLock<HashMap<String, FailedTask>>,
    completed_tasks: RwLock<HashMap<String, CompletedTask>>,
    storage: TaskStorage,
}

struct TaskPriority(u8);

impl TaskPriority {
    fn high() -> Self {
        TaskPriority(0)
    }

    fn normal() -> Self {
        TaskPriority(128)
    }

    fn low() -> Self {
        TaskPriority(255)
    }
}

impl PartialEq for TaskPriority {
    fn eq(&self, other: &Self) -> bool {
        self.0 == other.0
    }
}

impl Eq for TaskPriority {}

impl PartialOrd for TaskPriority {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for TaskPriority {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        // åå‘æ’åºä½¿è¾ƒå°çš„å€¼å…·æœ‰è¾ƒé«˜çš„ä¼˜å…ˆçº§
        other.0.cmp(&self.0)
    }
}

struct Task {
    id: String,
    name: String,
    task_type: String,
    parameters: HashMap<String, String>,
    priority: TaskPriority,
    dependencies: Vec<String>,
    retry_policy: RetryPolicy,
    timeout: Duration,
    created_at: DateTime<Utc>,
    scheduled_at: Option<DateTime<Utc>>,
    created_by: String,
    resources: HashMap<String, f64>,
    required_capabilities: HashSet<String>,
}

struct RetryPolicy {
    max_retries: u32,
    retry_delay: Duration,
    backoff_factor: f32,
    max_delay: Duration,
}

struct FailedTask {
    task: Task,
    error: String,
    failed_at: DateTime<Utc>,
    attempts: u32,
    next_retry: Option<DateTime<Utc>>,
}

struct CompletedTask {
    task: Task,
    result: TaskResult,
    completed_at: DateTime<Utc>,
    execution_time: Duration,
    worker_id: String,
}

struct TaskResult {
    success: bool,
    data: HashMap<String, String>,
    logs: String,
}

struct TaskStorage {
    data_dir: PathBuf,
}

struct Scheduler {
    scheduling_policies: Vec<Box<dyn SchedulingPolicy>>,
    scheduler_thread: Option<JoinHandle<()>>,
    running: AtomicBool,
    interval: Duration,
}

trait SchedulingPolicy: Send + Sync {
    fn name(&self) -> &str;
    fn select_worker(&self, task: &Task, workers: &HashMap<String, Worker>) -> Option<String>;
}

struct FifoSchedulingPolicy;

impl SchedulingPolicy for FifoSchedulingPolicy {
    fn name(&self) -> &str {
        "fifo"
    }

    fn select_worker(&self, task: &Task, workers: &HashMap<String, Worker>) -> Option<String> {
        // é€‰æ‹©ç¬¬ä¸€ä¸ªç©ºé—²ä¸”å…·æœ‰æ‰€éœ€èƒ½åŠ›çš„å·¥ä½œè€…
        for (id, worker) in workers {
            if worker.status == WorkerStatus::Idle &&
               task.required_capabilities.iter().all(|cap| worker.capabilities.contains(cap)) {
                return Some(id.clone());
            }
        }
        None
    }
}

struct RoundRobinSchedulingPolicy {
    last_worker: AtomicUsize,
}

impl SchedulingPolicy for RoundRobinSchedulingPolicy {
    fn name(&self) -> &str {
        "round_robin"
    }

    fn select_worker(&self, task: &Task, workers: &HashMap<String, Worker>) -> Option<String> {
        // æ”¶é›†æ‰€æœ‰ç©ºé—²ä¸”å…·æœ‰æ‰€éœ€èƒ½åŠ›çš„å·¥ä½œè€…
        let available_workers: Vec<_> = workers.iter()
            .filter(|(_, w)| w.status == WorkerStatus::Idle &&
                   task.required_capabilities.iter().all(|cap| w.capabilities.contains(cap)))
            .collect();

        if available_workers.is_empty() {
            return None;
        }

        // è·å–å¹¶æ›´æ–°ä¸Šæ¬¡ä½¿ç”¨çš„å·¥ä½œè€…ç´¢å¼•
        let current_index = self.last_worker.load(Ordering::SeqCst);
        let next_index = (current_index + 1) % available_workers.len();
        self.last_worker.store(next_index, Ordering::SeqCst);

        // è¿”å›é€‰æ‹©çš„å·¥ä½œè€…ID
        Some(available_workers[next_index].0.clone())
    }
}

struct ResourceAwareSchedulingPolicy;

impl SchedulingPolicy for ResourceAwareSchedulingPolicy {
    fn name(&self) -> &str {
        "resource_aware"
    }

    fn select_worker(&self, task: &Task, workers: &HashMap<String, Worker>) -> Option<String> {
        // ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œåªå®ç°ä¸€ä¸ªåŸºæœ¬çš„èµ„æºæ„ŸçŸ¥è°ƒåº¦
        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™å°†è€ƒè™‘èµ„æºåˆ©ç”¨ç‡å’Œèµ„æºè¯·æ±‚
        let available_workers: Vec<_> = workers.iter()
            .filter(|(_, w)| w.status == WorkerStatus::Idle &&
                   task.required_capabilities.iter().all(|cap| w.capabilities.contains(cap)))
            .collect();

        if available_workers.is_empty() {
            return None;
        }

        // é€‰æ‹©å®Œæˆä»»åŠ¡æœ€å°‘çš„å·¥ä½œè€…
        available_workers.iter()
            .min_by_key(|(_, w)| w.stats.tasks_completed)
            .map(|(id, _)| id.clone())
    }
}

struct LockService {
    locks: RwLock<HashMap<String, String>>,
}

struct TaskApiServer {
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

impl DistributedTaskScheduler {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str) -> Self {
        let task_storage = TaskStorage {
            data_dir: data_dir.join("tasks"),
        };

        let task_queue = TaskQueue {
            pending_tasks: RwLock::new(BTreeMap::new()),
            failed_tasks: RwLock::new(HashMap::new()),
            completed_tasks: RwLock::new(HashMap::new()),
            storage: task_storage,
        };

        let task_executor = TaskExecutor {
            workers: RwLock::new(HashMap::new()),
            worker_pool: ThreadPool::new(4),
            max_concurrent_tasks: 8,
            running_tasks: RwLock::new(HashMap::new()),
        };

        let scheduler = Scheduler {
            scheduling_policies: vec![
                Box::new(FifoSchedulingPolicy),
                Box::new(RoundRobinSchedulingPolicy {
                    last_worker: AtomicUsize::new(0),
                }),
                Box::new(ResourceAwareSchedulingPolicy),
            ],
            scheduler_thread: None,
            running: AtomicBool::new(false),
            interval: Duration::from_millis(100),
        };

        let lock_service = LockService {
            locks: RwLock::new(HashMap::new()),
        };

        let api_server = TaskApiServer {
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
        };

        DistributedTaskScheduler {
            node_id: node_id.to_string(),
            data_dir: data_dir.to_path_buf(),
            task_executor,
            task_queue,
            scheduler,
            lock_service,
            api_server,
            running: AtomicBool::new(false),
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        if let Err(e) = std::fs::create_dir_all(&self.task_queue.storage.data_dir) {
            return Err(format!("åˆ›å»ºä»»åŠ¡æ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        // åŠ è½½ä»»åŠ¡
        self.load_tasks()?;

        // æ³¨å†Œå·¥ä½œè€…
        self.register_workers()?;

        // å¯åŠ¨è°ƒåº¦å™¨
        self.start_scheduler()?;

        // å¯åŠ¨APIæœåŠ¡å™¨
        self.start_api_server()?;

        self.running.store(true, Ordering::SeqCst);

        Ok(())
    }

    fn load_tasks(&self) -> Result<(), String> {
        println!("åŠ è½½ä»»åŠ¡");

        let tasks_dir = self.task_queue.storage.data_dir.join("pending");

        if !tasks_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&tasks_dir) {
                return Err(format!("åˆ›å»ºå¾…å¤„ç†ä»»åŠ¡ç›®å½•å¤±è´¥: {}", e));
            }
            return Ok(());
        }

        // åŠ è½½å¾…å¤„ç†ä»»åŠ¡
        match std::fs::read_dir(&tasks_dir) {
            Ok(entries) => {
                for entry in entries {
                    if let Ok(entry) = entry {
                        let path = entry.path();

                        if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                            match self.load_task(&path) {
                                Ok(task) => {
                                    // æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ—
                                    let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
                                    let queue = pending_tasks.entry(task.priority).or_insert_with(VecDeque::new);
                                    queue.push_back(task);
                                },
                                Err(e) => println!("åŠ è½½ä»»åŠ¡å¤±è´¥: {} - {}", path.display(), e),
                            }
                        }
                    }
                }
            },
            Err(e) => return Err(format!("è¯»å–ä»»åŠ¡ç›®å½•å¤±è´¥: {}", e)),
        }

        // åŠ è½½å¤±è´¥ä»»åŠ¡
        let failed_dir = self.task_queue.storage.data_dir.join("failed");
        if failed_dir.exists() {
            match std::fs::read_dir(&failed_dir) {
                Ok(entries) => {
                    for entry in entries {
                        if let Ok(entry) = entry {
                            let path = entry.path();

                            if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½å¤±è´¥ä»»åŠ¡
                            }
                        }
                    }
                },
                Err(e) => println!("è¯»å–å¤±è´¥ä»»åŠ¡ç›®å½•å¤±è´¥: {}", e),
            }
        }

        // åŠ è½½å·²å®Œæˆä»»åŠ¡
        let completed_dir = self.task_queue.storage.data_dir.join("completed");
        if completed_dir.exists() {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½å·²å®Œæˆä»»åŠ¡
        }

        Ok(())
    }

    fn load_task(&self, path: &Path) -> Result<Task, String> {
        let content = match std::fs::read_to_string(path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£æä»»åŠ¡æ–‡ä»¶: {}", path.display());

        // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿä»»åŠ¡
        let now = Utc::now();

        Ok(Task {
            id: uuid::Uuid::new_v4().to_string(),
            name: format!("ä»»åŠ¡ {}", now.timestamp()),
            task_type: "process_data".to_string(),
            parameters: [
                ("input_file".to_string(), "data.csv".to_string()),
                ("output_format".to_string(), "json".to_string()),
            ].iter().cloned().collect(),
            priority: TaskPriority::normal(),
            dependencies: Vec::new(),
            retry_policy: RetryPolicy {
                max_retries: 3,
                retry_delay: Duration::from_secs(30),
                backoff_factor: 2.0,
                max_delay: Duration::from_secs(300),
            },
            timeout: Duration::from_secs(600),
            created_at: now,
            scheduled_at: None,
            created_by: "system".to_string(),
            resources: [
                ("cpu".to_string(), 1.0),
                ("memory".to_string(), 512.0),
            ].iter().cloned().collect(),
            required_capabilities: ["data_processing".to_string()].iter().cloned().collect(),
        })
    }

    fn register_workers(&self) -> Result<(), String> {
        println!("æ³¨å†Œå·¥ä½œè€…");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å‘ç°æˆ–ä»é…ç½®åŠ è½½å·¥ä½œè€…

        // æ³¨å†Œä¸€äº›æ¨¡æ‹Ÿå·¥ä½œè€…
        let mut workers = self.task_executor.workers.write().unwrap();

        for i in 1..=4 {
            let worker_id = format!("worker-{}", i);

            let worker = Worker {
                id: worker_id.clone(),
                capabilities: ["data_processing".to_string(), "computation".to_string()].iter().cloned().collect(),
                status: WorkerStatus::Idle,
                current_task: None,
                last_heartbeat: Utc::now(),
                stats: WorkerStats {
                    tasks_completed: 0,
                    tasks_failed: 0,
                    total_execution_time: Duration::from_secs(0),
                    avg_execution_time: Duration::from_secs(0),
                },
            };

            workers.insert(worker_id, worker);
        }

        Ok(())
    }

    fn start_scheduler(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨");

        let pending_tasks = self.task_queue.pending_tasks.clone();
        let workers = self.task_executor.workers.clone();
        let running_tasks = self.task_executor.running_tasks.clone();
        let max_concurrent_tasks = self.task_executor.max_concurrent_tasks;
        let worker_pool = self.task_executor.worker_pool;
        let policies = self.scheduler.scheduling_policies.clone();
        let interval = self.scheduler.interval;

        self.scheduler.running.store(true, Ordering::SeqCst);

        let running = self.scheduler.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // æ£€æŸ¥æ˜¯å¦æœ‰ç©ºé—²çš„å·¥ä½œè€…
                let workers_guard = workers.read().unwrap();
                let running_tasks_guard = running_tasks.read().unwrap();

                if running_tasks_guard.len() >= max_concurrent_tasks {
                    // å·²è¾¾åˆ°æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
                    drop(running_tasks_guard);
                    drop(workers_guard);
                    thread::sleep(interval);
                    continue;
                }

                let idle_workers: Vec<_> = workers_guard.iter()
                    .filter(|(_, w)| w.status == WorkerStatus::Idle)
                    .collect();

                if idle_workers.is_empty() {
                    // æ²¡æœ‰ç©ºé—²å·¥ä½œè€…
                    drop(running_tasks_guard);
                    drop(workers_guard);
                    thread::sleep(interval);
                    continue;
                }

                drop(running_tasks_guard);
                drop(workers_guard);

                // æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†ä»»åŠ¡
                let mut pending_tasks_guard = pending_tasks.write().unwrap();

                if pending_tasks_guard.is_empty() {
                    // æ²¡æœ‰å¾…å¤„ç†ä»»åŠ¡
                    drop(pending_tasks_guard);
                    thread::sleep(interval);
                    continue;
                }

                // æå–æœ€é«˜ä¼˜å…ˆçº§çš„ä»»åŠ¡
                let (priority, queue) = pending_tasks_guard.iter_mut().next().unwrap();
                let priority = priority.clone();

                if queue.is_empty() {
                    // åˆ é™¤ç©ºé˜Ÿåˆ—
                    pending_tasks_guard.remove(&priority);
                    drop(pending_tasks_guard);
                    thread::sleep(interval);
                    continue;
                }

                let task = match queue.pop_front() {
                    Some(task) => task,
                    None => {
                        // åˆ é™¤ç©ºé˜Ÿåˆ—
                        pending_tasks_guard.remove(&priority);
                        drop(pending_tasks_guard);
                        thread::sleep(interval);
                        continue;
                    }
                };

                // å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œåˆ é™¤å®ƒ
                if queue.is_empty() {
                    pending_tasks_guard.remove(&priority);
                }

                drop(pending_tasks_guard);

                // ä¸ºä»»åŠ¡é€‰æ‹©å·¥ä½œè€…
                let workers_guard = workers.read().unwrap();

                let mut selected_worker_id = None;

                // å°è¯•æ¯ä¸ªè°ƒåº¦ç­–ç•¥
                for policy in &policies {
                    if let Some(worker_id) = policy.select_worker(&task, &workers_guard) {
                        selected_worker_id = Some(worker_id);
                        break;
                    }
                }

                drop(workers_guard);

                if let Some(worker_id) = selected_worker_id {
                    // æ›´æ–°å·¥ä½œè€…çŠ¶æ€
                    let mut workers_guard = workers.write().unwrap();

                    if let Some(worker) = workers_guard.get_mut(&worker_id) {
                        worker.status = WorkerStatus::Busy;
                        worker.current_task = Some(task.id.clone());
                    }

                    drop(workers_guard);

                    // åˆ›å»ºè¿è¡Œä»»åŠ¡è®°å½•
                    let running_task = RunningTask {
                        task_id: task.id.clone(),
                        worker_id: worker_id.clone(),
                        started_at: Utc::now(),
                        progress: 0.0,
                        last_update: Utc::now(),
                    };

                    let mut running_tasks_guard = running_tasks.write().unwrap();
                    running_tasks_guard.insert(task.id.clone(), running_task);
                    drop(running_tasks_guard);

                    // æ‰§è¡Œä»»åŠ¡
                    let worker_id_clone = worker_id.clone();
                    let task_id = task.id.clone();

                    worker_pool.execute(move || {
                        println!("å·¥ä½œè€… {} æ‰§è¡Œä»»åŠ¡ {}", worker_id_clone, task_id);

                        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ‰§è¡Œå®é™…ä»»åŠ¡
                        thread::sleep(Duration::from_secs(5));

                        println!("å·¥ä½œè€… {} å®Œæˆä»»åŠ¡ {}", worker_id_clone, task_id);

                        // æ›´æ–°å·¥ä½œè€…çŠ¶æ€
                        let mut workers_guard = workers.write().unwrap();

                        if let Some(worker) = workers_guard.get_mut(&worker_id_clone) {
                            worker.status = WorkerStatus::Idle;
                            worker.current_task = None;
                            worker.stats.tasks_completed += 1;
                        }

                        drop(workers_guard);

                        // ç§»é™¤è¿è¡Œä»»åŠ¡è®°å½•
                        let mut running_tasks_guard = running_tasks.write().unwrap();
                        running_tasks_guard.remove(&task_id);
                    });
                } else {
                    // æ²¡æœ‰åˆé€‚çš„å·¥ä½œè€…ï¼Œå°†ä»»åŠ¡æ”¾å›é˜Ÿåˆ—
                    let mut pending_tasks_guard = pending_tasks.write().unwrap();
                    let queue = pending_tasks_guard.entry(priority).or_insert_with(VecDeque::new);
                    queue.push_back(task);
                }

                thread::sleep(interval);
            }
        });

        self.scheduler.scheduler_thread = Some(thread);

        Ok(())
    }

    fn start_api_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨ä»»åŠ¡è°ƒåº¦APIæœåŠ¡å™¨");

        let bind_address = self.api_server.bind_address.clone();

        self.api_server.running.store(true, Ordering::SeqCst);

        let running = self.api_server.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTP APIæœåŠ¡å™¨
            println!("ä»»åŠ¡è°ƒåº¦APIæœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†APIè¯·æ±‚
            }
        });

        self.api_server.server = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ");

        self.running.store(false, Ordering::SeqCst);

        // åœæ­¢APIæœåŠ¡å™¨
        self.api_server.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.api_server.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("APIæœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢è°ƒåº¦å™¨
        self.scheduler.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.scheduler.scheduler_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("è°ƒåº¦å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // ä¿å­˜æ‰€æœ‰å¾…å¤„ç†ä»»åŠ¡
        self.save_tasks()?;

        Ok(())
    }

    fn save_tasks(&self) -> Result<(), String> {
        println!("ä¿å­˜å¾…å¤„ç†ä»»åŠ¡");

        // ç¡®ä¿ç›®å½•å­˜åœ¨
        let pending_dir = self.task_queue.storage.data_dir.join("pending");
        if let Err(e) = std::fs::create_dir_all(&pending_dir) {
            return Err(format!("åˆ›å»ºå¾…å¤„ç†ä»»åŠ¡ç›®å½•å¤±è´¥: {}", e));
        }

        // ä¿å­˜å¾…å¤„ç†ä»»åŠ¡
        let pending_tasks = self.task_queue.pending_tasks.read().unwrap();

        for (priority, queue) in pending_tasks.iter() {
            for task in queue {
                let task_path = pending_dir.join(format!("{}.json", task.id));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–ä»»åŠ¡ä¸ºJSON
                let json = serde_json::to_string_pretty(task)
                    .map_err(|e| format!("åºåˆ—åŒ–ä»»åŠ¡å¤±è´¥: {}", e))?;

                if let Err(e) = std::fs::write(&task_path, json) {
                    println!("ä¿å­˜ä»»åŠ¡ {} å¤±è´¥: {}", task.id, e);
                }
            }
        }

        Ok(())
    }

    fn enqueue_task(&self, task: Task) -> Result<String, String> {
        println!("å°†ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—: {}", task.name);

        // ä¸ºä»»åŠ¡åˆ†é…IDï¼ˆå¦‚æœå°šæœªåˆ†é…ï¼‰
        let task_id = if task.id.is_empty() {
            uuid::Uuid::new_v4().to_string()
        } else {
            task.id.clone()
        };

        let mut task = task;
        task.id = task_id.clone();

        // æ£€æŸ¥ä¾èµ–é¡¹
        for dep_id in &task.dependencies {
            let completed_tasks = self.task_queue.completed_tasks.read().unwrap();

            if !completed_tasks.contains_key(dep_id) {
                // æ£€æŸ¥ä¾èµ–é¡¹æ˜¯å¦åœ¨å¾…å¤„ç†é˜Ÿåˆ—ä¸­
                let pending_tasks = self.task_queue.pending_tasks.read().unwrap();

                let mut found = false;
                for (_, queue) in pending_tasks.iter() {
                    if queue.iter().any(|t| t.id == *dep_id) {
                        found = true;
                        break;
                    }
                }

                if !found {
                    return Err(format!("ä¾èµ–ä»»åŠ¡ä¸å­˜åœ¨: {}", dep_id));
                }
            }
        }

        // å°†ä»»åŠ¡æ·»åŠ åˆ°é˜Ÿåˆ—
        let mut pending_tasks = self.task_queue.pending_tasks.write().unwrap();
        let queue = pending_tasks.entry(task.priority).or_insert_with(VecDeque::new);
        queue.push_back(task.clone());

        // ä¿å­˜ä»»åŠ¡åˆ°ç£ç›˜
        let pending_dir = self.task_queue.storage.data_dir.join("pending");
        if let Err(e) = std::fs::create_dir_all(&pending_dir) {
            return Err(format!("åˆ›å»ºå¾…å¤„ç†ä»»åŠ¡ç›®å½•å¤±è´¥: {}", e));
        }

        let task_path = pending_dir.join(format!("{}.json", task_id));

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–ä»»åŠ¡ä¸ºJSON
        let json = serde_json::to_string_pretty(&task)
            .map_err(|e| format!("åºåˆ—åŒ–ä»»åŠ¡å¤±è´¥: {}", e))?;

        if let Err(e) = std::fs::write(&task_path, json) {
            return Err(format!("ä¿å­˜ä»»åŠ¡å¤±è´¥: {}", e));
        }

        Ok(task_id)
    }

    fn get_task_status(&self, task_id: &str) -> Result<TaskStatus, String> {
        // æ£€æŸ¥è¿è¡Œä¸­çš„ä»»åŠ¡
        let running_tasks = self.task_executor.running_tasks.read().unwrap();

        if let Some(running) = running_tasks.get(task_id) {
            return Ok(TaskStatus::Running {
                worker_id: running.worker_id.clone(),
                started_at: running.started_at,
                progress: running.progress,
            });
        }

        // æ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
        let completed_tasks = self.task_queue.completed_tasks.read().unwrap();

        if let Some(completed) = completed_tasks.get(task_id) {
            return Ok(TaskStatus::Completed {
                completed_at: completed.completed_at,
                execution_time: completed.execution_time,
                result: completed.result.clone(),
            });
        }

        // æ£€æŸ¥å¤±è´¥çš„ä»»åŠ¡
        let failed_tasks = self.task_queue.failed_tasks.read().unwrap();

        if let Some(failed) = failed_tasks.get(task_id) {
            return Ok(TaskStatus::Failed {
                failed_at: failed.failed_at,
                error: failed.error.clone(),
                attempts: failed.attempts,
                next_retry: failed.next_retry,
            });
        }

        // æ£€æŸ¥å¾…å¤„ç†çš„ä»»åŠ¡
        let pending_tasks = self.task_queue.pending_tasks.read().unwrap();

        for (_, queue) in pending_tasks.iter() {
            for task in queue {
                if task.id == task_id {
                    return Ok(TaskStatus::Pending {
                        dependencies_met: task.dependencies.is_empty(),
                        scheduled_at: task.scheduled_at,
                        priority: task.priority.0,
                    });
                }
            }
        }

        Err(format!("ä»»åŠ¡ä¸å­˜åœ¨: {}", task_id))
    }
}

enum TaskStatus {
    Pending {
        dependencies_met: bool,
        scheduled_at: Option<DateTime<Utc>>,
        priority: u8,
    },
    Running {
        worker_id: String,
        started_at: DateTime<Utc>,
        progress: f32,
    },
    Completed {
        completed_at: DateTime<Utc>,
        execution_time: Duration,
        result: TaskResult,
    },
    Failed {
        failed_at: DateTime<Utc>,
        error: String,
        attempts: u32,
        next_retry: Option<DateTime<Utc>>,
    },
}

// å®Œå–„mainå‡½æ•°ï¼Œæ·»åŠ ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿæµ‹è¯•
fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("å¯åŠ¨åˆ†å¸ƒå¼ç³»ç»Ÿç¤ºä¾‹");

    let temp_dir = std::env::temp_dir().join("distributed_systems_demo");

    if temp_dir.exists() {
        std::fs::remove_dir_all(&temp_dir)?;
    }
    std::fs::create_dir_all(&temp_dir)?;

    println!("ä½¿ç”¨ä¸´æ—¶ç›®å½•: {:?}", temp_dir);

    // æµ‹è¯•åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ
    {
        println!("\n===== æµ‹è¯•åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ =====");

        let data_dir = temp_dir.join("task_scheduler");
        std::fs::create_dir_all(&data_dir)?;

        let mut task_scheduler = DistributedTaskScheduler::new("node1", &data_dir, "127.0.0.1:8070");

        task_scheduler.start()?;

        // æäº¤ä¸€äº›ä»»åŠ¡
        let now = Utc::now();

        let task1 = Task {
            id: String::new(), // è‡ªåŠ¨ç”ŸæˆID
            name: "æ•°æ®å¤„ç†ä»»åŠ¡".to_string(),
            task_type: "data_processing".to_string(),
            parameters: [
                ("input_file".to_string(), "data1.csv".to_string()),
                ("output_format".to_string(), "json".to_string()),
            ].iter().cloned().collect(),
            priority: TaskPriority::high(),
            dependencies: Vec::new(),
            retry_policy: RetryPolicy {
                max_retries: 3,
                retry_delay: Duration::from_secs(30),
                backoff_factor: 2.0,
                max_delay: Duration::from_secs(300),
            },
            timeout: Duration::from_secs(600),
            created_at: now,
            scheduled_at: None,
            created_by: "admin".to_string(),
            resources: [
                ("cpu".to_string(), 1.0),
                ("memory".to_string(), 512.0),
            ].iter().cloned().collect(),
            required_capabilities: ["data_processing".to_string()].iter().cloned().collect(),
        };

        let task2 = Task {
            id: String::new(), // è‡ªåŠ¨ç”ŸæˆID
            name: "æŠ¥å‘Šç”Ÿæˆä»»åŠ¡".to_string(),
            task_type: "report_generation".to_string(),
            parameters: [
                ("template".to_string(), "monthly_report.html".to_string()),
                ("output_format".to_string(), "pdf".to_string()),
            ].iter().cloned().collect(),
            priority: TaskPriority::normal(),
            dependencies: Vec::new(), // åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™å¯èƒ½ä¾èµ–äºtask1
            retry_policy: RetryPolicy {
                max_retries: 2,
                retry_delay: Duration::from_secs(60),
                backoff_factor: 1.5,
                max_delay: Duration::from_secs(300),
            },
            timeout: Duration::from_secs(300),
            created_at: now,
            scheduled_at: Some(now + Duration::from_secs(3600).into()), // 1å°æ—¶åè°ƒåº¦
            created_by: "admin".to_string(),
            resources: [
                ("cpu".to_string(), 2.0),
                ("memory".to_string(), 1024.0),
            ].iter().cloned().collect(),
            required_capabilities: ["computation".to_string()].iter().cloned().collect(),
        };

        let task1_id = task_scheduler.enqueue_task(task1)?;
        let task2_id = task_scheduler.enqueue_task(task2)?;

        println!("æäº¤çš„ä»»åŠ¡ID: {}, {}", task1_id, task2_id);

        // ç­‰å¾…ä»»åŠ¡å¤„ç†
        thread::sleep(Duration::from_secs(2));

        // æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        let status1 = task_scheduler.get_task_status(&task1_id)?;

        match status1 {
            TaskStatus::Pending { .. } => println!("ä»»åŠ¡1çŠ¶æ€: å¾…å¤„ç†"),
            TaskStatus::Running { worker_id, .. } => println!("ä»»åŠ¡1çŠ¶æ€: è¿è¡Œä¸­ï¼Œå·¥ä½œè€…: {}", worker_id),
            TaskStatus::Completed { .. } => println!("ä»»åŠ¡1çŠ¶æ€: å·²å®Œæˆ"),
            TaskStatus::Failed { error, .. } => println!("ä»»åŠ¡1çŠ¶æ€: å¤±è´¥ï¼Œé”™è¯¯: {}", error),
        }

        // å†ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè®©ä¸€äº›ä»»åŠ¡å®Œæˆ
        thread::sleep(Duration::from_secs(6));

        // åœæ­¢ä»»åŠ¡è°ƒåº¦å™¨
        task_scheduler.stop()?;
    }

    println!("\næ‰€æœ‰åˆ†å¸ƒå¼ç³»ç»Ÿæµ‹è¯•å®Œæˆ");

    // æ¸…ç†ä¸´æ—¶ç›®å½•
    std::fs::remove_dir_all(&temp_dir)?;

    Ok(())
}
```

### 1.17 ç»¼åˆåº”ç”¨17-åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ

```rust
// åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ
struct DistributedStreamProcessor {
    node_id: String,
    data_dir: PathBuf,
    source_connectors: RwLock<HashMap<String, SourceConnector>>,
    sink_connectors: RwLock<HashMap<String, SinkConnector>>,
    processors: RwLock<HashMap<String, StreamProcessor>>,
    streams: RwLock<HashMap<String, StreamDefinition>>,
    stream_engine: StreamEngine,
    checkpoint_manager: CheckpointManager,
    topology_manager: TopologyManager,
    state_store: StateStore,
    metrics_collector: StreamMetricsCollector,
    server: Option<JoinHandle<()>>,
    running: AtomicBool,
    bind_address: String,
}

struct SourceConnector {
    id: String,
    connector_type: String,
    config: HashMap<String, String>,
    status: ConnectorStatus,
    thread: Option<JoinHandle<()>>,
    running: AtomicBool,
    schema: Option<StreamSchema>,
    metrics: SourceMetrics,
}

struct SourceMetrics {
    records_read: AtomicU64,
    bytes_read: AtomicU64,
    last_record_time: AtomicI64,
    errors: AtomicU64,
}

struct SinkConnector {
    id: String,
    connector_type: String,
    config: HashMap<String, String>,
    status: ConnectorStatus,
    thread: Option<JoinHandle<()>>,
    running: AtomicBool,
    schema: Option<StreamSchema>,
    metrics: SinkMetrics,
}

struct SinkMetrics {
    records_written: AtomicU64,
    bytes_written: AtomicU64,
    last_record_time: AtomicI64,
    errors: AtomicU64,
}

enum ConnectorStatus {
    Running,
    Failed(String),
    Stopped,
    Paused,
}

struct StreamProcessor {
    id: String,
    processor_type: String,
    config: HashMap<String, String>,
    status: ProcessorStatus,
    threads: Vec<JoinHandle<()>>,
    running: AtomicBool,
    parallelism: usize,
    metrics: ProcessorMetrics,
}

struct ProcessorMetrics {
    records_processed: AtomicU64,
    processing_time: AtomicU64,
    last_processed_time: AtomicI64,
    errors: AtomicU64,
}

enum ProcessorStatus {
    Running,
    Failed(String),
    Stopped,
    Scaling,
}

struct StreamDefinition {
    id: String,
    name: String,
    schema: StreamSchema,
    partitioning: PartitioningStrategy,
    retention: RetentionPolicy,
    created_at: DateTime<Utc>,
    created_by: String,
}

struct StreamSchema {
    fields: Vec<FieldDefinition>,
    key_fields: Vec<String>,
    timestamp_field: Option<String>,
}

struct FieldDefinition {
    name: String,
    field_type: FieldType,
    nullable: bool,
    default_value: Option<String>,
    description: Option<String>,
}

enum FieldType {
    String,
    Integer,
    Long,
    Float,
    Double,
    Boolean,
    Timestamp,
    Bytes,
    Array(Box<FieldType>),
    Map(Box<FieldType>),
    Struct(Vec<FieldDefinition>),
}

enum PartitioningStrategy {
    Hash { fields: Vec<String>, partitions: usize },
    Range { field: String, boundaries: Vec<String> },
    Random { partitions: usize },
}

struct RetentionPolicy {
    time_based: Option<Duration>,
    size_based: Option<u64>,
}

struct StreamEngine {
    executors: Vec<StreamExecutor>,
    execution_graph: DirectedGraph,
    executor_pool: ThreadPool,
    running: AtomicBool,
    coordinator_thread: Option<JoinHandle<()>>,
}

struct StreamExecutor {
    id: String,
    thread: Option<JoinHandle<()>>,
    running: AtomicBool,
    input_queues: Vec<StreamQueue>,
    output_queues: Vec<StreamQueue>,
    metrics: ExecutorMetrics,
}

struct ExecutorMetrics {
    records_in: AtomicU64,
    records_out: AtomicU64,
    processing_time: AtomicU64,
    backpressure_time: AtomicU64,
    last_processed_time: AtomicI64,
}

struct StreamQueue {
    id: String,
    capacity: usize,
    queue: Arc<Mutex<VecDeque<Record>>>,
    metrics: QueueMetrics,
}

struct QueueMetrics {
    enqueued: AtomicU64,
    dequeued: AtomicU64,
    queue_time: AtomicU64,
    high_watermark: AtomicUsize,
}

struct Record {
    id: String,
    stream_id: String,
    partition: u32,
    key: Vec<u8>,
    value: Vec<u8>,
    timestamp: i64,
    headers: HashMap<String, Vec<u8>>,
}

struct DirectedGraph {
    nodes: HashMap<String, ProcessorNode>,
    edges: Vec<Edge>,
}

struct ProcessorNode {
    id: String,
    node_type: NodeType,
    config: HashMap<String, String>,
}

enum NodeType {
    Source(String),
    Processor(String),
    Sink(String),
}

struct Edge {
    from: String,
    to: String,
    stream_id: String,
    partitioning: EdgePartitioning,
}

enum EdgePartitioning {
    Forward,
    Key,
    Shuffle,
    Broadcast,
}

struct CheckpointManager {
    checkpoint_dir: PathBuf,
    checkpoint_interval: Duration,
    max_checkpoints: usize,
    checkpoints: RwLock<HashMap<String, Checkpoint>>,
    checkpoint_thread: Option<JoinHandle<()>>,
    running: AtomicBool,
}

struct Checkpoint {
    id: String,
    timestamp: DateTime<Utc>,
    processor_states: HashMap<String, Vec<u8>>,
    source_positions: HashMap<String, SourcePosition>,
    completed: bool,
}

struct SourcePosition {
    connector_id: String,
    position: Vec<u8>,
}

struct TopologyManager {
    topologies: RwLock<HashMap<String, Topology>>,
}

struct Topology {
    id: String,
    name: String,
    description: String,
    nodes: Vec<TopologyNode>,
    edges: Vec<TopologyEdge>,
    created_at: DateTime<Utc>,
    updated_at: DateTime<Utc>,
    version: u64,
    status: TopologyStatus,
}

struct TopologyNode {
    id: String,
    node_type: NodeType,
    config: HashMap<String, String>,
    parallelism: usize,
}

struct TopologyEdge {
    from: String,
    to: String,
    stream_id: String,
    partitioning: EdgePartitioning,
}

enum TopologyStatus {
    Created,
    Running,
    Paused,
    Failed(String),
    Stopped,
}

struct StateStore {
    store_dir: PathBuf,
    databases: RwLock<HashMap<String, KeyValueDatabase>>,
}

struct KeyValueDatabase {
    name: String,
    entries: RwLock<HashMap<Vec<u8>, Vec<u8>>>,
}

struct StreamMetricsCollector {
    metrics: RwLock<HashMap<String, Metric>>,
    collection_interval: Duration,
    collection_thread: Option<JoinHandle<()>>,
    running: AtomicBool,
}

struct Metric {
    name: String,
    value: f64,
    timestamp: i64,
    labels: HashMap<String, String>,
}

struct AtomicU64(AtomicUsize);

impl AtomicU64 {
    fn new(val: u64) -> Self {
        AtomicU64(AtomicUsize::new(val as usize))
    }

    fn load(&self) -> u64 {
        self.0.load(Ordering::SeqCst) as u64
    }

    fn store(&self, val: u64) {
        self.0.store(val as usize, Ordering::SeqCst);
    }

    fn fetch_add(&self, val: u64) -> u64 {
        self.0.fetch_add(val as usize, Ordering::SeqCst) as u64
    }
}

struct AtomicI64(AtomicIsize);

impl AtomicI64 {
    fn new(val: i64) -> Self {
        AtomicI64(AtomicIsize::new(val as isize))
    }

    fn load(&self) -> i64 {
        self.0.load(Ordering::SeqCst) as i64
    }

    fn store(&self, val: i64) {
        self.0.store(val as isize, Ordering::SeqCst);
    }
}

impl DistributedStreamProcessor {
    fn new(node_id: &str, data_dir: &Path, bind_address: &str) -> Self {
        let stream_engine = StreamEngine {
            executors: Vec::new(),
            execution_graph: DirectedGraph {
                nodes: HashMap::new(),
                edges: Vec::new(),
            },
            executor_pool: ThreadPool::new(4),
            running: AtomicBool::new(false),
            coordinator_thread: None,
        };

        let checkpoint_manager = CheckpointManager {
            checkpoint_dir: data_dir.join("checkpoints"),
            checkpoint_interval: Duration::from_secs(60),
            max_checkpoints: 5,
            checkpoints: RwLock::new(HashMap::new()),
            checkpoint_thread: None,
            running: AtomicBool::new(false),
        };

        let topology_manager = TopologyManager {
            topologies: RwLock::new(HashMap::new()),
        };

        let state_store = StateStore {
            store_dir: data_dir.join("state"),
            databases: RwLock::new(HashMap::new()),
        };

        let metrics_collector = StreamMetricsCollector {
            metrics: RwLock::new(HashMap::new()),
            collection_interval: Duration::from_secs(10),
            collection_thread: None,
            running: AtomicBool::new(false),
        };

        DistributedStreamProcessor {
            node_id: node_id.to_string(),
            data_dir: data_dir.to_path_buf(),
            source_connectors: RwLock::new(HashMap::new()),
            sink_connectors: RwLock::new(HashMap::new()),
            processors: RwLock::new(HashMap::new()),
            streams: RwLock::new(HashMap::new()),
            stream_engine,
            checkpoint_manager,
            topology_manager,
            state_store,
            metrics_collector,
            server: None,
            running: AtomicBool::new(false),
            bind_address: bind_address.to_string(),
        }
    }

    fn start(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ");

        // åˆ›å»ºå¿…è¦çš„ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.data_dir) {
            return Err(format!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {}", e));
        }

        if let Err(e) = std::fs::create_dir_all(&self.checkpoint_manager.checkpoint_dir) {
            return Err(format!("åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•å¤±è´¥: {}", e));
        }

        if let Err(e) = std::fs::create_dir_all(&self.state_store.store_dir) {
            return Err(format!("åˆ›å»ºçŠ¶æ€å­˜å‚¨ç›®å½•å¤±è´¥: {}", e));
        }

        // åŠ è½½æ‹“æ‰‘å®šä¹‰
        self.load_topologies()?;

        // åŠ è½½æµå®šä¹‰
        self.load_stream_definitions()?;

        // åˆå§‹åŒ–çŠ¶æ€å­˜å‚¨
        self.initialize_state_store()?;

        // å¯åŠ¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.start_checkpoint_manager()?;

        // å¯åŠ¨æŒ‡æ ‡æ”¶é›†å™¨
        self.start_metrics_collector()?;

        // å¯åŠ¨æµå¼•æ“
        self.start_stream_engine()?;

        // å¯åŠ¨æœåŠ¡å™¨
        self.start_server()?;

        self.running.store(true, Ordering::SeqCst);

        Ok(())
    }

    fn load_topologies(&self) -> Result<(), String> {
        println!("åŠ è½½æ‹“æ‰‘å®šä¹‰");

        let topologies_dir = self.data_dir.join("topologies");

        if !topologies_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&topologies_dir) {
                return Err(format!("åˆ›å»ºæ‹“æ‰‘ç›®å½•å¤±è´¥: {}", e));
            }
            return Ok(());
        }

        match std::fs::read_dir(&topologies_dir) {
            Ok(entries) => {
                for entry in entries {
                    if let Ok(entry) = entry {
                        let path = entry.path();

                        if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                            match self.load_topology(&path) {
                                Ok(topology) => {
                                    let mut topologies = self.topology_manager.topologies.write().unwrap();
                                    topologies.insert(topology.id.clone(), topology);
                                },
                                Err(e) => println!("åŠ è½½æ‹“æ‰‘å¤±è´¥: {} - {}", path.display(), e),
                            }
                        }
                    }
                }
            },
            Err(e) => return Err(format!("è¯»å–æ‹“æ‰‘ç›®å½•å¤±è´¥: {}", e)),
        }

        Ok(())
    }

    fn load_topology(&self, path: &Path) -> Result<Topology, String> {
        let content = match std::fs::read_to_string(path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–æ‹“æ‰‘æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£ææ‹“æ‰‘æ–‡ä»¶: {}", path.display());

        // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæ‹“æ‰‘
        let now = Utc::now();

        let topology = Topology {
            id: uuid::Uuid::new_v4().to_string(),
            name: "ç¤ºä¾‹æ‹“æ‰‘".to_string(),
            description: "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ•°æ®æµæ‹“æ‰‘".to_string(),
            nodes: vec![
                TopologyNode {
                    id: "source1".to_string(),
                    node_type: NodeType::Source("kafka".to_string()),
                    config: [
                        ("bootstrap.servers".to_string(), "localhost:9092".to_string()),
                        ("topic".to_string(), "input-topic".to_string()),
                        ("group.id".to_string(), "stream-processor".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 2,
                },
                TopologyNode {
                    id: "processor1".to_string(),
                    node_type: NodeType::Processor("filter".to_string()),
                    config: [
                        ("condition".to_string(), "value.temperature > 25".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 4,
                },
                TopologyNode {
                    id: "processor2".to_string(),
                    node_type: NodeType::Processor("transform".to_string()),
                    config: [
                        ("expression".to_string(), "value.temperature * 1.8 + 32".to_string()),
                        ("target_field".to_string(), "temperature_f".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 4,
                },
                TopologyNode {
                    id: "sink1".to_string(),
                    node_type: NodeType::Sink("elasticsearch".to_string()),
                    config: [
                        ("hosts".to_string(), "localhost:9200".to_string()),
                        ("index".to_string(), "temperatures".to_string()),
                        ("document_id".to_string(), "${id}".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 2,
                },
            ],
            edges: vec![
                TopologyEdge {
                    from: "source1".to_string(),
                    to: "processor1".to_string(),
                    stream_id: "raw_temperature".to_string(),
                    partitioning: EdgePartitioning::Key,
                },
                TopologyEdge {
                    from: "processor1".to_string(),
                    to: "processor2".to_string(),
                    stream_id: "filtered_temperature".to_string(),
                    partitioning: EdgePartitioning::Forward,
                },
                TopologyEdge {
                    from: "processor2".to_string(),
                    to: "sink1".to_string(),
                    stream_id: "processed_temperature".to_string(),
                    partitioning: EdgePartitioning::Key,
                },
            ],
            created_at: now,
            updated_at: now,
            version: 1,
            status: TopologyStatus::Created,
        };

        Ok(topology)
    }

    fn load_stream_definitions(&self) -> Result<(), String> {
        println!("åŠ è½½æµå®šä¹‰");

        let streams_dir = self.data_dir.join("streams");

        if !streams_dir.exists() {
            if let Err(e) = std::fs::create_dir_all(&streams_dir) {
                return Err(format!("åˆ›å»ºæµç›®å½•å¤±è´¥: {}", e));
            }
            return Ok(());
        }

        match std::fs::read_dir(&streams_dir) {
            Ok(entries) => {
                for entry in entries {
                    if let Ok(entry) = entry {
                        let path = entry.path();

                        if path.is_file() && path.extension().map_or(false, |ext| ext == "json") {
                            match self.load_stream_definition(&path) {
                                Ok(stream) => {
                                    let mut streams = self.streams.write().unwrap();
                                    streams.insert(stream.id.clone(), stream);
                                },
                                Err(e) => println!("åŠ è½½æµå®šä¹‰å¤±è´¥: {} - {}", path.display(), e),
                            }
                        }
                    }
                }
            },
            Err(e) => return Err(format!("è¯»å–æµç›®å½•å¤±è´¥: {}", e)),
        }

        // åˆ›å»ºç¤ºä¾‹æµå®šä¹‰
        if self.streams.read().unwrap().is_empty() {
            self.create_example_streams()?;
        }

        Ok(())
    }

    fn load_stream_definition(&self, path: &Path) -> Result<StreamDefinition, String> {
        let content = match std::fs::read_to_string(path) {
            Ok(content) => content,
            Err(e) => return Err(format!("è¯»å–æµå®šä¹‰æ–‡ä»¶å¤±è´¥: {}", e)),
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè§£æJSON
        println!("è§£ææµå®šä¹‰æ–‡ä»¶: {}", path.display());

        // åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿæµå®šä¹‰
        let now = Utc::now();

        let stream = StreamDefinition {
            id: uuid::Uuid::new_v4().to_string(),
            name: "ç¤ºä¾‹æµ".to_string(),
            schema: StreamSchema {
                fields: vec![
                    FieldDefinition {
                        name: "id".to_string(),
                        field_type: FieldType::String,
                        nullable: false,
                        default_value: None,
                        description: Some("è®°å½•ID".to_string()),
                    },
                    FieldDefinition {
                        name: "timestamp".to_string(),
                        field_type: FieldType::Timestamp,
                        nullable: false,
                        default_value: None,
                        description: Some("è®°å½•æ—¶é—´æˆ³".to_string()),
                    },
                    FieldDefinition {
                        name: "temperature".to_string(),
                        field_type: FieldType::Double,
                        nullable: false,
                        default_value: None,
                        description: Some("æ¸©åº¦ï¼ˆæ‘„æ°åº¦ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "location".to_string(),
                        field_type: FieldType::String,
                        nullable: true,
                        default_value: None,
                        description: Some("ä½ç½®".to_string()),
                    },
                ],
                key_fields: vec!["id".to_string()],
                timestamp_field: Some("timestamp".to_string()),
            },
            partitioning: PartitioningStrategy::Hash {
                fields: vec!["id".to_string()],
                partitions: 8,
            },
            retention: RetentionPolicy {
                time_based: Some(Duration::from_secs(86400 * 7)), // 7å¤©
                size_based: Some(1024 * 1024 * 1024 * 10), // 10GB
            },
            created_at: now,
            created_by: "system".to_string(),
        };

        Ok(stream)
    }

    fn create_example_streams(&self) -> Result<(), String> {
        println!("åˆ›å»ºç¤ºä¾‹æµå®šä¹‰");

        let now = Utc::now();

        // åŸå§‹æ¸©åº¦æµ
        let raw_temperature = StreamDefinition {
            id: "raw_temperature".to_string(),
            name: "åŸå§‹æ¸©åº¦æ•°æ®".to_string(),
            schema: StreamSchema {
                fields: vec![
                    FieldDefinition {
                        name: "id".to_string(),
                        field_type: FieldType::String,
                        nullable: false,
                        default_value: None,
                        description: Some("ä¼ æ„Ÿå™¨ID".to_string()),
                    },
                    FieldDefinition {
                        name: "timestamp".to_string(),
                        field_type: FieldType::Timestamp,
                        nullable: false,
                        default_value: None,
                        description: Some("è®°å½•æ—¶é—´æˆ³".to_string()),
                    },
                    FieldDefinition {
                        name: "temperature".to_string(),
                        field_type: FieldType::Double,
                        nullable: false,
                        default_value: None,
                        description: Some("æ¸©åº¦ï¼ˆæ‘„æ°åº¦ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "humidity".to_string(),
                        field_type: FieldType::Double,
                        nullable: true,
                        default_value: None,
                        description: Some("æ¹¿åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "location".to_string(),
                        field_type: FieldType::String,
                        nullable: true,
                        default_value: None,
                        description: Some("ä½ç½®".to_string()),
                    },
                ],
                key_fields: vec!["id".to_string()],
                timestamp_field: Some("timestamp".to_string()),
            },
            partitioning: PartitioningStrategy::Hash {
                fields: vec!["id".to_string()],
                partitions: 8,
            },
            retention: RetentionPolicy {
                time_based: Some(Duration::from_secs(86400 * 3)), // 3å¤©
                size_based: Some(1024 * 1024 * 1024 * 5), // 5GB
            },
            created_at: now,
            created_by: "system".to_string(),
        };

        // è¿‡æ»¤åçš„æ¸©åº¦æµ
        let filtered_temperature = StreamDefinition {
            id: "filtered_temperature".to_string(),
            name: "è¿‡æ»¤åçš„æ¸©åº¦æ•°æ®".to_string(),
            schema: StreamSchema {
                fields: vec![
                    FieldDefinition {
                        name: "id".to_string(),
                        field_type: FieldType::String,
                        nullable: false,
                        default_value: None,
                        description: Some("ä¼ æ„Ÿå™¨ID".to_string()),
                    },
                    FieldDefinition {
                        name: "timestamp".to_string(),
                        field_type: FieldType::Timestamp,
                        nullable: false,
                        default_value: None,
                        description: Some("è®°å½•æ—¶é—´æˆ³".to_string()),
                    },
                    FieldDefinition {
                        name: "temperature".to_string(),
                        field_type: FieldType::Double,
                        nullable: false,
                        default_value: None,
                        description: Some("æ¸©åº¦ï¼ˆæ‘„æ°åº¦ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "humidity".to_string(),
                        field_type: FieldType::Double,
                        nullable: true,
                        default_value: None,
                        description: Some("æ¹¿åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "location".to_string(),
                        field_type: FieldType::String,
                        nullable: true,
                        default_value: None,
                        description: Some("ä½ç½®".to_string()),
                    },
                ],
                key_fields: vec!["id".to_string()],
                timestamp_field: Some("timestamp".to_string()),
            },
            partitioning: PartitioningStrategy::Hash {
                fields: vec!["id".to_string()],
                partitions: 8,
            },
            retention: RetentionPolicy {
                time_based: Some(Duration::from_secs(86400 * 3)), // 3å¤©
                size_based: Some(1024 * 1024 * 1024 * 3), // 3GB
            },
            created_at: now,
            created_by: "system".to_string(),
        };

        // å¤„ç†åçš„æ¸©åº¦æµ
        let processed_temperature = StreamDefinition {
            id: "processed_temperature".to_string(),
            name: "å¤„ç†åçš„æ¸©åº¦æ•°æ®".to_string(),
            schema: StreamSchema {
                fields: vec![
                    FieldDefinition {
                        name: "id".to_string(),
                        field_type: FieldType::String,
                        nullable: false,
                        default_value: None,
                        description: Some("ä¼ æ„Ÿå™¨ID".to_string()),
                    },
                    FieldDefinition {
                        name: "timestamp".to_string(),
                        field_type: FieldType::Timestamp,
                        nullable: false,
                        default_value: None,
                        description: Some("è®°å½•æ—¶é—´æˆ³".to_string()),
                    },
                    FieldDefinition {
                        name: "temperature".to_string(),
                        field_type: FieldType::Double,
                        nullable: false,
                        default_value: None,
                        description: Some("æ¸©åº¦ï¼ˆæ‘„æ°åº¦ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "temperature_f".to_string(),
                        field_type: FieldType::Double,
                        nullable: false,
                        default_value: None,
                        description: Some("æ¸©åº¦ï¼ˆåæ°åº¦ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "humidity".to_string(),
                        field_type: FieldType::Double,
                        nullable: true,
                        default_value: None,
                        description: Some("æ¹¿åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰".to_string()),
                    },
                    FieldDefinition {
                        name: "location".to_string(),
                        field_type: FieldType::String,
                        nullable: true,
                        default_value: None,
                        description: Some("ä½ç½®".to_string()),
                    },
                ],
                key_fields: vec!["id".to_string()],
                timestamp_field: Some("timestamp".to_string()),
            },
            partitioning: PartitioningStrategy::Hash {
                fields: vec!["id".to_string()],
                partitions: 8,
            },
            retention: RetentionPolicy {
                time_based: Some(Duration::from_secs(86400 * 7)), // 7å¤©
                size_based: Some(1024 * 1024 * 1024 * 5), // 5GB
            },
            created_at: now,
            created_by: "system".to_string(),
        };

        let mut streams = self.streams.write().unwrap();
        streams.insert(raw_temperature.id.clone(), raw_temperature);
        streams.insert(filtered_temperature.id.clone(), filtered_temperature);
        streams.insert(processed_temperature.id.clone(), processed_temperature);

        // ä¿å­˜æµå®šä¹‰
        self.save_stream_definitions()?;

        Ok(())
    }

    fn save_stream_definitions(&self) -> Result<(), String> {
        println!("ä¿å­˜æµå®šä¹‰");

        let streams_dir = self.data_dir.join("streams");

        if let Err(e) = std::fs::create_dir_all(&streams_dir) {
            return Err(format!("åˆ›å»ºæµç›®å½•å¤±è´¥: {}", e));
        }

        let streams = self.streams.read().unwrap();

        for (id, stream) in streams.iter() {
            let stream_path = streams_dir.join(format!("{}.json", id));

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–ä¸ºJSON
            let json = serde_json::to_string_pretty(stream)
                .map_err(|e| format!("åºåˆ—åŒ–æµå®šä¹‰å¤±è´¥: {}", e))?;

            if let Err(e) = std::fs::write(&stream_path, json) {
                println!("ä¿å­˜æµå®šä¹‰ {} å¤±è´¥: {}", id, e);
            }
        }

        Ok(())
    }

    fn initialize_state_store(&self) -> Result<(), String> {
        println!("åˆå§‹åŒ–çŠ¶æ€å­˜å‚¨");

        // åˆ›å»ºçŠ¶æ€å­˜å‚¨ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.state_store.store_dir) {
            return Err(format!("åˆ›å»ºçŠ¶æ€å­˜å‚¨ç›®å½•å¤±è´¥: {}", e));
        }

        // åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®åº“
        let mut databases = self.state_store.databases.write().unwrap();

        // æ¸©åº¦ç»Ÿè®¡æ•°æ®åº“
        let temperature_stats = KeyValueDatabase {
            name: "temperature_stats".to_string(),
            entries: RwLock::new(HashMap::new()),
        };

        // ä¼ æ„Ÿå™¨å…ƒæ•°æ®æ•°æ®åº“
        let sensor_metadata = KeyValueDatabase {
            name: "sensor_metadata".to_string(),
            entries: RwLock::new(HashMap::new()),
        };

        databases.insert("temperature_stats".to_string(), temperature_stats);
        databases.insert("sensor_metadata".to_string(), sensor_metadata);

        Ok(())
    }

    fn start_checkpoint_manager(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨");

        // åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        if let Err(e) = std::fs::create_dir_all(&self.checkpoint_manager.checkpoint_dir) {
            return Err(format!("åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•å¤±è´¥: {}", e));
        }

        let checkpoint_dir = self.checkpoint_manager.checkpoint_dir.clone();
        let interval = self.checkpoint_manager.checkpoint_interval;
        let checkpoints = self.checkpoint_manager.checkpoints.clone();
        let source_connectors = self.source_connectors.clone();
        let processors = self.processors.clone();

        self.checkpoint_manager.running.store(true, Ordering::SeqCst);

        let running = self.checkpoint_manager.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // åˆ›å»ºæ–°æ£€æŸ¥ç‚¹
                let checkpoint_id = uuid::Uuid::new_v4().to_string();
                let now = Utc::now();

                println!("åˆ›å»ºæ£€æŸ¥ç‚¹: {}", checkpoint_id);

                // æ”¶é›†å¤„ç†å™¨çŠ¶æ€
                let mut processor_states = HashMap::new();
                let processors_guard = processors.read().unwrap();

                for (id, processor) in processors_guard.iter() {
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè·å–å¤„ç†å™¨çš„çŠ¶æ€
                    processor_states.insert(id.clone(), Vec::new());
                }

                drop(processors_guard);

                // æ”¶é›†æºè¿æ¥å™¨ä½ç½®
                let mut source_positions = HashMap::new();
                let sources_guard = source_connectors.read().unwrap();

                for (id, source) in sources_guard.iter() {
                    // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè·å–æºè¿æ¥å™¨çš„ä½ç½®
                    source_positions.insert(id.clone(), SourcePosition {
                        connector_id: id.clone(),
                        position: Vec::new(),
                    });
                }

                drop(sources_guard);

                // åˆ›å»ºæ£€æŸ¥ç‚¹
                let checkpoint = Checkpoint {
                    id: checkpoint_id.clone(),
                    timestamp: now,
                    processor_states,
                    source_positions,
                    completed: true,
                };

                // ä¿å­˜æ£€æŸ¥ç‚¹
                let mut checkpoints_guard = checkpoints.write().unwrap();
                checkpoints_guard.insert(checkpoint_id.clone(), checkpoint);

                // é™åˆ¶æ£€æŸ¥ç‚¹æ•°é‡
                if checkpoints_guard.len() > 5 {
                    // åˆ é™¤æœ€æ—§çš„æ£€æŸ¥ç‚¹
                    let oldest = checkpoints_guard.iter()
                        .min_by_key(|(_, c)| c.timestamp)
                        .map(|(id, _)| id.clone());

                    if let Some(id) = oldest {
                        checkpoints_guard.remove(&id);
                    }
                }

                drop(checkpoints_guard);

                // ä¿å­˜æ£€æŸ¥ç‚¹åˆ°ç£ç›˜
                let checkpoint_path = checkpoint_dir.join(format!("{}.json", checkpoint_id));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–æ£€æŸ¥ç‚¹ä¸ºJSON

                // ç­‰å¾…ä¸‹ä¸€ä¸ªæ£€æŸ¥ç‚¹é—´éš”
                thread::sleep(interval);
            }
        });

        self.checkpoint_manager.checkpoint_thread = Some(thread);

        Ok(())
    }

    fn start_metrics_collector(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æŒ‡æ ‡æ”¶é›†å™¨");

        let interval = self.metrics_collector.collection_interval;
        let metrics = self.metrics_collector.metrics.clone();
        let source_connectors = self.source_connectors.clone();
        let sink_connectors = self.sink_connectors.clone();
        let processors = self.processors.clone();

        self.metrics_collector.running.store(true, Ordering::SeqCst);

        let running = self.metrics_collector.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                let now = Utc::now().timestamp();

                // æ”¶é›†æºè¿æ¥å™¨æŒ‡æ ‡
                let sources_guard = source_connectors.read().unwrap();

                for (id, source) in sources_guard.iter() {
                    let mut metric_guard = metrics.write().unwrap();

                    metric_guard.insert(format!("source.{}.records_read", id), Metric {
                        name: format!("source.{}.records_read", id),
                        value: source.metrics.records_read.load() as f64,
                        timestamp: now,
                        labels: [
                            ("source".to_string(), id.clone()),
                            ("type".to_string(), source.connector_type.clone()),
                        ].iter().cloned().collect(),
                    });

                    metric_guard.insert(format!("source.{}.bytes_read", id), Metric {
                        name: format!("source.{}.bytes_read", id),
                        value: source.metrics.bytes_read.load() as f64,
                        timestamp: now,
                        labels: [
                            ("source".to_string(), id.clone()),
                            ("type".to_string(), source.connector_type.clone()),
                        ].iter().cloned().collect(),
                    });

                    metric_guard.insert(format!("source.{}.errors", id), Metric {
                        name: format!("source.{}.errors", id),
                        value: source.metrics.errors.load() as f64,
                        timestamp: now,
                        labels: [
                            ("source".to_string

```rust
                            ("source".to_string(), id.clone()),
                            ("type".to_string(), source.connector_type.clone()),
                        ].iter().cloned().collect(),
                    });
                }

                drop(sources_guard);

                // æ”¶é›†æ¥æ”¶å™¨è¿æ¥å™¨æŒ‡æ ‡
                let sinks_guard = sink_connectors.read().unwrap();

                for (id, sink) in sinks_guard.iter() {
                    let mut metric_guard = metrics.write().unwrap();

                    metric_guard.insert(format!("sink.{}.records_written", id), Metric {
                        name: format!("sink.{}.records_written", id),
                        value: sink.metrics.records_written.load() as f64,
                        timestamp: now,
                        labels: [
                            ("sink".to_string(), id.clone()),
                            ("type".to_string(), sink.connector_type.clone()),
                        ].iter().cloned().collect(),
                    });

                    metric_guard.insert(format!("sink.{}.bytes_written", id), Metric {
                        name: format!("sink.{}.bytes_written", id),
                        value: sink.metrics.bytes_written.load() as f64,
                        timestamp: now,
                        labels: [
                            ("sink".to_string(), id.clone()),
                            ("type".to_string(), sink.connector_type.clone()),
                        ].iter().cloned().collect(),
                    });

                    metric_guard.insert(format!("sink.{}.errors", id), Metric {
                        name: format!("sink.{}.errors", id),
                        value: sink.metrics.errors.load() as f64,
                        timestamp: now,
                        labels: [
                            ("sink".to_string(), id.clone()),
                            ("type".to_string(), sink.connector_type.clone()),
                        ].iter().cloned().collect(),
                    });
                }

                drop(sinks_guard);

                // æ”¶é›†å¤„ç†å™¨æŒ‡æ ‡
                let processors_guard = processors.read().unwrap();

                for (id, processor) in processors_guard.iter() {
                    let mut metric_guard = metrics.write().unwrap();

                    metric_guard.insert(format!("processor.{}.records_processed", id), Metric {
                        name: format!("processor.{}.records_processed", id),
                        value: processor.metrics.records_processed.load() as f64,
                        timestamp: now,
                        labels: [
                            ("processor".to_string(), id.clone()),
                            ("type".to_string(), processor.processor_type.clone()),
                        ].iter().cloned().collect(),
                    });

                    metric_guard.insert(format!("processor.{}.processing_time", id), Metric {
                        name: format!("processor.{}.processing_time", id),
                        value: processor.metrics.processing_time.load() as f64,
                        timestamp: now,
                        labels: [
                            ("processor".to_string(), id.clone()),
                            ("type".to_string(), processor.processor_type.clone()),
                        ].iter().cloned().collect(),
                    });

                    metric_guard.insert(format!("processor.{}.errors", id), Metric {
                        name: format!("processor.{}.errors", id),
                        value: processor.metrics.errors.load() as f64,
                        timestamp: now,
                        labels: [
                            ("processor".to_string(), id.clone()),
                            ("type".to_string(), processor.processor_type.clone()),
                        ].iter().cloned().collect(),
                    });
                }

                // ç­‰å¾…ä¸‹ä¸€ä¸ªæ”¶é›†å‘¨æœŸ
                thread::sleep(interval);
            }
        });

        self.metrics_collector.collection_thread = Some(thread);

        Ok(())
    }

    fn start_stream_engine(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æµå¤„ç†å¼•æ“");

        // åˆ›å»ºæ‰§è¡Œå›¾
        self.build_execution_graph()?;

        // å¯åŠ¨æ‰§è¡Œå™¨
        self.start_executors()?;

        // å¯åŠ¨åè°ƒå™¨
        self.start_coordinator()?;

        Ok(())
    }

    fn build_execution_graph(&self) -> Result<(), String> {
        println!("æ„å»ºæ‰§è¡Œå›¾");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»æ‹“æ‰‘å®šä¹‰æ„å»ºæ‰§è¡Œå›¾

        // è·å–æ‹“æ‰‘å®šä¹‰
        let topologies = self.topology_manager.topologies.read().unwrap();

        if topologies.is_empty() {
            println!("æ²¡æœ‰å®šä¹‰æ‹“æ‰‘ï¼Œè·³è¿‡æ„å»ºæ‰§è¡Œå›¾");
            return Ok(());
        }

        // ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‹“æ‰‘
        let topology = topologies.values().next().unwrap();

        // åˆ›å»ºå¤„ç†å™¨èŠ‚ç‚¹
        let mut nodes = HashMap::new();

        for node in &topology.nodes {
            let processor_node = ProcessorNode {
                id: node.id.clone(),
                node_type: node.node_type.clone(),
                config: node.config.clone(),
            };

            nodes.insert(node.id.clone(), processor_node);
        }

        // åˆ›å»ºè¾¹
        let edges = topology.edges.clone();

        // è®¾ç½®æ‰§è¡Œå›¾
        let execution_graph = DirectedGraph {
            nodes,
            edges,
        };

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ‰§è¡Œå›¾å­˜å‚¨åœ¨æµå¼•æ“ä¸­

        Ok(())
    }

    fn start_executors(&self) -> Result<(), String> {
        println!("å¯åŠ¨æ‰§è¡Œå™¨");

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ ¹æ®æ‰§è¡Œå›¾å¯åŠ¨æ‰§è¡Œå™¨

        // ç®€åŒ–ï¼šåˆ›å»ºä¸€äº›æ¨¡æ‹Ÿæ‰§è¡Œå™¨
        for i in 1..=4 {
            let executor_id = format!("executor-{}", i);

            let executor = StreamExecutor {
                id: executor_id.clone(),
                thread: None,
                running: AtomicBool::new(false),
                input_queues: Vec::new(),
                output_queues: Vec::new(),
                metrics: ExecutorMetrics {
                    records_in: AtomicU64::new(0),
                    records_out: AtomicU64::new(0),
                    processing_time: AtomicU64::new(0),
                    backpressure_time: AtomicU64::new(0),
                    last_processed_time: AtomicI64::new(0),
                },
            };

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ‰§è¡Œå™¨æ·»åŠ åˆ°æµå¼•æ“
        }

        Ok(())
    }

    fn start_coordinator(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨åè°ƒå™¨");

        self.stream_engine.running.store(true, Ordering::SeqCst);

        let running = self.stream_engine.running.clone();

        let thread = thread::spawn(move || {
            while running.load(Ordering::SeqCst) {
                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåè°ƒæ‰§è¡Œå™¨ã€ç›‘æ§è¿›åº¦å’Œå¤„ç†æ•…éšœ

                // ä¼‘çœ ä¸€æ®µæ—¶é—´
                thread::sleep(Duration::from_millis(100));
            }
        });

        self.stream_engine.coordinator_thread = Some(thread);

        Ok(())
    }

    fn start_server(&mut self) -> Result<(), String> {
        println!("å¯åŠ¨æµå¤„ç†æœåŠ¡å™¨");

        let bind_address = self.bind_address.clone();

        self.running.store(true, Ordering::SeqCst);

        let running = self.running.clone();

        let thread = thread::spawn(move || {
            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨ä¸€ä¸ªHTTP APIæœåŠ¡å™¨
            println!("æµå¤„ç†æœåŠ¡å™¨ç»‘å®šåˆ°: {}", bind_address);

            while running.load(Ordering::SeqCst) {
                // æ¨¡æ‹ŸæœåŠ¡å™¨å¾ªç¯
                thread::sleep(Duration::from_millis(100));

                // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†APIè¯·æ±‚
            }
        });

        self.server = Some(thread);

        Ok(())
    }

    fn stop(&mut self) -> Result<(), String> {
        println!("åœæ­¢åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ");

        self.running.store(false, Ordering::SeqCst);

        // åœæ­¢æœåŠ¡å™¨
        if let Some(thread) = self.server.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æœåŠ¡å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢æµå¼•æ“
        self.stream_engine.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.stream_engine.coordinator_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("åè°ƒå™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢æŒ‡æ ‡æ”¶é›†å™¨
        self.metrics_collector.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.metrics_collector.collection_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æŒ‡æ ‡æ”¶é›†å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.checkpoint_manager.running.store(false, Ordering::SeqCst);
        if let Some(thread) = self.checkpoint_manager.checkpoint_thread.take() {
            match thread.join() {
                Ok(_) => {},
                Err(e) => println!("æ£€æŸ¥ç‚¹ç®¡ç†å™¨çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", e),
            }
        }

        // åœæ­¢æ‰€æœ‰æºè¿æ¥å™¨
        let mut sources = self.source_connectors.write().unwrap();

        for (id, source) in sources.iter_mut() {
            source.running.store(false, Ordering::SeqCst);

            if let Some(thread) = source.thread.take() {
                match thread.join() {
                    Ok(_) => {},
                    Err(e) => println!("æºè¿æ¥å™¨ {} çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", id, e),
                }
            }
        }

        // åœæ­¢æ‰€æœ‰æ¥æ”¶å™¨è¿æ¥å™¨
        let mut sinks = self.sink_connectors.write().unwrap();

        for (id, sink) in sinks.iter_mut() {
            sink.running.store(false, Ordering::SeqCst);

            if let Some(thread) = sink.thread.take() {
                match thread.join() {
                    Ok(_) => {},
                    Err(e) => println!("æ¥æ”¶å™¨è¿æ¥å™¨ {} çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", id, e),
                }
            }
        }

        // åœæ­¢æ‰€æœ‰å¤„ç†å™¨
        let mut processors = self.processors.write().unwrap();

        for (id, processor) in processors.iter_mut() {
            processor.running.store(false, Ordering::SeqCst);

            for thread in processor.threads.drain(..) {
                match thread.join() {
                    Ok(_) => {},
                    Err(e) => println!("å¤„ç†å™¨ {} çº¿ç¨‹é€€å‡ºé”™è¯¯: {:?}", id, e),
                }
            }
        }

        // ä¿å­˜çŠ¶æ€
        self.save_state()?;

        Ok(())
    }

    fn save_state(&self) -> Result<(), String> {
        println!("ä¿å­˜çŠ¶æ€");

        // ä¿å­˜æµå®šä¹‰
        self.save_stream_definitions()?;

        // ä¿å­˜æ£€æŸ¥ç‚¹
        self.save_checkpoints()?;

        Ok(())
    }

    fn save_checkpoints(&self) -> Result<(), String> {
        println!("ä¿å­˜æ£€æŸ¥ç‚¹");

        // ç¡®ä¿ç›®å½•å­˜åœ¨
        if let Err(e) = std::fs::create_dir_all(&self.checkpoint_manager.checkpoint_dir) {
            return Err(format!("åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•å¤±è´¥: {}", e));
        }

        // ä¿å­˜æ‰€æœ‰æ£€æŸ¥ç‚¹
        let checkpoints = self.checkpoint_manager.checkpoints.read().unwrap();

        for (id, checkpoint) in checkpoints.iter() {
            let checkpoint_path = self.checkpoint_manager.checkpoint_dir.join(format!("{}.json", id));

            // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–æ£€æŸ¥ç‚¹ä¸ºJSON
            let json = serde_json::to_string_pretty(checkpoint)
                .map_err(|e| format!("åºåˆ—åŒ–æ£€æŸ¥ç‚¹å¤±è´¥: {}", e))?;

            if let Err(e) = std::fs::write(&checkpoint_path, json) {
                println!("ä¿å­˜æ£€æŸ¥ç‚¹ {} å¤±è´¥: {}", id, e);
            }
        }

        Ok(())
    }

    fn create_stream(&self, stream: StreamDefinition) -> Result<String, String> {
        println!("åˆ›å»ºæµ: {}", stream.name);

        // éªŒè¯æµå®šä¹‰
        if stream.name.is_empty() {
            return Err("æµåç§°ä¸èƒ½ä¸ºç©º".to_string());
        }

        if stream.schema.fields.is_empty() {
            return Err("æµæ¶æ„å¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªå­—æ®µ".to_string());
        }

        // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåæµ
        let streams = self.streams.read().unwrap();

        for (_, existing) in streams.iter() {
            if existing.name == stream.name {
                return Err(format!("å·²å­˜åœ¨åŒåæµ: {}", stream.name));
            }
        }

        drop(streams);

        // ç”ŸæˆæµIDï¼ˆå¦‚æœå°šæœªè®¾ç½®ï¼‰
        let stream_id = if stream.id.is_empty() {
            uuid::Uuid::new_v4().to_string()
        } else {
            stream.id.clone()
        };

        let mut stream = stream;
        stream.id = stream_id.clone();

        // æ·»åŠ æµå®šä¹‰
        let mut streams = self.streams.write().unwrap();
        streams.insert(stream_id.clone(), stream);

        // ä¿å­˜æµå®šä¹‰
        drop(streams);
        self.save_stream_definitions()?;

        Ok(stream_id)
    }

    fn deploy_topology(&self, topology: Topology) -> Result<(), String> {
        println!("éƒ¨ç½²æ‹“æ‰‘: {}", topology.name);

        // éªŒè¯æ‹“æ‰‘
        if topology.name.is_empty() {
            return Err("æ‹“æ‰‘åç§°ä¸èƒ½ä¸ºç©º".to_string());
        }

        if topology.nodes.is_empty() {
            return Err("æ‹“æ‰‘å¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªèŠ‚ç‚¹".to_string());
        }

        if topology.edges.is_empty() {
            return Err("æ‹“æ‰‘å¿…é¡»è‡³å°‘æœ‰ä¸€æ¡è¾¹".to_string());
        }

        // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåæ‹“æ‰‘
        let topologies = self.topology_manager.topologies.read().unwrap();

        for (_, existing) in topologies.iter() {
            if existing.name == topology.name && existing.id != topology.id {
                return Err(format!("å·²å­˜åœ¨åŒåæ‹“æ‰‘: {}", topology.name));
            }
        }

        drop(topologies);

        // ä¿å­˜æ‹“æ‰‘
        let topology_id = topology.id.clone();

        let mut topologies = self.topology_manager.topologies.write().unwrap();
        topologies.insert(topology_id.clone(), topology);

        // ä¿å­˜æ‹“æ‰‘å®šä¹‰
        drop(topologies);
        self.save_topology(&topology_id)?;

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨æ‹“æ‰‘

        Ok(())
    }

    fn save_topology(&self, topology_id: &str) -> Result<(), String> {
        println!("ä¿å­˜æ‹“æ‰‘: {}", topology_id);

        // è·å–æ‹“æ‰‘
        let topologies = self.topology_manager.topologies.read().unwrap();

        let topology = match topologies.get(topology_id) {
            Some(topology) => topology,
            None => return Err(format!("æ‹“æ‰‘ä¸å­˜åœ¨: {}", topology_id)),
        };

        // ç¡®ä¿ç›®å½•å­˜åœ¨
        let topologies_dir = self.data_dir.join("topologies");

        if let Err(e) = std::fs::create_dir_all(&topologies_dir) {
            return Err(format!("åˆ›å»ºæ‹“æ‰‘ç›®å½•å¤±è´¥: {}", e));
        }

        // ä¿å­˜æ‹“æ‰‘å®šä¹‰
        let topology_path = topologies_dir.join(format!("{}.json", topology_id));

        // åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåºåˆ—åŒ–æ‹“æ‰‘ä¸ºJSON
        let json = serde_json::to_string_pretty(topology)
            .map_err(|e| format!("åºåˆ—åŒ–æ‹“æ‰‘å¤±è´¥: {}", e))?;

        if let Err(e) = std::fs::write(&topology_path, json) {
            return Err(format!("ä¿å­˜æ‹“æ‰‘æ–‡ä»¶å¤±è´¥: {}", e));
        }

        Ok(())
    }
}

// å®Œå–„mainå‡½æ•°ï¼Œæ·»åŠ æµå¤„ç†ç³»ç»Ÿæµ‹è¯•
fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("å¯åŠ¨åˆ†å¸ƒå¼ç³»ç»Ÿç¤ºä¾‹");

    let temp_dir = std::env::temp_dir().join("distributed_systems_demo");

    if temp_dir.exists() {
        std::fs::remove_dir_all(&temp_dir)?;
    }
    std::fs::create_dir_all(&temp_dir)?;

    println!("ä½¿ç”¨ä¸´æ—¶ç›®å½•: {:?}", temp_dir);

    // æµ‹è¯•åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ
    {
        println!("\n===== æµ‹è¯•åˆ†å¸ƒå¼æµå¤„ç†ç³»ç»Ÿ =====");

        let data_dir = temp_dir.join("stream_processor");
        std::fs::create_dir_all(&data_dir)?;

        let mut stream_processor = DistributedStreamProcessor::new("node1", &data_dir, "127.0.0.1:8090");

        stream_processor.start()?;

        // åˆ›å»ºæµ
        let now = Utc::now();

        let clickstream = StreamDefinition {
            id: String::new(), // è‡ªåŠ¨ç”ŸæˆID
            name: "ç‚¹å‡»æµæ•°æ®".to_string(),
            schema: StreamSchema {
                fields: vec![
                    FieldDefinition {
                        name: "user_id".to_string(),
                        field_type: FieldType::String,
                        nullable: false,
                        default_value: None,
                        description: Some("ç”¨æˆ·ID".to_string()),
                    },
                    FieldDefinition {
                        name: "page_id".to_string(),
                        field_type: FieldType::String,
                        nullable: false,
                        default_value: None,
                        description: Some("é¡µé¢ID".to_string()),
                    },
                    FieldDefinition {
                        name: "timestamp".to_string(),
                        field_type: FieldType::Timestamp,
                        nullable: false,
                        default_value: None,
                        description: Some("ç‚¹å‡»æ—¶é—´æˆ³".to_string()),
                    },
                    FieldDefinition {
                        name: "referrer".to_string(),
                        field_type: FieldType::String,
                        nullable: true,
                        default_value: None,
                        description: Some("å¼•èæ¥æº".to_string()),
                    },
                    FieldDefinition {
                        name: "user_agent".to_string(),
                        field_type: FieldType::String,
                        nullable: true,
                        default_value: None,
                        description: Some("ç”¨æˆ·ä»£ç†".to_string()),
                    },
                ],
                key_fields: vec!["user_id".to_string()],
                timestamp_field: Some("timestamp".to_string()),
            },
            partitioning: PartitioningStrategy::Hash {
                fields: vec!["user_id".to_string()],
                partitions: 8,
            },
            retention: RetentionPolicy {
                time_based: Some(Duration::from_secs(86400 * 7)), // 7å¤©
                size_based: Some(1024 * 1024 * 1024 * 10), // 10GB
            },
            created_at: now,
            created_by: "admin".to_string(),
        };

        let clickstream_id = stream_processor.create_stream(clickstream)?;

        println!("åˆ›å»ºçš„æµID: {}", clickstream_id);

        // åˆ›å»ºå¹¶éƒ¨ç½²æ‹“æ‰‘
        let clickstream_topology = Topology {
            id: uuid::Uuid::new_v4().to_string(),
            name: "ç‚¹å‡»æµå¤„ç†".to_string(),
            description: "å¤„ç†ç½‘ç«™ç‚¹å‡»æµæ•°æ®".to_string(),
            nodes: vec![
                TopologyNode {
                    id: "source1".to_string(),
                    node_type: NodeType::Source("kafka".to_string()),
                    config: [
                        ("bootstrap.servers".to_string(), "localhost:9092".to_string()),
                        ("topic".to_string(), "clickstream".to_string()),
                        ("group.id".to_string(), "clickstream-processor".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 2,
                },
                TopologyNode {
                    id: "processor1".to_string(),
                    node_type: NodeType::Processor("sessionize".to_string()),
                    config: [
                        ("session_timeout".to_string(), "1800".to_string()), // 30åˆ†é’Ÿä¼šè¯è¶…æ—¶
                    ].iter().cloned().collect(),
                    parallelism: 4,
                },
                TopologyNode {
                    id: "processor2".to_string(),
                    node_type: NodeType::Processor("enrich".to_string()),
                    config: [
                        ("user_db".to_string(), "users".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 4,
                },
                TopologyNode {
                    id: "sink1".to_string(),
                    node_type: NodeType::Sink("elasticsearch".to_string()),
                    config: [
                        ("hosts".to_string(), "localhost:9200".to_string()),
                        ("index".to_string(), "clickstream_sessions".to_string()),
                        ("document_id".to_string(), "${session_id}".to_string()),
                    ].iter().cloned().collect(),
                    parallelism: 2,
                },
            ],
            edges: vec![
                TopologyEdge {
                    from: "source1".to_string(),
                    to: "processor1".to_string(),
                    stream_id: clickstream_id.clone(),
                    partitioning: EdgePartitioning::Key,
                },
                TopologyEdge {
                    from: "processor1".to_string(),
                    to: "processor2".to_string(),
                    stream_id: "sessionized_clicks".to_string(),
                    partitioning: EdgePartitioning::Forward,
                },
                TopologyEdge {
                    from: "processor2".to_string(),
                    to: "sink1".to_string(),
                    stream_id: "enriched_sessions".to_string(),
                    partitioning: EdgePartitioning::Key,
                },
            ],
            created_at: now,
            updated_at: now,
            version: 1,
            status: TopologyStatus::Created,
        };

        stream_processor.deploy_topology(clickstream_topology)?;

        println!("éƒ¨ç½²æ‹“æ‰‘æˆåŠŸ");

        // ç­‰å¾…ä¸€ä¼šå„¿ï¼Œè®©ç³»ç»Ÿè¿è¡Œ
        thread::sleep(Duration::from_secs(3));

        // åœæ­¢æµå¤„ç†ç³»ç»Ÿ
        stream_processor.stop()?;
    }

    println!("\næ‰€æœ‰åˆ†å¸ƒå¼ç³»ç»Ÿæµ‹è¯•å®Œæˆ");

    // æ¸…ç†ä¸´æ—¶ç›®å½•
    std::fs::remove_dir_all(&temp_dir)?;

    Ok(())
}
```
