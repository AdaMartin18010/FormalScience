# å¤æ‚åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸å®ç°æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•

- [1 ä¸€ã€æ•´ä½“æ¶æ„è®¾è®¡](#1-ä¸€æ•´ä½“æ¶æ„è®¾è®¡)
  - [1.1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#11-ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
  - [1.2 æ ¸å¿ƒå±‚æ¬¡ä¸ç»„ä»¶](#12-æ ¸å¿ƒå±‚æ¬¡ä¸ç»„ä»¶)
    - [2.2.1 åº”ç”¨å±‚](#221-åº”ç”¨å±‚)
    - [2.2.2 é¢†åŸŸå±‚](#222-é¢†åŸŸå±‚)
    - [2.2.3 åŸºç¡€è®¾æ–½å±‚](#223-åŸºç¡€è®¾æ–½å±‚)
- [2 äºŒã€å­ç³»ç»Ÿè¯¦ç»†è®¾è®¡](#2-äºŒå­ç³»ç»Ÿè¯¦ç»†è®¾è®¡)
  - [2.1 å‘½ä»¤å¤„ç†å­ç³»ç»Ÿ](#21-å‘½ä»¤å¤„ç†å­ç³»ç»Ÿ)
    - [1.1.1 å‘½ä»¤å¤„ç†æµç¨‹](#111-å‘½ä»¤å¤„ç†æµç¨‹)
    - [1.1.2 ä»£è¡¨æ€§å‘½ä»¤å¤„ç†å™¨å®ç°](#112-ä»£è¡¨æ€§å‘½ä»¤å¤„ç†å™¨å®ç°)
  - [2.2 äº‹ä»¶å¤„ç†å­ç³»ç»Ÿ](#22-äº‹ä»¶å¤„ç†å­ç³»ç»Ÿ)
    - [2.2.1 äº‹ä»¶æ€»çº¿å®ç°](#221-äº‹ä»¶æ€»çº¿å®ç°)
    - [2.2.2 è®¢å•åˆ›å»ºäº‹ä»¶å¤„ç†](#222-è®¢å•åˆ›å»ºäº‹ä»¶å¤„ç†)
  - [2.3 å·¥ä½œæµå­ç³»ç»Ÿ](#23-å·¥ä½œæµå­ç³»ç»Ÿ)
    - [3.3.1 Temporalå·¥ä½œæµé›†æˆ](#331-temporalå·¥ä½œæµé›†æˆ)
    - [3.3.2 å·¥ä½œæµæ´»åŠ¨å®ç°](#332-å·¥ä½œæµæ´»åŠ¨å®ç°)
  - [2.4 å¤–éƒ¨ç³»ç»Ÿé›†æˆå­ç³»ç»Ÿ](#24-å¤–éƒ¨ç³»ç»Ÿé›†æˆå­ç³»ç»Ÿ)
    - [4.4.1 æŠ½è±¡æœåŠ¡å®¢æˆ·ç«¯](#441-æŠ½è±¡æœåŠ¡å®¢æˆ·ç«¯)
  - [2.5 å¤–éƒ¨ç³»ç»Ÿé›†æˆå­ç³»ç»Ÿç»­](#25-å¤–éƒ¨ç³»ç»Ÿé›†æˆå­ç³»ç»Ÿç»­)
    - [5.5.1 æŠ½è±¡æœåŠ¡å®¢æˆ·ç«¯ç»­](#551-æŠ½è±¡æœåŠ¡å®¢æˆ·ç«¯ç»­)
    - [5.5.2 å¤–éƒ¨ç³»ç»Ÿé€‚é…å™¨å·¥å‚](#552-å¤–éƒ¨ç³»ç»Ÿé€‚é…å™¨å·¥å‚)
  - [2.6 æŸ¥è¯¢æœåŠ¡å­ç³»ç»Ÿ](#26-æŸ¥è¯¢æœåŠ¡å­ç³»ç»Ÿ)
    - [6.6.1 CQRSæŸ¥è¯¢å±‚](#661-cqrsæŸ¥è¯¢å±‚)
    - [6.6.2 APIç«¯ç‚¹](#662-apiç«¯ç‚¹)
  - [2.7 ç›‘æ§ä¸å¯è§‚æµ‹æ€§å­ç³»ç»Ÿ](#27-ç›‘æ§ä¸å¯è§‚æµ‹æ€§å­ç³»ç»Ÿ)
    - [7.7.1 åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ](#771-åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ)
    - [7.7.2 æŒ‡æ ‡ç›‘æ§å®ç°](#772-æŒ‡æ ‡ç›‘æ§å®ç°)
  - [2.8 ç›‘æ§ä¸å¯è§‚æµ‹æ€§å­ç³»ç»Ÿç»­](#28-ç›‘æ§ä¸å¯è§‚æµ‹æ€§å­ç³»ç»Ÿç»­)
    - [8.8.1 æŒ‡æ ‡ç›‘æ§å®ç°ç»­](#881-æŒ‡æ ‡ç›‘æ§å®ç°ç»­)
  - [2.9 é…ç½®ä¸æœåŠ¡å‘ç°å­ç³»ç»Ÿ](#29-é…ç½®ä¸æœåŠ¡å‘ç°å­ç³»ç»Ÿ)
    - [9.9.1 åŠ¨æ€é…ç½®ç®¡ç†](#991-åŠ¨æ€é…ç½®ç®¡ç†)
    - [9.9.2 æœåŠ¡æ³¨å†Œä¸å‘ç°](#992-æœåŠ¡æ³¨å†Œä¸å‘ç°)
- [3 ä¸‰ã€æ•°æ®æ¨¡å‹è®¾è®¡](#3-ä¸‰æ•°æ®æ¨¡å‹è®¾è®¡)
  - [3.1 äº‹ä»¶å­˜å‚¨è¡¨ç»“æ„](#31-äº‹ä»¶å­˜å‚¨è¡¨ç»“æ„)
  - [3.2 è¯»æ¨¡å‹è¡¨ç»“æ„](#32-è¯»æ¨¡å‹è¡¨ç»“æ„)
- [4 å››ã€éƒ¨ç½²æ¶æ„](#4-å››éƒ¨ç½²æ¶æ„)
  - [4.1 å®¹å™¨åŒ–ä¸æœåŠ¡ç¼–æ’](#41-å®¹å™¨åŒ–ä¸æœåŠ¡ç¼–æ’)
- [5 å››ã€éƒ¨ç½²æ¶æ„ç»­](#5-å››éƒ¨ç½²æ¶æ„ç»­)
  - [5.1 å®¹å™¨åŒ–ä¸æœåŠ¡ç¼–æ’ç»­](#51-å®¹å™¨åŒ–ä¸æœåŠ¡ç¼–æ’ç»­)
  - [5.2 Prometheusé…ç½®](#52-prometheusé…ç½®)
  - [5.3 Kuberneteséƒ¨ç½²é…ç½®](#53-kuberneteséƒ¨ç½²é…ç½®)
- [6 äº”ã€ç³»ç»Ÿå¯åŠ¨ä¸é›†æˆ](#6-äº”ç³»ç»Ÿå¯åŠ¨ä¸é›†æˆ)
  - [6.1 åº”ç”¨å¯åŠ¨åºåˆ—](#61-åº”ç”¨å¯åŠ¨åºåˆ—)
  - [6.2 Dockerfile](#62-dockerfile)
- [7 å…­ã€æ€»ç»“ä¸æœ€ä½³å®è·µ](#7-å…­æ€»ç»“ä¸æœ€ä½³å®è·µ)
  - [7.1 æ¶æ„è®¾è®¡å…³é”®ç‚¹](#71-æ¶æ„è®¾è®¡å…³é”®ç‚¹)
  - [7.2 Rustå®ç°ä¼˜åŠ¿](#72-rustå®ç°ä¼˜åŠ¿)
  - [7.3 é›†æˆå¼€æºåº“æœ€ä½³å®è·µ](#73-é›†æˆå¼€æºåº“æœ€ä½³å®è·µ)
  - [7.4 æœ€ç»ˆæ¶æ„ç‰¹ç‚¹](#74-æœ€ç»ˆæ¶æ„ç‰¹ç‚¹)

---

## 1 ä¸€ã€æ•´ä½“æ¶æ„è®¾è®¡

åŸºäºå‰é¢çš„åˆ†æ,æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªå¯æ»¡è¶³æ‚¨éœ€æ±‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„,é‡‡ç”¨é¢†åŸŸé©±åŠ¨è®¾è®¡å’Œå¾®æœåŠ¡æ¶æ„,ç»“åˆäº‹ä»¶é©±åŠ¨å’ŒCQRSæ¨¡å¼ã€‚

### 1.1 ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

![ç³»ç»Ÿæ¶æ„å›¾](æ¶æ„è®¾è®¡)

```text
+-----------------------------------------------------+
|                   å‰ç«¯åº”ç”¨å±‚                          |
+-----------------------------------------------------+
                        â†“
+-----------------------------------------------------+
|                 APIç½‘å…³ (actix-web)                  |
+-----------------------------------------------------+
            â†™          â†“           â†˜
+---------------+ +---------------+ +---------------+
|  å‘½ä»¤æœåŠ¡      | |   æŸ¥è¯¢æœåŠ¡     | |   é›†æˆæœåŠ¡     |
| (å¤„ç†å†™æ“ä½œ)   | | (å¤„ç†è¯»æ“ä½œ)   | | (å¤–éƒ¨ç³»ç»Ÿé›†æˆ)  |
+---------------+ +---------------+ +---------------+
      â†“    â†‘                â†‘               â†‘   â†“
+---------------+           |               |   |
|  äº‹ä»¶æ€»çº¿      |-----------|---------------|---|
| (rdkafka)     |                                   
+---------------+                               
      â†“                                    â†“   â†‘
+---------------+                     +---------------+
|  å·¥ä½œæµå¼•æ“    |                     |  å¤–éƒ¨ç³»ç»Ÿé€‚é…å™¨ |
| (temporal)    |                     | (tonic clients)|
+---------------+                     +---------------+
      â†“    â†‘                                â†“   â†‘
+---------------+ +---------------+ +---------------+
|  äº‹ä»¶å­˜å‚¨      | |   è¯»æ¨¡å‹å­˜å‚¨   | |   é›†æˆå­˜å‚¨     |
| (sqlx+Postgres)| | (sqlx+MongoDB) | | (redis-rs)    |
+---------------+ +---------------+ +---------------+
```

### 1.2 æ ¸å¿ƒå±‚æ¬¡ä¸ç»„ä»¶

#### 2.2.1 åº”ç”¨å±‚

- **APIç½‘å…³**: ä½¿ç”¨actix-webå®ç°,å¤„ç†è®¤è¯ã€æˆæƒã€è·¯ç”±ã€é™æµ
- **å‘½ä»¤æœåŠ¡**: å¤„ç†å†™æ“ä½œ,éªŒè¯ã€è½¬æ¢å‘½ä»¤ä¸ºé¢†åŸŸäº‹ä»¶
- **æŸ¥è¯¢æœåŠ¡**: å¤„ç†è¯»æ“ä½œ,ä»ä¼˜åŒ–çš„è¯»æ¨¡å‹ä¸­è·å–æ•°æ®
- **é›†æˆæœåŠ¡**: ç®¡ç†ä¸å¤–éƒ¨ç³»ç»Ÿçš„äº¤äº’

#### 2.2.2 é¢†åŸŸå±‚

- **èšåˆæ ¹**: ä¸šåŠ¡å®ä½“å’Œè§„åˆ™çš„å°è£…
- **é¢†åŸŸäº‹ä»¶**: ç³»ç»Ÿä¸­å‘ç”Ÿçš„é‡è¦å˜åŒ–
- **å·¥ä½œæµå¼•æ“**: ç®¡ç†é•¿æ—¶é—´è¿è¡Œçš„ä¸šåŠ¡æµç¨‹
- **é¢†åŸŸæœåŠ¡**: è·¨èšåˆæ ¹çš„ä¸šåŠ¡é€»è¾‘

#### 2.2.3 åŸºç¡€è®¾æ–½å±‚

- **äº‹ä»¶æ€»çº¿**: ä½¿ç”¨rdkafkaå®ç°çš„å‘å¸ƒ/è®¢é˜…æœºåˆ¶
- **äº‹ä»¶å­˜å‚¨**: ä½¿ç”¨PostgreSQLå­˜å‚¨é¢†åŸŸäº‹ä»¶
- **è¯»æ¨¡å‹å­˜å‚¨**: ä½¿ç”¨MongoDBå­˜å‚¨ä¼˜åŒ–çš„è¯»æ¨¡å‹
- **é›†æˆå­˜å‚¨**: ä½¿ç”¨Redisç¼“å­˜é›†æˆæ•°æ®

## 2 äºŒã€å­ç³»ç»Ÿè¯¦ç»†è®¾è®¡

### 2.1 å‘½ä»¤å¤„ç†å­ç³»ç»Ÿ

#### 1.1.1 å‘½ä»¤å¤„ç†æµç¨‹

```rust
use tokio::sync::mpsc;
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use chrono::{DateTime, Utc};
use tracing::{info, error, instrument};

/// å‘½ä»¤ç‰¹è´¨
pub trait Command: Send + Sync + 'static {
    /// å‘½ä»¤ç±»å‹æ ‡è¯†
    fn command_type(&self) -> &'static str;
    
    /// å‘½ä»¤å”¯ä¸€æ ‡è¯†
    fn command_id(&self) -> Uuid;
    
    /// ç”¨äºè¿½è¸ªçš„ç›¸å…³ID
    fn correlation_id(&self) -> Option<String>;
}

/// å‘½ä»¤å¤„ç†å™¨ç‰¹è´¨
#[async_trait]
pub trait CommandHandler<C: Command>: Send + Sync + 'static {
    /// å¤„ç†å‘½ä»¤
    async fn handle(&self, command: C) -> Result<Vec<DomainEvent>, CommandError>;
}

/// å‘½ä»¤æ€»çº¿
pub struct CommandBus {
    handlers: HashMap<&'static str, Box<dyn AnyCommandHandler>>,
    event_producer: Arc<EventProducer>,
}

#[async_trait]
pub trait AnyCommandHandler: Send + Sync {
    async fn handle_any(&self, command: Box<dyn Command>) -> Result<Vec<DomainEvent>, CommandError>;
}

impl CommandBus {
    pub fn new(event_producer: Arc<EventProducer>) -> Self {
        Self {
            handlers: HashMap::new(),
            event_producer,
        }
    }
    
    /// æ³¨å†Œå‘½ä»¤å¤„ç†å™¨
    pub fn register<C, H>(&mut self, handler: H)
    where
        C: Command + 'static,
        H: CommandHandler<C> + 'static,
    {
        let type_id = TypeId::of::<C>();
        let handler_wrapper = CommandHandlerWrapper { handler };
        self.handlers.insert(type_id, Box::new(handler_wrapper));
    }
    
    /// åˆ†æ´¾å‘½ä»¤
    #[instrument(skip(self), fields(command_id = %command.command_id(), command_type = %command.command_type()))]
    pub async fn dispatch<C: Command>(&self, command: C) -> Result<Vec<DomainEvent>, CommandError> {
        info!("å¤„ç†å‘½ä»¤");
        
        let type_id = TypeId::of::<C>();
        
        let handler = self.handlers.get(&type_id)
            .ok_or_else(|| CommandError::HandlerNotFound(command.command_type().to_string()))?;
            
        // å¤„ç†å‘½ä»¤
        let events = handler.handle_any(Box::new(command)).await?;
        
        // å‘å¸ƒäº‹ä»¶
        for event in &events {
            self.event_producer.publish_event(event).await?;
        }
        
        Ok(events)
    }
}
```

#### 1.1.2 ä»£è¡¨æ€§å‘½ä»¤å¤„ç†å™¨å®ç°

```rust
use sqlx::{PgPool, postgres::PgQueryResult};
use uuid::Uuid;
use serde::{Serialize, Deserialize};

/// åˆ›å»ºè®¢å•å‘½ä»¤
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CreateOrderCommand {
    pub command_id: Uuid,
    pub customer_id: String,
    pub items: Vec<OrderItem>,
    pub shipping_address: Address,
    pub correlation_id: Option<String>,
}

impl Command for CreateOrderCommand {
    fn command_type(&self) -> &'static str { "create_order" }
    fn command_id(&self) -> Uuid { self.command_id }
    fn correlation_id(&self) -> Option<String> { self.correlation_id.clone() }
}

/// åˆ›å»ºè®¢å•å¤„ç†å™¨
pub struct CreateOrderHandler {
    db_pool: PgPool,
    inventory_client: Arc<InventoryServiceClient>,
}

#[async_trait]
impl CommandHandler<CreateOrderCommand> for CreateOrderHandler {
    #[instrument(skip(self), fields(command_id = %command.command_id))]
    async fn handle(&self, command: CreateOrderCommand) -> Result<Vec<DomainEvent>, CommandError> {
        // 1. éªŒè¯åº“å­˜å¯ç”¨æ€§
        let inventory_check = self.inventory_client.check_availability(&command.items).await
            .map_err(|e| CommandError::ValidationError(format!("åº“å­˜æ£€æŸ¥å¤±è´¥: {}", e)))?;
            
        if !inventory_check.all_available {
            return Err(CommandError::ValidationError(
                format!("éƒ¨åˆ†å•†å“åº“å­˜ä¸è¶³: {:?}", inventory_check.unavailable_items)
            ));
        }
        
        // 2. åˆ›å»ºè®¢å•èšåˆæ ¹
        let order = Order::create(
            OrderId::new(),
            CustomerId::from(command.customer_id),
            command.items,
            command.shipping_address,
        )?;
        
        // 3. å°†è®¢å•ä¿å­˜åˆ°äº‹ä»¶å­˜å‚¨
        let events = order.uncommitted_events();
        
        // ä½¿ç”¨äº‹åŠ¡ä¿å­˜äº‹ä»¶
        let mut tx = self.db_pool.begin().await
            .map_err(|e| CommandError::InfrastructureError(e.to_string()))?;
            
        for event in &events {
            sqlx::query(
                "INSERT INTO event_store (aggregate_id, aggregate_type, event_type, event_data, sequence, occurred_at)
                 VALUES ($1, $2, $3, $4, $5, $6)"
            )
            .bind(event.aggregate_id().to_string())
            .bind("order")
            .bind(event.event_type())
            .bind(serde_json::to_value(event).map_err(|e| CommandError::SerializationError(e.to_string()))?)
            .bind(event.sequence() as i32)
            .bind(event.occurred_at())
            .execute(&mut tx)
            .await
            .map_err(|e| CommandError::InfrastructureError(format!("äº‹ä»¶ä¿å­˜å¤±è´¥: {}", e)))?;
        }
        
        tx.commit().await
            .map_err(|e| CommandError::InfrastructureError(format!("äº‹åŠ¡æäº¤å¤±è´¥: {}", e)))?;
            
        info!(order_id = %order.id(), "è®¢å•åˆ›å»ºæˆåŠŸ");
        
        Ok(events)
    }
}
```

### 2.2 äº‹ä»¶å¤„ç†å­ç³»ç»Ÿ

#### 2.2.1 äº‹ä»¶æ€»çº¿å®ç°

```rust
use rdkafka::producer::{FutureProducer, FutureRecord};
use rdkafka::config::ClientConfig;
use rdkafka::message::OwnedHeaders;
use std::time::Duration;

/// é¢†åŸŸäº‹ä»¶ç‰¹è´¨
pub trait DomainEvent: Send + Sync + 'static {
    fn event_type(&self) -> &'static str;
    fn aggregate_id(&self) -> &str;
    fn sequence(&self) -> u64;
    fn occurred_at(&self) -> DateTime<Utc>;
}

/// äº‹ä»¶ç”Ÿäº§è€…
pub struct EventProducer {
    producer: FutureProducer,
    topic: String,
}

impl EventProducer {
    pub fn new(brokers: &str, topic: &str) -> Result<Self, rdkafka::error::KafkaError> {
        let producer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("message.timeout.ms", "5000")
            .create()?;
            
        Ok(Self {
            producer,
            topic: topic.to_string(),
        })
    }
    
    #[instrument(skip(self, event), fields(event_type = %event.event_type(), aggregate_id = %event.aggregate_id()))]
    pub async fn publish_event<E: DomainEvent + Serialize>(&self, event: &E) -> Result<(), EventBusError> {
        let event_data = serde_json::to_string(event)
            .map_err(|e| EventBusError::SerializationError(e.to_string()))?;
            
        let headers = OwnedHeaders::new()
            .add("event_type", event.event_type())
            .add("aggregate_id", event.aggregate_id())
            .add("sequence", &event.sequence().to_string())
            .add("occurred_at", &event.occurred_at().to_rfc3339());
            
        let delivery_status = self.producer
            .send(
                FutureRecord::to(&self.topic)
                    .payload(&event_data)
                    .key(event.aggregate_id())
                    .headers(headers),
                Duration::from_secs(5),
            )
            .await
            .map_err(|(e, _)| EventBusError::PublishError(e.to_string()))?;
            
        info!(
            offset = delivery_status.0,
            partition = delivery_status.1,
            "äº‹ä»¶å·²å‘å¸ƒ"
        );
        
        Ok(())
    }
}

/// äº‹ä»¶æ¶ˆè´¹è€…
pub struct EventConsumer {
    consumer: StreamConsumer,
    event_handlers: HashMap<&'static str, Vec<Box<dyn AnyEventHandler>>>,
}

#[async_trait]
pub trait EventHandler<E: DomainEvent>: Send + Sync + 'static {
    async fn handle(&self, event: E) -> Result<(), EventHandlerError>;
}

#[async_trait]
pub trait AnyEventHandler: Send + Sync {
    async fn handle_any(&self, event_type: &str, event_data: &str) -> Result<(), EventHandlerError>;
}

impl EventConsumer {
    pub async fn start(
        &self,
        topics: &[&str],
        group_id: &str,
        brokers: &str,
    ) -> Result<(), EventConsumerError> {
        let consumer: StreamConsumer = ClientConfig::new()
            .set("group.id", group_id)
            .set("bootstrap.servers", brokers)
            .set("enable.auto.commit", "false")
            .set("auto.offset.reset", "earliest")
            .create()?
            .subscribe(topics)?;
            
        let mut message_stream = consumer.stream();
        
        while let Some(message) = message_stream.next().await {
            match message {
                Ok(msg) => {
                    let payload = match msg.payload_view::<str>() {
                        Some(Ok(s)) => s,
                        Some(Err(e)) => {
                            error!("æ¶ˆæ¯è§£æå¤±è´¥: {:?}", e);
                            continue;
                        }
                        None => {
                            error!("ç©ºæ¶ˆæ¯");
                            continue;
                        }
                    };
                    
                    let event_type = match msg.headers() {
                        Some(headers) => {
                            match headers.get(0) {
                                Some(header) if header.key == "event_type" => {
                                    match std::str::from_utf8(header.value.unwrap_or_default()) {
                                        Ok(s) => s,
                                        Err(_) => {
                                            error!("æ— æ³•è§£æäº‹ä»¶ç±»å‹");
                                            continue;
                                        }
                                    }
                                }
                                _ => {
                                    error!("æ¶ˆæ¯ä¸­ç¼ºå°‘äº‹ä»¶ç±»å‹");
                                    continue;
                                }
                            }
                        }
                        None => {
                            error!("æ¶ˆæ¯ä¸­ç¼ºå°‘å¤´ä¿¡æ¯");
                            continue;
                        }
                    };
                    
                    if let Some(handlers) = self.event_handlers.get(event_type) {
                        for handler in handlers {
                            if let Err(e) = handler.handle_any(event_type, payload).await {
                                error!(error = ?e, "äº‹ä»¶å¤„ç†å¤±è´¥");
                            }
                        }
                    }
                    
                    consumer.commit_message(&msg, CommitMode::Async).unwrap();
                }
                Err(e) => {
                    error!("æ¶ˆè´¹æ¶ˆæ¯é”™è¯¯: {:?}", e);
                }
            }
        }
        
        Ok(())
    }
}
```

#### 2.2.2 è®¢å•åˆ›å»ºäº‹ä»¶å¤„ç†

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OrderCreatedEvent {
    pub event_id: Uuid,
    pub aggregate_id: String,
    pub sequence: u64,
    pub occurred_at: DateTime<Utc>,
    pub customer_id: String,
    pub order_items: Vec<OrderItem>,
    pub shipping_address: Address,
    pub order_status: OrderStatus,
}

impl DomainEvent for OrderCreatedEvent {
    fn event_type(&self) -> &'static str { "order_created" }
    fn aggregate_id(&self) -> &str { &self.aggregate_id }
    fn sequence(&self) -> u64 { self.sequence }
    fn occurred_at(&self) -> DateTime<Utc> { self.occurred_at }
}

/// è®¢å•æŸ¥è¯¢æ¨¡å‹æ›´æ–°å™¨
pub struct OrderReadModelUpdater {
    db_pool: sqlx::PgPool,
}

#[async_trait]
impl EventHandler<OrderCreatedEvent> for OrderReadModelUpdater {
    #[instrument(skip(self, event), fields(order_id = %event.aggregate_id))]
    async fn handle(&self, event: OrderCreatedEvent) -> Result<(), EventHandlerError> {
        // æ›´æ–°è®¢å•æŸ¥è¯¢æ¨¡å‹
        let result = sqlx::query(
            "INSERT INTO order_view (
                id, customer_id, status, total_amount, item_count, shipping_address, created_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7)"
        )
        .bind(&event.aggregate_id)
        .bind(&event.customer_id)
        .bind(event.order_status.to_string())
        .bind(calculate_total_amount(&event.order_items))
        .bind(event.order_items.len() as i32)
        .bind(serde_json::to_value(&event.shipping_address).map_err(|e| EventHandlerError::SerializationError(e.to_string()))?)
        .bind(event.occurred_at)
        .execute(&self.db_pool)
        .await
        .map_err(|e| EventHandlerError::DatabaseError(e.to_string()))?;
        
        // æ’å…¥è®¢å•é¡¹
        for item in &event.order_items {
            sqlx::query(
                "INSERT INTO order_item_view (
                    order_id, product_id, quantity, unit_price
                ) VALUES ($1, $2, $3, $4)"
            )
            .bind(&event.aggregate_id)
            .bind(&item.product_id)
            .bind(item.quantity as i32)
            .bind(item.unit_price)
            .execute(&self.db_pool)
            .await
            .map_err(|e| EventHandlerError::DatabaseError(e.to_string()))?;
        }
        
        info!("å·²æ›´æ–°è®¢å•æŸ¥è¯¢æ¨¡å‹");
        Ok(())
    }
}

/// å·¥ä½œæµå¯åŠ¨å¤„ç†å™¨
pub struct OrderWorkflowStarter {
    temporal_client: Arc<TemporalClient>,
}

#[async_trait]
impl EventHandler<OrderCreatedEvent> for OrderWorkflowStarter {
    #[instrument(skip(self, event), fields(order_id = %event.aggregate_id))]
    async fn handle(&self, event: OrderCreatedEvent) -> Result<(), EventHandlerError> {
        // å¯åŠ¨è®¢å•å¤„ç†å·¥ä½œæµ
        let workflow_input = OrderProcessingWorkflowInput {
            order_id: event.aggregate_id.clone(),
            customer_id: event.customer_id.clone(),
        };
        
        let workflow_id = format!("order-processing-{}", event.aggregate_id);
        
        self.temporal_client.start_workflow(
            "OrderProcessingWorkflow",
            workflow_input,
            &workflow_id,
            &WorkflowOptions {
                task_queue: "order-processing".to_string(),
                workflow_execution_timeout: Some(Duration::from_secs(86400)), // 24å°æ—¶
                ..Default::default()
            },
        ).await.map_err(|e| EventHandlerError::WorkflowError(e.to_string()))?;
        
        info!(workflow_id = %workflow_id, "å·²å¯åŠ¨è®¢å•å¤„ç†å·¥ä½œæµ");
        Ok(())
    }
}

/// åº“å­˜é¢„ç•™å¤„ç†å™¨
pub struct InventoryReserver {
    inventory_client: Arc<InventoryServiceClient>,
}

#[async_trait]
impl EventHandler<OrderCreatedEvent> for InventoryReserver {
    #[instrument(skip(self, event), fields(order_id = %event.aggregate_id))]
    async fn handle(&self, event: OrderCreatedEvent) -> Result<(), EventHandlerError> {
        // é¢„ç•™åº“å­˜
        let reserve_request = ReserveInventoryRequest {
            order_id: event.aggregate_id.clone(),
            items: event.order_items.iter().map(|item| InventoryItem {
                product_id: item.product_id.clone(),
                quantity: item.quantity,
            }).collect(),
        };
        
        match self.inventory_client.reserve_inventory(reserve_request).await {
            Ok(_) => {
                info!("åº“å­˜é¢„ç•™æˆåŠŸ");
                Ok(())
            },
            Err(e) => {
                error!(error = %e, "åº“å­˜é¢„ç•™å¤±è´¥");
                Err(EventHandlerError::ExternalServiceError(format!("åº“å­˜é¢„ç•™å¤±è´¥: {}", e)))
            }
        }
    }
}
```

### 2.3 å·¥ä½œæµå­ç³»ç»Ÿ

#### 3.3.1 Temporalå·¥ä½œæµé›†æˆ

```rust
use temporal_sdk::{WfContext, WfExitValue, WorkflowResult, ActivityOptions};
use temporal_sdk_core_protos::coresdk::workflow_commands::workflow_command::Variant;
use std::time::Duration;
use serde::{Serialize, Deserialize};
use uuid::Uuid;

/// è®¢å•å¤„ç†å·¥ä½œæµè¾“å…¥
#[derive(Clone, Serialize, Deserialize)]
pub struct OrderProcessingWorkflowInput {
    pub order_id: String,
    pub customer_id: String,
}

/// è®¢å•å¤„ç†å·¥ä½œæµ
#[temporal_sdk::workflow]
pub async fn order_processing_workflow(ctx: WfContext, input: OrderProcessingWorkflowInput) -> WorkflowResult<String> {
    // è®¾ç½®å·¥ä½œæµè¶…æ—¶
    ctx.set_workflow_timeout(std::time::Duration::from_hours(24));
    
    // 1. éªŒè¯è®¢å•
    let validate_result = ctx.activity("validate_order")
        .options(ActivityOptions {
            start_to_close_timeout: Some(Duration::from_secs(30)),
            retry_policy: Some(RetryPolicy {
                initial_interval: Duration::from_secs(1),
                backoff_coefficient: 2.0,
                maximum_interval: Duration::from_secs(100),
                maximum_attempts: 3,
                ..Default::default()
            }),
            ..Default::default()
        })
        .args(input.order_id.clone())
        .run::<ValidateOrderResult>()
        .await?;
        
    if !validate_result.is_valid {
        return Ok(WfExitValue::Normal(format!("è®¢å• {} éªŒè¯å¤±è´¥: {}", input.order_id, validate_result.reason.unwrap_or_default())));
    }
    
    // 2. å¤„ç†æ”¯ä»˜
    let payment_result = ctx.activity("process_payment")
        .options(ActivityOptions {
            start_to_close_timeout: Some(Duration::from_secs(60)),
            retry_policy: Some(RetryPolicy {
                initial_interval: Duration::from_secs(1),
                backoff_coefficient: 2.0,
                maximum_interval: Duration::from_secs(100),
                maximum_attempts: 5,
                non_retryable_error_types: vec!["PaymentDeclined".to_string()],
                ..Default::default()
            }),
            ..Default::default()
        })
        .args(ProcessPaymentInput {
            order_id: input.order_id.clone(),
            customer_id: input.customer_id.clone(),
        })
        .run::<ProcessPaymentResult>()
        .await?;
        
    if payment_result.status != "completed" {
        // æ”¯ä»˜å¤±è´¥,é‡Šæ”¾åº“å­˜
        ctx.activity("release_inventory")
            .args(input.order_id.clone())
            .run::<()>()
            .await?;
            
        return Ok(WfExitValue::Normal(format!("è®¢å• {} æ”¯ä»˜å¤±è´¥: {}", 
            input.order_id, 
            payment_result.failure_reason.unwrap_or_default()
        )));
    }
    
    // 3. åˆ›å»ºé…é€å•
    let shipment_result = ctx.activity("create_shipment")
        .args(CreateShipmentInput {
            order_id: input.order_id.clone(),
        })
        .run::<CreateShipmentResult>()
        .await?;
        
    // 4. å‘é€è®¢å•ç¡®è®¤é€šçŸ¥
    ctx.activity("send_order_confirmation")
        .args(SendOrderConfirmationInput {
            order_id: input.order_id.clone(),
            customer_id: input.customer_id.clone(),
            shipment_id: shipment_result.shipment_id,
            estimated_delivery: shipment_result.estimated_delivery,
        })
        .run::<()>()
        .await?;
        
    Ok(WfExitValue::Normal(format!("è®¢å• {} å¤„ç†å®Œæˆ", input.order_id)))
}
```

#### 3.3.2 å·¥ä½œæµæ´»åŠ¨å®ç°

```rust
use temporal_sdk::{ActivityContext, ActivityResult};
use serde::{Serialize, Deserialize};
use chrono::{DateTime, Utc};
use uuid::Uuid;
use sqlx::PgPool;

/// éªŒè¯è®¢å•æ´»åŠ¨
#[derive(Serialize, Deserialize)]
pub struct ValidateOrderResult {
    pub is_valid: bool,
    pub reason: Option<String>,
}

#[temporal_sdk::activity]
pub async fn validate_order(ctx: ActivityContext, order_id: String) -> ActivityResult<ValidateOrderResult> {
    let tracer = global::tracer("validate_order");
    let span = tracer.start_with_context("validate_order", &ctx);
    
    let state = ctx.state();
    let db_pool = state.db_pool.clone();
    
    // éªŒè¯è®¢å•æ˜¯å¦å­˜åœ¨
    let order = sqlx::query_as::<_, Order>(
        "SELECT id, customer_id, status FROM orders WHERE id = $1"
    )
    .bind(&order_id)
    .fetch_optional(&db_pool)
    .await
    .map_err(|e| anyhow::anyhow!("æ•°æ®åº“é”™è¯¯: {}", e))?;
    
    if let Some(order) = order {
        if order.status == "created" {
            Ok(ValidateOrderResult {
                is_valid: true,
                reason: None,
            })
        } else {
            Ok(ValidateOrderResult {
                is_valid: false,
                reason: Some(format!("è®¢å•çŠ¶æ€æ— æ•ˆ: {}", order.status)),
            })
        }
    } else {
        Ok(ValidateOrderResult {
            is_valid: false,
            reason: Some("è®¢å•ä¸å­˜åœ¨".to_string()),
        })
    }
}

/// å¤„ç†æ”¯ä»˜è¾“å…¥
#[derive(Serialize, Deserialize)]
pub struct ProcessPaymentInput {
    pub order_id: String,
    pub customer_id: String,
}

/// å¤„ç†æ”¯ä»˜ç»“æœ
#[derive(Serialize, Deserialize)]
pub struct ProcessPaymentResult {
    pub status: String,
    pub transaction_id: Option<String>,
    pub failure_reason: Option<String>,
}

#[temporal_sdk::activity]
pub async fn process_payment(ctx: ActivityContext, input: ProcessPaymentInput) -> ActivityResult<ProcessPaymentResult> {
    let state = ctx.state();
    let payment_service = state.payment_service.clone();
    
    // è·å–è®¢å•é‡‘é¢
    let order_amount = sqlx::query_scalar::<_, f64>(
        "SELECT total_amount FROM order_view WHERE id = $1"
    )
    .bind(&input.order_id)
    .fetch_one(&state.db_pool)
    .await
    .map_err(|e| anyhow::anyhow!("è·å–è®¢å•é‡‘é¢å¤±è´¥: {}", e))?;
    
    // å¤„ç†æ”¯ä»˜
    let payment_request = PaymentRequest {
        order_id: input.order_id.clone(),
        customer_id: input.customer_id.clone(),
        amount: order_amount,
        currency: "CNY".to_string(),
        idempotency_key: Uuid::new_v4().to_string(),
    };
    
    match payment_service.process_payment(payment_request).await {
        Ok(response) => {
            // æ›´æ–°è®¢å•æ”¯ä»˜çŠ¶æ€
            sqlx::query(
                "INSERT INTO order_payment (order_id, transaction_id, status, amount, processed_at)
                 VALUES ($1, $2, $3, $4, $5)"
            )
            .bind(&input.order_id)
            .bind(&response.transaction_id)
            .bind(&response.status)
            .bind(order_amount)
            .bind(Utc::now())
            .execute(&state.db_pool)
            .await
            .map_err(|e| anyhow::anyhow!("ä¿å­˜æ”¯ä»˜è®°å½•å¤±è´¥: {}", e))?;
            
            Ok(ProcessPaymentResult {
                status: response.status,
                transaction_id: Some(response.transaction_id),
                failure_reason: response.failure_reason,
            })
        },
        Err(e) => {
            // è®°å½•æ”¯ä»˜é”™è¯¯
            sqlx::query(
                "INSERT INTO order_payment (order_id, status, amount, failure_reason, processed_at)
                 VALUES ($1, $2, $3, $4, $5)"
            )
            .bind(&input.order_id)
            .bind("failed")
            .bind(order_amount)
            .bind(e.to_string())
            .bind(Utc::now())
            .execute(&state.db_pool)
            .await
            .map_err(|e| anyhow::anyhow!("ä¿å­˜æ”¯ä»˜è®°å½•å¤±è´¥: {}", e))?;
            
            Ok(ProcessPaymentResult {
                status: "failed".to_string(),
                transaction_id: None,
                failure_reason: Some(e.to_string()),
            })
        }
    }
}
```

### 2.4 å¤–éƒ¨ç³»ç»Ÿé›†æˆå­ç³»ç»Ÿ

#### 4.4.1 æŠ½è±¡æœåŠ¡å®¢æˆ·ç«¯

```rust
use async_trait::async_trait;
use serde::{Serialize, Deserialize};
use std::time::Duration;
use tonic::transport::{Channel, Endpoint};
use tracing::{info, error, instrument};
use resilient::{CircuitBreaker, CircuitBreakerConfig};

/// åº“å­˜æœåŠ¡å®¢æˆ·ç«¯ç‰¹è´¨
#[async_trait]
pub trait InventoryService: Send + Sync + 'static {
    /// æ£€æŸ¥åº“å­˜å¯ç”¨æ€§
    async fn check_availability(&self, items: &
# å¤æ‚åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸å®ç°æ–¹æ¡ˆï¼ˆç»­ï¼‰

### 2.5 å¤–éƒ¨ç³»ç»Ÿé›†æˆå­ç³»ç»Ÿç»­

#### 5.5.1 æŠ½è±¡æœåŠ¡å®¢æˆ·ç«¯ç»­

```rust
use async_trait::async_trait;
use serde::{Serialize, Deserialize};
use std::time::Duration;
use tonic::transport::{Channel, Endpoint};
use tracing::{info, error, instrument};
use resilient::{CircuitBreaker, CircuitBreakerConfig};

/// åº“å­˜æœåŠ¡å®¢æˆ·ç«¯ç‰¹è´¨
#[async_trait]
pub trait InventoryService: Send + Sync + 'static {
    /// æ£€æŸ¥åº“å­˜å¯ç”¨æ€§
    async fn check_availability(&self, items: &[OrderItem]) -> Result<InventoryCheckResult, ServiceError>;
    
    /// é¢„ç•™åº“å­˜
    async fn reserve_inventory(&self, request: ReserveInventoryRequest) -> Result<ReserveInventoryResponse, ServiceError>;
    
    /// é‡Šæ”¾åº“å­˜
    async fn release_inventory(&self, order_id: &str) -> Result<(), ServiceError>;
}

/// åº“å­˜æ£€æŸ¥ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InventoryCheckResult {
    pub all_available: bool,
    pub unavailable_items: Vec<UnavailableItem>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UnavailableItem {
    pub product_id: String,
    pub requested_quantity: u32,
    pub available_quantity: u32,
}

/// é¢„ç•™åº“å­˜è¯·æ±‚
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReserveInventoryRequest {
    pub order_id: String,
    pub items: Vec<InventoryItem>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InventoryItem {
    pub product_id: String,
    pub quantity: u32,
}

/// é¢„ç•™åº“å­˜å“åº”
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReserveInventoryResponse {
    pub reservation_id: String,
    pub status: String,
    pub reserved_at: DateTime<Utc>,
}

/// gRPCåº“å­˜æœåŠ¡å®¢æˆ·ç«¯å®ç°
pub struct GrpcInventoryServiceClient {
    inner_client: inventory_proto::inventory_client::InventoryClient<Channel>,
    circuit_breaker: CircuitBreaker,
}

impl GrpcInventoryServiceClient {
    pub async fn new(service_url: &str) -> Result<Self, ServiceError> {
        let channel = Endpoint::from_shared(service_url.to_string())?
            .timeout(Duration::from_secs(5))
            .connect()
            .await?;
            
        let inner_client = inventory_proto::inventory_client::InventoryClient::new(channel);
        
        let breaker_config = CircuitBreakerConfig {
            failure_threshold: 5,
            success_threshold: 2,
            open_duration: Duration::from_secs(30),
            ..Default::default()
        };
        
        let circuit_breaker = CircuitBreaker::new("inventory_service", breaker_config);
        
        Ok(Self {
            inner_client,
            circuit_breaker,
        })
    }
}

#[async_trait]
impl InventoryService for GrpcInventoryServiceClient {
    #[instrument(skip(self, items), fields(items_count = items.len()))]
    async fn check_availability(&self, items: &[OrderItem]) -> Result<InventoryCheckResult, ServiceError> {
        // ä½¿ç”¨æ–­è·¯å™¨æ‰§è¡Œè¯·æ±‚
        self.circuit_breaker.execute(|| async {
            let request = inventory_proto::CheckAvailabilityRequest {
                items: items.iter().map(|item| inventory_proto::InventoryItem {
                    product_id: item.product_id.clone(),
                    quantity: item.quantity,
                }).collect(),
            };
            
            let response = self.inner_client.clone().check_availability(request).await?;
            let result = response.into_inner();
            
            Ok(InventoryCheckResult {
                all_available: result.all_available,
                unavailable_items: result.unavailable_items.into_iter().map(|item| UnavailableItem {
                    product_id: item.product_id,
                    requested_quantity: item.requested_quantity,
                    available_quantity: item.available_quantity,
                }).collect(),
            })
        }).await.map_err(|e| match e {
            resilient::Error::CircuitOpen => ServiceError::ServiceUnavailable("åº“å­˜æœåŠ¡æš‚æ—¶ä¸å¯ç”¨".to_string()),
            resilient::Error::OperationError(inner) => inner,
        })
    }
    
    #[instrument(skip(self, request), fields(order_id = %request.order_id, items_count = request.items.len()))]
    async fn reserve_inventory(&self, request: ReserveInventoryRequest) -> Result<ReserveInventoryResponse, ServiceError> {
        self.circuit_breaker.execute(|| async {
            let proto_request = inventory_proto::ReserveInventoryRequest {
                order_id: request.order_id.clone(),
                items: request.items.iter().map(|item| inventory_proto::InventoryItem {
                    product_id: item.product_id.clone(),
                    quantity: item.quantity,
                }).collect(),
            };
            
            let response = self.inner_client.clone().reserve_inventory(proto_request).await?;
            let result = response.into_inner();
            
            let reserved_at = DateTime::<Utc>::from_str(&result.reserved_at)
                .map_err(|e| ServiceError::ParsingError(format!("æ—¥æœŸè§£æé”™è¯¯: {}", e)))?;
                
            Ok(ReserveInventoryResponse {
                reservation_id: result.reservation_id,
                status: result.status,
                reserved_at,
            })
        }).await.map_err(|e| match e {
            resilient::Error::CircuitOpen => ServiceError::ServiceUnavailable("åº“å­˜æœåŠ¡æš‚æ—¶ä¸å¯ç”¨".to_string()),
            resilient::Error::OperationError(inner) => inner,
        })
    }
    
    #[instrument(skip(self), fields(order_id = %order_id))]
    async fn release_inventory(&self, order_id: &str) -> Result<(), ServiceError> {
        self.circuit_breaker.execute(|| async {
            let request = inventory_proto::ReleaseInventoryRequest {
                order_id: order_id.to_string(),
            };
            
            let _response = self.inner_client.clone().release_inventory(request).await?;
            Ok(())
        }).await.map_err(|e| match e {
            resilient::Error::CircuitOpen => ServiceError::ServiceUnavailable("åº“å­˜æœåŠ¡æš‚æ—¶ä¸å¯ç”¨".to_string()),
            resilient::Error::OperationError(inner) => inner,
        })
    }
}
```

#### 5.5.2 å¤–éƒ¨ç³»ç»Ÿé€‚é…å™¨å·¥å‚

```rust
use std::sync::Arc;
use tokio::sync::RwLock;

/// å¤–éƒ¨æœåŠ¡å·¥å‚
pub struct ExternalServiceFactory {
    config: Arc<ServiceConfig>,
    service_registry: Arc<ServiceRegistry>,
    inventory_client: RwLock<Option<Arc<dyn InventoryService>>>,
    payment_client: RwLock<Option<Arc<dyn PaymentService>>>,
    shipping_client: RwLock<Option<Arc<dyn ShippingService>>>,
    notification_client: RwLock<Option<Arc<dyn NotificationService>>>,
}

impl ExternalServiceFactory {
    pub fn new(config: ServiceConfig, service_registry: Arc<ServiceRegistry>) -> Self {
        Self {
            config: Arc::new(config),
            service_registry,
            inventory_client: RwLock::new(None),
            payment_client: RwLock::new(None),
            shipping_client: RwLock::new(None),
            notification_client: RwLock::new(None),
        }
    }
    
    /// è·å–åº“å­˜æœåŠ¡å®¢æˆ·ç«¯
    pub async fn get_inventory_service(&self) -> Arc<dyn InventoryService> {
        let mut client = self.inventory_client.write().await;
        
        if let Some(ref c) = *client {
            return c.clone();
        }
        
        // ä»æœåŠ¡æ³¨å†Œè¡¨è·å–æœåŠ¡åœ°å€
        let service_url = match self.service_registry.get_service_url("inventory-service").await {
            Ok(url) => url,
            Err(_) => self.config.inventory_service_fallback_url.clone(),
        };
        
        // åˆ›å»ºæ–°å®¢æˆ·ç«¯
        let new_client: Arc<dyn InventoryService> = match self.config.inventory_service_type.as_str() {
            "grpc" => Arc::new(GrpcInventoryServiceClient::new(&service_url).await
                .expect("æ— æ³•åˆ›å»ºåº“å­˜æœåŠ¡å®¢æˆ·ç«¯")),
            "rest" => Arc::new(RestInventoryServiceClient::new(&service_url)
                .expect("æ— æ³•åˆ›å»ºåº“å­˜æœåŠ¡å®¢æˆ·ç«¯")),
            _ => panic!("ä¸æ”¯æŒçš„åº“å­˜æœåŠ¡ç±»å‹: {}", self.config.inventory_service_type),
        };
        
        *client = Some(new_client.clone());
        new_client
    }
    
    /// è·å–æ”¯ä»˜æœåŠ¡å®¢æˆ·ç«¯
    pub async fn get_payment_service(&self) -> Arc<dyn PaymentService> {
        let mut client = self.payment_client.write().await;
        
        if let Some(ref c) = *client {
            return c.clone();
        }
        
        // ä»æœåŠ¡æ³¨å†Œè¡¨è·å–æœåŠ¡åœ°å€
        let service_url = match self.service_registry.get_service_url("payment-service").await {
            Ok(url) => url,
            Err(_) => self.config.payment_service_fallback_url.clone(),
        };
        
        // åˆ›å»ºæ–°å®¢æˆ·ç«¯
        let new_client: Arc<dyn PaymentService> = match self.config.payment_service_type.as_str() {
            "grpc" => Arc::new(GrpcPaymentServiceClient::new(&service_url).await
                .expect("æ— æ³•åˆ›å»ºæ”¯ä»˜æœåŠ¡å®¢æˆ·ç«¯")),
            "rest" => Arc::new(RestPaymentServiceClient::new(&service_url)
                .expect("æ— æ³•åˆ›å»ºæ”¯ä»˜æœåŠ¡å®¢æˆ·ç«¯")),
            _ => panic!("ä¸æ”¯æŒçš„æ”¯ä»˜æœåŠ¡ç±»å‹: {}", self.config.payment_service_type),
        };
        
        *client = Some(new_client.clone());
        new_client
    }
    
    // å…¶ä»–æœåŠ¡çš„è·å–æ–¹æ³•ç±»ä¼¼...
}
```

### 2.6 æŸ¥è¯¢æœåŠ¡å­ç³»ç»Ÿ

#### 6.6.1 CQRSæŸ¥è¯¢å±‚

```rust
use actix_web::{web, HttpResponse, Responder};
use sqlx::PgPool;
use serde::{Serialize, Deserialize};
use tracing::{info, error, instrument};
use redis::AsyncCommands;

/// è®¢å•æŸ¥è¯¢æœåŠ¡
pub struct OrderQueryService {
    db_pool: PgPool,
    redis_client: redis::Client,
    metrics: Arc<Metrics>,
}

impl OrderQueryService {
    pub fn new(db_pool: PgPool, redis_client: redis::Client, metrics: Arc<Metrics>) -> Self {
        Self {
            db_pool,
            redis_client,
            metrics,
        }
    }
    
    /// è·å–è®¢å•è¯¦æƒ…
    #[instrument(skip(self), fields(order_id = %order_id))]
    pub async fn get_order(&self, order_id: &str) -> Result<Option<OrderDetailsDto>, QueryError> {
        let timer = self.metrics.start_timer("order_query_get_order");
        
        // å°è¯•ä»ç¼“å­˜è·å–
        let mut redis_conn = self.redis_client.get_async_connection().await
            .map_err(|e| QueryError::CacheError(e.to_string()))?;
            
        let cache_key = format!("order:{}", order_id);
        
        if let Ok(cached_data) = redis_conn.get::<_, String>(&cache_key).await {
            info!("ä»ç¼“å­˜è·å–è®¢å•");
            self.metrics.increment_counter("order_query_cache_hit");
            
            return serde_json::from_str(&cached_data)
                .map(Some)
                .map_err(|e| QueryError::DeserializationError(e.to_string()));
        }
        
        self.metrics.increment_counter("order_query_cache_miss");
        
        // ä»æ•°æ®åº“è·å–è®¢å•åŸºæœ¬ä¿¡æ¯
        let order = sqlx::query_as::<_, OrderDetails>(
            r#"
            SELECT 
                o.id, o.customer_id, o.status, o.total_amount, o.created_at,
                c.first_name, c.last_name, c.email,
                a.street, a.city, a.state, a.country, a.postal_code
            FROM 
                order_view o
                LEFT JOIN customer c ON o.customer_id = c.id
                LEFT JOIN address a ON o.shipping_address_id = a.id
            WHERE 
                o.id = $1
            "#
        )
        .bind(order_id)
        .fetch_optional(&self.db_pool)
        .await
        .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
        
        if let Some(order) = order {
            // è·å–è®¢å•é¡¹
            let items = sqlx::query_as::<_, OrderItemDto>(
                "SELECT product_id, name, quantity, unit_price FROM order_item_view WHERE order_id = $1"
            )
            .bind(order_id)
            .fetch_all(&self.db_pool)
            .await
            .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
            
            // è·å–æ”¯ä»˜ä¿¡æ¯
            let payment = sqlx::query_as::<_, PaymentInfoDto>(
                "SELECT transaction_id, status, amount, processed_at FROM order_payment WHERE order_id = $1"
            )
            .bind(order_id)
            .fetch_optional(&self.db_pool)
            .await
            .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
            
            // è·å–é…é€ä¿¡æ¯
            let shipment = sqlx::query_as::<_, ShipmentInfoDto>(
                "SELECT shipment_id, carrier, tracking_number, status, estimated_delivery FROM order_shipment WHERE order_id = $1"
            )
            .bind(order_id)
            .fetch_optional(&self.db_pool)
            .await
            .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
            
            let result = OrderDetailsDto {
                id: order.id,
                customer: CustomerDto {
                    id: order.customer_id,
                    first_name: order.first_name,
                    last_name: order.last_name,
                    email: order.email,
                },
                items,
                shipping_address: AddressDto {
                    street: order.street,
                    city: order.city,
                    state: order.state,
                    country: order.country,
                    postal_code: order.postal_code,
                },
                payment,
                shipment,
                status: order.status,
                total_amount: order.total_amount,
                created_at: order.created_at,
            };
            
            // ç¼“å­˜ç»“æœ
            if let Ok(json) = serde_json::to_string(&result) {
                let _: Result<(), _> = redis_conn.set_ex(&cache_key, json, 300).await; // 5åˆ†é’Ÿè¿‡æœŸ
            }
            
            info!("æˆåŠŸè·å–è®¢å•è¯¦æƒ…");
            timer.observe_duration();
            
            Ok(Some(result))
        } else {
            timer.observe_duration();
            Ok(None)
        }
    }
    
    /// è·å–å®¢æˆ·è®¢å•åˆ—è¡¨
    #[instrument(skip(self), fields(customer_id = %customer_id))]
    pub async fn get_customer_orders(
        &self, 
        customer_id: &str,
        page: i64,
        page_size: i64
    ) -> Result<CustomerOrdersResult, QueryError> {
        let offset = (page - 1) * page_size;
        
        // è·å–æ€»è®¢å•æ•°
        let total_count: i64 = sqlx::query_scalar(
            "SELECT COUNT(*) FROM order_view WHERE customer_id = $1"
        )
        .bind(customer_id)
        .fetch_one(&self.db_pool)
        .await
        .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
        
        // è·å–åˆ†é¡µè®¢å•åˆ—è¡¨
        let orders = sqlx::query_as::<_, CustomerOrderDto>(
            r#"
            SELECT 
                id, status, total_amount, created_at,
                (SELECT COUNT(*) FROM order_item_view WHERE order_id = order_view.id) as item_count
            FROM 
                order_view 
            WHERE 
                customer_id = $1
            ORDER BY 
                created_at DESC
            LIMIT $2 OFFSET $3
            "#
        )
        .bind(customer_id)
        .bind(page_size)
        .bind(offset)
        .fetch_all(&self.db_pool)
        .await
        .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
        
        Ok(CustomerOrdersResult {
            customer_id: customer_id.to_string(),
            orders,
            page,
            page_size,
            total_count,
            total_pages: (total_count + page_size - 1) / page_size,
        })
    }
    
    /// æœç´¢è®¢å•
    #[instrument(skip(self), fields(query = %query))]
    pub async fn search_orders(&self, query: &str, page: i64, page_size: i64) -> Result<OrderSearchResult, QueryError> {
        let offset = (page - 1) * page_size;
        
        // ä½¿ç”¨å…¨æ–‡æ£€ç´¢æœç´¢è®¢å•
        let search_query = format!("%{}%", query);
        
        // è·å–åŒ¹é…è®¢å•æ€»æ•°
        let total_count: i64 = sqlx::query_scalar(
            r#"
            SELECT COUNT(*) FROM order_view o
            LEFT JOIN customer c ON o.customer_id = c.id
            WHERE 
                o.id::text ILIKE $1 OR
                c.email ILIKE $1 OR
                c.first_name ILIKE $1 OR
                c.last_name ILIKE $1
            "#
        )
        .bind(&search_query)
        .fetch_one(&self.db_pool)
        .await
        .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
        
        // è·å–åˆ†é¡µæœç´¢ç»“æœ
        let orders = sqlx::query_as::<_, SearchOrderDto>(
            r#"
            SELECT 
                o.id, o.status, o.total_amount, o.created_at,
                c.id as customer_id, c.first_name, c.last_name, c.email
            FROM 
                order_view o
                LEFT JOIN customer c ON o.customer_id = c.id
            WHERE 
                o.id::text ILIKE $1 OR
                c.email ILIKE $1 OR
                c.first_name ILIKE $1 OR
                c.last_name ILIKE $1
            ORDER BY
                o.created_at DESC
            LIMIT $2 OFFSET $3
            "#
        )
        .bind(&search_query)
        .bind(page_size)
        .bind(offset)
        .fetch_all(&self.db_pool)
        .await
        .map_err(|e| QueryError::DatabaseError(e.to_string()))?;
        
        Ok(OrderSearchResult {
            query: query.to_string(),
            orders,
            page,
            page_size,
            total_count,
            total_pages: (total_count + page_size - 1) / page_size,
        })
    }
}
```

#### 6.6.2 APIç«¯ç‚¹

```rust
use actix_web::{web, HttpResponse, Responder, get, post};
use serde::{Serialize, Deserialize};

/// è®¢å•æ§åˆ¶å™¨
pub struct OrderController {
    query_service: Arc<OrderQueryService>,
    command_bus: Arc<CommandBus>,
}

impl OrderController {
    pub fn new(query_service: Arc<OrderQueryService>, command_bus: Arc<CommandBus>) -> Self {
        Self {
            query_service,
            command_bus,
        }
    }
}

/// æ³¨å†ŒAPIè·¯ç”±
pub fn register_routes(config: &mut web::ServiceConfig, controller: Arc<OrderController>) {
    config.service(
        web::scope("/api/v1/orders")
            .route("", web::post().to(create_order))
            .route("/{id}", web::get().to(get_order))
            .route("/{id}/cancel", web::post().to(cancel_order))
            .route("/customer/{customer_id}", web::get().to(get_customer_orders))
            .route("/search", web::get().to(search_orders))
    );
}

/// åˆ›å»ºè®¢å•è¯·æ±‚
#[derive(Serialize, Deserialize)]
struct CreateOrderRequest {
    customer_id: String,
    items: Vec<OrderItemRequest>,
    shipping_address: AddressRequest,
}

/// åˆ›å»ºè®¢å•ç«¯ç‚¹
#[instrument(skip(req, data), fields(customer_id = %req.customer_id))]
async fn create_order(
    req: web::Json<CreateOrderRequest>,
    data: web::Data<Arc<OrderController>>,
) -> impl Responder {
    let command = CreateOrderCommand {
        command_id: Uuid::new_v4(),
        customer_id: req.customer_id.clone(),
        items: req.items.iter().map(|item| OrderItem {
            product_id: item.product_id.clone(),
            quantity: item.quantity,
            unit_price: item.unit_price,
        }).collect(),
        shipping_address: Address {
            street: req.shipping_address.street.clone(),
            city: req.shipping_address.city.clone(),
            state: req.shipping_address.state.clone(),
            country: req.shipping_address.country.clone(),
            postal_code: req.shipping_address.postal_code.clone(),
        },
        correlation_id: None,
    };
    
    match data.command_bus.dispatch(command).await {
        Ok(events) => {
            // ä»äº‹ä»¶ä¸­æå–è®¢å•ID
            if let Some(event) = events.iter().find(|e| e.event_type() == "order_created") {
                if let Ok(created_event) = serde_json::from_value::<OrderCreatedEvent>(serde_json::to_value(event).unwrap()) {
                    return HttpResponse::Created().json(json!({
                        "order_id": created_event.aggregate_id,
                        "status": "created",
                        "message": "è®¢å•åˆ›å»ºæˆåŠŸ"
                    }));
                }
            }
            
            HttpResponse::InternalServerError().json(json!({
                "error": "å¤„ç†è®¢å•åˆ›å»ºå¤±è´¥"
            }))
        },
        Err(e) => {
            error!(error = %e, "è®¢å•åˆ›å»ºå¤±è´¥");
            
            let status_code = match e {
                CommandError::ValidationError(_) => StatusCode::BAD_REQUEST,
                CommandError::NotFound(_) => StatusCode::NOT_FOUND,
                CommandError::Unauthorized(_) => StatusCode::UNAUTHORIZED,
                _ => StatusCode::INTERNAL_SERVER_ERROR,
            };
            
            HttpResponse::build(status_code).json(json!({
                "error": e.to_string()
            }))
        }
    }
}

/// è·å–è®¢å•ç«¯ç‚¹
#[instrument(skip(data), fields(order_id = %id))]
async fn get_order(
    id: web::Path<String>,
    data: web::Data<Arc<OrderController>>,
) -> impl Responder {
    match data.query_service.get_order(&id).await {
        Ok(Some(order)) => HttpResponse::Ok().json(order),
        Ok(None) => HttpResponse::NotFound().json(json!({
            "error": "è®¢å•ä¸å­˜åœ¨"
        })),
        Err(e) => {
            error!(error = %e, "è·å–è®¢å•å¤±è´¥");
            HttpResponse::InternalServerError().json(json!({
                "error": "è·å–è®¢å•å¤±è´¥",
                "details": e.to_string()
            }))
        }
    }
}
```

### 2.7 ç›‘æ§ä¸å¯è§‚æµ‹æ€§å­ç³»ç»Ÿ

#### 7.7.1 åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ

```rust
use opentelemetry::{global, sdk::propagation::TraceContextPropagator};
use opentelemetry::sdk::trace::{self, IdGenerator, Sampler};
use opentelemetry::sdk::Resource;
use opentelemetry_jaeger::{Propagator, new_pipeline};
use opentelemetry::KeyValue;
use tracing_opentelemetry::OpenTelemetrySpanExt;
use tracing_subscriber::{layer::SubscriberExt, Registry};
use tracing_subscriber::layer::Layer;

/// åˆå§‹åŒ–è¿½è¸ªç³»ç»Ÿ
pub fn init_tracer(service_name: &str, jaeger_endpoint: &str) -> Result<(), Box<dyn Error>> {
    // è®¾ç½®å…¨å±€ä¼ æ’­å™¨
    global::set_text_map_propagator(TraceContextPropagator::new());
    
    // åˆ›å»ºJaegerå¯¼å‡ºå™¨
    let tracer = new_pipeline()
        .with_service_name(service_name)
        .with_collector_endpoint(jaeger_endpoint)
        .with_trace_config(
            trace::config()
                .with_sampler(Sampler::AlwaysOn)
                .with_id_generator(IdGenerator::default())
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", service_name.to_string()),
                    KeyValue::new("service.version", env!("CARGO_PKG_VERSION").to_string()),
                    KeyValue::new("deployment.environment", std::env::var("ENVIRONMENT").unwrap_or_else(|_| "development".to_string())),
                ]))
        )
        .install_batch(opentelemetry::runtime::Tokio)?;
    
    // åˆ›å»ºOpenTelemetry tracingå±‚
    let telemetry = tracing_opentelemetry::layer().with_tracer(tracer);
    
    // åˆ›å»ºæ ¼å¼åŒ–æ—¥å¿—å±‚
    let fmt_layer = tracing_subscriber::fmt::layer()
        .with_ansi(true)
        .with_target(true);
    
    // åˆ›å»ºEnvFilterå±‚
    let filter_layer = tracing_subscriber::EnvFilter::try_from_default_env()
        .or_else(|_| tracing_subscriber::EnvFilter::try_new("info"))?;
    
    // æ³¨å†Œè®¢é˜…è€…
    Registry::default()
        .with(filter_layer)
        .with(fmt_layer)
        .with(telemetry)
        .init();
    
    Ok(())
}

/// æå–è¯·æ±‚ä¸Šä¸‹æ–‡
pub fn extract_tracing_context(req: &HttpRequest) -> tracing::Span {
    let parent_cx = global::get_text_map_propagator(|propagator| {
        propagator.extract(&RequestHeaderCarrier(req.headers()))
    });
    
    tracing::span!(
        tracing::Level::INFO, 
        "http_request",
        method = %req.method(),
        uri = %req.uri(),
        trace_id = tracing::field::Empty,
        span_id = tracing::field::Empty
    ).with_subscriber(move |id, _| {
        id.with_current(|current| {
            current.record("trace_id", &tracing::field::display(parent_cx.span().span_context().trace_id()));
            current.record("span_id", &tracing::field::display(parent_cx.span().span_context().span_id()));
        })
    })
}

/// HTTPè¯·æ±‚å¤´è½½ä½“
struct RequestHeaderCarrier<'a>(&'a HeaderMap);

impl<'a> Extractor for RequestHeaderCarrier<'a> {
    fn get(&self, key: &str) -> Option<&str> {
        self.0.get(key).and_then(|v| v.to_str().ok())
    }
    
    fn keys(&self) -> Vec<&str> {
        self.0.keys().map(|k| k.as_str()).collect()
    }
}
```

#### 7.7.2 æŒ‡æ ‡ç›‘æ§å®ç°

```rust
use metrics::{counter, gauge, histogram};
use metrics_exporter_prometheus::{PrometheusBuilder, PrometheusHandle};
use std::time::{Duration, Instant};
use tokio::time::interval;
use std::sync::Arc;

/// åº”ç”¨æŒ‡æ ‡
pub struct Metrics {
    prometheus_handle: PrometheusHandle,
}

impl Metrics {
    pub fn new() -> Self {
        let builder = PrometheusBuilder::new();
        let handle = builder
            .install()
            .expect("æ— æ³•å®‰è£…Prometheuså¯¼å‡ºå™¨");
            
        Self {
            prometheus_handle: handle,
        }
    }
    
    /// è·å–PrometheusæŒ‡æ ‡å¯¼å‡ºç‚¹
    pub fn get_prometheus_handler(&self) -> PrometheusHandle {
        self.prometheus_handle.clone()
    }
    
    /// é€’å¢è®¡æ•°å™¨
    pub fn increment_counter(&self, name: &str) {
        counter!(name, 1);
    }
    
    /// é€’å¢å¸¦æ ‡ç­¾çš„è®¡æ•°å™¨
    pub fn increment_counter_with_labels(&self, name: &str, labels: &[(&str, &str)]) {
        let labels: Vec<(String, String)> = labels.iter()
            .map(|(k, v)| (k.to_string(), v.to_string()))
            .collect();
            
        counter!(name, 1, &labels);
    }
    
    /// è®°å½•æ•°å€¼
    pub fn set_gauge(&self, name: &str, value: f64) {
        gauge!(name, value);
    }
    
    /// è®°å½•ç›´æ–¹å›¾æ•°æ®
# å¤æ‚åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸å®ç°æ–¹æ¡ˆï¼ˆç»­ï¼‰

### 2.8 ç›‘æ§ä¸å¯è§‚æµ‹æ€§å­ç³»ç»Ÿç»­

#### 8.8.1 æŒ‡æ ‡ç›‘æ§å®ç°ç»­

```rust
use metrics::{counter, gauge, histogram};
use metrics_exporter_prometheus::{PrometheusBuilder, PrometheusHandle};
use std::time::{Duration, Instant};
use tokio::time::interval;
use std::sync::Arc;

/// åº”ç”¨æŒ‡æ ‡
pub struct Metrics {
    prometheus_handle: PrometheusHandle,
}

impl Metrics {
    pub fn new() -> Self {
        let builder = PrometheusBuilder::new();
        let handle = builder
            .install()
            .expect("æ— æ³•å®‰è£…Prometheuså¯¼å‡ºå™¨");
            
        Self {
            prometheus_handle: handle,
        }
    }
    
    /// è·å–PrometheusæŒ‡æ ‡å¯¼å‡ºç‚¹
    pub fn get_prometheus_handler(&self) -> PrometheusHandle {
        self.prometheus_handle.clone()
    }
    
    /// é€’å¢è®¡æ•°å™¨
    pub fn increment_counter(&self, name: &str) {
        counter!(name, 1);
    }
    
    /// é€’å¢å¸¦æ ‡ç­¾çš„è®¡æ•°å™¨
    pub fn increment_counter_with_labels(&self, name: &str, labels: &[(&str, &str)]) {
        let labels: Vec<(String, String)> = labels.iter()
            .map(|(k, v)| (k.to_string(), v.to_string()))
            .collect();
            
        counter!(name, 1, &labels);
    }
    
    /// è®°å½•æ•°å€¼
    pub fn set_gauge(&self, name: &str, value: f64) {
        gauge!(name, value);
    }
    
    /// è®°å½•ç›´æ–¹å›¾æ•°æ®
    pub fn record_histogram(&self, name: &str, value: f64) {
        histogram!(name, value);
    }
    
    /// å¼€å§‹è®¡æ—¶å™¨
    pub fn start_timer(&self, name: &str) -> OperationTimer {
        OperationTimer {
            name: name.to_string(),
            start: Instant::now(),
        }
    }
    
    /// å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†
    pub async fn start_system_metrics_collector(&self) -> tokio::task::JoinHandle<()> {
        let metrics_interval = Duration::from_secs(15);
        
        tokio::spawn(async move {
            let mut interval = interval(metrics_interval);
            
            // è¿™é‡Œå¯ä»¥ä½¿ç”¨ç³»ç»Ÿåº“å¦‚sysinfoæ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            // è¿™é‡Œä»¥ç®€å•æ¨¡æ‹Ÿä¸ºä¾‹
            loop {
                interval.tick().await;
                
                // æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                gauge!("system.memory.used_mb", 1024.0);
                gauge!("system.cpu.usage_percent", 45.0);
                gauge!("system.disk.free_gb", 100.0);
            }
        })
    }
}

/// æ“ä½œè®¡æ—¶å™¨
pub struct OperationTimer {
    name: String,
    start: Instant,
}

impl OperationTimer {
    pub fn observe_duration(&self) {
        let duration = self.start.elapsed();
        histogram!(&self.name, duration.as_secs_f64());
    }
}

impl Drop for OperationTimer {
    fn drop(&mut self) {
        self.observe_duration();
    }
}

/// å¥åº·æ£€æŸ¥æœåŠ¡
pub struct HealthCheckService {
    db_pool: PgPool,
    redis_client: redis::Client,
    kafka_producer: Arc<EventProducer>,
}

impl HealthCheckService {
    pub fn new(
        db_pool: PgPool,
        redis_client: redis::Client,
        kafka_producer: Arc<EventProducer>,
    ) -> Self {
        Self {
            db_pool,
            redis_client,
            kafka_producer,
        }
    }
    
    /// æ£€æŸ¥æ‰€æœ‰ä¾èµ–å¥åº·çŠ¶æ€
    pub async fn check_health(&self) -> HealthStatus {
        let db_check = self.check_database().await;
        let redis_check = self.check_redis().await;
        let kafka_check = self.check_kafka().await;
        
        let status = if db_check.status == "up" 
                       && redis_check.status == "up" 
                       && kafka_check.status == "up" {
            "up"
        } else {
            "down"
        };
        
        HealthStatus {
            status: status.to_string(),
            components: vec![
                db_check,
                redis_check,
                kafka_check,
            ],
            timestamp: Utc::now(),
        }
    }
    
    async fn check_database(&self) -> ComponentHealth {
        let start = Instant::now();
        
        match sqlx::query("SELECT 1").execute(&self.db_pool).await {
            Ok(_) => ComponentHealth {
                name: "database".to_string(),
                status: "up".to_string(),
                details: None,
                response_time_ms: start.elapsed().as_millis() as u64,
            },
            Err(e) => ComponentHealth {
                name: "database".to_string(),
                status: "down".to_string(),
                details: Some(format!("æ•°æ®åº“è¿æ¥å¤±è´¥: {}", e)),
                response_time_ms: start.elapsed().as_millis() as u64,
            },
        }
    }
    
    async fn check_redis(&self) -> ComponentHealth {
        let start = Instant::now();
        
        match self.redis_client.get_async_connection().await {
            Ok(mut conn) => {
                match conn.ping::<String>().await {
                    Ok(_) => ComponentHealth {
                        name: "redis".to_string(),
                        status: "up".to_string(),
                        details: None,
                        response_time_ms: start.elapsed().as_millis() as u64,
                    },
                    Err(e) => ComponentHealth {
                        name: "redis".to_string(),
                        status: "down".to_string(),
                        details: Some(format!("Redis pingå¤±è´¥: {}", e)),
                        response_time_ms: start.elapsed().as_millis() as u64,
                    },
                }
            },
            Err(e) => ComponentHealth {
                name: "redis".to_string(),
                status: "down".to_string(),
                details: Some(format!("Redisè¿æ¥å¤±è´¥: {}", e)),
                response_time_ms: start.elapsed().as_millis() as u64,
            },
        }
    }
    
    async fn check_kafka(&self) -> ComponentHealth {
        let start = Instant::now();
        
        // ç®€å•çš„kafkaå¥åº·æ£€æŸ¥
        match self.kafka_producer.check_connectivity().await {
            Ok(_) => ComponentHealth {
                name: "kafka".to_string(),
                status: "up".to_string(),
                details: None,
                response_time_ms: start.elapsed().as_millis() as u64,
            },
            Err(e) => ComponentHealth {
                name: "kafka".to_string(),
                status: "down".to_string(),
                details: Some(format!("Kafkaè¿æ¥å¤±è´¥: {}", e)),
                response_time_ms: start.elapsed().as_millis() as u64,
            },
        }
    }
}

/// å¥åº·çŠ¶æ€
#[derive(Serialize)]
struct HealthStatus {
    status: String,
    components: Vec<ComponentHealth>,
    timestamp: DateTime<Utc>,
}

/// ç»„ä»¶å¥åº·çŠ¶æ€
#[derive(Serialize)]
struct ComponentHealth {
    name: String,
    status: String,
    details: Option<String>,
    response_time_ms: u64,
}
```

### 2.9 é…ç½®ä¸æœåŠ¡å‘ç°å­ç³»ç»Ÿ

#### 9.9.1 åŠ¨æ€é…ç½®ç®¡ç†

```rust
use config::{Config, ConfigError, Environment, File};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::watch;
use tokio::time::{Duration, interval};
use tracing::{info, error};

/// åº”ç”¨é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AppConfig {
    pub server: ServerConfig,
    pub database: DatabaseConfig,
    pub redis: RedisConfig,
    pub kafka: KafkaConfig,
    pub temporal: TemporalConfig,
    pub tracing: TracingConfig,
    pub external_services: ExternalServicesConfig,
    pub circuit_breakers: CircuitBreakersConfig,
    pub cache: CacheConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServerConfig {
    pub host: String,
    pub port: u16,
    pub workers: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatabaseConfig {
    pub url: String,
    pub max_connections: u32,
    pub min_connections: u32,
    pub max_lifetime: Duration,
    pub idle_timeout: Duration,
}

// å…¶ä»–å…·ä½“é…ç½®éƒ¨åˆ†çœç•¥...

/// é…ç½®ç®¡ç†å™¨
pub struct ConfigManager {
    current_config: Arc<AppConfig>,
    config_sender: watch::Sender<Arc<AppConfig>>,
    config_receiver: watch::Receiver<Arc<AppConfig>>,
    config_source: Box<dyn ConfigSource>,
}

impl ConfigManager {
    /// ä»æ–‡ä»¶åˆ›å»ºé…ç½®
    pub fn from_file(config_path: &str) -> Result<Self, ConfigError> {
        let config = Self::load_config_from_file(config_path)?;
        let (tx, rx) = watch::channel(Arc::new(config.clone()));
        
        let config_source: Box<dyn ConfigSource> = if config_path.ends_with(".json") {
            Box::new(JsonFileConfigSource::new(config_path.to_string()))
        } else {
            Box::new(TomlFileConfigSource::new(config_path.to_string()))
        };
        
        Ok(Self {
            current_config: Arc::new(config),
            config_sender: tx,
            config_receiver: rx,
            config_source,
        })
    }
    
    /// ä»Consulåˆ›å»ºé…ç½®
    pub fn from_consul(consul_url: &str, app_name: &str) -> Result<Self, ConfigError> {
        let consul_source = ConsulConfigSource::new(consul_url.to_string(), app_name.to_string());
        let config = consul_source.load_config().map_err(|e| {
            ConfigError::Foreign(Box::new(e))
        })?;
        
        let (tx, rx) = watch::channel(Arc::new(config.clone()));
        
        Ok(Self {
            current_config: Arc::new(config),
            config_sender: tx,
            config_receiver: rx,
            config_source: Box::new(consul_source),
        })
    }
    
    /// åŠ è½½é…ç½®æ–‡ä»¶
    fn load_config_from_file(config_path: &str) -> Result<AppConfig, ConfigError> {
        let builder = Config::builder()
            .add_source(File::with_name(config_path))
            .add_source(Environment::with_prefix("APP").separator("__"));
            
        let config = builder.build()?;
        config.try_deserialize()
    }
    
    /// è·å–å½“å‰é…ç½®
    pub fn get_config(&self) -> Arc<AppConfig> {
        self.current_config.clone()
    }
    
    /// è·å–é…ç½®è§‚å¯Ÿå™¨
    pub fn get_config_watcher(&self) -> watch::Receiver<Arc<AppConfig>> {
        self.config_receiver.clone()
    }
    
    /// å¯åŠ¨é…ç½®ç›‘å¬
    pub async fn start_config_watch(&self, refresh_interval: Duration) -> tokio::task::JoinHandle<()> {
        let config_source = self.config_source.clone_box();
        let sender = self.config_sender.clone();
        let mut current_config = self.current_config.clone();
        
        tokio::spawn(async move {
            let mut interval = interval(refresh_interval);
            
            loop {
                interval.tick().await;
                
                match config_source.load_config() {
                    Ok(new_config) => {
                        if !config_equals(&current_config, &Arc::new(new_config.clone())) {
                            info!("æ£€æµ‹åˆ°é…ç½®å˜æ›´,æ›´æ–°é…ç½®");
                            
                            let new_config_arc = Arc::new(new_config);
                            current_config = new_config_arc.clone();
                            
                            if let Err(e) = sender.send(new_config_arc) {
                                error!("å‘é€é…ç½®æ›´æ–°å¤±è´¥: {:?}", e);
                            }
                        }
                    },
                    Err(e) => {
                        error!("åŠ è½½é…ç½®å¤±è´¥: {:?}", e);
                    }
                }
            }
        })
    }
}

/// æ¯”è¾ƒé…ç½®æ˜¯å¦ç›¸ç­‰
fn config_equals(a: &Arc<AppConfig>, b: &Arc<AppConfig>) -> bool {
    let a_json = serde_json::to_string(a).unwrap_or_default();
    let b_json = serde_json::to_string(b).unwrap_or_default();
    a_json == b_json
}

/// é…ç½®æºç‰¹è´¨
#[async_trait]
pub trait ConfigSource: Send + Sync {
    fn load_config(&self) -> Result<AppConfig, Box<dyn std::error::Error>>;
    fn clone_box(&self) -> Box<dyn ConfigSource>;
}

/// JSONæ–‡ä»¶é…ç½®æº
pub struct JsonFileConfigSource {
    file_path: String,
}

impl JsonFileConfigSource {
    pub fn new(file_path: String) -> Self {
        Self { file_path }
    }
}

impl ConfigSource for JsonFileConfigSource {
    fn load_config(&self) -> Result<AppConfig, Box<dyn std::error::Error>> {
        let content = std::fs::read_to_string(&self.file_path)?;
        let config: AppConfig = serde_json::from_str(&content)?;
        Ok(config)
    }
    
    fn clone_box(&self) -> Box<dyn ConfigSource> {
        Box::new(Self {
            file_path: self.file_path.clone(),
        })
    }
}

/// Consulé…ç½®æº
pub struct ConsulConfigSource {
    consul_url: String,
    app_name: String,
}

impl ConsulConfigSource {
    pub fn new(consul_url: String, app_name: String) -> Self {
        Self { consul_url, app_name }
    }
}

impl ConfigSource for ConsulConfigSource {
    fn load_config(&self) -> Result<AppConfig, Box<dyn std::error::Error>> {
        // åˆ›å»ºConsulå®¢æˆ·ç«¯
        let client = consul_rs::Client::new(&self.consul_url)?;
        
        // è·å–é…ç½®
        let (_, data) = client.kv().get(&format!("config/{}", self.app_name)).map_err(|e| {
            format!("ä»Consulè·å–é…ç½®å¤±è´¥: {}", e)
        })?.ok_or_else(|| {
            "Consulä¸­ä¸å­˜åœ¨é…ç½®".to_string()
        })?;
        
        // è§£æé…ç½®
        let config: AppConfig = serde_json::from_slice(&data)?;
        Ok(config)
    }
    
    fn clone_box(&self) -> Box<dyn ConfigSource> {
        Box::new(Self {
            consul_url: self.consul_url.clone(),
            app_name: self.app_name.clone(),
        })
    }
}
```

#### 9.9.2 æœåŠ¡æ³¨å†Œä¸å‘ç°

```rust
use consul_rs::{Client, Config, RegisterServiceRequest, ServiceEntry};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{Duration, interval};
use resilient::CircuitBreaker;
use rand::{thread_rng, seq::SliceRandom};
use serde::{Serialize, Deserialize};
use uuid::Uuid;
use std::collections::HashMap;
use std::net::SocketAddr;
use tracing::{info, error, warn};

/// æœåŠ¡æ³¨å†Œç®¡ç†å™¨
pub struct ServiceRegistration {
    consul_client: Client,
    service_id: String,
    service_name: String,
    host: String,
    port: u16,
    health_check_path: String,
    health_check_interval: String,
}

impl ServiceRegistration {
    pub fn new(
        consul_url: &str,
        service_name: &str,
        host: &str,
        port: u16,
        health_check_path: &str,
        health_check_interval: &str,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let consul_client = Client::new(consul_url)?;
        
        let service_id = format!("{}-{}", service_name, Uuid::new_v4());
        
        Ok(Self {
            consul_client,
            service_id,
            service_name: service_name.to_string(),
            host: host.to_string(),
            port,
            health_check_path: health_check_path.to_string(),
            health_check_interval: health_check_interval.to_string(),
        })
    }
    
    /// æ³¨å†ŒæœåŠ¡
    pub async fn register(&self) -> Result<(), Box<dyn std::error::Error>> {
        let check_url = format!("http://{}:{}{}", self.host, self.port, self.health_check_path);
        
        let request = RegisterServiceRequest {
            id: Some(self.service_id.clone()),
            name: self.service_name.clone(),
            address: Some(self.host.clone()),
            port: Some(self.port),
            tags: Some(vec!["rust".to_string(), "v1".to_string()]),
            check: Some(consul_rs::CheckInfo {
                http: Some(check_url),
                interval: Some(self.health_check_interval.clone()),
                timeout: Some("5s".to_string()),
                ..Default::default()
            }),
            ..Default::default()
        };
        
        self.consul_client.agent().register_service_with_request(request).await?;
        
        info!("æœåŠ¡å·²æ³¨å†Œåˆ°Consul: id={}, name={}", self.service_id, self.service_name);
        Ok(())
    }
    
    /// æ³¨é”€æœåŠ¡
    pub async fn deregister(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.consul_client.agent().deregister_service(&self.service_id).await?;
        info!("æœåŠ¡å·²ä»Consulæ³¨é”€: id={}", self.service_id);
        Ok(())
    }
}

/// æœåŠ¡å‘ç°ç®¡ç†å™¨
pub struct ServiceDiscovery {
    consul_client: Client,
    service_cache: RwLock<HashMap<String, ServiceCacheEntry>>,
    circuit_breaker: Arc<CircuitBreaker>,
}

struct ServiceCacheEntry {
    services: Vec<ServiceInfo>,
    last_updated: chrono::DateTime<chrono::Utc>,
}

#[derive(Clone, Debug)]
pub struct ServiceInfo {
    pub id: String,
    pub service_name: String,
    pub address: String,
    pub port: u16,
    pub tags: Vec<String>,
    pub healthy: bool,
}

impl ServiceDiscovery {
    pub fn new(consul_url: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let consul_client = Client::new(consul_url)?;
        
        let circuit_breaker = Arc::new(CircuitBreaker::new(
            "consul_client",
            resilient::CircuitBreakerConfig {
                failure_threshold: 5,
                success_threshold: 2,
                open_duration: Duration::from_secs(30),
                ..Default::default()
            }
        ));
        
        Ok(Self {
            consul_client,
            service_cache: RwLock::new(HashMap::new()),
            circuit_breaker,
        })
    }
    
    /// å‘ç°æœåŠ¡
    pub async fn discover_service(&self, service_name: &str) -> Result<Vec<ServiceInfo>, ServiceDiscoveryError> {
        // å…ˆæ£€æŸ¥ç¼“å­˜
        {
            let cache = self.service_cache.read().await;
            if let Some(entry) = cache.get(service_name) {
                // å¦‚æœç¼“å­˜æœªè¿‡æœŸ,ç›´æ¥ä½¿ç”¨
                if entry.last_updated + chrono::Duration::seconds(30) > chrono::Utc::now() {
                    return Ok(entry.services.clone());
                }
            }
        }
        
        // ä½¿ç”¨æ–­è·¯å™¨è°ƒç”¨Consul
        let services = self.circuit_breaker.execute(|| async {
            let services = self.consul_client.health().service(service_name, None, true).await?;
            
            let service_infos = services.into_iter().map(|s| {
                ServiceInfo {
                    id: s.service.id,
                    service_name: s.service.service,
                    address: s.service.address.unwrap_or_else(|| "127.0.0.1".to_string()),
                    port: s.service.port.unwrap_or(80),
                    tags: s.service.tags.unwrap_or_default(),
                    healthy: s.checks.iter().all(|c| c.status == "passing"),
                }
            }).collect::<Vec<_>>();
            
            Ok::<Vec<ServiceInfo>, Box<dyn std::error::Error>>(service_infos)
        }).await.map_err(|e| match e {
            resilient::Error::CircuitOpen => ServiceDiscoveryError::ServiceUnavailable("æœåŠ¡å‘ç°æš‚æ—¶ä¸å¯ç”¨".to_string()),
            resilient::Error::OperationError(inner) => ServiceDiscoveryError::ConsulError(inner.to_string()),
        })?;
        
        // æ›´æ–°ç¼“å­˜
        {
            let mut cache = self.service_cache.write().await;
            cache.insert(service_name.to_string(), ServiceCacheEntry {
                services: services.clone(),
                last_updated: chrono::Utc::now(),
            });
        }
        
        if services.is_empty() {
            return Err(ServiceDiscoveryError::ServiceNotFound(service_name.to_string()));
        }
        
        Ok(services)
    }
    
    /// è·å–æœåŠ¡URL(ä½¿ç”¨éšæœºè´Ÿè½½å‡è¡¡)
    pub async fn get_service_url(&self, service_name: &str) -> Result<String, ServiceDiscoveryError> {
        let services = self.discover_service(service_name).await?;
        
        // è¿‡æ»¤å‡ºå¥åº·çš„æœåŠ¡
        let healthy_services: Vec<_> = services.into_iter()
            .filter(|s| s.healthy)
            .collect();
            
        if healthy_services.is_empty() {
            return Err(ServiceDiscoveryError::NoHealthyService(service_name.to_string()));
        }
        
        // éšæœºé€‰æ‹©ä¸€ä¸ªå¥åº·æœåŠ¡
        let mut rng = thread_rng();
        let service = healthy_services.choose(&mut rng)
            .ok_or_else(|| ServiceDiscoveryError::NoHealthyService(service_name.to_string()))?;
            
        let scheme = if service.tags.contains(&"secure".to_string()) { "https" } else { "http" };
        let url = format!("{}://{}:{}", scheme, service.address, service.port);
        
        Ok(url)
    }
    
    /// å¯åŠ¨ç¼“å­˜åˆ·æ–°ä»»åŠ¡
    pub async fn start_cache_refresh(&self, refresh_interval: Duration) -> tokio::task::JoinHandle<()> {
        let service_discovery = Arc::new(self.clone());
        
        tokio::spawn(async move {
            let mut interval = interval(refresh_interval);
            
            loop {
                interval.tick().await;
                
                let services_to_refresh = {
                    let cache = service_discovery.service_cache.read().await;
                    cache.keys().cloned().collect::<Vec<_>>()
                };
                
                for service_name in services_to_refresh {
                    if let Err(e) = service_discovery.discover_service(&service_name).await {
                        warn!("åˆ·æ–°æœåŠ¡ {} ç¼“å­˜å¤±è´¥: {:?}", service_name, e);
                    }
                }
            }
        })
    }
}

impl Clone for ServiceDiscovery {
    fn clone(&self) -> Self {
        Self {
            consul_client: self.consul_client.clone(),
            service_cache: RwLock::new(HashMap::new()),
            circuit_breaker: self.circuit_breaker.clone(),
        }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum ServiceDiscoveryError {
    #[error("æœåŠ¡ {0} æœªæ‰¾åˆ°")]
    ServiceNotFound(String),
    
    #[error("æœåŠ¡ {0} æ²¡æœ‰å¥åº·å®ä¾‹")]
    NoHealthyService(String),
    
    #[error("Consulé”™è¯¯: {0}")]
    ConsulError(String),
    
    #[error("æœåŠ¡å‘ç°æš‚æ—¶ä¸å¯ç”¨: {0}")]
    ServiceUnavailable(String),
}
```

## 3 ä¸‰ã€æ•°æ®æ¨¡å‹è®¾è®¡

### 3.1 äº‹ä»¶å­˜å‚¨è¡¨ç»“æ„

```sql
-- äº‹ä»¶å­˜å‚¨è¡¨
CREATE TABLE event_store (
    id SERIAL PRIMARY KEY,
    aggregate_id VARCHAR(50) NOT NULL,
    aggregate_type VARCHAR(50) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    event_data JSONB NOT NULL,
    sequence INTEGER NOT NULL,
    occurred_at TIMESTAMP WITH TIME ZONE NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_event_store_aggregate_id ON event_store(aggregate_id);
CREATE INDEX idx_event_store_aggregate_type ON event_store(aggregate_type);
CREATE INDEX idx_event_store_event_type ON event_store(event_type);
CREATE INDEX idx_event_store_occurred_at ON event_store(occurred_at);

-- èšåˆå¿«ç…§è¡¨
CREATE TABLE aggregate_snapshots (
    id SERIAL PRIMARY KEY,
    aggregate_id VARCHAR(50) NOT NULL,
    aggregate_type VARCHAR(50) NOT NULL,
    version INTEGER NOT NULL,
    state JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    UNIQUE(aggregate_id, aggregate_type, version)
);
```

### 3.2 è¯»æ¨¡å‹è¡¨ç»“æ„

```sql
-- è®¢å•è§†å›¾è¡¨
CREATE TABLE order_view (
    id VARCHAR(50) PRIMARY KEY,
    customer_id VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL,
    total_amount DECIMAL(12, 2) NOT NULL,
    item_count INTEGER NOT NULL,
    shipping_address_id VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL,
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

-- è®¢å•é¡¹è§†å›¾è¡¨
CREATE TABLE order_item_view (
    id SERIAL PRIMARY KEY,
    order_id VARCHAR(50) NOT NULL REFERENCES order_view(id),
    product_id VARCHAR(50) NOT NULL,
    name VARCHAR(200) NOT NULL,
    quantity INTEGER NOT NULL,
    unit_price DECIMAL(12, 2) NOT NULL,
    UNIQUE(order_id, product_id)
);

-- æ”¯ä»˜ä¿¡æ¯è¡¨
CREATE TABLE order_payment (
    id SERIAL PRIMARY KEY,
    order_id VARCHAR(50) NOT NULL REFERENCES order_view(id),
    transaction_id VARCHAR(100),
    status VARCHAR(20) NOT NULL,
    amount DECIMAL(12, 2) NOT NULL,
    failure_reason TEXT,
    processed_at TIMESTAMP WITH TIME ZONE NOT NULL
);

-- é…é€ä¿¡æ¯è¡¨
CREATE TABLE order_shipment (
    id SERIAL PRIMARY KEY,
    order_id VARCHAR(50) NOT NULL REFERENCES order_view(id),
    shipment_id VARCHAR(100) NOT NULL,
    carrier VARCHAR(50) NOT NULL,
    tracking_number VARCHAR(100),
    status VARCHAR(20) NOT NULL,
    estimated_delivery TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_order_view_customer_id ON order_view(customer_id);
CREATE INDEX idx_order_view_status ON order_view(status);
CREATE INDEX idx_order_view_created_at ON order_view(created_at);
CREATE INDEX idx_order_payment_status ON order_payment(status);
CREATE INDEX idx_order_shipment_status ON order_shipment(status);
```

## 4 å››ã€éƒ¨ç½²æ¶æ„

### 4.1 å®¹å™¨åŒ–ä¸æœåŠ¡ç¼–æ’

ä»¥ä¸‹æ˜¯ä½¿ç”¨Docker Composeå®šä¹‰çš„æœåŠ¡éƒ¨ç½²é…ç½®:

```yaml
version: '3.8'

services:
  # åº”ç”¨æœåŠ¡
  order-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: order-service:latest
    environment:
      - RUST_LOG=info
      - APP__DATABASE__URL=postgres://postgres:password@postgres:5432/orderdb
      - APP__REDIS__URL=redis://redis:6379
      - APP__KAFKA__BROKERS=kafka:9092
      - APP__TEMPORAL__URL=temporal:7233
      - APP__CONSUL__URL=http://consul:8500
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - redis
      - kafka
      - temporal
      - consul
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
        
  # æ•°æ®åº“
  postgres:
    image: postgres:14-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=orderdb
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "5432:5432"
  
  # Redisç¼“å­˜
  redis:
    image: redis:6-alpine
    ports:
      
# å¤æ‚åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„è®¾è®¡ä¸å®ç°æ–¹æ¡ˆï¼ˆç»­ï¼‰

## 5 å››ã€éƒ¨ç½²æ¶æ„ç»­

### 5.1 å®¹å™¨åŒ–ä¸æœåŠ¡ç¼–æ’ç»­

```yaml
version: '3.8'

services:
  # åº”ç”¨æœåŠ¡
  order-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: order-service:latest
    environment:
      - RUST_LOG=info
      - APP__DATABASE__URL=postgres://postgres:password@postgres:5432/orderdb
      - APP__REDIS__URL=redis://redis:6379
      - APP__KAFKA__BROKERS=kafka:9092
      - APP__TEMPORAL__URL=temporal:7233
      - APP__CONSUL__URL=http://consul:8500
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - redis
      - kafka
      - temporal
      - consul
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
        
  # æ•°æ®åº“
  postgres:
    image: postgres:14-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=orderdb
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "5432:5432"
  
  # Redisç¼“å­˜
  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
      
  # Kafkaæ¶ˆæ¯é˜Ÿåˆ—
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      
  kafka:
    image: confluentinc/cp-kafka:7.2.1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
      
  # Temporalå·¥ä½œæµå¼•æ“
  temporal:
    image: temporalio/auto-setup:1.16.2
    environment:
      - DB=postgresql
      - DB_PORT=5432
      - POSTGRES_USER=postgres
      - POSTGRES_PWD=password
      - POSTGRES_SEEDS=postgres
    ports:
      - "7233:7233"
    depends_on:
      - postgres
      
  temporal-web:
    image: temporalio/web:1.15.0
    environment:
      - TEMPORAL_GRPC_ENDPOINT=temporal:7233
    ports:
      - "8088:8088"
    depends_on:
      - temporal
      
  # ConsulæœåŠ¡å‘ç°
  consul:
    image: hashicorp/consul:1.12.3
    ports:
      - "8500:8500"
    volumes:
      - consul-data:/consul/data
    command: agent -server -bootstrap-expect=1 -ui -client=0.0.0.0
      
  # Jaegeråˆ†å¸ƒå¼è¿½è¸ª
  jaeger:
    image: jaegertracing/all-in-one:1.36
    ports:
      - "16686:16686"  # UI
      - "14268:14268"  # Collector HTTP
      
  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:v2.36.1
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      
  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:9.0.4
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana-datasources:/etc/grafana/provisioning/datasources
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus

volumes:
  postgres-data:
  redis-data:
  zookeeper-data:
  kafka-data:
  consul-data:
  prometheus-data:
  grafana-data:
```

### 5.2 Prometheusé…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'order-service'
    consul_sd_configs:
      - server: 'consul:8500'
        services: ['order-service']
    metrics_path: /metrics
    
  - job_name: 'temporal'
    static_configs:
      - targets: ['temporal:9090']
        
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

### 5.3 Kuberneteséƒ¨ç½²é…ç½®

```yaml
# order-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
  labels:
    app: order-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: order-service
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: order-service
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        prometheus.io/port: "8080"
    spec:
      containers:
      - name: order-service
        image: order-service:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        env:
        - name: RUST_LOG
          value: "info"
        - name: APP__SERVER__PORT
          value: "8080"
        - name: APP__DATABASE__URL
          valueFrom:
            secretKeyRef:
              name: order-service-secrets
              key: database-url
        - name: APP__REDIS__URL
          valueFrom:
            secretKeyRef:
              name: order-service-secrets
              key: redis-url
        - name: APP__KAFKA__BROKERS
          value: "kafka-headless.kafka.svc.cluster.local:9092"
        - name: APP__TEMPORAL__URL
          value: "temporal-frontend.temporal.svc.cluster.local:7233"
        - name: APP__CONSUL__URL
          value: "http://consul-server.consul.svc.cluster.local:8500"
        resources:
          limits:
            cpu: "1"
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "512Mi"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 20
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 20
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        fsGroup: 10001
      terminationGracePeriodSeconds: 60

---
# order-service-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: order-service
  labels:
    app: order-service
spec:
  selector:
    app: order-service
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
  type: ClusterIP

---
# order-service-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## 6 äº”ã€ç³»ç»Ÿå¯åŠ¨ä¸é›†æˆ

### 6.1 åº”ç”¨å¯åŠ¨åºåˆ—

```rust
use std::sync::Arc;
use actix_web::{web, App, HttpServer, middleware};
use sqlx::postgres::PgPoolOptions;
use rdkafka::config::ClientConfig;
use opentelemetry::global;

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    // 1. åŠ è½½é…ç½®
    let config_manager = ConfigManager::from_file("config/config.toml")
        .expect("æ— æ³•åŠ è½½é…ç½®");
    let config = config_manager.get_config();
    
    // 2. åˆå§‹åŒ–æ—¥å¿—å’Œè¿½è¸ª
    init_tracer(&config.tracing.service_name, &config.tracing.jaeger_endpoint)
        .expect("æ— æ³•åˆå§‹åŒ–åˆ†å¸ƒå¼è¿½è¸ª");
    
    tracing::info!("åº”ç”¨å¯åŠ¨ä¸­...");
    
    // 3. åˆ›å»ºæ•°æ®åº“è¿æ¥æ± 
    let db_pool = PgPoolOptions::new()
        .max_connections(config.database.max_connections)
        .min_connections(config.database.min_connections)
        .max_lifetime(config.database.max_lifetime)
        .idle_timeout(config.database.idle_timeout)
        .connect(&config.database.url)
        .await
        .expect("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“");
        
    // æ‰§è¡Œè¿ç§»
    sqlx::migrate!("./migrations")
        .run(&db_pool)
        .await
        .expect("æ— æ³•æ‰§è¡Œæ•°æ®åº“è¿ç§»");
    
    // 4. åˆ›å»ºRediså®¢æˆ·ç«¯
    let redis_client = redis::Client::open(config.redis.url.as_str())
        .expect("æ— æ³•åˆ›å»ºRediså®¢æˆ·ç«¯");
        
    // 5. åˆ›å»ºKafkaç”Ÿäº§è€…
    let kafka_producer = EventProducer::new(
        &config.kafka.brokers,
        &config.kafka.topic,
    ).expect("æ— æ³•åˆ›å»ºKafkaç”Ÿäº§è€…");
    let kafka_producer = Arc::new(kafka_producer);
    
    // 6. è®¾ç½®æœåŠ¡æ³¨å†Œ
    let service_registration = ServiceRegistration::new(
        &config.consul.url,
        &config.server.service_name,
        &config.server.host,
        config.server.port,
        "/health",
        "15s",
    ).expect("æ— æ³•åˆ›å»ºæœåŠ¡æ³¨å†Œ");
    
    service_registration.register().await
        .expect("æ— æ³•æ³¨å†ŒæœåŠ¡");
        
    // æ³¨å†Œå…³é—­é’©å­,ç¡®ä¿æœåŠ¡ä¼˜é›…é€€å‡º
    let service_registration_clone = service_registration.clone();
    ctrlc::set_handler(move || {
        let registration = service_registration_clone.clone();
        tokio::runtime::Builder::new_current_thread()
            .enable_all()
            .build()
            .unwrap()
            .block_on(async {
                tracing::info!("æ¥æ”¶åˆ°ç»ˆæ­¢ä¿¡å·,æ­£åœ¨æ³¨é”€æœåŠ¡...");
                if let Err(e) = registration.deregister().await {
                    tracing::error!("æ³¨é”€æœåŠ¡å¤±è´¥: {:?}", e);
                }
                std::process::exit(0);
            });
    }).expect("æ— æ³•è®¾ç½®Ctrl-Cå¤„ç†å™¨");
    
    // 7. åˆ›å»ºæœåŠ¡å‘ç°
    let service_discovery = Arc::new(ServiceDiscovery::new(&config.consul.url)
        .expect("æ— æ³•åˆ›å»ºæœåŠ¡å‘ç°"));
        
    // å¯åŠ¨ç¼“å­˜åˆ·æ–°ä»»åŠ¡
    service_discovery.clone().start_cache_refresh(std::time::Duration::from_secs(60)).await;
    
    // 8. åˆ›å»ºå¤–éƒ¨æœåŠ¡å·¥å‚
    let external_service_factory = Arc::new(ExternalServiceFactory::new(
        config.external_services.clone(),
        service_discovery.clone(),
    ));
    
    // 9. åˆå§‹åŒ–å‘½ä»¤æ€»çº¿
    let command_bus = Arc::new(CommandBus::new(kafka_producer.clone()));
    
    // æ³¨å†Œå‘½ä»¤å¤„ç†å™¨
    let mut command_handlers = CommandHandlerRegistry::new();
    
    let create_order_handler = CreateOrderHandler::new(
        db_pool.clone(),
        external_service_factory.clone(),
    );
    command_handlers.register_handler(Box::new(create_order_handler));
    
    let cancel_order_handler = CancelOrderHandler::new(
        db_pool.clone(),
        external_service_factory.clone(),
    );
    command_handlers.register_handler(Box::new(cancel_order_handler));
    
    // å°†å¤„ç†å™¨æ³¨å†Œåˆ°å‘½ä»¤æ€»çº¿
    command_bus.register_handlers(command_handlers).await;
    
    // 10. åˆå§‹åŒ–äº‹ä»¶æ¶ˆè´¹è€…
    let event_consumer = EventConsumer::new();
    
    // æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
    let order_read_model_updater = OrderReadModelUpdater::new(db_pool.clone());
    event_consumer.register_handler("order_created", Box::new(order_read_model_updater));
    
    let order_workflow_starter = OrderWorkflowStarter::new(
        TemporalClientFactory::new_client(&config.temporal.url)
            .await
            .expect("æ— æ³•åˆ›å»ºTemporalå®¢æˆ·ç«¯")
    );
    event_consumer.register_handler("order_created", Box::new(order_workflow_starter));
    
    let inventory_reserver = InventoryReserver::new(
        external_service_factory.clone(),
    );
    event_consumer.register_handler("order_created", Box::new(inventory_reserver));
    
    // å¯åŠ¨äº‹ä»¶æ¶ˆè´¹
    let event_consumer_clone = event_consumer.clone();
    tokio::spawn(async move {
        if let Err(e) = event_consumer_clone.start(
            &["orders"],
            "order-service-group",
            &config.kafka.brokers,
        ).await {
            tracing::error!("äº‹ä»¶æ¶ˆè´¹è€…å¯åŠ¨å¤±è´¥: {:?}", e);
        }
    });
    
    // 11. åˆå§‹åŒ–æŸ¥è¯¢æœåŠ¡
    let metrics = Arc::new(Metrics::new());
    
    let order_query_service = Arc::new(OrderQueryService::new(
        db_pool.clone(),
        redis_client.clone(),
        metrics.clone(),
    ));
    
    // 12. å¯åŠ¨æŒ‡æ ‡æ”¶é›†
    metrics.start_system_metrics_collector().await;
    
    // 13. åˆ›å»ºAPIæ§åˆ¶å™¨
    let order_controller = Arc::new(OrderController::new(
        order_query_service.clone(),
        command_bus.clone(),
    ));
    
    // 14. åˆ›å»ºå¥åº·æ£€æŸ¥æœåŠ¡
    let health_check_service = Arc::new(HealthCheckService::new(
        db_pool.clone(),
        redis_client.clone(),
        kafka_producer.clone(),
    ));
    
    // 15. å¯åŠ¨HTTPæœåŠ¡å™¨
    tracing::info!("å¯åŠ¨HTTPæœåŠ¡å™¨åœ¨ {}:{}", config.server.host, config.server.port);
    
    HttpServer::new(move || {
        App::new()
            // ä¸­é—´ä»¶
            .wrap(middleware::Logger::default())
            .wrap(middleware::Compress::default())
            .wrap(middleware::NormalizePath::trim())
            .wrap(TracingMiddleware)
            .wrap(ErrorHandlerMiddleware)
            .wrap(RequestMetricsMiddleware::new(metrics.clone()))
            
            // åº”ç”¨çŠ¶æ€
            .app_data(web::Data::new(order_controller.clone()))
            .app_data(web::Data::new(health_check_service.clone()))
            .app_data(web::Data::new(config.clone()))
            
            // è·¯ç”±
            .configure(|cfg| register_routes(cfg, order_controller.clone()))
            
            // å¥åº·æ£€æŸ¥
            .route("/health", web::get().to(health_check))
            
            // æŒ‡æ ‡ç«¯ç‚¹
            .route("/metrics", web::get().to(metrics_handler))
    })
    .workers(config.server.workers)
    .bind(format!("{}:{}", config.server.host, config.server.port))?
    .run()
    .await
}

/// å¥åº·æ£€æŸ¥å¤„ç†å™¨
async fn health_check(
    service: web::Data<Arc<HealthCheckService>>,
) -> impl Responder {
    match service.check_health().await {
        health_status if health_status.status == "up" => {
            HttpResponse::Ok().json(health_status)
        },
        health_status => {
            HttpResponse::ServiceUnavailable().json(health_status)
        }
    }
}

/// æŒ‡æ ‡ç«¯ç‚¹å¤„ç†å™¨
async fn metrics_handler(
    metrics: web::Data<Arc<Metrics>>,
) -> impl Responder {
    let handle = metrics.get_prometheus_handler();
    HttpResponse::Ok()
        .content_type("text/plain")
        .body(handle.render())
}
```

### 6.2 Dockerfile

```dockerfile
FROM rust:1.62 as builder

# åˆ›å»ºæ–°çš„ç©ºé¡¹ç›®
WORKDIR /usr/src/app
RUN USER=root cargo new --bin order-service
WORKDIR /usr/src/app/order-service

# å¤åˆ¶é¡¹ç›®æ¸…å•
COPY Cargo.toml Cargo.lock ./

# æ„å»ºä¾èµ–é¡¹
RUN cargo build --release
RUN rm src/*.rs

# å¤åˆ¶æºä»£ç 
COPY src ./src
COPY migrations ./migrations
COPY config ./config

# æ„å»ºåº”ç”¨
RUN cargo build --release

# è¿è¡Œé˜¶æ®µ
FROM debian:bullseye-slim

# å®‰è£…ä¾èµ–
RUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*

# è®¾ç½®ç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

# åˆ›å»ºå·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶äºŒè¿›åˆ¶æ–‡ä»¶å’Œé…ç½®
COPY --from=builder /usr/src/app/order-service/target/release/order-service /app/
COPY --from=builder /usr/src/app/order-service/config /app/config
COPY --from=builder /usr/src/app/order-service/migrations /app/migrations

# è®¾ç½®æƒé™
RUN chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¯åŠ¨å‘½ä»¤
CMD ["/app/order-service"]
```

## 7 å…­ã€æ€»ç»“ä¸æœ€ä½³å®è·µ

### 7.1 æ¶æ„è®¾è®¡å…³é”®ç‚¹

1. **é¢†åŸŸé©±åŠ¨è®¾è®¡(DDD)**: é€šè¿‡æ˜ç¡®çš„é™ç•Œä¸Šä¸‹æ–‡åˆ†ç¦»ä¸šåŠ¡å…³æ³¨ç‚¹,èšåˆæ ¹ç¡®ä¿ä¸šåŠ¡è§„åˆ™å®Œæ•´æ€§

2. **äº‹ä»¶é©±åŠ¨æ¶æ„(EDA)**: ä½¿ç”¨äº‹ä»¶æ€»çº¿(Kafka)å®ç°æ¾è€¦åˆ,æé«˜ç³»ç»Ÿå¼¹æ€§å’Œæ‰©å±•æ€§

3. **å‘½ä»¤æŸ¥è¯¢è´£ä»»åˆ†ç¦»(CQRS)**: åˆ†ç¦»è¯»å†™æ¨¡å‹,ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½å¹¶ç®€åŒ–å¤æ‚ä¸šåŠ¡é€»è¾‘

4. **äº‹ä»¶æº¯æº**: ä¿å­˜æ‰€æœ‰é¢†åŸŸäº‹ä»¶,å®ç°å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€å®¡è®¡å’Œé‡å»ºèƒ½åŠ›

5. **å¾®æœåŠ¡æ¶æ„**: æ ¹æ®ä¸šåŠ¡èƒ½åŠ›åˆ’åˆ†æœåŠ¡è¾¹ç•Œ,å®ç°ç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•

6. **å·¥ä½œæµå¼•æ“**: ä½¿ç”¨Temporalç®¡ç†é•¿æ—¶é—´è¿è¡Œçš„ä¸šåŠ¡æµç¨‹,æä¾›æŒä¹…æ€§å’Œæ•…éšœæ¢å¤

### 7.2 Rustå®ç°ä¼˜åŠ¿

1. **ç±»å‹ç³»ç»Ÿå®‰å…¨**: åˆ©ç”¨Rustç±»å‹ç³»ç»Ÿåœ¨ç¼–è¯‘æ—¶æ•è·é”™è¯¯,å¢å¼ºç³»ç»Ÿå¯é æ€§

2. **é›¶æˆæœ¬æŠ½è±¡**: å®ç°é«˜çº§æŠ½è±¡è€Œä¸ç‰ºç‰²æ€§èƒ½

3. **å†…å­˜å®‰å…¨**: æ— éœ€åƒåœ¾å›æ”¶å³å¯ä¿è¯å†…å­˜å®‰å…¨,æé«˜ç³»ç»Ÿä¸€è‡´æ€§å’Œå¯é¢„æµ‹æ€§

4. **å¹¶å‘å®‰å…¨**: é€šè¿‡æ‰€æœ‰æƒæ¨¡å‹åœ¨ç¼–è¯‘æ—¶é˜²æ­¢æ•°æ®ç«äº‰å’Œå…¶ä»–å¹¶å‘é—®é¢˜

5. **é«˜æ•ˆèµ„æºåˆ©ç”¨**: ä½å†…å­˜å ç”¨å’ŒCPUä½¿ç”¨ç‡,æ”¯æŒé«˜ååé‡å¤„ç†

### 7.3 é›†æˆå¼€æºåº“æœ€ä½³å®è·µ

1. **åˆ†å±‚æŠ½è±¡**: ä½¿ç”¨æŠ½è±¡å±‚å°è£…ç¬¬ä¸‰æ–¹åº“,é™ä½è€¦åˆå¹¶ç®€åŒ–æ›¿æ¢

   ```rust
   // ç¤ºä¾‹: æŠ½è±¡æ•°æ®åº“è®¿é—®å±‚
   #[async_trait]
   pub trait OrderRepository: Send + Sync {
       async fn save(&self, order: &Order) -> Result<(), RepositoryError>;
       async fn find_by_id(&self, id: &str) -> Result<Option<Order>, RepositoryError>;
       async fn find_by_customer(&self, customer_id: &str, limit: i64, offset: i64) -> Result<Vec<Order>, RepositoryError>;
   }
   
   // å…·ä½“å®ç°ä½¿ç”¨sqlx
   pub struct PgOrderRepository {
       pool: PgPool,
   }
   
   #[async_trait]
   impl OrderRepository for PgOrderRepository {
       // å…·ä½“å®ç°...
   }
   ```

2. **è‡ªåŠ¨é‡è¿ä¸æ–­è·¯å™¨**: å¯¹å¤–éƒ¨ä¾èµ–åº”ç”¨æ–­è·¯å™¨æ¨¡å¼,é˜²æ­¢çº§è”æ•…éšœ

   ```rust
   // ç¤ºä¾‹: å¯¹å¤–éƒ¨æœåŠ¡è°ƒç”¨ä½¿ç”¨æ–­è·¯å™¨
   async fn call_external_service(&self, request: &Request) -> Result<Response, ServiceError> {
       self.circuit_breaker.execute(|| async {
           self.client.send_request(request).await
       }).await.map_err(|e| match e {
           resilient::Error::CircuitOpen => ServiceError::ServiceUnavailable("æœåŠ¡æš‚æ—¶ä¸å¯ç”¨".to_string()),
           resilient::Error::OperationError(inner) => inner,
       })
   }
   ```

3. **ç»“åˆç¼“å­˜æé«˜æ€§èƒ½**: å¯¹é¢‘ç¹è®¿é—®æ•°æ®ä½¿ç”¨å¤šçº§ç¼“å­˜ç­–ç•¥

   ```rust
   // ç¤ºä¾‹: ç¼“å­˜æŸ¥è¯¢ç»“æœ
   async fn get_cached_data(&self, key: &str) -> Result<Data, CacheError> {
       // 1. å°è¯•ä»æœ¬åœ°ç¼“å­˜è·å–
       if let Some(data) = self.local_cache.get(key).await {
           return Ok(data);
       }

       // 2. å°è¯•ä»Redisè·å–
       let mut redis_conn = self.redis_client.get_async_connection().await?;
       if let Ok(data_string) = redis_conn.get::<_, String>(key).await {
           let data = serde_json::from_str(&data_string)?;
           self.local_cache.insert(key.to_string(), data.clone()).await;
           return Ok(data);
       }

       // 3. ä»æ•°æ®åº“è·å–
       let data = self.db_repository.get_data(key).await?;
       
       // 4. æ›´æ–°ç¼“å­˜
       let data_string = serde_json::to_string(&data)?;
       let _: Result<(), _> = redis_conn.set_ex(key, data_string, 300).await;
       self.local_cache.insert(key.to_string(), data.clone()).await;
       
       Ok(data)
   }
   ```

4. **åˆ†å¸ƒå¼è¿½è¸ªä¸€è‡´æ€§**: ç¡®ä¿è·¨æœåŠ¡ä¼ é€’è¿½è¸ªä¸Šä¸‹æ–‡

   ```rust
   // ç¤ºä¾‹: åœ¨æœåŠ¡è°ƒç”¨ä¸­ä¼ é€’è¿½è¸ªä¸Šä¸‹æ–‡
   async fn call_service(&self, req: Request) -> Result<Response, Error> {
       let current_context = global::get_text_map_propagator(|propagator| {
           propagator.extract(&RequestHeaderCarrier(req.headers()))
       });
       
       let mut new_request = create_downstream_request(&req);
       global::get_text_map_propagator(|propagator| {
           propagator.inject_context(&current_context, &mut HeaderInjector(new_request.headers_mut()))
       });
       
       self.client.call(new_request).await
   }
   ```

### 7.4 æœ€ç»ˆæ¶æ„ç‰¹ç‚¹

1. **é«˜å¯é æ€§**: é€šè¿‡æ–­è·¯å™¨ã€é‡è¯•ã€é™çº§ç­‰å¼¹æ€§æ¨¡å¼åº”å¯¹æ•…éšœ

2. **é«˜å¯æ‰©å±•æ€§**: äº‹ä»¶é©±åŠ¨æ¶æ„æ”¯æŒç³»ç»Ÿæ°´å¹³æ‰©å±•

3. **å¯è§‚æµ‹æ€§**: å®Œæ•´çš„æ—¥å¿—ã€æŒ‡æ ‡å’Œåˆ†å¸ƒå¼è¿½è¸ª

4. **ä¸€è‡´æ€§ä¿è¯**: é€šè¿‡äº‹ä»¶æº¯æºå’ŒSagaæ¨¡å¼ç¡®ä¿åˆ†å¸ƒå¼ä¸€è‡´æ€§

5. **åŠ¨æ€é…ç½®**: æ”¯æŒåœ¨è¿è¡Œæ—¶è°ƒæ•´ç³»ç»Ÿå‚æ•°å’Œç­–ç•¥

6. **ç»´æŠ¤æ€§**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„å’Œä¾èµ–åè½¬åŸåˆ™ä½¿ç³»ç»Ÿæ˜“äºç†è§£å’Œç»´æŠ¤

é€šè¿‡ä»¥ä¸Šè®¾è®¡å’Œå®ç°,ç³»ç»Ÿèƒ½å¤Ÿæ»¡è¶³æ‚¨æè¿°çš„å¤æ‚ä¸šåŠ¡æµç¨‹ã€é•¿æ—¶é—´è¿è¡Œæ“ä½œã€é«˜å¯é æ€§ã€å¤šçº§åˆ†å¸ƒå¼ç³»ç»Ÿã€åŠ¨æ€ç­–ç•¥è°ƒæ•´ã€åˆè§„æ€§å’Œå®¡è®¡éœ€æ±‚ä»¥åŠå¤æ‚å¼‚å¸¸å¤„ç†ç­‰è¦æ±‚ã€‚
åŒæ—¶,åˆ©ç”¨Rustçš„ç±»å‹ç³»ç»Ÿå’Œå®‰å…¨ä¿è¯,ä¸ºç³»ç»Ÿæä¾›é¢å¤–çš„å¯é æ€§å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚
