# 18.1 ä¿¡æ¯ç†µç†è®º (Information Entropy Theory)

**åˆ›å»ºæ—¶é—´**: 2025-01-17
**æœ€åæ›´æ–°**: 2025-01-17
**æ–‡æ¡£çŠ¶æ€**: æ´»è·ƒ
**å…³è”æ¨¡å—**: `18_Information_Theory`

## ğŸ“ æ¦‚è¿°

ä¿¡æ¯ç†µç†è®ºæ˜¯ä¿¡æ¯è®ºçš„åŸºç¡€ï¼Œç ”ç©¶ä¿¡æ¯çš„ä¸ç¡®å®šæ€§åº¦é‡å’Œä¿¡æ¯é‡çš„æ•°å­¦æ€§è´¨ã€‚æœ¬æ–‡æ¡£æ¶µç›–é¦™å†œç†µã€æ¡ä»¶ç†µã€äº’ä¿¡æ¯ã€ç›¸å¯¹ç†µç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥åŠç†µçš„è®¡ç®—ã€ç¼–ç åº”ç”¨å’Œæ€§èƒ½åˆ†æã€‚

## ğŸ”¬ ç†è®ºåŸºç¡€

### ä¿¡æ¯ç†µå½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 18.1.1** (ä¿¡æ¯é‡)
å¯¹äºæ¦‚ç‡ä¸º $p$ çš„äº‹ä»¶ï¼Œå…¶ä¿¡æ¯é‡å®šä¹‰ä¸ºï¼š
$I(p) = -\log_2 p$

å…¶ä¸­å¯¹æ•°ä»¥2ä¸ºåº•ï¼Œå•ä½ä¸ºæ¯”ç‰¹(bit)ã€‚

**å®šä¹‰ 18.1.2** (é¦™å†œç†µ)
ç¦»æ•£éšæœºå˜é‡ $X$ çš„é¦™å†œç†µå®šä¹‰ä¸ºï¼š
$H(X) = \mathbb{E}[I(X)] = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)$

å…¶ä¸­ $p(x) = P(X = x)$ æ˜¯æ¦‚ç‡è´¨é‡å‡½æ•°ã€‚

**å®šä¹‰ 18.1.3** (è”åˆç†µ)
éšæœºå˜é‡ $X$ å’Œ $Y$ çš„è”åˆç†µå®šä¹‰ä¸ºï¼š
$H(X, Y) = -\sum_{x,y} p(x, y) \log_2 p(x, y)$

**å®šä¹‰ 18.1.4** (æ¡ä»¶ç†µ)
ç»™å®š $Y$ æ—¶ $X$ çš„æ¡ä»¶ç†µå®šä¹‰ä¸ºï¼š
$H(X|Y) = -\sum_{x,y} p(x, y) \log_2 p(x|y)$

### ç†µçš„åŸºæœ¬æ€§è´¨

**å®šç† 18.1.1** (ç†µçš„éè´Ÿæ€§)
å¯¹äºä»»æ„éšæœºå˜é‡ $X$ï¼Œ$H(X) \geq 0$ï¼Œå½“ä¸”ä»…å½“ $X$ æ˜¯ç¡®å®šæ€§å˜é‡æ—¶ç­‰å·æˆç«‹ã€‚

**å®šç† 18.1.2** (ç†µçš„ä¸Šç•Œ)
å¯¹äº $|\mathcal{X}| = n$ çš„éšæœºå˜é‡ $X$ï¼Œ$H(X) \leq \log_2 n$ï¼Œå½“ä¸”ä»…å½“ $X$ æ˜¯å‡åŒ€åˆ†å¸ƒæ—¶ç­‰å·æˆç«‹ã€‚

**å®šç† 18.1.3** (é“¾å¼æ³•åˆ™)
$H(X, Y) = H(X) + H(Y|X)$

**è¯æ˜**:
$H(X, Y) = -\sum_{x,y} p(x, y) \log_2 p(x, y)$
$= -\sum_{x,y} p(x, y) \log_2 [p(x) p(y|x)]$
$= -\sum_{x,y} p(x, y) \log_2 p(x) - \sum_{x,y} p(x, y) \log_2 p(y|x)$
$= H(X) + H(Y|X)$

**å®šç† 18.1.4** (æ•°æ®å‹ç¼©ä¸ç­‰å¼)
å¯¹äºä»»æ„ç¼–ç å‡½æ•° $f: \mathcal{X} \rightarrow \{0,1\}^*$ï¼Œå¹³å‡ç é•¿æ»¡è¶³ï¼š
$\mathbb{E}[|f(X)|] \geq H(X)$

### äº’ä¿¡æ¯å’Œç›¸å¯¹ç†µ

**å®šä¹‰ 18.1.5** (äº’ä¿¡æ¯)
éšæœºå˜é‡ $X$ å’Œ $Y$ çš„äº’ä¿¡æ¯å®šä¹‰ä¸ºï¼š
$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)$

**å®šä¹‰ 18.1.6** (KLæ•£åº¦)
æ¦‚ç‡åˆ†å¸ƒ $P$ å’Œ $Q$ çš„KLæ•£åº¦å®šä¹‰ä¸ºï¼š
$D_{KL}(P \| Q) = \sum_x p(x) \log_2 \frac{p(x)}{q(x)}$

**å®šç† 18.1.5** (äº’ä¿¡æ¯æ€§è´¨)

1. éè´Ÿæ€§ï¼š$I(X; Y) \geq 0$
2. å¯¹ç§°æ€§ï¼š$I(X; Y) = I(Y; X)$
3. æ•°æ®å¤„ç†ä¸ç­‰å¼ï¼š$I(X; Y) \geq I(X; f(Y))$

## ğŸ—ï¸ ç†µè®¡ç®—å’Œç¼–ç å®ç°

### Python ä¿¡æ¯ç†µæ¡†æ¶

```python
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from scipy.stats import entropy

class InformationEntropy:
    """ä¿¡æ¯ç†µè®¡ç®—æ¡†æ¶"""

    def __init__(self):
        self.epsilon = 1e-10  # é¿å…log(0)

    def calculate_entropy(self, probabilities, base=2):
        """è®¡ç®—é¦™å†œç†µ"""
        # ç¡®ä¿æ¦‚ç‡å’Œä¸º1
        probabilities = np.array(probabilities)
        probabilities = probabilities[probabilities > 0]  # ç§»é™¤é›¶æ¦‚ç‡
        probabilities = probabilities / np.sum(probabilities)

        # è®¡ç®—ç†µ
        entropy_value = -np.sum(probabilities * np.log(probabilities) / np.log(base))
        return entropy_value

    def calculate_conditional_entropy(self, joint_prob, base=2):
        """è®¡ç®—æ¡ä»¶ç†µ H(X|Y)"""
        # è®¡ç®—è¾¹ç¼˜æ¦‚ç‡
        p_y = np.sum(joint_prob, axis=0)
        p_y = p_y[p_y > 0]  # ç§»é™¤é›¶æ¦‚ç‡

        # è®¡ç®—æ¡ä»¶æ¦‚ç‡
        conditional_entropy = 0
        for j in range(joint_prob.shape[1]):
            if p_y[j] > 0:
                p_x_given_y = joint_prob[:, j] / p_y[j]
                p_x_given_y = p_x_given_y[p_x_given_y > 0]
                if len(p_x_given_y) > 0:
                    entropy_given_y = -np.sum(p_x_given_y * np.log(p_x_given_y) / np.log(base))
                    conditional_entropy += p_y[j] * entropy_given_y

        return conditional_entropy

    def calculate_mutual_information(self, joint_prob, base=2):
        """è®¡ç®—äº’ä¿¡æ¯ I(X; Y)"""
        # è®¡ç®—è¾¹ç¼˜æ¦‚ç‡
        p_x = np.sum(joint_prob, axis=1)
        p_y = np.sum(joint_prob, axis=0)

        # è®¡ç®—è”åˆç†µ
        joint_entropy = self.calculate_entropy(joint_prob.flatten(), base)

        # è®¡ç®—è¾¹ç¼˜ç†µ
        entropy_x = self.calculate_entropy(p_x, base)
        entropy_y = self.calculate_entropy(p_y, base)

        # äº’ä¿¡æ¯ = H(X) + H(Y) - H(X,Y)
        mutual_info = entropy_x + entropy_y - joint_entropy
        return mutual_info

    def calculate_kl_divergence(self, p, q, base=2):
        """è®¡ç®—KLæ•£åº¦ D_KL(P||Q)"""
        # ç¡®ä¿æ¦‚ç‡å’Œä¸º1
        p = np.array(p)
        q = np.array(q)
        p = p / np.sum(p)
        q = q / np.sum(q)

        # åªè€ƒè™‘p > 0çš„ä½ç½®
        mask = p > 0
        p = p[mask]
        q = q[mask]

        # è®¡ç®—KLæ•£åº¦
        kl_div = np.sum(p * np.log(p / q) / np.log(base))
        return kl_div

    def calculate_cross_entropy(self, p, q, base=2):
        """è®¡ç®—äº¤å‰ç†µ H(P,Q)"""
        p = np.array(p)
        q = np.array(q)
        p = p / np.sum(p)
        q = q / np.sum(q)

        # åªè€ƒè™‘p > 0çš„ä½ç½®
        mask = p > 0
        p = p[mask]
        q = q[mask]

        cross_entropy = -np.sum(p * np.log(q) / np.log(base))
        return cross_entropy

class EntropyCoding:
    """ç†µç¼–ç å®ç°"""

    def __init__(self):
        self.codes = {}

    def huffman_encoding(self, symbols, probabilities):
        """Huffmanç¼–ç """
        from heapq import heappush, heappop, heapify

        # åˆ›å»ºä¼˜å…ˆé˜Ÿåˆ—
        heap = [[prob, [symbol, ""]] for symbol, prob in zip(symbols, probabilities)]
        heapify(heap)

        # æ„å»ºHuffmanæ ‘
        while len(heap) > 1:
            lo = heappop(heap)
            hi = heappop(heap)

            for pair in lo[1:]:
                pair[1] = '0' + pair[1]
            for pair in hi[1:]:
                pair[1] = '1' + pair[1]

            heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])

        # æå–ç¼–ç 
        huffman_codes = {}
        for pair in heap[0][1:]:
            huffman_codes[pair[0]] = pair[1]

        return huffman_codes

    def arithmetic_encoding(self, symbols, probabilities, message):
        """ç®—æœ¯ç¼–ç """
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        cumulative_prob = [0]
        for p in probabilities:
            cumulative_prob.append(cumulative_prob[-1] + p)

        # ç¼–ç 
        low, high = 0.0, 1.0
        for symbol in message:
            symbol_idx = symbols.index(symbol)
            range_size = high - low
            high = low + range_size * cumulative_prob[symbol_idx + 1]
            low = low + range_size * cumulative_prob[symbol_idx]

        return (low + high) / 2

    def calculate_average_code_length(self, symbols, probabilities, codes):
        """è®¡ç®—å¹³å‡ç é•¿"""
        avg_length = 0
        for symbol, prob in zip(symbols, probabilities):
            if symbol in codes:
                avg_length += prob * len(codes[symbol])
        return avg_length

class EntropyAnalysis:
    """ç†µåˆ†æå·¥å…·"""

    def __init__(self):
        self.entropy_calculator = InformationEntropy()

    def analyze_text_entropy(self, text):
        """åˆ†ææ–‡æœ¬ç†µ"""
        # è®¡ç®—å­—ç¬¦é¢‘ç‡
        char_counts = Counter(text)
        total_chars = len(text)

        # è®¡ç®—æ¦‚ç‡
        probabilities = [count / total_chars for count in char_counts.values()]
        symbols = list(char_counts.keys())

        # è®¡ç®—ç†µ
        entropy = self.entropy_calculator.calculate_entropy(probabilities)

        # Huffmanç¼–ç 
        coding = EntropyCoding()
        huffman_codes = coding.huffman_encoding(symbols, probabilities)
        avg_length = coding.calculate_average_code_length(symbols, probabilities, huffman_codes)

        return {
            'entropy': entropy,
            'huffman_codes': huffman_codes,
            'average_code_length': avg_length,
            'compression_ratio': entropy / avg_length if avg_length > 0 else 0,
            'symbols': symbols,
            'probabilities': probabilities
        }

    def analyze_image_entropy(self, image):
        """åˆ†æå›¾åƒç†µ"""
        # å°†å›¾åƒè½¬æ¢ä¸ºç°åº¦å€¼
        if len(image.shape) == 3:
            image = np.mean(image, axis=2)

        # è®¡ç®—åƒç´ å€¼åˆ†å¸ƒ
        pixel_counts = Counter(image.flatten())
        total_pixels = image.size

        # è®¡ç®—æ¦‚ç‡
        probabilities = [count / total_pixels for count in pixel_counts.values()]

        # è®¡ç®—ç†µ
        entropy = self.entropy_calculator.calculate_entropy(probabilities)

        return {
            'entropy': entropy,
            'pixel_distribution': dict(pixel_counts),
            'total_pixels': total_pixels
        }

    def entropy_rate_analysis(self, data, window_size=100):
        """ç†µç‡åˆ†æ"""
        if isinstance(data, str):
            data = list(data)

        entropy_rates = []
        for i in range(0, len(data) - window_size + 1, window_size // 2):
            window = data[i:i + window_size]
            analysis = self.analyze_text_entropy(''.join(window))
            entropy_rates.append(analysis['entropy'])

        return {
            'entropy_rates': entropy_rates,
            'mean_entropy_rate': np.mean(entropy_rates),
            'std_entropy_rate': np.std(entropy_rates)
        }

# ä½¿ç”¨ç¤ºä¾‹
def entropy_examples():
    """ç†µè®¡ç®—ç¤ºä¾‹"""
    ie = InformationEntropy()

    # ç¤ºä¾‹1: å…¬å¹³ç¡¬å¸
    fair_coin = [0.5, 0.5]
    entropy_fair = ie.calculate_entropy(fair_coin)
    print(f"å…¬å¹³ç¡¬å¸ç†µ: {entropy_fair:.4f} bits")

    # ç¤ºä¾‹2: æœ‰åç¡¬å¸
    biased_coin = [0.8, 0.2]
    entropy_biased = ie.calculate_entropy(biased_coin)
    print(f"æœ‰åç¡¬å¸ç†µ: {entropy_biased:.4f} bits")

    # ç¤ºä¾‹3: è”åˆåˆ†å¸ƒ
    joint_prob = np.array([[0.3, 0.2], [0.1, 0.4]])
    mutual_info = ie.calculate_mutual_information(joint_prob)
    print(f"äº’ä¿¡æ¯: {mutual_info:.4f} bits")

    # ç¤ºä¾‹4: æ–‡æœ¬ç†µåˆ†æ
    text = "hello world this is a test message"
    ea = EntropyAnalysis()
    text_analysis = ea.analyze_text_entropy(text)
    print(f"æ–‡æœ¬ç†µ: {text_analysis['entropy']:.4f} bits")
    print(f"å¹³å‡ç é•¿: {text_analysis['average_code_length']:.4f} bits")
    print(f"å‹ç¼©æ¯”: {text_analysis['compression_ratio']:.4f}")

if __name__ == "__main__":
    entropy_examples()
```

## ğŸ“Š æ€§èƒ½åˆ†æå’Œè¯„ä¼°

### ç†µç¼–ç æ€§èƒ½åˆ†æ

```python
class EntropyPerformanceAnalysis:
    """ç†µç¼–ç æ€§èƒ½åˆ†æ"""

    def __init__(self):
        self.entropy_calculator = InformationEntropy()
        self.coding = EntropyCoding()

    def compare_encoding_methods(self, symbols, probabilities, test_message):
        """æ¯”è¾ƒä¸åŒç¼–ç æ–¹æ³•"""
        # Huffmanç¼–ç 
        huffman_codes = self.coding.huffman_encoding(symbols, probabilities)
        huffman_length = self.coding.calculate_average_code_length(symbols, probabilities, huffman_codes)

        # å›ºå®šé•¿åº¦ç¼–ç 
        fixed_length = len(symbols).bit_length()

        # ç†è®ºç†µ
        entropy = self.entropy_calculator.calculate_entropy(probabilities)

        # ç¼–ç æ•ˆç‡
        huffman_efficiency = entropy / huffman_length if huffman_length > 0 else 0
        fixed_efficiency = entropy / fixed_length if fixed_length > 0 else 0

        return {
            'entropy': entropy,
            'huffman_average_length': huffman_length,
            'fixed_length': fixed_length,
            'huffman_efficiency': huffman_efficiency,
            'fixed_efficiency': fixed_efficiency,
            'compression_improvement': (fixed_length - huffman_length) / fixed_length
        }

    def analyze_entropy_convergence(self, data_generator, max_samples=10000):
        """åˆ†æç†µçš„æ”¶æ•›æ€§"""
        entropies = []
        sample_sizes = []

        for n in range(100, max_samples, 100):
            data = data_generator(n)
            analysis = EntropyAnalysis()
            if isinstance(data, str):
                result = analysis.analyze_text_entropy(data)
            else:
                result = analysis.analyze_image_entropy(data)

            entropies.append(result['entropy'])
            sample_sizes.append(n)

        return {
            'sample_sizes': sample_sizes,
            'entropies': entropies,
            'convergence_rate': self._calculate_convergence_rate(entropies)
        }

    def _calculate_convergence_rate(self, entropies):
        """è®¡ç®—æ”¶æ•›ç‡"""
        if len(entropies) < 2:
            return 0

        # è®¡ç®—ç›¸é‚»ç†µå€¼çš„å·®å¼‚
        differences = [abs(entropies[i] - entropies[i-1]) for i in range(1, len(entropies))]

        # è®¡ç®—æ”¶æ•›ç‡ï¼ˆå·®å¼‚çš„è¡°å‡ç‡ï¼‰
        if len(differences) > 1:
            convergence_rate = np.mean(differences[-len(differences)//2:]) / np.mean(differences[:len(differences)//2])
        else:
            convergence_rate = 1

        return convergence_rate

    def entropy_complexity_analysis(self, data, max_order=5):
        """ç†µå¤æ‚åº¦åˆ†æ"""
        entropies = []
        orders = []

        for order in range(1, max_order + 1):
            # è®¡ç®—né˜¶ç†µ
            if isinstance(data, str):
                n_gram_entropy = self._calculate_n_gram_entropy(data, order)
            else:
                n_gram_entropy = self._calculate_n_gram_entropy_image(data, order)

            entropies.append(n_gram_entropy)
            orders.append(order)

        return {
            'orders': orders,
            'entropies': entropies,
            'entropy_rate': entropies[-1] if entropies else 0
        }

    def _calculate_n_gram_entropy(self, text, n):
        """è®¡ç®—n-gramç†µ"""
        n_grams = [text[i:i+n] for i in range(len(text)-n+1)]
        counts = Counter(n_grams)
        total = len(n_grams)

        probabilities = [count / total for count in counts.values()]
        return self.entropy_calculator.calculate_entropy(probabilities)

    def _calculate_n_gram_entropy_image(self, image, n):
        """è®¡ç®—å›¾åƒçš„n-gramç†µ"""
        # ç®€åŒ–ä¸º1Dåºåˆ—
        if len(image.shape) == 3:
            image = np.mean(image, axis=2)

        # å°†å›¾åƒè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        image_str = ''.join([chr(int(pixel)) for pixel in image.flatten()])
        return self._calculate_n_gram_entropy(image_str, n)
```

## ğŸ”— ä¸æ¨¡å—å†…ä¸»é¢˜çš„å…³ç³»

- **18.2 ä¿¡é“å®¹é‡ç†è®º**: ç†µä¸ä¿¡é“å®¹é‡çš„å…³ç³»
- **18.3 ç¼–ç ç†è®º**: ç†µç¼–ç å’Œæ— æŸå‹ç¼©
- **18.4 æ•°æ®å‹ç¼©ç†è®º**: åŸºäºç†µçš„å‹ç¼©ç®—æ³•
- **18.5 é”™è¯¯çº æ­£ç†è®º**: ç†µä¸çº é”™ç çš„å…³ç³»

## ğŸ§­ æ‰¹åˆ¤æ€§åˆ†æ

### å“²å­¦ç»´åº¦

- **ä¿¡æ¯å“²å­¦**: ç†µä½œä¸ºä¿¡æ¯é‡çš„åº¦é‡ï¼Œä½“ç°äº†"ä¿¡æ¯å³ä¸ç¡®å®šæ€§å‡å°‘"çš„å“²å­¦è§‚ç‚¹
- **è®¤è¯†è®ºåŸºç¡€**: ç†µç†è®ºåæ˜ äº†äººç±»å¯¹ä¸ç¡®å®šæ€§å’Œéšæœºæ€§çš„è®¤çŸ¥æ¨¡å¼
- **æœ¬ä½“è®ºåæ€**: ç†µä½œä¸ºç‰©ç†é‡ï¼Œå…¶å­˜åœ¨å½¢å¼ä»‹äºæ•°å­¦æŠ½è±¡å’Œç‰©ç†ç°å®ä¹‹é—´

### æ–¹æ³•è®ºç»´åº¦

- **è®¡ç®—æ–¹æ³•**: ä¸åŒç†µè®¡ç®—æ–¹æ³•çš„ç²¾åº¦å’Œæ•ˆç‡æ¯”è¾ƒ
- **ä¼°è®¡æ–¹æ³•**: æœ‰é™æ ·æœ¬ä¸‹ç†µä¼°è®¡çš„åå·®å’Œæ–¹å·®
- **åº”ç”¨èŒƒå›´**: ç†µç†è®ºåœ¨ä¸åŒé¢†åŸŸçš„é€‚ç”¨æ€§åˆ†æ

### å·¥ç¨‹ç»´åº¦

- **è®¡ç®—å¤æ‚åº¦**: å¤§è§„æ¨¡æ•°æ®ç†µè®¡ç®—çš„æ•ˆç‡é—®é¢˜
- **æ•°å€¼ç¨³å®šæ€§**: æ¦‚ç‡æ¥è¿‘é›¶æ—¶çš„æ•°å€¼è®¡ç®—ç¨³å®šæ€§
- **å®ç°ç²¾åº¦**: æµ®ç‚¹æ•°è¿ç®—å¯¹ç†µè®¡ç®—ç²¾åº¦çš„å½±å“

### ç¤¾ä¼šæŠ€æœ¯ç»´åº¦

- **ä¿¡æ¯éšç§**: ç†µåˆ†æå¯¹ä¿¡æ¯éšç§çš„æ½œåœ¨å¨èƒ
- **æ•°æ®ä¼¦ç†**: ä¿¡æ¯é‡åº¦é‡çš„å…¬å¹³æ€§å’Œé€æ˜æ€§
- **åº”ç”¨è´£ä»»**: ç†µç¼–ç åœ¨æ•°æ®å‹ç¼©ä¸­çš„è´£ä»»å½’å±

## ğŸ“š å‚è§

- [18.2 ä¿¡é“å®¹é‡ç†è®º](./18.2_Channel_Capacity_Theory.md)
- [18.3 ç¼–ç ç†è®º](./18.3_Coding_Theory.md)
- [ç»Ÿä¸€æœ¯è¯­è¡¨](../../04_Type_Theory/TERMINOLOGY_TABLE.md)

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. Shannon, C. E. (1948). A mathematical theory of communication. _Bell System Technical Journal_, 27(3), 379-423.
2. Cover, T. M., & Thomas, J. A. (2006). _Elements of Information Theory_. Wiley.
3. MacKay, D. J. C. (2003). _Information Theory, Inference, and Learning Algorithms_. Cambridge University Press.
4. Yeung, R. W. (2008). _Information Theory and Network Coding_. Springer.
5. Gray, R. M. (2011). _Entropy and Information Theory_. Springer.
