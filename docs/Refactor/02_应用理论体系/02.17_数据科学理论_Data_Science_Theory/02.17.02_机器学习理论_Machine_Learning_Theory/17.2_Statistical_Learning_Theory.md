# 17.2 ç»Ÿè®¡å­¦ä¹ ç†è®º (Statistical Learning Theory)

**åˆ›å»ºæ—¶é—´**: 2025-01-17  
**æœ€åæ›´æ–°**: 2025-01-17  
**æ–‡æ¡£çŠ¶æ€**: æ´»è·ƒ  
**å…³è”æ¨¡å—**: `17_Data_Science_Theory`

## ğŸ“‹ ç›®å½•

- [1 æ¦‚è¿°](#1-æ¦‚è¿°)
- [2 ç†è®ºåŸºç¡€](#2-ç†è®ºåŸºç¡€)
  - [2.1 ç»Ÿè®¡å­¦ä¹ å½¢å¼åŒ–å®šä¹‰](#21-ç»Ÿè®¡å­¦ä¹ å½¢å¼åŒ–å®šä¹‰)
  - [2.2 VCç»´ç†è®º](#22-vcç»´ç†è®º)
  - [2.3 Rademacherå¤æ‚åº¦](#23-rademacherå¤æ‚åº¦)
  - [2.4 ç»Ÿè®¡æ¨æ–­ç†è®º](#24-ç»Ÿè®¡æ¨æ–­ç†è®º)
- [3 ç»Ÿè®¡å­¦ä¹ ç®—æ³•å®ç°](#3-ç»Ÿè®¡å­¦ä¹ ç®—æ³•å®ç°)
  - [3.1 Python ç»Ÿè®¡å­¦ä¹ æ¡†æ¶](#31-python-ç»Ÿè®¡å­¦ä¹ æ¡†æ¶)
- [4 æ€§èƒ½åˆ†æå’Œè¯„ä¼°](#4-æ€§èƒ½åˆ†æå’Œè¯„ä¼°)
  - [4.1 ç»Ÿè®¡å­¦ä¹ è¯„ä¼°æŒ‡æ ‡](#41-ç»Ÿè®¡å­¦ä¹ è¯„ä¼°æŒ‡æ ‡)
- [5 ä¸æ¨¡å—å†…ä¸»é¢˜çš„å…³ç³»](#5-ä¸æ¨¡å—å†…ä¸»é¢˜çš„å…³ç³»)
- [6 æ‰¹åˆ¤æ€§åˆ†æ](#6-æ‰¹åˆ¤æ€§åˆ†æ)
  - [6.1 å“²å­¦ç»´åº¦](#61-å“²å­¦ç»´åº¦)
  - [6.2 æ–¹æ³•è®ºç»´åº¦](#62-æ–¹æ³•è®ºç»´åº¦)
  - [6.3 å·¥ç¨‹ç»´åº¦](#63-å·¥ç¨‹ç»´åº¦)
  - [6.4 ç¤¾ä¼šæŠ€æœ¯ç»´åº¦](#64-ç¤¾ä¼šæŠ€æœ¯ç»´åº¦)
- [7 å‚è§](#7-å‚è§)

---

## 1 æ¦‚è¿°

ç»Ÿè®¡å­¦ä¹ ç†è®ºæ˜¯æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ï¼Œç ”ç©¶å­¦ä¹ ç®—æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œç»Ÿè®¡æ€§è´¨ã€‚æœ¬æ–‡æ¡£æ¶µç›–VCç»´ç†è®ºã€Rademacherå¤æ‚åº¦ã€ç»Ÿè®¡æ¨æ–­ã€å‡è®¾æ£€éªŒç­‰æ ¸å¿ƒæ¦‚å¿µã€‚

## 2 ç†è®ºåŸºç¡€

### 2.1 ç»Ÿè®¡å­¦ä¹ å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 17.2.1** (ç»Ÿè®¡å­¦ä¹ é—®é¢˜)
ç»Ÿè®¡å­¦ä¹ é—®é¢˜æ˜¯å¯»æ‰¾å‡½æ•° $f: \mathcal{X} \rightarrow \mathcal{Y}$ æœ€å°åŒ–æœŸæœ›é£é™©ï¼š
$R(f) = \mathbb{E}_{(x,y) \sim \mathcal{D}}[L(f(x), y)]$

å…¶ä¸­ $\mathcal{D}$ æ˜¯æ•°æ®åˆ†å¸ƒï¼Œ$L$ æ˜¯æŸå¤±å‡½æ•°ã€‚

**å®šä¹‰ 17.2.2** (ç»éªŒé£é™©)
ç»éªŒé£é™©æ˜¯è®­ç»ƒé›†ä¸Šçš„å¹³å‡æŸå¤±ï¼š
$\hat{R}(f) = \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)$

**å®šä¹‰ 17.2.3** (æ³›åŒ–è¯¯å·®)
æ³›åŒ–è¯¯å·®æ˜¯æœŸæœ›é£é™©ä¸ç»éªŒé£é™©çš„å·®å¼‚ï¼š
$E_{gen} = R(f) - \hat{R}(f)$

### 2.2 VCç»´ç†è®º

**å®šä¹‰ 17.2.4** (VCç»´)
å‡è®¾ç±» $\mathcal{H}$ çš„VCç»´æ˜¯èƒ½è¢« $\mathcal{H}$ å®Œå…¨åˆ†ç±»çš„æœ€å¤§æ ·æœ¬æ•°ã€‚

**å®šç† 17.2.1** (VCç»´æ³›åŒ–ç•Œ)
å¯¹äºVCç»´ä¸º $d$ çš„å‡è®¾ç±» $\mathcal{H}$ï¼Œä»¥æ¦‚ç‡è‡³å°‘ $1-\delta$ï¼š
$R(f) \leq \hat{R}(f) + \sqrt{\frac{d \log(n/d) + \log(1/\delta)}{n}}$

**è¯æ˜**:
ä½¿ç”¨Hoeffdingä¸ç­‰å¼å’ŒVCç»´çš„Sauerå¼•ç†ã€‚

**å®šç† 17.2.2** (çº¿æ€§åˆ†ç±»å™¨VCç»´)
$d$ ç»´ç©ºé—´ä¸­çº¿æ€§åˆ†ç±»å™¨çš„VCç»´ä¸º $d+1$ã€‚

### 2.3 Rademacherå¤æ‚åº¦

**å®šä¹‰ 17.2.5** (Rademacherå¤æ‚åº¦)
å‡è®¾ç±» $\mathcal{H}$ çš„Rademacherå¤æ‚åº¦ï¼š
$\mathcal{R}_n(\mathcal{H}) = \mathbb{E}_{\sigma} \left[ \sup_{f \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \sigma_i f(x_i) \right]$

å…¶ä¸­ $\sigma_i$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„Rademacheréšæœºå˜é‡ã€‚

**å®šç† 17.2.3** (Rademacheræ³›åŒ–ç•Œ)
ä»¥æ¦‚ç‡è‡³å°‘ $1-\delta$ï¼š
$R(f) \leq \hat{R}(f) + 2\mathcal{R}_n(\mathcal{H}) + 3\sqrt{\frac{\log(2/\delta)}{2n}}$

### 2.4 ç»Ÿè®¡æ¨æ–­ç†è®º

**å®šä¹‰ 17.2.6** (ç½®ä¿¡åŒºé—´)
å‚æ•° $\theta$ çš„ $(1-\alpha)$ ç½®ä¿¡åŒºé—´æ˜¯éšæœºåŒºé—´ $[L, U]$ï¼Œä½¿å¾—ï¼š
$P(L \leq \theta \leq U) \geq 1-\alpha$

**å®šç† 17.2.4** (æ­£æ€åˆ†å¸ƒç½®ä¿¡åŒºé—´)
å¯¹äºæ­£æ€åˆ†å¸ƒ $N(\mu, \sigma^2)$ï¼Œå‡å€¼ $\mu$ çš„ $(1-\alpha)$ ç½®ä¿¡åŒºé—´ï¼š
$\left[ \bar{x} - z_{\alpha/2} \frac{\sigma}{\sqrt{n}}, \bar{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}} \right]$

## 3 ç»Ÿè®¡å­¦ä¹ ç®—æ³•å®ç°

### 3.1 Python ç»Ÿè®¡å­¦ä¹ æ¡†æ¶

```python
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

class StatisticalLearning:
    """ç»Ÿè®¡å­¦ä¹ æ¡†æ¶"""
    
    def __init__(self):
        self.models = {}
        self.results = {}
    
    def linear_regression_with_inference(self, X, y, alpha=0.05):
        """å¸¦ç»Ÿè®¡æ¨æ–­çš„çº¿æ€§å›å½’"""
        n, p = X.shape
        
        # æœ€å°äºŒä¹˜ä¼°è®¡
        X_with_intercept = np.column_stack([np.ones(n), X])
        beta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y
        
        # æ®‹å·®
        y_hat = X_with_intercept @ beta_hat
        residuals = y - y_hat
        
        # æ®‹å·®æ–¹å·®ä¼°è®¡
        sigma2_hat = np.sum(residuals**2) / (n - p - 1)
        
        # åæ–¹å·®çŸ©é˜µ
        cov_beta = sigma2_hat * np.linalg.inv(X_with_intercept.T @ X_with_intercept)
        
        # æ ‡å‡†è¯¯å·®
        se_beta = np.sqrt(np.diag(cov_beta))
        
        # tç»Ÿè®¡é‡
        t_stats = beta_hat / se_beta
        
        # på€¼
        p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), n - p - 1))
        
        # ç½®ä¿¡åŒºé—´
        t_critical = stats.t.ppf(1 - alpha/2, n - p - 1)
        ci_lower = beta_hat - t_critical * se_beta
        ci_upper = beta_hat + t_critical * se_beta
        
        return {
            'coefficients': beta_hat,
            'standard_errors': se_beta,
            't_statistics': t_stats,
            'p_values': p_values,
            'confidence_intervals': list(zip(ci_lower, ci_upper)),
            'r_squared': 1 - np.sum(residuals**2) / np.sum((y - np.mean(y))**2)
        }
    
    def hypothesis_testing(self, data1, data2, test_type='t_test'):
        """å‡è®¾æ£€éªŒ"""
        if test_type == 't_test':
            # ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ
            t_stat, p_value = stats.ttest_ind(data1, data2)
            return {
                'test_type': 'Independent t-test',
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        elif test_type == 'wilcoxon':
            # Wilcoxonç§©å’Œæ£€éªŒ
            stat, p_value = stats.ranksums(data1, data2)
            return {
                'test_type': 'Wilcoxon rank-sum test',
                'statistic': stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
    
    def confidence_interval(self, data, confidence=0.95):
        """ç½®ä¿¡åŒºé—´è®¡ç®—"""
        n = len(data)
        mean = np.mean(data)
        std = np.std(data, ddof=1)
        
        # tåˆ†å¸ƒä¸´ç•Œå€¼
        t_critical = stats.t.ppf((1 + confidence) / 2, n - 1)
        
        # ç½®ä¿¡åŒºé—´
        margin_of_error = t_critical * std / np.sqrt(n)
        ci_lower = mean - margin_of_error
        ci_upper = mean + margin_of_error
        
        return {
            'mean': mean,
            'std': std,
            'confidence_level': confidence,
            'confidence_interval': (ci_lower, ci_upper),
            'margin_of_error': margin_of_error
        }
    
    def bootstrap_confidence_interval(self, data, statistic_func, n_bootstrap=1000, confidence=0.95):
        """Bootstrapç½®ä¿¡åŒºé—´"""
        n = len(data)
        bootstrap_statistics = []
        
        for _ in range(n_bootstrap):
            # é‡é‡‡æ ·
            bootstrap_sample = np.random.choice(data, size=n, replace=True)
            bootstrap_stat = statistic_func(bootstrap_sample)
            bootstrap_statistics.append(bootstrap_stat)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        alpha = 1 - confidence
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = np.percentile(bootstrap_statistics, lower_percentile)
        ci_upper = np.percentile(bootstrap_statistics, upper_percentile)
        
        return {
            'bootstrap_statistics': bootstrap_statistics,
            'confidence_interval': (ci_lower, ci_upper),
            'confidence_level': confidence
        }

class ModelSelection:
    """æ¨¡å‹é€‰æ‹©æ¡†æ¶"""
    
    def __init__(self):
        self.models = {}
        self.criteria = {}
    
    def aic_criterion(self, model, X, y):
        """AICå‡†åˆ™"""
        n = len(y)
        k = len(model.coef_) + 1  # å‚æ•°ä¸ªæ•°
        rss = np.sum((y - model.predict(X))**2)
        aic = n * np.log(rss/n) + 2*k
        return aic
    
    def bic_criterion(self, model, X, y):
        """BICå‡†åˆ™"""
        n = len(y)
        k = len(model.coef_) + 1  # å‚æ•°ä¸ªæ•°
        rss = np.sum((y - model.predict(X))**2)
        bic = n * np.log(rss/n) + k * np.log(n)
        return bic
    
    def cross_validation_selection(self, models, X, y, cv=5):
        """äº¤å‰éªŒè¯æ¨¡å‹é€‰æ‹©"""
        results = {}
        
        for name, model in models.items():
            scores = cross_val_score(model, X, y, cv=cv, scoring='neg_mean_squared_error')
            mse_scores = -scores  # è½¬æ¢ä¸ºMSE
            
            results[name] = {
                'mean_mse': np.mean(mse_scores),
                'std_mse': np.std(mse_scores),
                'cv_scores': mse_scores
            }
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model = min(results.items(), key=lambda x: x[1]['mean_mse'])
        
        return {
            'results': results,
            'best_model': best_model[0],
            'best_score': best_model[1]['mean_mse']
        }

class StatisticalTests:
    """ç»Ÿè®¡æ£€éªŒæ¡†æ¶"""
    
    def normality_test(self, data):
        """æ­£æ€æ€§æ£€éªŒ"""
        # Shapiro-Wilkæ£€éªŒ
        shapiro_stat, shapiro_p = stats.shapiro(data)
        
        # Kolmogorov-Smirnovæ£€éªŒ
        ks_stat, ks_p = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))
        
        return {
            'shapiro_wilk': {
                'statistic': shapiro_stat,
                'p_value': shapiro_p,
                'is_normal': shapiro_p > 0.05
            },
            'kolmogorov_smirnov': {
                'statistic': ks_stat,
                'p_value': ks_p,
                'is_normal': ks_p > 0.05
            }
        }
    
    def homogeneity_test(self, groups):
        """æ–¹å·®é½æ€§æ£€éªŒ"""
        # Leveneæ£€éªŒ
        levene_stat, levene_p = stats.levene(*groups)
        
        # Bartlettæ£€éªŒ
        bartlett_stat, bartlett_p = stats.bartlett(*groups)
        
        return {
            'levene': {
                'statistic': levene_stat,
                'p_value': levene_p,
                'homogeneous': levene_p > 0.05
            },
            'bartlett': {
                'statistic': bartlett_stat,
                'p_value': bartlett_p,
                'homogeneous': bartlett_p > 0.05
            }
        }
    
    def correlation_test(self, x, y, method='pearson'):
        """ç›¸å…³æ€§æ£€éªŒ"""
        if method == 'pearson':
            corr, p_value = stats.pearsonr(x, y)
        elif method == 'spearman':
            corr, p_value = stats.spearmanr(x, y)
        elif method == 'kendall':
            corr, p_value = stats.kendalltau(x, y)
        
        return {
            'correlation': corr,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'method': method
        }

# ä½¿ç”¨ç¤ºä¾‹
def statistical_learning_example():
    """ç»Ÿè®¡å­¦ä¹ ç¤ºä¾‹"""
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n = 100
    X = np.random.randn(n, 2)
    y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(n) * 0.5
    
    # ç»Ÿè®¡å­¦ä¹ åˆ†æ
    sl = StatisticalLearning()
    
    # çº¿æ€§å›å½’æ¨æ–­
    results = sl.linear_regression_with_inference(X, y)
    print("Linear Regression Results:")
    print(f"RÂ²: {results['r_squared']:.4f}")
    print(f"Intercept: {results['coefficients'][0]:.4f} (p={results['p_values'][0]:.4f})")
    print(f"X1 coefficient: {results['coefficients'][1]:.4f} (p={results['p_values'][1]:.4f})")
    print(f"X2 coefficient: {results['coefficients'][2]:.4f} (p={results['p_values'][2]:.4f})")
    
    # ç½®ä¿¡åŒºé—´
    ci_results = sl.confidence_interval(y)
    print(f"\nConfidence Interval for y:")
    print(f"Mean: {ci_results['mean']:.4f}")
    print(f"95% CI: ({ci_results['confidence_interval'][0]:.4f}, {ci_results['confidence_interval'][1]:.4f})")
    
    # ç»Ÿè®¡æ£€éªŒ
    st = StatisticalTests()
    normality = st.normality_test(y)
    print(f"\nNormality Test:")
    print(f"Shapiro-Wilk p-value: {normality['shapiro_wilk']['p_value']:.4f}")
    print(f"Is normal: {normality['shapiro_wilk']['is_normal']}")

if __name__ == "__main__":
    statistical_learning_example()
```

## 4 æ€§èƒ½åˆ†æå’Œè¯„ä¼°

### 4.1 ç»Ÿè®¡å­¦ä¹ è¯„ä¼°æŒ‡æ ‡

```python
class StatisticalEvaluation:
    """ç»Ÿè®¡å­¦ä¹ è¯„ä¼°"""
    
    def __init__(self):
        self.metrics = {}
    
    def regression_metrics(self, y_true, y_pred):
        """å›å½’è¯„ä¼°æŒ‡æ ‡"""
        n = len(y_true)
        
        # åŸºæœ¬æŒ‡æ ‡
        mse = np.mean((y_true - y_pred)**2)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(y_true - y_pred))
        
        # RÂ²å’Œè°ƒæ•´RÂ²
        ss_res = np.sum((y_true - y_pred)**2)
        ss_tot = np.sum((y_true - np.mean(y_true))**2)
        r2 = 1 - (ss_res / ss_tot)
        
        # è°ƒæ•´RÂ²
        p = 2  # å‡è®¾æœ‰2ä¸ªç‰¹å¾
        adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2,
            'Adjusted_RÂ²': adj_r2
        }
    
    def classification_metrics(self, y_true, y_pred, y_prob=None):
        """åˆ†ç±»è¯„ä¼°æŒ‡æ ‡"""
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='weighted')
        recall = recall_score(y_true, y_pred, average='weighted')
        f1 = f1_score(y_true, y_pred, average='weighted')
        
        metrics = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1_Score': f1
        }
        
        if y_prob is not None:
            from sklearn.metrics import roc_auc_score, log_loss
            try:
                auc = roc_auc_score(y_true, y_prob, multi_class='ovr')
                logloss = log_loss(y_true, y_prob)
                metrics.update({
                    'AUC': auc,
                    'Log_Loss': logloss
                })
            except:
                pass
        
        return metrics
    
    def statistical_significance_test(self, model1_scores, model2_scores, alpha=0.05):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        # tæ£€éªŒ
        t_stat, p_value = stats.ttest_rel(model1_scores, model2_scores)
        
        # Wilcoxonç¬¦å·ç§©æ£€éªŒ
        w_stat, w_p_value = stats.wilcoxon(model1_scores, model2_scores)
        
        return {
            't_test': {
                'statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < alpha
            },
            'wilcoxon': {
                'statistic': w_stat,
                'p_value': w_p_value,
                'significant': w_p_value < alpha
            }
        }
```

## 5 ä¸æ¨¡å—å†…ä¸»é¢˜çš„å…³ç³»

- **17.1 æœºå™¨å­¦ä¹ ç†è®º**: æä¾›æœºå™¨å­¦ä¹ ç®—æ³•åŸºç¡€
- **17.3 æ•°æ®æŒ–æ˜ç†è®º**: æ¨¡å¼å‘ç°å’Œå…³è”è§„åˆ™
- **17.4 æ•°æ®å¯è§†åŒ–ç†è®º**: ç»Ÿè®¡ç»“æœå¯è§†åŒ–
- **17.5 æ•°æ®ä¼¦ç†ç†è®º**: ç»Ÿè®¡æ¨æ–­çš„ä¼¦ç†è€ƒè™‘

## 6 æ‰¹åˆ¤æ€§åˆ†æ

### 6.1 å“²å­¦ç»´åº¦

- **ç»Ÿè®¡å“²å­¦**: ç»Ÿè®¡å­¦ä¹ ä½“ç°äº†"æ¦‚ç‡å³çŸ¥è¯†"çš„å“²å­¦è§‚ç‚¹ï¼Œä½†æ¦‚ç‡è§£é‡Šå­˜åœ¨äº‰è®®
- **è®¤è¯†è®ºåŸºç¡€**: ç»Ÿè®¡æ¨æ–­åæ˜ äº†äººç±»å¯¹ä¸ç¡®å®šæ€§çš„è®¤çŸ¥æ¨¡å¼
- **æœ¬ä½“è®ºåæ€**: ç»Ÿè®¡æ¨¡å‹ä½œä¸ºæ¦‚ç‡å¯¹è±¡ï¼Œå…¶å­˜åœ¨å½¢å¼ä»‹äºç¡®å®šæ€§æ•°å­¦å’Œéšæœºç°è±¡ä¹‹é—´

### 6.2 æ–¹æ³•è®ºç»´åº¦

- **æ¨æ–­æ–¹æ³•æ¯”è¾ƒ**: é¢‘ç‡å­¦æ´¾å’Œè´å¶æ–¯å­¦æ´¾å„æœ‰ä¼˜ç¼ºç‚¹
- **æ¨¡å‹é€‰æ‹©**: æ¨¡å‹å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›çš„æƒè¡¡
- **å‡è®¾æ£€éªŒ**: på€¼çš„æ­£ç¡®ä½¿ç”¨å’Œè¯¯è§£é—®é¢˜

### 6.3 å·¥ç¨‹ç»´åº¦

- **è®¡ç®—å¤æ‚åº¦**: ç»Ÿè®¡æ¨æ–­çš„è®¡ç®—æˆæœ¬
- **æ•°å€¼ç¨³å®šæ€§**: çŸ©é˜µè¿ç®—çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜
- **å¯æ‰©å±•æ€§**: å¤§è§„æ¨¡æ•°æ®çš„ç»Ÿè®¡æ¨æ–­æŒ‘æˆ˜

### 6.4 ç¤¾ä¼šæŠ€æœ¯ç»´åº¦

- **ç»Ÿè®¡ç´ å…»**: å…¬ä¼—å¯¹ç»Ÿè®¡æ¦‚å¿µçš„ç†è§£ä¸è¶³
- **è¯¯ç”¨é£é™©**: ç»Ÿè®¡æ–¹æ³•çš„è¯¯ç”¨å’Œæ»¥ç”¨
- **é€æ˜åº¦**: ç»Ÿè®¡æ¨æ–­è¿‡ç¨‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§

## 7 å‚è§

- [17.1 æœºå™¨å­¦ä¹ ç†è®º](./17.1_Machine_Learning_Theory.md)
- [17.3 æ•°æ®æŒ–æ˜ç†è®º](../17.1_Data_Mining_Theory.md)
- [ç»Ÿä¸€æœ¯è¯­è¡¨](../../04_Type_Theory/TERMINOLOGY_TABLE.md)

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. Vapnik, V. N. (1998). *Statistical Learning Theory*. Wiley.
2. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.
3. Wasserman, L. (2004). *All of Statistics*. Springer.
4. Casella, G., & Berger, R. L. (2002). *Statistical Inference*. Duxbury.
5. Efron, B., & Tibshirani, R. J. (1994). *An Introduction to the Bootstrap*. CRC Press.
