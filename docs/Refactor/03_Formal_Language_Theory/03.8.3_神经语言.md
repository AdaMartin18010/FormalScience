# 03.8.3 ç¥ç»è¯­è¨€

## ğŸ“‹ æ¦‚è¿°

ç¥ç»è¯­è¨€æ˜¯å½¢å¼è¯­è¨€ç†è®ºåœ¨ç¥ç»ç½‘ç»œå’Œç¥ç»è®¡ç®—é¢†åŸŸçš„åº”ç”¨ï¼Œç ”ç©¶ç¥ç»ç½‘ç»œçš„å½¢å¼åŒ–è¡¨è¾¾å’Œç¥ç»ç¼–ç¨‹è¯­è¨€çš„ç†è®ºåŸºç¡€ã€‚æœ¬æ–‡æ¡£å»ºç«‹ä¸¥æ ¼çš„ç¥ç»è¯­è¨€ç†è®ºæ¡†æ¶ï¼ŒåŒ…å«ç¥ç»ç½‘ç»œæ¨¡å‹ã€ç¥ç»è®¡ç®—ã€ç¥ç»ç¬¦å·ç³»ç»Ÿç­‰å†…å®¹ã€‚

## ğŸ¯ æ ¸å¿ƒç›®æ ‡

1. å»ºç«‹ç¥ç»è¯­è¨€çš„åŸºæœ¬æ¦‚å¿µå’Œå½¢å¼åŒ–å®šä¹‰
2. åˆ†æç¥ç»ç½‘ç»œæ¨¡å‹ä¸å½¢å¼è¯­è¨€çš„å…³ç³»
3. ç ”ç©¶ç¥ç»ç¼–ç¨‹è¯­è¨€çš„è®¾è®¡åŸç†
4. æä¾›ç¥ç»ç¬¦å·ç³»ç»Ÿçš„å½¢å¼åŒ–æ–¹æ³•

## ğŸ“š ç›®å½•

1. [åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
2. [å½¢å¼åŒ–å®šä¹‰](#2-å½¢å¼åŒ–å®šä¹‰)
3. [å®šç†ä¸è¯æ˜](#3-å®šç†ä¸è¯æ˜)
4. [ä»£ç å®ç°](#4-ä»£ç å®ç°)
5. [åº”ç”¨ç¤ºä¾‹](#5-åº”ç”¨ç¤ºä¾‹)
6. [ç›¸å…³ç†è®º](#6-ç›¸å…³ç†è®º)
7. [å‚è€ƒæ–‡çŒ®](#7-å‚è€ƒæ–‡çŒ®)

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 ç¥ç»ç½‘ç»œåŸºç¡€

**å®šä¹‰ 1.1.1** (ç¥ç»å…ƒ)
ç¥ç»å…ƒæ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œæ•°å­¦è¡¨ç¤ºä¸ºï¼š
$$y = f(\sum_{i=1}^{n} w_i x_i + b)$$
å…¶ä¸­ $x_i$ æ˜¯è¾“å…¥ï¼Œ$w_i$ æ˜¯æƒé‡ï¼Œ$b$ æ˜¯åç½®ï¼Œ$f$ æ˜¯æ¿€æ´»å‡½æ•°ã€‚

**å®šä¹‰ 1.1.2** (ç¥ç»ç½‘ç»œ)
ç¥ç»ç½‘ç»œæ˜¯ç”±å¤šä¸ªç¥ç»å…ƒç»„æˆçš„è®¡ç®—å›¾ï¼Œè¡¨ç¤ºä¸ºæœ‰å‘å›¾ $G = (V, E)$ï¼Œå…¶ä¸­ï¼š

- $V$ æ˜¯ç¥ç»å…ƒé›†åˆ
- $E$ æ˜¯è¿æ¥é›†åˆ
- æ¯ä¸ªè¿æ¥éƒ½æœ‰æƒé‡ $w_{ij}$

**å®šä¹‰ 1.1.3** (ç¥ç»è¯­è¨€)
ç¥ç»è¯­è¨€æ˜¯ç”¨äºæè¿°ç¥ç»ç½‘ç»œç»“æ„å’Œè®¡ç®—è¿‡ç¨‹çš„å½¢å¼è¯­è¨€ï¼ŒåŒ…æ‹¬ï¼š

- ç½‘ç»œæ‹“æ‰‘æè¿°è¯­è¨€
- æƒé‡æ›´æ–°è¯­è¨€
- æ¿€æ´»å‡½æ•°è¯­è¨€
- è®­ç»ƒç®—æ³•è¯­è¨€

### 1.2 ç¥ç»è¯­è¨€çš„åŸºæœ¬ç‰¹å¾

**å®šä¹‰ 1.2.1** (ç¥ç»è¯­æ³•)
ç¥ç»è¯­æ³• $G_N = (V_N, \Sigma_N, R_N, S_N)$ å…¶ä¸­ï¼š

- $V_N$ æ˜¯ç¥ç»éç»ˆç»“ç¬¦é›†åˆ
- $\Sigma_N$ æ˜¯ç¥ç»ç»ˆç»“ç¬¦é›†åˆ
- $R_N$ æ˜¯ç¥ç»é‡å†™è§„åˆ™é›†åˆ
- $S_N$ æ˜¯ç¥ç»å¼€å§‹ç¬¦å·

**å®šä¹‰ 1.2.2** (ç¥ç»è¯­ä¹‰)
ç¥ç»è¯­ä¹‰å°†ç¥ç»è¡¨è¾¾å¼æ˜ å°„åˆ°è®¡ç®—è¿‡ç¨‹ï¼š

- å‰å‘ä¼ æ’­è¯­ä¹‰
- åå‘ä¼ æ’­è¯­ä¹‰
- æƒé‡æ›´æ–°è¯­ä¹‰
- æ¢¯åº¦è®¡ç®—è¯­ä¹‰

**å®šä¹‰ 1.2.3** (ç¥ç»è®¡ç®—æ¨¡å‹)
ç¥ç»è®¡ç®—æ¨¡å‹åŒ…æ‹¬ï¼š

- å‰é¦ˆç¥ç»ç½‘ç»œ
- å¾ªç¯ç¥ç»ç½‘ç»œ
- å·ç§¯ç¥ç»ç½‘ç»œ
- æ³¨æ„åŠ›æœºåˆ¶

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 ç¥ç»ç½‘ç»œè¯­æ³•

**å®šä¹‰ 2.1.1** (ç½‘ç»œæ‹“æ‰‘è¯­æ³•)
ç½‘ç»œæ‹“æ‰‘è¯­æ³•å®šä¹‰ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼š

```text
Network ::= Layer+
Layer ::= Neuron+
Neuron ::= Input | Hidden | Output
Connection ::= Neuron -> Neuron [Weight]
```

**å®šä¹‰ 2.1.2** (æ¿€æ´»å‡½æ•°è¯­æ³•)
æ¿€æ´»å‡½æ•°è¯­æ³•å®šä¹‰ç¥ç»å…ƒçš„æ¿€æ´»æ–¹å¼ï¼š

```text
Activation ::= Sigmoid | Tanh | ReLU | Softmax
Sigmoid ::= 1 / (1 + exp(-x))
Tanh ::= (exp(x) - exp(-x)) / (exp(x) + exp(-x))
ReLU ::= max(0, x)
```

**å®šä¹‰ 2.1.3** (è®­ç»ƒè¯­æ³•)
è®­ç»ƒè¯­æ³•å®šä¹‰å­¦ä¹ è¿‡ç¨‹ï¼š

```text
Training ::= Forward Backward Update
Forward ::= Compute(Input) -> Output
Backward ::= Compute(Gradient) -> WeightGradient
Update ::= Weight = Weight - LearningRate * WeightGradient
```

### 2.2 ç¥ç»è¯­ä¹‰

**å®šä¹‰ 2.2.1** (å‰å‘ä¼ æ’­è¯­ä¹‰)
å‰å‘ä¼ æ’­è¯­ä¹‰ $\llbracket \cdot \rrbracket_F$ å®šä¹‰ï¼š
$$\llbracket y = f(\sum w_i x_i + b) \rrbracket_F = f(\sum w_i \llbracket x_i \rrbracket_F + b)$$

**å®šä¹‰ 2.2.2** (åå‘ä¼ æ’­è¯­ä¹‰)
åå‘ä¼ æ’­è¯­ä¹‰ $\llbracket \cdot \rrbracket_B$ å®šä¹‰ï¼š
$$\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w_i} = \frac{\partial L}{\partial y} \cdot x_i \cdot f'(\sum w_i x_i + b)$$

**å®šä¹‰ 2.2.3** (æƒé‡æ›´æ–°è¯­ä¹‰)
æƒé‡æ›´æ–°è¯­ä¹‰å®šä¹‰ï¼š
$$w_{new} = w_{old} - \alpha \cdot \nabla_w L$$
å…¶ä¸­ $\alpha$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\nabla_w L$ æ˜¯æŸå¤±å‡½æ•°å¯¹æƒé‡çš„æ¢¯åº¦ã€‚

## 3. å®šç†ä¸è¯æ˜

### 3.1 ç¥ç»ç½‘ç»œåŸºæœ¬å®šç†

**å®šç† 3.1.1** (é€šç”¨é€¼è¿‘å®šç†)
å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•°ã€‚

**è¯æ˜**ï¼š
è®¾ $f: [0,1]^n \to \mathbb{R}$ æ˜¯è¿ç»­å‡½æ•°ï¼Œ$\epsilon > 0$ã€‚
å­˜åœ¨ä¸€ä¸ªå…·æœ‰ä¸€ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œ $N$ï¼Œä½¿å¾—ï¼š
$$\sup_{x \in [0,1]^n} |f(x) - N(x)| < \epsilon$$

è¿™é€šè¿‡æ„é€ æ€§è¯æ˜ï¼Œä½¿ç”¨Stone-Weierstrasså®šç†å’Œç¥ç»ç½‘ç»œçš„çº¿æ€§ç»„åˆæ€§è´¨ã€‚

**å®šç† 3.1.2** (æ¢¯åº¦æ¶ˆå¤±å®šç†)
åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œæ¢¯åº¦å¯èƒ½åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¶ˆå¤±ã€‚

**è¯æ˜**ï¼š
å¯¹äºsigmoidæ¿€æ´»å‡½æ•° $f(x) = \frac{1}{1 + e^{-x}}$ï¼Œå…¶å¯¼æ•° $f'(x) = f(x)(1-f(x)) \leq \frac{1}{4}$ã€‚
åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæ¢¯åº¦ä¸ºï¼š
$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y} \cdot \prod_{i=1}^{n} f'(z_i) \cdot w_i$$
å½“å±‚æ•° $n$ å¾ˆå¤§æ—¶ï¼Œ$\prod_{i=1}^{n} f'(z_i)$ è¶‹è¿‘äº0ã€‚

### 3.2 ç¥ç»è®¡ç®—å¤æ‚åº¦

**å®šç† 3.2.1** (ç¥ç»ç½‘ç»œè®¡ç®—å¤æ‚åº¦)
å‰å‘ä¼ æ’­çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(|E|)$ï¼Œå…¶ä¸­ $|E|$ æ˜¯è¾¹çš„æ•°é‡ã€‚

**è¯æ˜**ï¼š
æ¯ä¸ªç¥ç»å…ƒéœ€è¦è®¡ç®—ä¸€æ¬¡åŠ æƒå’Œå’Œæ¿€æ´»å‡½æ•°ï¼Œæ€»è®¡ç®—é‡ä¸ºè¾¹æ•°ã€‚

**å®šç† 3.2.2** (åå‘ä¼ æ’­å¤æ‚åº¦)
åå‘ä¼ æ’­çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(|E|)$ã€‚

**è¯æ˜**ï¼š
åå‘ä¼ æ’­éœ€è¦è®¡ç®—æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œè®¡ç®—é‡ä¸è¾¹æ•°æˆæ­£æ¯”ã€‚

## 4. ä»£ç å®ç°

### 4.1 ç¥ç»ç½‘ç»œåŸºç¡€å®ç°

```rust
use std::collections::HashMap;

/// æ¿€æ´»å‡½æ•°æšä¸¾
#[derive(Debug, Clone)]
enum ActivationFunction {
    Sigmoid,
    Tanh,
    ReLU,
    Softmax,
}

impl ActivationFunction {
    /// åº”ç”¨æ¿€æ´»å‡½æ•°
    fn apply(&self, x: f64) -> f64 {
        match self {
            ActivationFunction::Sigmoid => 1.0 / (1.0 + (-x).exp()),
            ActivationFunction::Tanh => x.tanh(),
            ActivationFunction::ReLU => x.max(0.0),
            ActivationFunction::Softmax => x.exp(), // éœ€è¦å½’ä¸€åŒ–
        }
    }
    
    /// æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
    fn derivative(&self, x: f64) -> f64 {
        match self {
            ActivationFunction::Sigmoid => {
                let fx = self.apply(x);
                fx * (1.0 - fx)
            },
            ActivationFunction::Tanh => {
                let fx = x.tanh();
                1.0 - fx * fx
            },
            ActivationFunction::ReLU => if x > 0.0 { 1.0 } else { 0.0 },
            ActivationFunction::Softmax => 1.0, // ç®€åŒ–å¤„ç†
        }
    }
}

/// ç¥ç»å…ƒ
#[derive(Debug, Clone)]
struct Neuron {
    weights: Vec<f64>,
    bias: f64,
    activation: ActivationFunction,
    last_input: Vec<f64>,
    last_output: f64,
}

impl Neuron {
    /// åˆ›å»ºæ–°ç¥ç»å…ƒ
    fn new(input_size: usize, activation: ActivationFunction) -> Self {
        let mut weights = Vec::with_capacity(input_size);
        for _ in 0..input_size {
            weights.push(rand::random::<f64>() * 2.0 - 1.0); // éšæœºåˆå§‹åŒ–
        }
        
        Neuron {
            weights,
            bias: rand::random::<f64>() * 2.0 - 1.0,
            activation,
            last_input: Vec::new(),
            last_output: 0.0,
        }
    }
    
    /// å‰å‘ä¼ æ’­
    fn forward(&mut self, inputs: &[f64]) -> f64 {
        self.last_input = inputs.to_vec();
        
        let sum: f64 = inputs.iter()
            .zip(&self.weights)
            .map(|(x, w)| x * w)
            .sum::<f64>() + self.bias;
        
        self.last_output = self.activation.apply(sum);
        self.last_output
    }
    
    /// è®¡ç®—æ¢¯åº¦
    fn compute_gradients(&self, output_gradient: f64) -> (Vec<f64>, f64) {
        let activation_gradient = self.activation.derivative(
            self.last_input.iter()
                .zip(&self.weights)
                .map(|(x, w)| x * w)
                .sum::<f64>() + self.bias
        );
        
        let total_gradient = output_gradient * activation_gradient;
        
        let weight_gradients: Vec<f64> = self.last_input.iter()
            .map(|&x| x * total_gradient)
            .collect();
        
        let bias_gradient = total_gradient;
        
        (weight_gradients, bias_gradient)
    }
    
    /// æ›´æ–°æƒé‡
    fn update_weights(&mut self, weight_gradients: &[f64], bias_gradient: f64, learning_rate: f64) {
        for (weight, gradient) in self.weights.iter_mut().zip(weight_gradients) {
            *weight -= learning_rate * gradient;
        }
        self.bias -= learning_rate * bias_gradient;
    }
}

/// ç¥ç»ç½‘ç»œå±‚
#[derive(Debug)]
struct Layer {
    neurons: Vec<Neuron>,
}

impl Layer {
    /// åˆ›å»ºæ–°å±‚
    fn new(input_size: usize, output_size: usize, activation: ActivationFunction) -> Self {
        let neurons = (0..output_size)
            .map(|_| Neuron::new(input_size, activation.clone()))
            .collect();
        
        Layer { neurons }
    }
    
    /// å‰å‘ä¼ æ’­
    fn forward(&mut self, inputs: &[f64]) -> Vec<f64> {
        self.neurons.iter_mut()
            .map(|neuron| neuron.forward(inputs))
            .collect()
    }
    
    /// åå‘ä¼ æ’­
    fn backward(&mut self, output_gradients: &[f64], learning_rate: f64) -> Vec<f64> {
        let mut input_gradients = vec![0.0; self.neurons[0].weights.len()];
        
        for (neuron, &output_gradient) in self.neurons.iter_mut().zip(output_gradients) {
            let (weight_gradients, bias_gradient) = neuron.compute_gradients(output_gradient);
            
            // ç´¯åŠ è¾“å…¥æ¢¯åº¦
            for (input_grad, weight_grad) in input_gradients.iter_mut().zip(weight_gradients) {
                *input_grad += weight_grad;
            }
            
            // æ›´æ–°ç¥ç»å…ƒæƒé‡
            neuron.update_weights(&weight_gradients, bias_gradient, learning_rate);
        }
        
        input_gradients
    }
}

/// ç¥ç»ç½‘ç»œ
#[derive(Debug)]
struct NeuralNetwork {
    layers: Vec<Layer>,
}

impl NeuralNetwork {
    /// åˆ›å»ºæ–°ç¥ç»ç½‘ç»œ
    fn new(layer_sizes: &[usize], activations: &[ActivationFunction]) -> Self {
        assert_eq!(layer_sizes.len() - 1, activations.len());
        
        let mut layers = Vec::new();
        for i in 0..layer_sizes.len() - 1 {
            let layer = Layer::new(
                layer_sizes[i],
                layer_sizes[i + 1],
                activations[i].clone()
            );
            layers.push(layer);
        }
        
        NeuralNetwork { layers }
    }
    
    /// å‰å‘ä¼ æ’­
    fn forward(&mut self, inputs: &[f64]) -> Vec<f64> {
        let mut current_inputs = inputs.to_vec();
        
        for layer in &mut self.layers {
            current_inputs = layer.forward(&current_inputs);
        }
        
        current_inputs
    }
    
    /// è®­ç»ƒ
    fn train(&mut self, inputs: &[f64], targets: &[f64], learning_rate: f64) -> f64 {
        // å‰å‘ä¼ æ’­
        let outputs = self.forward(inputs);
        
        // è®¡ç®—æŸå¤±
        let loss: f64 = outputs.iter()
            .zip(targets)
            .map(|(o, t)| 0.5 * (o - t).powi(2))
            .sum();
        
        // è®¡ç®—è¾“å‡ºå±‚æ¢¯åº¦
        let mut output_gradients: Vec<f64> = outputs.iter()
            .zip(targets)
            .map(|(o, t)| o - t)
            .collect();
        
        // åå‘ä¼ æ’­
        for layer in self.layers.iter_mut().rev() {
            output_gradients = layer.backward(&output_gradients, learning_rate);
        }
        
        loss
    }
}
```

### 4.2 å·ç§¯ç¥ç»ç½‘ç»œå®ç°

```rust
/// å·ç§¯å±‚
#[derive(Debug)]
struct ConvLayer {
    filters: Vec<Vec<Vec<f64>>>, // [num_filters][height][width]
    bias: Vec<f64>,
    stride: usize,
    padding: usize,
}

impl ConvLayer {
    /// åˆ›å»ºå·ç§¯å±‚
    fn new(num_filters: usize, filter_size: usize, stride: usize, padding: usize) -> Self {
        let filters = (0..num_filters)
            .map(|_| {
                (0..filter_size)
                    .map(|_| {
                        (0..filter_size)
                            .map(|_| rand::random::<f64>() * 2.0 - 1.0)
                            .collect()
                    })
                    .collect()
            })
            .collect();
        
        let bias = (0..num_filters)
            .map(|_| rand::random::<f64>() * 2.0 - 1.0)
            .collect();
        
        ConvLayer {
            filters,
            bias,
            stride,
            padding,
        }
    }
    
    /// å·ç§¯æ“ä½œ
    fn convolve(&self, input: &[Vec<f64>], filter: &[Vec<f64>]) -> Vec<Vec<f64>> {
        let input_height = input.len();
        let input_width = input[0].len();
        let filter_size = filter.len();
        
        let output_height = (input_height + 2 * self.padding - filter_size) / self.stride + 1;
        let output_width = (input_width + 2 * self.padding - filter_size) / self.stride + 1;
        
        let mut output = vec![vec![0.0; output_width]; output_height];
        
        for i in 0..output_height {
            for j in 0..output_width {
                let mut sum = 0.0;
                
                for fi in 0..filter_size {
                    for fj in 0..filter_size {
                        let input_i = i * self.stride + fi;
                        let input_j = j * self.stride + fj;
                        
                        if input_i < input_height && input_j < input_width {
                            sum += input[input_i][input_j] * filter[fi][fj];
                        }
                    }
                }
                
                output[i][j] = sum;
            }
        }
        
        output
    }
    
    /// å‰å‘ä¼ æ’­
    fn forward(&self, input: &[Vec<Vec<f64>>]) -> Vec<Vec<Vec<f64>>> {
        let mut outputs = Vec::new();
        
        for (filter_idx, filter) in self.filters.iter().enumerate() {
            let mut channel_output = Vec::new();
            
            for input_channel in input {
                let conv_result = self.convolve(input_channel, filter);
                channel_output.push(conv_result);
            }
            
            // æ·»åŠ åç½®
            for row in &mut channel_output {
                for val in row.iter_mut() {
                    *val += self.bias[filter_idx];
                }
            }
            
            outputs.push(channel_output);
        }
        
        outputs
    }
}

/// æ± åŒ–å±‚
#[derive(Debug)]
struct PoolingLayer {
    pool_size: usize,
    stride: usize,
    pool_type: PoolType,
}

#[derive(Debug, Clone)]
enum PoolType {
    Max,
    Average,
}

impl PoolingLayer {
    /// åˆ›å»ºæ± åŒ–å±‚
    fn new(pool_size: usize, stride: usize, pool_type: PoolType) -> Self {
        PoolingLayer {
            pool_size,
            stride,
            pool_type,
        }
    }
    
    /// æ± åŒ–æ“ä½œ
    fn pool(&self, input: &[Vec<f64>]) -> Vec<Vec<f64>> {
        let input_height = input.len();
        let input_width = input[0].len();
        
        let output_height = (input_height - self.pool_size) / self.stride + 1;
        let output_width = (input_width - self.pool_size) / self.stride + 1;
        
        let mut output = vec![vec![0.0; output_width]; output_height];
        
        for i in 0..output_height {
            for j in 0..output_width {
                let start_i = i * self.stride;
                let start_j = j * self.stride;
                
                let mut values = Vec::new();
                for fi in 0..self.pool_size {
                    for fj in 0..self.pool_size {
                        let input_i = start_i + fi;
                        let input_j = start_j + fj;
                        
                        if input_i < input_height && input_j < input_width {
                            values.push(input[input_i][input_j]);
                        }
                    }
                }
                
                output[i][j] = match self.pool_type {
                    PoolType::Max => values.into_iter().fold(f64::NEG_INFINITY, f64::max),
                    PoolType::Average => values.iter().sum::<f64>() / values.len() as f64,
                };
            }
        }
        
        output
    }
    
    /// å‰å‘ä¼ æ’­
    fn forward(&self, input: &[Vec<Vec<f64>>]) -> Vec<Vec<Vec<f64>>> {
        input.iter()
            .map(|channel| self.pool(channel))
            .collect()
    }
}
```

### 4.3 å¾ªç¯ç¥ç»ç½‘ç»œå®ç°

```rust
/// å¾ªç¯ç¥ç»ç½‘ç»œå±‚
#[derive(Debug)]
struct RNNLayer {
    input_weights: Vec<Vec<f64>>,  // W_x
    hidden_weights: Vec<Vec<f64>>, // W_h
    output_weights: Vec<Vec<f64>>, // W_y
    input_bias: Vec<f64>,
    hidden_bias: Vec<f64>,
    output_bias: Vec<f64>,
    hidden_size: usize,
    activation: ActivationFunction,
}

impl RNNLayer {
    /// åˆ›å»ºRNNå±‚
    fn new(input_size: usize, hidden_size: usize, output_size: usize, activation: ActivationFunction) -> Self {
        let input_weights = (0..hidden_size)
            .map(|_| (0..input_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect())
            .collect();
        
        let hidden_weights = (0..hidden_size)
            .map(|_| (0..hidden_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect())
            .collect();
        
        let output_weights = (0..output_size)
            .map(|_| (0..hidden_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect())
            .collect();
        
        let input_bias = (0..hidden_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect();
        let hidden_bias = (0..hidden_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect();
        let output_bias = (0..output_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect();
        
        RNNLayer {
            input_weights,
            hidden_weights,
            output_weights,
            input_bias,
            hidden_bias,
            output_bias,
            hidden_size,
            activation,
        }
    }
    
    /// å‰å‘ä¼ æ’­
    fn forward(&self, inputs: &[Vec<f64>], initial_hidden: Option<Vec<f64>>) -> (Vec<Vec<f64>>, Vec<Vec<f64>>) {
        let mut hidden_states = Vec::new();
        let mut outputs = Vec::new();
        
        let mut hidden = initial_hidden.unwrap_or_else(|| vec![0.0; self.hidden_size]);
        
        for input in inputs {
            // è®¡ç®—éšè—çŠ¶æ€: h_t = f(W_x * x_t + W_h * h_{t-1} + b_h)
            let mut new_hidden = vec![0.0; self.hidden_size];
            
            for i in 0..self.hidden_size {
                let mut sum = self.input_bias[i];
                
                // è¾“å…¥æƒé‡
                for j in 0..input.len() {
                    sum += self.input_weights[i][j] * input[j];
                }
                
                // éšè—æƒé‡
                for j in 0..self.hidden_size {
                    sum += self.hidden_weights[i][j] * hidden[j];
                }
                
                new_hidden[i] = self.activation.apply(sum);
            }
            
            // è®¡ç®—è¾“å‡º: y_t = W_y * h_t + b_y
            let mut output = vec![0.0; self.output_weights.len()];
            for i in 0..output.len() {
                let mut sum = self.output_bias[i];
                for j in 0..self.hidden_size {
                    sum += self.output_weights[i][j] * new_hidden[j];
                }
                output[i] = sum;
            }
            
            hidden_states.push(new_hidden.clone());
            outputs.push(output);
            hidden = new_hidden;
        }
        
        (hidden_states, outputs)
    }
}
```

## 5. åº”ç”¨ç¤ºä¾‹

### 5.1 ç®€å•åˆ†ç±»ä»»åŠ¡

```rust
// ç®€å•åˆ†ç±»ç¤ºä¾‹
fn classification_example() {
    // åˆ›å»ºç¥ç»ç½‘ç»œï¼š2è¾“å…¥ -> 4éšè— -> 2è¾“å‡º
    let layer_sizes = vec![2, 4, 2];
    let activations = vec![
        ActivationFunction::ReLU,
        ActivationFunction::Softmax,
    ];
    
    let mut network = NeuralNetwork::new(&layer_sizes, &activations);
    
    // è®­ç»ƒæ•°æ®ï¼šXORé—®é¢˜
    let training_data = vec![
        (vec![0.0, 0.0], vec![1.0, 0.0]),
        (vec![0.0, 1.0], vec![0.0, 1.0]),
        (vec![1.0, 0.0], vec![0.0, 1.0]),
        (vec![1.0, 1.0], vec![1.0, 0.0]),
    ];
    
    // è®­ç»ƒ
    let learning_rate = 0.1;
    for epoch in 0..1000 {
        let mut total_loss = 0.0;
        
        for (inputs, targets) in &training_data {
            let loss = network.train(inputs, targets, learning_rate);
            total_loss += loss;
        }
        
        if epoch % 100 == 0 {
            println!("Epoch {}, Loss: {:.4}", epoch, total_loss);
        }
    }
    
    // æµ‹è¯•
    println!("\næµ‹è¯•ç»“æœ:");
    for (inputs, _) in &training_data {
        let output = network.forward(inputs);
        println!("è¾“å…¥: {:?}, è¾“å‡º: {:?}", inputs, output);
    }
}
```

### 5.2 å›¾åƒåˆ†ç±»

```rust
// å›¾åƒåˆ†ç±»ç¤ºä¾‹
fn image_classification_example() {
    // åˆ›å»ºç®€å•çš„CNN
    let conv_layer = ConvLayer::new(4, 3, 1, 1);
    let pool_layer = PoolingLayer::new(2, 2, PoolType::Max);
    
    // æ¨¡æ‹Ÿè¾“å…¥å›¾åƒ (3é€šé“, 6x6)
    let input_image = vec![
        vec![vec![1.0; 6]; 6], // çº¢è‰²é€šé“
        vec![vec![0.5; 6]; 6], // ç»¿è‰²é€šé“
        vec![vec![0.2; 6]; 6], // è“è‰²é€šé“
    ];
    
    // å·ç§¯å±‚
    let conv_output = conv_layer.forward(&input_image);
    println!("å·ç§¯è¾“å‡ºå½¢çŠ¶: {}x{}x{}", 
             conv_output.len(), 
             conv_output[0].len(), 
             conv_output[0][0].len());
    
    // æ± åŒ–å±‚
    let pool_output = pool_layer.forward(&conv_output);
    println!("æ± åŒ–è¾“å‡ºå½¢çŠ¶: {}x{}x{}", 
             pool_output.len(), 
             pool_output[0].len(), 
             pool_output[0][0].len());
}
```

### 5.3 åºåˆ—é¢„æµ‹

```rust
// åºåˆ—é¢„æµ‹ç¤ºä¾‹
fn sequence_prediction_example() {
    // åˆ›å»ºRNN
    let rnn = RNNLayer::new(1, 4, 1, ActivationFunction::Tanh);
    
    // è®­ç»ƒæ•°æ®ï¼šç®€å•åºåˆ— [1, 2, 3, 4, 5]
    let input_sequence = vec![
        vec![1.0],
        vec![2.0],
        vec![3.0],
        vec![4.0],
        vec![5.0],
    ];
    
    let target_sequence = vec![
        vec![2.0],
        vec![3.0],
        vec![4.0],
        vec![5.0],
        vec![6.0],
    ];
    
    // å‰å‘ä¼ æ’­
    let (hidden_states, outputs) = rnn.forward(&input_sequence, None);
    
    println!("åºåˆ—é¢„æµ‹ç»“æœ:");
    for (i, (input, output)) in input_sequence.iter().zip(outputs.iter()).enumerate() {
        println!("æ—¶é—´æ­¥ {}: è¾“å…¥={:?}, è¾“å‡º={:?}", i, input, output);
    }
}
```

## 6. ç›¸å…³ç†è®º

### 6.1 ä¸å½¢å¼è¯­è¨€ç†è®ºçš„å…³ç³»

ç¥ç»è¯­è¨€ä¸ç»å…¸å½¢å¼è¯­è¨€ç†è®ºçš„å…³ç³»ï¼š

1. **è¯­æ³•æ‰©å±•**ï¼šç¥ç»è¯­è¨€æ‰©å±•äº†ä¼ ç»Ÿè¯­æ³•ï¼ŒåŒ…å«ç½‘ç»œæ‹“æ‰‘å’Œè®¡ç®—è§„åˆ™
2. **è¯­ä¹‰ä¸°å¯Œ**ï¼šç¥ç»è¯­è¨€å…·æœ‰ä¸°å¯Œçš„è®¡ç®—è¯­ä¹‰å’Œè®­ç»ƒè¯­ä¹‰
3. **åŠ¨æ€æ€§**ï¼šç¥ç»è¯­è¨€æ”¯æŒåŠ¨æ€æƒé‡æ›´æ–°å’Œå­¦ä¹ è¿‡ç¨‹
4. **å¹¶è¡Œæ€§**ï¼šç¥ç»è¯­è¨€å¤©ç„¶æ”¯æŒå¹¶è¡Œè®¡ç®—

### 6.2 ä¸è®¡ç®—ç†è®ºçš„å…³ç³»

ç¥ç»è¯­è¨€ä¸è®¡ç®—ç†è®ºçš„å…³ç³»ï¼š

1. **è®¡ç®—æ¨¡å‹**ï¼šç¥ç»ç½‘ç»œæ˜¯ä¸€ç§æ–°çš„è®¡ç®—æ¨¡å‹
2. **å¤æ‚åº¦ç†è®º**ï¼šç¥ç»è®¡ç®—çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦åˆ†æ
3. **å­¦ä¹ ç†è®º**ï¼šç¥ç»ç½‘ç»œçš„æ”¶æ•›æ€§å’Œæ³›åŒ–èƒ½åŠ›
4. **ä¼˜åŒ–ç†è®º**ï¼šæ¢¯åº¦ä¸‹é™å’Œä¼˜åŒ–ç®—æ³•çš„ç†è®ºåŸºç¡€

### 6.3 ä¸äººå·¥æ™ºèƒ½çš„å…³ç³»

ç¥ç»è¯­è¨€ä¸äººå·¥æ™ºèƒ½çš„å…³ç³»ï¼š

1. **æœºå™¨å­¦ä¹ **ï¼šç¥ç»ç½‘ç»œæ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦å·¥å…·
2. **æ·±åº¦å­¦ä¹ **ï¼šç¥ç»è¯­è¨€æ”¯æŒæ·±åº¦ç½‘ç»œçš„è®¾è®¡å’Œè®­ç»ƒ
3. **è®¤çŸ¥å»ºæ¨¡**ï¼šç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººç±»è®¤çŸ¥è¿‡ç¨‹
4. **æ™ºèƒ½ç³»ç»Ÿ**ï¼šç¥ç»è¯­è¨€æ„å»ºæ™ºèƒ½ç³»ç»Ÿçš„ç†è®ºåŸºç¡€

## 7. å‚è€ƒæ–‡çŒ®

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.
2. Haykin, S. (2009). Neural networks and learning machines. Pearson Education.
3. Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.
4. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.
5. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.

---

**ç›¸å…³æ–‡æ¡£**ï¼š

- [03.1.1 æœ‰é™è‡ªåŠ¨æœº](../03.1.1_æœ‰é™è‡ªåŠ¨æœº.md)
- [03.2.2 ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•](../03.2.2_ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•.md)
- [03.5.1 æ“ä½œè¯­ä¹‰](../03.5.1_æ“ä½œè¯­ä¹‰.md)
- [04.2.1 çº¿æ€§ç±»å‹åŸºç¡€](../04_Type_Theory/04.2.1_çº¿æ€§ç±»å‹åŸºç¡€.md)
