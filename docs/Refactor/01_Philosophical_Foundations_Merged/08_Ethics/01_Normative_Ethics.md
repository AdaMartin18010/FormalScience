# è§„èŒƒä¼¦ç†å­¦ (Normative Ethics)

## ğŸ“‹ 1. æ¦‚è¿°

è§„èŒƒä¼¦ç†å­¦æ˜¯ä¼¦ç†å­¦çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œç ”ç©¶ä»€ä¹ˆæ ·çš„è¡Œä¸ºæ˜¯é“å¾·çš„ã€ä»€ä¹ˆæ ·çš„è¡Œä¸ºæ˜¯ä¸é“å¾·çš„ï¼Œå¹¶è¯•å›¾å»ºç«‹åˆ¤æ–­è¡Œä¸ºé“å¾·æ€§çš„ä¸€èˆ¬æ€§æ ‡å‡†ã€‚æœ¬æ–‡æ¡£ä»å½¢å¼ç§‘å­¦çš„è§’åº¦åˆ†æä¸‰å¤§ä¸»è¦è§„èŒƒä¼¦ç†å­¦ç†è®ºï¼šåŠŸåˆ©ä¸»ä¹‰ã€ä¹‰åŠ¡è®ºå’Œå¾·æ€§ä¼¦ç†å­¦ï¼Œä¸ºå®ƒä»¬æä¾›å½¢å¼åŒ–çš„è¡¨è¾¾å’Œè®¡ç®—å®ç°ã€‚

## ğŸ¯ 2. è§„èŒƒä¼¦ç†å­¦çš„æ ¸å¿ƒé—®é¢˜

è§„èŒƒä¼¦ç†å­¦è¯•å›¾å›ç­”ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š

1. ä»€ä¹ˆä½¿è¡Œä¸ºåœ¨é“å¾·ä¸Šæ˜¯æ­£ç¡®çš„ï¼Ÿ
2. é“å¾·è¯„ä»·çš„åŸºç¡€æ˜¯ä»€ä¹ˆï¼Ÿ
3. é“å¾·åˆ¤æ–­å¦‚ä½•é€‚ç”¨äºä¸åŒæƒ…å¢ƒï¼Ÿ
4. å¦‚ä½•æƒè¡¡ä¸åŒçš„é“å¾·ä»·å€¼ï¼Ÿ

## ğŸ“š 3. æ ¸å¿ƒç†è®ºå½¢å¼åŒ–

### 3.1 åŠŸåˆ©ä¸»ä¹‰

åŠŸåˆ©ä¸»ä¹‰è®¤ä¸ºï¼Œè¡Œä¸ºçš„é“å¾·ä»·å€¼å–å†³äºå…¶ç»“æœäº§ç”Ÿçš„æ•ˆç”¨æˆ–ç¦åˆ©ã€‚

#### 3.1.1 åŸºæœ¬åŸç†

åŠŸåˆ©ä¸»ä¹‰çš„æ ¸å¿ƒåŸåˆ™å¯å½¢å¼åŒ–ä¸ºï¼š

$$
Right(A) \iff \forall B \in Actions, Utility(A) \geq Utility(B)
$$

å…¶ä¸­ï¼š

- $Right(A)$ è¡¨ç¤ºè¡Œä¸º $A$ åœ¨é“å¾·ä¸Šæ˜¯æ­£ç¡®çš„
- $Actions$ è¡¨ç¤ºæ‰€æœ‰å¯èƒ½è¡Œä¸ºçš„é›†åˆ
- $Utility(X)$ è¡¨ç¤ºè¡Œä¸º $X$ äº§ç”Ÿçš„æ€»æ•ˆç”¨

#### 3.1.2 æ•ˆç”¨è®¡ç®—

æ•ˆç”¨è®¡ç®—å…¬å¼ï¼š

$$
Utility(A) = \sum_{i=1}^{n} w_i \times Welfare_i(A)
$$

å…¶ä¸­ï¼š

- $n$ æ˜¯å—å½±å“ä¸ªä½“çš„æ•°é‡
- $w_i$ æ˜¯ä¸ªä½“ $i$ çš„æƒé‡
- $Welfare_i(A)$ æ˜¯è¡Œä¸º $A$ å¯¹ä¸ªä½“ $i$ çš„ç¦åˆ©å½±å“

#### 3.1.3 å˜ä½“å½¢å¼åŒ–

1. **è¡Œä¸ºåŠŸåˆ©ä¸»ä¹‰**ï¼šæ¯ä¸ªå…·ä½“è¡Œä¸ºåº”æœ€å¤§åŒ–æ•ˆç”¨

    $$
    Right(A) \iff Utility(A) = \max_{X \in Actions} Utility(X)
    $$

2. **è§„åˆ™åŠŸåˆ©ä¸»ä¹‰**ï¼šéµå¾ªèƒ½æœ€å¤§åŒ–æ•ˆç”¨çš„è§„åˆ™

    $$
    Right(A) \iff A \text{ ç¬¦åˆè§„åˆ™ } R \land R \in \arg\max_{R' \in Rules} Utility(Society(R'))
    $$

3. **åå¥½åŠŸåˆ©ä¸»ä¹‰**ï¼šæ»¡è¶³åå¥½è€Œéä»…è€ƒè™‘å¿«ä¹

    $$
    Utility(A) = \sum_{i=1}^{n} w_i \times PreferenceSatisfaction_i(A)
    $$

### 3.2 ä¹‰åŠ¡è®º

ä¹‰åŠ¡è®ºè®¤ä¸ºï¼Œè¡Œä¸ºçš„é“å¾·ä»·å€¼å–å†³äºè¡Œä¸ºæœ¬èº«çš„ç‰¹æ€§ï¼Œè€Œä¸æ˜¯å…¶ç»“æœã€‚

#### 3.2.1 åŸºæœ¬åŸç†

åº·å¾·çš„ç»å¯¹å‘½ä»¤å¯å½¢å¼åŒ–ä¸ºï¼š

$$
Right(A) \iff Universalizable(A) \land RespectsPerson(A)
$$

å…¶ä¸­ï¼š

- $Universalizable(A)$ è¡¨ç¤ºè¡Œä¸º $A$ çš„å‡†åˆ™å¯ä»¥ä¸è‡ªç›¸çŸ›ç›¾åœ°è¢«æ™®éåŒ–
- $RespectsPerson(A)$ è¡¨ç¤ºè¡Œä¸º $A$ å°Šé‡äººçš„å°Šä¸¥ï¼Œå°†äººè§†ä¸ºç›®çš„è€Œéæ‰‹æ®µ

#### 3.2.2 é“å¾·è§„åˆ™ç³»ç»Ÿ

ä¹‰åŠ¡è®ºæ„å»ºå½¢å¼åŒ–é“å¾·è§„åˆ™ç³»ç»Ÿï¼š

$$
MoralSystem = \langle P, D, R \rangle
$$

å…¶ä¸­ï¼š

- $P$ æ˜¯é“å¾·åŸåˆ™é›†åˆ
- $D$ æ˜¯é“å¾·ä¹‰åŠ¡é›†åˆ
- $R$ æ˜¯é“å¾·æ¨ç†è§„åˆ™é›†åˆ

#### 3.2.3 é“å¾·å†²çªè§£å†³

ä¹‰åŠ¡è®ºä¸­å†²çªè§£å†³å¯è¡¨ç¤ºä¸ºï¼š

$$
Priority(D_i, D_j, C) = \begin{cases}
1 & \text{å¦‚æœä¹‰åŠ¡ } D_i \text{ åœ¨æƒ…å¢ƒ } C \text{ ä¸­ä¼˜å…ˆäº } D_j \\
0 & \text{å¦‚æœä¹‰åŠ¡ } D_j \text{ åœ¨æƒ…å¢ƒ } C \text{ ä¸­ä¼˜å…ˆäº } D_i \\
undefined & \text{å¦‚æœä¹‰åŠ¡ä¸å¯æ¯”è¾ƒ}
\end{cases}
$$

### 3.3 å¾·æ€§ä¼¦ç†å­¦

å¾·æ€§ä¼¦ç†å­¦å…³æ³¨è¡Œä¸ºä¸»ä½“çš„å“æ ¼ç‰¹è´¨ï¼Œè€Œéå•ä¸€è¡Œä¸ºæˆ–è§„åˆ™ã€‚

#### 3.3.1 åŸºæœ¬åŸç†

å¾·æ€§ä¼¦ç†å­¦å¯å½¢å¼åŒ–ä¸ºï¼š

$$
Right(A, P) \iff Virtuous(P) \land InAccordanceWithVirtue(A, P)
$$

å…¶ä¸­ï¼š

- $P$ è¡¨ç¤ºè¡Œä¸ºä¸»ä½“
- $Virtuous(P)$ è¡¨ç¤ºä¸»ä½“ $P$ å…·æœ‰å¾·æ€§
- $InAccordanceWithVirtue(A, P)$ è¡¨ç¤ºè¡Œä¸º $A$ ç¬¦åˆä¸»ä½“ $P$ çš„å¾·æ€§

#### 3.3.2 å¾·æ€§ç©ºé—´

å¾·æ€§å¯åœ¨å¤šç»´ç©ºé—´ä¸­è¡¨ç¤ºï¼š

$$
VirtueSpace = \langle V_1, V_2, ..., V_n \rangle
$$

å…¶ä¸­æ¯ä¸ª $V_i$ ä»£è¡¨ä¸€ç§å¾·æ€§ç»´åº¦ï¼Œæ¯ä¸ªäººå¯è¡¨ç¤ºä¸ºæ­¤ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ã€‚

#### 3.3.3 å¾·æ€§ä¸ä¸­é“

äºšé‡Œå£«å¤šå¾·çš„ä¸­é“ç†è®ºå¯å½¢å¼åŒ–ä¸ºï¼š

$$
Virtue_i = Mean(Excess_i, Deficiency_i)
$$

å…¶ä¸­ï¼š

- $Virtue_i$ æ˜¯ç¬¬ $i$ ç§å¾·æ€§
- $Excess_i$ æ˜¯è¿‡åº¦çš„æç«¯
- $Deficiency_i$ æ˜¯ä¸è¶³çš„æç«¯

## ğŸ” 4. ç†è®ºæ¯”è¾ƒä¸ç»¼åˆ

### 4.1 å½¢å¼åŒ–æ¯”è¾ƒæ¡†æ¶

ä¸‰ç§ç†è®ºå¯åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹æ¯”è¾ƒï¼š

$$
Evaluation(T, A, C) = \langle Consistency(T), Intuitiveness(T, A, C), Applicability(T, C) \rangle
$$

å…¶ä¸­ï¼š

- $T$ æ˜¯é“å¾·ç†è®º
- $A$ æ˜¯è¡Œä¸º
- $C$ æ˜¯æƒ…å¢ƒ
- $Consistency(T)$ æµ‹é‡ç†è®ºçš„å†…éƒ¨ä¸€è‡´æ€§
- $Intuitiveness(T, A, C)$ æµ‹é‡ç†è®ºåˆ¤æ–­ä¸ç›´è§‰çš„å»åˆåº¦
- $Applicability(T, C)$ æµ‹é‡ç†è®ºåœ¨æƒ…å¢ƒä¸­çš„é€‚ç”¨æ€§

### 4.2 ç†è®ºç»¼åˆ

æ•´åˆæ–¹æ³•å¯å½¢å¼åŒ–ä¸ºï¼š

$$
IntegratedJudgment(A) = \alpha \times Utilitarian(A) + \beta \times Deontological(A) + \gamma \times Virtue(A)
$$

å…¶ä¸­ï¼š

- $\alpha, \beta, \gamma$ æ˜¯æƒé‡å‚æ•°ï¼Œä¸” $\alpha + \beta + \gamma = 1$
- $Utilitarian(A)$ã€$Deontological(A)$ å’Œ $Virtue(A)$ åˆ†åˆ«æ˜¯ä¸‰ç§ç†è®ºå¯¹è¡Œä¸º $A$ çš„è¯„ä»·

## ğŸ’» 5. è®¡ç®—å®ç°

ä»¥ä¸‹ä»£ç å®ç°äº†è§„èŒƒä¼¦ç†å­¦çš„æ ¸å¿ƒæ¦‚å¿µï¼š

```rust
// åŸºç¡€ç±»å‹å®šä¹‰
pub type AgentId = u64;
pub type ActionId = u64;
pub type ContextId = u64;
pub type Utility = f64;

// é“å¾·ç†è®ºç‰¹å¾
pub trait MoralTheory {
    fn evaluate(&self, action: &Action, context: &Context) -> MoralEvaluation;
    fn is_right(&self, action: &Action, context: &Context) -> bool;
    fn rank_actions(&self, actions: &[Action], context: &Context) -> Vec<(ActionId, MoralEvaluation)>;
    fn resolve_dilemma(&self, actions: &[Action], context: &Context) -> ActionId;
}

// åŠŸåˆ©ä¸»ä¹‰å®ç°
pub struct Utilitarianism {
    pub weight_fn: Box<dyn Fn(AgentId, ContextId) -> f64>,
}

impl MoralTheory for Utilitarianism {
    fn evaluate(&self, action: &Action, context: &Context) -> MoralEvaluation {
        let mut total_utility = 0.0;
        
        for agent in context.affected_agents() {
            let weight = (self.weight_fn)(agent.id, context.id);
            let welfare = action.welfare_impact(agent.id, context);
            total_utility += weight * welfare;
        }
        
        MoralEvaluation {
            theory: "Utilitarianism".to_string(),
            value: total_utility,
            classification: if total_utility > 0.0 { "Right" } else { "Wrong" }.to_string(),
            justification: format!("è¡Œä¸ºäº§ç”Ÿæ€»æ•ˆç”¨: {}", total_utility),
        }
    }
    
    fn is_right(&self, action: &Action, context: &Context) -> bool {
        // ä¸€ä¸ªè¡Œä¸ºæ˜¯æ­£ç¡®çš„ï¼Œå½“ä¸”ä»…å½“æ²¡æœ‰å…¶ä»–å¯è¡Œè¡Œä¸ºäº§ç”Ÿæ›´é«˜çš„æ•ˆç”¨
        let this_utility = self.evaluate(action, context).value;
        
        for alt_action in context.available_actions() {
            if alt_action.id != action.id {
                let alt_utility = self.evaluate(&alt_action, context).value;
                if alt_utility > this_utility {
                    return false;
                }
            }
        }
        
        true
    }
    
    // å…¶ä½™æ–¹æ³•å®ç°ç•¥
}

// ä¹‰åŠ¡è®ºå®ç°
pub struct Deontology {
    pub principles: Vec<MoralPrinciple>,
    pub priority_matrix: HashMap<(PrincipleId, PrincipleId), i32>,
}

impl MoralTheory for Deontology {
    fn evaluate(&self, action: &Action, context: &Context) -> MoralEvaluation {
        // æ£€æŸ¥è¡Œä¸ºæ˜¯å¦ç¬¦åˆé“å¾·åŸåˆ™
        let violations = self.principles.iter()
            .filter(|p| !p.is_respected_by(action, context))
            .collect::<Vec<_>>();
            
        let is_permissible = violations.is_empty();
        
        MoralEvaluation {
            theory: "Deontology".to_string(),
            value: if is_permissible { 1.0 } else { -1.0 },
            classification: if is_permissible { "Right" } else { "Wrong" }.to_string(),
            justification: if is_permissible {
                "è¡Œä¸ºç¬¦åˆæ‰€æœ‰é“å¾·åŸåˆ™".to_string()
            } else {
                format!("è¡Œä¸ºè¿ååŸåˆ™: {}", 
                    violations.iter()
                        .map(|p| p.name.clone())
                        .collect::<Vec<_>>()
                        .join(", ")
                )
            },
        }
    }
    
    // å…¶ä½™æ–¹æ³•å®ç°ç•¥
}

// å¾·æ€§ä¼¦ç†å­¦å®ç°
pub struct VirtueEthics {
    pub virtues: Vec<Virtue>,
    pub virtue_thresholds: HashMap<VirtueId, f64>,
}

impl MoralTheory for VirtueEthics {
    fn evaluate(&self, action: &Action, context: &Context) -> MoralEvaluation {
        // è¯„ä¼°è¡Œä¸ºæ˜¯å¦å±•ç°äº†å¾·æ€§
        let mut virtue_scores = Vec::new();
        
        for virtue in &self.virtues {
            let score = virtue.exhibited_in(action, context);
            virtue_scores.push((virtue.id, score));
        }
        
        let avg_virtue = virtue_scores.iter()
            .map(|(_, score)| *score)
            .sum::<f64>() / virtue_scores.len() as f64;
            
        MoralEvaluation {
            theory: "Virtue Ethics".to_string(),
            value: avg_virtue,
            classification: if avg_virtue >= 0.5 { "Right" } else { "Wrong" }.to_string(),
            justification: format!("è¡Œä¸ºå±•ç°å¾·æ€§å¹³å‡åˆ†: {:.2}", avg_virtue),
        }
    }
    
    // å…¶ä½™æ–¹æ³•å®ç°ç•¥
}

// ç»¼åˆä¼¦ç†è¯„ä¼°ç³»ç»Ÿ
pub struct IntegratedEthics {
    pub utilitarian: Utilitarianism,
    pub deontological: Deontology, 
    pub virtue_ethics: VirtueEthics,
    pub weights: (f64, f64, f64), // (åŠŸåˆ©ä¸»ä¹‰æƒé‡, ä¹‰åŠ¡è®ºæƒé‡, å¾·æ€§ä¼¦ç†æƒé‡)
}

impl MoralTheory for IntegratedEthics {
    fn evaluate(&self, action: &Action, context: &Context) -> MoralEvaluation {
        let util_eval = self.utilitarian.evaluate(action, context);
        let deont_eval = self.deontological.evaluate(action, context);
        let virtue_eval = self.virtue_ethics.evaluate(action, context);
        
        let integrated_value = 
            self.weights.0 * util_eval.value + 
            self.weights.1 * deont_eval.value + 
            self.weights.2 * virtue_eval.value;
            
        MoralEvaluation {
            theory: "Integrated Ethics".to_string(),
            value: integrated_value,
            classification: if integrated_value > 0.0 { "Right" } else { "Wrong" }.to_string(),
            justification: format!(
                "ç»¼åˆè¯„ä¼°:\nåŠŸåˆ©: {:.2} (æƒé‡: {:.2})\nä¹‰åŠ¡: {:.2} (æƒé‡: {:.2})\nå¾·æ€§: {:.2} (æƒé‡: {:.2})",
                util_eval.value, self.weights.0,
                deont_eval.value, self.weights.1,
                virtue_eval.value, self.weights.2
            ),
        }
    }
    
    // å…¶ä½™æ–¹æ³•å®ç°ç•¥
}
```

## ğŸ“Š 6. æ¡ˆä¾‹åˆ†æ

### 6.1 ç”µè½¦é—®é¢˜å½¢å¼åŒ–

ç”µè½¦é—®é¢˜å¯å½¢å¼åŒ–ä¸ºï¼š

$$
Trolley = \langle A_{divert}, A_{nothing}, Lives(Track1), Lives(Track2) \rangle
$$

ä¸‰ç§ä¼¦ç†ç†è®ºçš„è¯„ä»·ï¼š

$$
\begin{align}
Utilitarian(A_{divert}) &= 1 \times Lives(Track1) - 5 \times Lives(Track2) \\
Deontological(A_{divert}) &= Violates(DoNoHarm) \land Violates(UseAsEnd) \\
Virtue(A_{divert}) &= Compassion(save5) - Ruthlessness(kill1)
\end{align}
$$

### 6.2 å¤æ‚æ¡ˆä¾‹ï¼šè‡ªåŠ¨é©¾é©¶ä¼¦ç†å†³ç­–

è‡ªåŠ¨é©¾é©¶å†³ç­–å¯å½¢å¼åŒ–ä¸ºï¼š

$$
AutonomousDecision = \langle A_{options}, Risks, Values, Constraints \rangle
$$

å„ç†è®ºçš„é›†æˆï¼š

$$
Decision = \argmax_{A \in A_{options}} \left( \alpha U(A) + \beta D(A) + \gamma V(A) \right)
$$

å…¶ä¸­ï¼š

- $U(A)$ æ˜¯åŠŸåˆ©ä¸»ä¹‰è¯„åˆ†
- $D(A)$ æ˜¯ä¹‰åŠ¡è®ºè¯„åˆ†
- $V(A)$ æ˜¯å¾·æ€§ä¼¦ç†è¯„åˆ†
- $\alpha, \beta, \gamma$ æ˜¯æƒé‡

## ğŸ”— 7. äº¤å‰å¼•ç”¨

- [å…ƒä¼¦ç†å­¦](./02_Meta_Ethics.md) - ä¼¦ç†å­¦çš„å…ƒç†è®ºåŸºç¡€
- [åº”ç”¨ä¼¦ç†å­¦](./03_Applied_Ethics.md) - è§„èŒƒä¼¦ç†å­¦çš„å…·ä½“åº”ç”¨
- [AIä¼¦ç†å­¦](./04_AI_Ethics.md) - AIç³»ç»Ÿä¸­çš„é“å¾·å†³ç­–
- [è®¤è¯†è®º](../02_Epistemology/README.md) - é“å¾·çŸ¥è¯†çš„è®¤è¯†è®ºåŸºç¡€

## ğŸ“š 8. å‚è€ƒæ–‡çŒ®

1. Bentham, J. (1789). *An Introduction to the Principles of Morals and Legislation*
2. Kant, I. (1785). *Groundwork for the Metaphysics of Morals*
3. Aristotle. (c. 350 BCE). *Nicomachean Ethics*
4. Mill, J. S. (1863). *Utilitarianism*
5. Ross, W. D. (1930). *The Right and the Good*
6. Hursthouse, R. (1999). *On Virtue Ethics*
7. Rawls, J. (1971). *A Theory of Justice*
8. Parfit, D. (1984). *Reasons and Persons*
9. Korsgaard, C. M. (1996). *The Sources of Normativity*
10. Scanlon, T. M. (1998). *What We Owe to Each Other*
