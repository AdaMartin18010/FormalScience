# 06.1.1 分布式系统基础

## 📋 概述

分布式系统基础是分布式计算理论的核心分支，研究由多个独立计算机组成的系统如何协同工作以完成共同的任务。该理论为现代分布式应用、云计算和分布式数据库提供了理论基础，是构建大规模、高可用系统的关键。

## 🎯 核心目标

1. **建立分布式系统的数学基础**
2. **定义分布式系统的基本概念**
3. **研究分布式系统的特性**
4. **提供分布式系统设计方法**
5. **分析分布式系统在工程中的应用**

## 📚 目录

1. [基本概念](#1-基本概念)
2. [形式化定义](#2-形式化定义)
3. [定理与证明](#3-定理与证明)
4. [代码实现](#4-代码实现)
5. [应用示例](#5-应用示例)
6. [相关理论](#6-相关理论)
7. [参考文献](#7-参考文献)

## 1. 基本概念

### 1.1 分布式系统定义

**定义 1.1.1 (分布式系统)**
分布式系统是由多个独立计算机（节点）组成的系统，这些节点通过网络连接，协同工作以完成共同的任务，但对用户表现为单一系统。

**定义 1.1.2 (节点)**
节点是分布式系统中的独立计算单元，具有自己的处理器、内存和存储设备。

**定义 1.1.3 (网络)**
网络是连接分布式系统中各个节点的通信基础设施，可以是局域网、广域网或互联网。

### 1.2 分布式系统特性

**定义 1.2.1 (透明性)**
透明性是分布式系统的一个重要特性，包括：

- **位置透明性**：用户不需要知道资源的具体位置
- **迁移透明性**：资源可以在节点间移动而不影响用户
- **复制透明性**：用户不需要知道资源的副本数量
- **并发透明性**：多个用户可以同时访问资源

**定义 1.2.2 (开放性)**
开放性是指分布式系统能够与异构系统互操作，支持不同的硬件、操作系统和网络协议。

**定义 1.2.3 (可扩展性)**
可扩展性是指分布式系统能够通过增加节点来提高性能和容量。

### 1.3 分布式系统挑战

**定义 1.3.1 (部分故障)**
部分故障是指分布式系统中的某些节点或网络链路发生故障，而其他部分仍然正常工作。

**定义 1.3.2 (网络分区)**
网络分区是指网络故障导致分布式系统被分割成多个无法通信的子网络。

**定义 1.3.3 (时钟同步)**
时钟同步是指分布式系统中各个节点的时钟可能存在偏差，需要协调一致。

## 2. 形式化定义

### 2.1 分布式系统模型

**定义 2.1.1 (分布式系统模型)**
分布式系统可以形式化为一个三元组 $DS = (N, E, P)$，其中：

- $N = \{n_1, n_2, \ldots, n_m\}$ 是节点集合
- $E \subseteq N \times N$ 是边集合，表示节点间的连接
- $P = \{p_1, p_2, \ldots, p_k\}$ 是进程集合

**定义 2.1.2 (消息传递模型)**
消息传递模型描述节点间的通信：
$$send(n_i, n_j, m) \rightarrow receive(n_j, n_i, m)$$

其中 $m$ 是消息内容。

**定义 2.1.3 (状态转换)**
节点的状态转换可以描述为：
$$\delta: S \times M \rightarrow S \times M^*$$

其中 $S$ 是状态集合，$M$ 是消息集合。

### 2.2 分布式算法

**定义 2.2.1 (分布式算法)**
分布式算法是在分布式系统上执行的算法，每个节点执行算法的一部分。

**定义 2.2.2 (同步模型)**
在同步模型中，所有节点按照全局时钟同步执行，消息传递有固定的延迟。

**定义 2.2.3 (异步模型)**
在异步模型中，节点执行速度不同，消息传递延迟不确定，但消息最终会被传递。

### 2.3 一致性模型

**定义 2.3.1 (强一致性)**
强一致性要求所有节点看到相同的操作顺序，任何读取操作都能看到最近一次写入的值。

**定义 2.3.2 (最终一致性)**
最终一致性允许系统暂时不一致，但最终会收敛到一致状态。

**定义 2.3.3 (因果一致性)**
因果一致性要求因果相关的操作在所有节点上以相同的顺序执行。

## 3. 定理与证明

### 3.1 CAP定理

**定理 3.1.1 (CAP定理)**
在分布式系统中，不可能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）三个属性。

**证明**：
假设存在一个分布式系统同时满足CAP三个属性。当网络分区发生时：

1. 为了保证可用性，系统必须继续响应请求
2. 为了保证一致性，所有节点必须看到相同的数据
3. 由于网络分区，节点间无法通信，无法保证一致性

这导致矛盾，因此CAP三个属性不可能同时满足。$\square$

**推论 3.1.1 (CAP定理的应用)**
根据CAP定理，分布式系统设计需要在一致性、可用性和分区容错性之间做出权衡。

### 3.2 FLP不可能性定理

**定理 3.2.1 (FLP不可能性定理)**
在异步分布式系统中，即使只有一个节点可能发生故障，也不可能设计出一个确定性算法来解决共识问题。

**证明**：

1. 假设存在一个确定性算法能够解决共识问题
2. 在异步系统中，消息传递延迟不确定
3. 当某个节点故障时，其他节点无法区分该节点是故障还是延迟
4. 这导致算法无法确定是否达成共识

因此，在异步系统中无法设计出确定性的共识算法。$\square$

### 3.3 分布式系统的基本限制

**定理 3.3.1 (分布式系统的基本限制)**
分布式系统存在以下基本限制：

1. 网络延迟不可预测
2. 时钟不同步
3. 部分故障不可避免
4. 消息可能丢失或重复

**证明**：
这些限制是分布式系统的物理和逻辑约束：

1. 网络延迟受物理距离和网络拥塞影响
2. 时钟同步受硬件精度和环境因素影响
3. 部分故障是硬件和软件的固有特性
4. 消息传递受网络协议和硬件故障影响

因此，这些限制是分布式系统的基本特性。$\square$

## 4. 代码实现

### 4.1 分布式节点实现

```rust
use std::collections::HashMap;
use std::net::{TcpListener, TcpStream};
use std::sync::{Arc, Mutex};
use std::thread;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// 节点ID
pub type NodeId = String;

/// 消息类型
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Message {
    Ping { from: NodeId, to: NodeId },
    Pong { from: NodeId, to: NodeId },
    Data { from: NodeId, to: NodeId, content: String },
    Heartbeat { from: NodeId },
}

/// 节点状态
#[derive(Debug, Clone)]
pub struct NodeState {
    pub id: NodeId,
    pub address: String,
    pub port: u16,
    pub neighbors: HashMap<NodeId, String>,
    pub data: HashMap<String, String>,
    pub timestamp: u64,
}

/// 分布式节点
pub struct DistributedNode {
    pub state: Arc<Mutex<NodeState>>,
    pub message_queue: Arc<Mutex<Vec<Message>>>,
    pub running: Arc<Mutex<bool>>,
}

impl DistributedNode {
    /// 创建新节点
    pub fn new(id: NodeId, address: String, port: u16) -> Self {
        Self {
            state: Arc::new(Mutex::new(NodeState {
                id,
                address,
                port,
                neighbors: HashMap::new(),
                data: HashMap::new(),
                timestamp: 0,
            })),
            message_queue: Arc::new(Mutex::new(Vec::new())),
            running: Arc::new(Mutex::new(true)),
        }
    }

    /// 启动节点
    pub fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        let state = Arc::clone(&self.state);
        let message_queue = Arc::clone(&self.message_queue);
        let running = Arc::clone(&self.running);

        // 启动消息处理线程
        thread::spawn(move || {
            Self::message_processor(state, message_queue, running);
        });

        // 启动网络监听
        let listener = TcpListener::bind(format!("{}:{}", 
            state.lock().unwrap().address, 
            state.lock().unwrap().port))?;

        println!("节点启动在 {}:{}", 
            state.lock().unwrap().address, 
            state.lock().unwrap().port);

        for stream in listener.incoming() {
            match stream {
                Ok(stream) => {
                    let message_queue = Arc::clone(&self.message_queue);
                    thread::spawn(move || {
                        Self::handle_connection(stream, message_queue);
                    });
                }
                Err(e) => eprintln!("连接错误: {}", e),
            }
        }

        Ok(())
    }

    /// 消息处理器
    fn message_processor(
        state: Arc<Mutex<NodeState>>,
        message_queue: Arc<Mutex<Vec<Message>>>,
        running: Arc<Mutex<bool>>,
    ) {
        while *running.lock().unwrap() {
            let message = {
                let mut queue = message_queue.lock().unwrap();
                queue.pop()
            };

            if let Some(msg) = message {
                Self::process_message(&state, msg);
            }

            thread::sleep(std::time::Duration::from_millis(10));
        }
    }

    /// 处理消息
    fn process_message(state: &Arc<Mutex<NodeState>>, message: Message) {
        match message {
            Message::Ping { from, to } => {
                let state_guard = state.lock().unwrap();
                if state_guard.id == to {
                    println!("收到Ping来自: {}", from);
                    // 发送Pong响应
                    let response = Message::Pong {
                        from: state_guard.id.clone(),
                        to: from,
                    };
                    // 这里应该发送响应
                }
            }
            Message::Pong { from, to } => {
                let state_guard = state.lock().unwrap();
                if state_guard.id == to {
                    println!("收到Pong来自: {}", from);
                }
            }
            Message::Data { from, to, content } => {
                let mut state_guard = state.lock().unwrap();
                if state_guard.id == to {
                    println!("收到数据来自: {}: {}", from, content);
                    state_guard.data.insert(from, content);
                }
            }
            Message::Heartbeat { from } => {
                let state_guard = state.lock().unwrap();
                println!("收到心跳来自: {}", from);
            }
        }
    }

    /// 处理连接
    fn handle_connection(stream: TcpStream, message_queue: Arc<Mutex<Vec<Message>>>) {
        // 简化的消息接收处理
        use std::io::{Read, Write};
        
        let mut buffer = [0; 1024];
        if let Ok(n) = stream.try_clone().unwrap().read(&mut buffer) {
            if n > 0 {
                if let Ok(message) = serde_json::from_slice::<Message>(&buffer[..n]) {
                    message_queue.lock().unwrap().push(message);
                }
            }
        }
    }

    /// 发送消息
    pub fn send_message(&self, message: Message) -> Result<(), Box<dyn std::error::Error>> {
        let state_guard = self.state.lock().unwrap();
        
        match &message {
            Message::Ping { to, .. } | Message::Pong { to, .. } | Message::Data { to, .. } => {
                if let Some(address) = state_guard.neighbors.get(to) {
                    let mut stream = TcpStream::connect(address)?;
                    let message_json = serde_json::to_string(&message)?;
                    stream.write_all(message_json.as_bytes())?;
                }
            }
            Message::Heartbeat { .. } => {
                // 广播心跳到所有邻居
                for address in state_guard.neighbors.values() {
                    if let Ok(mut stream) = TcpStream::connect(address) {
                        let message_json = serde_json::to_string(&message)?;
                        let _ = stream.write_all(message_json.as_bytes());
                    }
                }
            }
        }
        
        Ok(())
    }

    /// 添加邻居节点
    pub fn add_neighbor(&self, id: NodeId, address: String) {
        self.state.lock().unwrap().neighbors.insert(id, address);
    }

    /// 存储数据
    pub fn store_data(&self, key: String, value: String) {
        self.state.lock().unwrap().data.insert(key, value);
    }

    /// 获取数据
    pub fn get_data(&self, key: &str) -> Option<String> {
        self.state.lock().unwrap().data.get(key).cloned()
    }
}

/// 分布式系统管理器
pub struct DistributedSystem {
    pub nodes: HashMap<NodeId, DistributedNode>,
}

impl DistributedSystem {
    /// 创建新的分布式系统
    pub fn new() -> Self {
        Self {
            nodes: HashMap::new(),
        }
    }

    /// 添加节点
    pub fn add_node(&mut self, id: NodeId, address: String, port: u16) -> Result<(), Box<dyn std::error::Error>> {
        let node = DistributedNode::new(id.clone(), address, port);
        self.nodes.insert(id, node);
        Ok(())
    }

    /// 启动所有节点
    pub fn start_all(&self) -> Result<(), Box<dyn std::error::Error>> {
        for (id, node) in &self.nodes {
            println!("启动节点: {}", id);
            node.start()?;
        }
        Ok(())
    }

    /// 连接节点
    pub fn connect_nodes(&self, from: &NodeId, to: &NodeId) -> Result<(), Box<dyn std::error::Error>> {
        if let (Some(from_node), Some(to_node)) = (self.nodes.get(from), self.nodes.get(to)) {
            let to_address = format!("{}:{}", 
                to_node.state.lock().unwrap().address,
                to_node.state.lock().unwrap().port);
            from_node.add_neighbor(to.clone(), to_address);
        }
        Ok(())
    }
}

/// 一致性协议实现
pub struct ConsistencyProtocol;

impl ConsistencyProtocol {
    /// 两阶段提交协议
    pub fn two_phase_commit(
        coordinator: &DistributedNode,
        participants: &[&DistributedNode],
        transaction: &str,
    ) -> bool {
        // 阶段1：准备阶段
        let mut all_prepared = true;
        for participant in participants {
            // 发送准备消息
            let prepare_msg = Message::Data {
                from: coordinator.state.lock().unwrap().id.clone(),
                to: participant.state.lock().unwrap().id.clone(),
                content: format!("PREPARE:{}", transaction),
            };
            if coordinator.send_message(prepare_msg).is_err() {
                all_prepared = false;
                break;
            }
        }

        if !all_prepared {
            // 发送中止消息
            for participant in participants {
                let abort_msg = Message::Data {
                    from: coordinator.state.lock().unwrap().id.clone(),
                    to: participant.state.lock().unwrap().id.clone(),
                    content: format!("ABORT:{}", transaction),
                };
                let _ = coordinator.send_message(abort_msg);
            }
            return false;
        }

        // 阶段2：提交阶段
        for participant in participants {
            let commit_msg = Message::Data {
                from: coordinator.state.lock().unwrap().id.clone(),
                to: participant.state.lock().unwrap().id.clone(),
                content: format!("COMMIT:{}", transaction),
            };
            if coordinator.send_message(commit_msg).is_err() {
                return false;
            }
        }

        true
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_distributed_node() {
        let node = DistributedNode::new(
            "node1".to_string(),
            "127.0.0.1".to_string(),
            8080,
        );
        
        node.store_data("key1".to_string(), "value1".to_string());
        assert_eq!(node.get_data("key1"), Some("value1".to_string()));
    }

    #[test]
    fn test_distributed_system() {
        let mut system = DistributedSystem::new();
        
        system.add_node("node1".to_string(), "127.0.0.1".to_string(), 8081).unwrap();
        system.add_node("node2".to_string(), "127.0.0.1".to_string(), 8082).unwrap();
        
        assert_eq!(system.nodes.len(), 2);
    }
}
```

### 4.2 Haskell分布式系统实现

```haskell
-- 节点ID类型
type NodeId = String

-- 消息类型
data Message = Ping NodeId NodeId
             | Pong NodeId NodeId
             | Data NodeId NodeId String
             | Heartbeat NodeId
             deriving (Show, Read)

-- 节点状态
data NodeState = NodeState
    { nodeId :: NodeId
    , address :: String
    , port :: Int
    , neighbors :: Map NodeId String
    , dataStore :: Map String String
    , timestamp :: Integer
    } deriving (Show)

-- 分布式节点
data DistributedNode = DistributedNode
    { state :: MVar NodeState
    , messageQueue :: MVar [Message]
    , running :: MVar Bool
    }

-- 创建新节点
createNode :: NodeId -> String -> Int -> IO DistributedNode
createNode id addr port = do
    initialState <- newMVar $ NodeState id addr port empty empty 0
    queue <- newMVar []
    running <- newMVar True
    return $ DistributedNode initialState queue running

-- 启动节点
startNode :: DistributedNode -> IO ()
startNode node = do
    putStrLn $ "启动节点: " ++ nodeId (readMVar (state node))
    
    -- 启动消息处理线程
    forkIO $ messageProcessor node
    
    -- 启动网络监听
    let addr = address (readMVar (state node))
    let port = port (readMVar (state node))
    listenSocket <- listenOn $ PortNumber $ fromIntegral port
    
    putStrLn $ "监听在 " ++ addr ++ ":" ++ show port
    
    -- 接受连接
    forever $ do
        (handle, host, port) <- accept listenSocket
        forkIO $ handleConnection handle node

-- 消息处理器
messageProcessor :: DistributedNode -> IO ()
messageProcessor node = do
    isRunning <- readMVar (running node)
    if isRunning
        then do
            message <- takeMVar (messageQueue node)
            processMessage node message
            messageProcessor node
        else return ()

-- 处理消息
processMessage :: DistributedNode -> Message -> IO ()
processMessage node message = do
    currentState <- readMVar (state node)
    case message of
        Ping from to -> 
            when (nodeId currentState == to) $ do
                putStrLn $ "收到Ping来自: " ++ from
                -- 发送Pong响应
                let response = Pong (nodeId currentState) from
                sendMessage node response
        
        Pong from to ->
            when (nodeId currentState == to) $ do
                putStrLn $ "收到Pong来自: " ++ from
        
        Data from to content ->
            when (nodeId currentState == to) $ do
                putStrLn $ "收到数据来自: " ++ from ++ ": " ++ content
                modifyMVar_ (state node) $ \s -> 
                    return s { dataStore = insert from content (dataStore s) }
        
        Heartbeat from ->
            putStrLn $ "收到心跳来自: " ++ from

-- 处理连接
handleConnection :: Handle -> DistributedNode -> IO ()
handleConnection handle node = do
    content <- hGetContents handle
    case reads content of
        [(message, _)] -> do
            modifyMVar_ (messageQueue node) $ \queue -> 
                return (message : queue)
        _ -> putStrLn "无效消息格式"
    hClose handle

-- 发送消息
sendMessage :: DistributedNode -> Message -> IO ()
sendMessage node message = do
    currentState <- readMVar (state node)
    case message of
        Ping _ to | Just addr <- lookup to (neighbors currentState) -> do
            handle <- connectTo "localhost" (PortNumber $ fromIntegral (read addr))
            hPutStrLn handle (show message)
            hClose handle
        
        Pong _ to | Just addr <- lookup to (neighbors currentState) -> do
            handle <- connectTo "localhost" (PortNumber $ fromIntegral (read addr))
            hPutStrLn handle (show message)
            hClose handle
        
        Data _ to content | Just addr <- lookup to (neighbors currentState) -> do
            handle <- connectTo "localhost" (PortNumber $ fromIntegral (read addr))
            hPutStrLn handle (show message)
            hClose handle
        
        Heartbeat _ -> do
            -- 广播心跳到所有邻居
            mapM_ (\addr -> do
                handle <- connectTo "localhost" (PortNumber $ fromIntegral (read addr))
                hPutStrLn handle (show message)
                hClose handle) (elems (neighbors currentState))
        
        _ -> return ()

-- 添加邻居节点
addNeighbor :: DistributedNode -> NodeId -> String -> IO ()
addNeighbor node id addr = do
    modifyMVar_ (state node) $ \s -> 
        return s { neighbors = insert id addr (neighbors s) }

-- 存储数据
storeData :: DistributedNode -> String -> String -> IO ()
storeData node key value = do
    modifyMVar_ (state node) $ \s -> 
        return s { dataStore = insert key value (dataStore s) }

-- 获取数据
getData :: DistributedNode -> String -> IO (Maybe String)
getData node key = do
    currentState <- readMVar (state node)
    return $ lookup key (dataStore currentState)

-- 分布式系统管理器
data DistributedSystem = DistributedSystem
    { nodes :: MVar (Map NodeId DistributedNode)
    }

-- 创建分布式系统
createDistributedSystem :: IO DistributedSystem
createDistributedSystem = do
    nodesMap <- newMVar empty
    return $ DistributedSystem nodesMap

-- 添加节点
addNode :: DistributedSystem -> NodeId -> String -> Int -> IO ()
addNode system id addr port = do
    node <- createNode id addr port
    modifyMVar_ (nodes system) $ \nodesMap -> 
        return $ insert id node nodesMap

-- 连接节点
connectNodes :: DistributedSystem -> NodeId -> NodeId -> IO ()
connectNodes system from to = do
    nodesMap <- readMVar (nodes system)
    case (lookup from nodesMap, lookup to nodesMap) of
        (Just fromNode, Just toNode) -> do
            currentState <- readMVar (state toNode)
            let addr = address currentState ++ ":" ++ show (port currentState)
            addNeighbor fromNode to addr
        _ -> putStrLn "节点不存在"

-- 一致性协议
-- 两阶段提交
twoPhaseCommit :: DistributedNode -> [DistributedNode] -> String -> IO Bool
twoPhaseCommit coordinator participants transaction = do
    -- 阶段1：准备阶段
    prepareResults <- mapM (\participant -> do
        let prepareMsg = Data (nodeId (readMVar (state coordinator))) 
                              (nodeId (readMVar (state participant))) 
                              ("PREPARE:" ++ transaction)
        sendMessage coordinator prepareMsg
        return True) participants
    
    let allPrepared = and prepareResults
    
    if not allPrepared
        then do
            -- 发送中止消息
            mapM_ (\participant -> do
                let abortMsg = Data (nodeId (readMVar (state coordinator))) 
                                   (nodeId (readMVar (state participant))) 
                                   ("ABORT:" ++ transaction)
                sendMessage coordinator abortMsg) participants
            return False
        else do
            -- 阶段2：提交阶段
            commitResults <- mapM (\participant -> do
                let commitMsg = Data (nodeId (readMVar (state coordinator))) 
                                    (nodeId (readMVar (state participant))) 
                                    ("COMMIT:" ++ transaction)
                sendMessage coordinator commitMsg
                return True) participants
            return $ and commitResults

-- 测试函数
testDistributedSystem :: IO ()
testDistributedSystem = do
    putStrLn "测试分布式系统:"
    
    -- 创建分布式系统
    system <- createDistributedSystem
    
    -- 添加节点
    addNode system "node1" "127.0.0.1" 8081
    addNode system "node2" "127.0.0.1" 8082
    
    -- 连接节点
    connectNodes system "node1" "node2"
    
    putStrLn "分布式系统创建完成"
```

## 5. 应用示例

### 5.1 分布式键值存储

```rust
/// 分布式键值存储
pub struct DistributedKeyValueStore {
    pub nodes: HashMap<NodeId, DistributedNode>,
    pub replication_factor: usize,
}

impl DistributedKeyValueStore {
    /// 创建新的分布式键值存储
    pub fn new(replication_factor: usize) -> Self {
        Self {
            nodes: HashMap::new(),
            replication_factor,
        }
    }

    /// 添加节点
    pub fn add_node(&mut self, id: NodeId, address: String, port: u16) -> Result<(), Box<dyn std::error::Error>> {
        let node = DistributedNode::new(id.clone(), address, port);
        self.nodes.insert(id, node);
        Ok(())
    }

    /// 存储键值对
    pub fn put(&self, key: String, value: String) -> Result<(), Box<dyn std::error::Error>> {
        let node_ids: Vec<NodeId> = self.nodes.keys().cloned().collect();
        let hash = self.hash_key(&key);
        let primary_node_id = &node_ids[hash % node_ids.len()];
        
        if let Some(primary_node) = self.nodes.get(primary_node_id) {
            // 存储到主节点
            primary_node.store_data(key.clone(), value.clone());
            
            // 复制到其他节点
            let replica_nodes: Vec<&NodeId> = node_ids
                .iter()
                .filter(|&id| id != primary_node_id)
                .take(self.replication_factor - 1)
                .collect();
            
            for replica_id in replica_nodes {
                if let Some(replica_node) = self.nodes.get(replica_id) {
                    replica_node.store_data(key.clone(), value.clone());
                }
            }
        }
        
        Ok(())
    }

    /// 获取值
    pub fn get(&self, key: &str) -> Option<String> {
        let node_ids: Vec<NodeId> = self.nodes.keys().cloned().collect();
        let hash = self.hash_key(key);
        let primary_node_id = &node_ids[hash % node_ids.len()];
        
        if let Some(primary_node) = self.nodes.get(primary_node_id) {
            if let Some(value) = primary_node.get_data(key) {
                return Some(value);
            }
        }
        
        // 如果主节点没有数据，尝试从副本节点获取
        for (node_id, node) in &self.nodes {
            if node_id != primary_node_id {
                if let Some(value) = node.get_data(key) {
                    return Some(value);
                }
            }
        }
        
        None
    }

    /// 简单的哈希函数
    fn hash_key(&self, key: &str) -> usize {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        hasher.finish() as usize
    }
}

#[test]
fn test_distributed_kv_store() {
    let mut store = DistributedKeyValueStore::new(3);
    
    store.add_node("node1".to_string(), "127.0.0.1".to_string(), 8081).unwrap();
    store.add_node("node2".to_string(), "127.0.0.1".to_string(), 8082).unwrap();
    store.add_node("node3".to_string(), "127.0.0.1".to_string(), 8083).unwrap();
    
    store.put("key1".to_string(), "value1".to_string()).unwrap();
    assert_eq!(store.get("key1"), Some("value1".to_string()));
}
```

### 5.2 分布式锁

```rust
/// 分布式锁
pub struct DistributedLock {
    pub nodes: Vec<DistributedNode>,
    pub lock_key: String,
    pub timeout: std::time::Duration,
}

impl DistributedLock {
    /// 创建新的分布式锁
    pub fn new(nodes: Vec<DistributedNode>, lock_key: String, timeout: std::time::Duration) -> Self {
        Self {
            nodes,
            lock_key,
            timeout,
        }
    }

    /// 尝试获取锁
    pub fn try_lock(&self, node_id: &NodeId) -> bool {
        let lock_value = format!("{}:{}", node_id, chrono::Utc::now().timestamp());
        
        // 尝试在所有节点上创建锁
        let mut acquired_count = 0;
        let required_count = (self.nodes.len() / 2) + 1; // 多数派
        
        for node in &self.nodes {
            if let Ok(()) = node.store_data(self.lock_key.clone(), lock_value.clone()) {
                acquired_count += 1;
            }
        }
        
        acquired_count >= required_count
    }

    /// 释放锁
    pub fn unlock(&self, node_id: &NodeId) -> bool {
        let mut released_count = 0;
        
        for node in &self.nodes {
            if let Some(lock_value) = node.get_data(&self.lock_key) {
                if lock_value.starts_with(&format!("{}:", node_id)) {
                    // 删除锁
                    node.store_data(self.lock_key.clone(), "".to_string());
                    released_count += 1;
                }
            }
        }
        
        released_count > 0
    }
}

#[test]
fn test_distributed_lock() {
    let nodes = vec![
        DistributedNode::new("node1".to_string(), "127.0.0.1".to_string(), 8081),
        DistributedNode::new("node2".to_string(), "127.0.0.1".to_string(), 8082),
        DistributedNode::new("node3".to_string(), "127.0.0.1".to_string(), 8083),
    ];
    
    let lock = DistributedLock::new(
        nodes,
        "test_lock".to_string(),
        std::time::Duration::from_secs(10),
    );
    
    assert!(lock.try_lock(&"node1".to_string()));
    assert!(lock.unlock(&"node1".to_string()));
}
```

## 6. 相关理论

### 6.1 与网络理论的关系

**定理 6.1.1 (分布式系统与网络理论)**
分布式系统的性能受网络拓扑和通信协议的影响。

**证明**：

1. 网络拓扑决定了节点间的连接关系
2. 通信协议决定了消息传递的可靠性和效率
3. 网络延迟和带宽影响系统性能
4. 网络分区影响系统可用性

因此，网络理论是分布式系统理论的基础。$\square$

### 6.2 与并发理论的关系

**定理 6.2.1 (分布式系统与并发理论)**
分布式系统是并发系统的特例，具有网络延迟和部分故障的特性。

**证明**：

1. 分布式系统中的节点并发执行
2. 节点间通过消息传递通信
3. 需要考虑网络延迟和故障
4. 并发控制算法需要适应分布式环境

因此，并发理论为分布式系统提供了重要的理论基础。$\square$

### 6.3 与数据库理论的关系

**定理 6.3.1 (分布式系统与数据库理论)**
分布式数据库是分布式系统的重要应用，需要解决一致性和可用性问题。

**证明**：

1. 分布式数据库需要在多个节点上存储数据
2. 需要保证数据的一致性和可用性
3. 需要处理网络分区和节点故障
4. 需要实现分布式事务和共识算法

因此，数据库理论为分布式系统提供了重要的应用场景。$\square$

## 7. 参考文献

1. Tanenbaum, A. S., & Van Steen, M. (2017). *Distributed systems: Principles and paradigms* (3rd ed.). Pearson.

2. Coulouris, G., Dollimore, J., Kindberg, T., & Blair, G. (2011). *Distributed systems: Concepts and design* (5th ed.). Addison-Wesley.

3. Lamport, L. (1978). Time, clocks, and the ordering of events in a distributed system. *Communications of the ACM*, 21(7), 558-565.

4. Fischer, M. J., Lynch, N. A., & Paterson, M. S. (1985). Impossibility of distributed consensus with one faulty process. *Journal of the ACM*, 32(2), 374-382.

5. Brewer, E. A. (2012). CAP twelve years later: How the "rules" have changed. *Computer*, 45(2), 23-29.

6. Lamport, L. (1998). The part-time parliament. *ACM Transactions on Computer Systems*, 16(2), 133-169.

7. Chandra, T. D., Griesemer, R., & Redstone, J. (2007). Paxos made live: An engineering perspective. *Proceedings of the Twenty-Sixth Annual ACM Symposium on Principles of Distributed Computing*, 398-407.

8. Ongaro, D., & Ousterhout, J. (2014). In search of an understandable consensus algorithm. *Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference*, 305-319.

---

**相关文档**：

- [06.1.2 共识理论](06.1.2_共识理论.md)
- [06.1.3 一致性理论](06.1.3_一致性理论.md)
- [06.1.4 分布式算法](06.1.4_分布式算法.md)
- [02.4.1 函数概念](../02_Mathematical_Foundation/02.4.1_函数概念.md)
- [02.5.1 关系概念](../02_Mathematical_Foundation/02.5.1_关系概念.md)
