# 13.1.1 æœºå™¨å­¦ä¹ ç†è®º

## ç›®å½•

- [13.1.1 æœºå™¨å­¦ä¹ ç†è®º](#1311-æœºå™¨å­¦ä¹ ç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [1. åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
    - [1.1 æœºå™¨å­¦ä¹ å®šä¹‰](#11-æœºå™¨å­¦ä¹ å®šä¹‰)
    - [1.2 å­¦ä¹ ç±»å‹åˆ†ç±»](#12-å­¦ä¹ ç±»å‹åˆ†ç±»)
  - [2. å½¢å¼åŒ–å®šä¹‰](#2-å½¢å¼åŒ–å®šä¹‰)
    - [2.1 å­¦ä¹ é—®é¢˜](#21-å­¦ä¹ é—®é¢˜)
    - [2.2 å‡è®¾ç©ºé—´](#22-å‡è®¾ç©ºé—´)
    - [2.3 ç»éªŒé£é™©](#23-ç»éªŒé£é™©)
  - [3. å®šç†ä¸è¯æ˜](#3-å®šç†ä¸è¯æ˜)
    - [3.1 æ²¡æœ‰å…è´¹åˆé¤å®šç†](#31-æ²¡æœ‰å…è´¹åˆé¤å®šç†)
    - [3.2 æ³›åŒ–ç•Œå®šç†](#32-æ³›åŒ–ç•Œå®šç†)
  - [4. æ ¸å¿ƒç®—æ³•ç†è®º](#4-æ ¸å¿ƒç®—æ³•ç†è®º)
    - [4.1 æ”¯æŒå‘é‡æœºç†è®º](#41-æ”¯æŒå‘é‡æœºç†è®º)
    - [4.2 é›†æˆå­¦ä¹ ç†è®º](#42-é›†æˆå­¦ä¹ ç†è®º)
    - [4.3 èšç±»ç®—æ³•ç†è®º](#43-èšç±»ç®—æ³•ç†è®º)
    - [4.4 æ¨¡å‹è¯„ä¼°ç†è®º](#44-æ¨¡å‹è¯„ä¼°ç†è®º)
    - [4.5 ä¼˜åŒ–ç†è®º](#45-ä¼˜åŒ–ç†è®º)
    - [4.6 è”é‚¦å­¦ä¹ ç†è®º](#46-è”é‚¦å­¦ä¹ ç†è®º)
    - [4.7 å› æœæ¨ç†ç†è®º](#47-å› æœæ¨ç†ç†è®º)
  - [5. Rustä»£ç å®ç°](#5-rustä»£ç å®ç°)
    - [5.1 çº¿æ€§å›å½’å®ç°](#51-çº¿æ€§å›å½’å®ç°)
    - [5.4 æ”¯æŒå‘é‡æœºå®ç°](#54-æ”¯æŒå‘é‡æœºå®ç°)
    - [5.5 æ¨¡å‹è¯„ä¼°å®ç°](#55-æ¨¡å‹è¯„ä¼°å®ç°)
  - [6. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨](#6-ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨)
  - [7. å‚è€ƒæ–‡çŒ®](#7-å‚è€ƒæ–‡çŒ®)
  - [æ‰¹åˆ¤æ€§åˆ†æ](#æ‰¹åˆ¤æ€§åˆ†æ)
    - [ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†](#ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†)
    - [ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ](#ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ)
    - [ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ](#ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ)
    - [åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›](#åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›)
    - [å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»](#å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»)

## ğŸ“‹ æ¦‚è¿°

æœºå™¨å­¦ä¹ ç†è®ºç ”ç©¶å¦‚ä½•è®©è®¡ç®—æœºç³»ç»Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ã€‚è¯¥ç†è®ºæ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿæ„å»ºæä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 æœºå™¨å­¦ä¹ å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆæœºå™¨å­¦ä¹ ï¼‰
æœºå™¨å­¦ä¹ æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚

### 1.2 å­¦ä¹ ç±»å‹åˆ†ç±»

| å­¦ä¹ ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹ç®—æ³•         |
|--------------|------------------|------------------------------|------------------|
| ç›‘ç£å­¦ä¹      | Supervised       | ä»æ ‡è®°æ•°æ®ä¸­å­¦ä¹              | çº¿æ€§å›å½’, SVM    |
| æ— ç›‘ç£å­¦ä¹    | Unsupervised     | ä»æ— æ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼       | K-means, PCA     |
| å¼ºåŒ–å­¦ä¹      | Reinforcement    | é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ ç­–ç•¥       | Q-learning, DQN  |
| åŠç›‘ç£å­¦ä¹    | Semi-supervised  | ç»“åˆæ ‡è®°å’Œæœªæ ‡è®°æ•°æ®         | è‡ªè®­ç»ƒ, å›¾å­¦ä¹    |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 å­¦ä¹ é—®é¢˜

**å®šä¹‰ 2.1**ï¼ˆå­¦ä¹ é—®é¢˜ï¼‰
å­¦ä¹ é—®é¢˜æ˜¯ç»™å®šè¾“å…¥ç©ºé—´ $X$ å’Œè¾“å‡ºç©ºé—´ $Y$ï¼Œå¯»æ‰¾æ˜ å°„ $f: X \rightarrow Y$ çš„è¿‡ç¨‹ã€‚

### 2.2 å‡è®¾ç©ºé—´

**å®šä¹‰ 2.2**ï¼ˆå‡è®¾ç©ºé—´ï¼‰
å‡è®¾ç©ºé—´æ˜¯æ‰€æœ‰å¯èƒ½å‡è®¾çš„é›†åˆï¼Œè®°ä½œ $H = \{h: X \rightarrow Y\}$ã€‚

### 2.3 ç»éªŒé£é™©

**å®šä¹‰ 2.3**ï¼ˆç»éªŒé£é™©ï¼‰
ç»éªŒé£é™©æ˜¯å‡è®¾ $h$ åœ¨è®­ç»ƒé›† $S$ ä¸Šçš„å¹³å‡æŸå¤±ï¼š
$R_{emp}(h) = \frac{1}{n}\sum_{i=1}^{n} L(h(x_i), y_i)$

## 3. å®šç†ä¸è¯æ˜

### 3.1 æ²¡æœ‰å…è´¹åˆé¤å®šç†

**å®šç† 3.1**ï¼ˆæ²¡æœ‰å…è´¹åˆé¤å®šç†ï¼‰
åœ¨æ‰€æœ‰å¯èƒ½çš„é—®é¢˜ä¸Šï¼Œä»»ä½•å­¦ä¹ ç®—æ³•çš„å¹³å‡æ€§èƒ½éƒ½æ˜¯ç›¸åŒçš„ã€‚

**è¯æ˜**ï¼š
å¯¹äºä»»æ„ä¸¤ä¸ªç®—æ³• $A$ å’Œ $B$ï¼Œåœ¨æ‰€æœ‰å¯èƒ½çš„ç›®æ ‡å‡½æ•°ä¸Šï¼Œå®ƒä»¬çš„æœŸæœ›æ€§èƒ½ç›¸ç­‰ã€‚â–¡

### 3.2 æ³›åŒ–ç•Œå®šç†

**å®šç† 3.2**ï¼ˆæ³›åŒ–ç•Œï¼‰
å¯¹äºå‡è®¾ç©ºé—´ $H$ å’Œè®­ç»ƒé›† $S$ï¼Œä»¥æ¦‚ç‡ $1-\delta$ æœ‰ï¼š
$R(h) \leq R_{emp}(h) + \sqrt{\frac{\log|H| + \log(1/\delta)}{2n}}$

**è¯æ˜**ï¼š
ä½¿ç”¨Hoeffdingä¸ç­‰å¼å’Œè”åˆç•Œï¼Œè¯æ˜çœŸå®é£é™©ä¸ç»éªŒé£é™©çš„å·®è·ã€‚â–¡

## 4. æ ¸å¿ƒç®—æ³•ç†è®º

### 4.1 æ”¯æŒå‘é‡æœºç†è®º

**å®šä¹‰ 4.1**ï¼ˆæ”¯æŒå‘é‡æœºï¼‰
æ”¯æŒå‘é‡æœºæ˜¯ä¸€ç§äºŒåˆ†ç±»æ¨¡å‹ï¼Œå…¶åŸºæœ¬æ¨¡å‹æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„é—´éš”æœ€å¤§çš„çº¿æ€§åˆ†ç±»å™¨ã€‚

**å®šç† 4.1**ï¼ˆæœ€å¤§é—´éš”å®šç†ï¼‰
å¯¹äºçº¿æ€§å¯åˆ†çš„æ•°æ®é›†ï¼Œå­˜åœ¨å”¯ä¸€çš„è¶…å¹³é¢ä½¿å¾—ä¸¤ç±»æ ·æœ¬çš„é—´éš”æœ€å¤§ã€‚

**è¯æ˜**ï¼š
è®¾è¶…å¹³é¢ä¸º $w^T x + b = 0$ï¼Œåˆ™é—´éš”ä¸º $\frac{2}{\|w\|}$ã€‚æœ€å¤§åŒ–é—´éš”ç­‰ä»·äºæœ€å°åŒ– $\frac{1}{2}\|w\|^2$ã€‚â–¡

### 4.2 é›†æˆå­¦ä¹ ç†è®º

**å®šä¹‰ 4.2**ï¼ˆé›†æˆå­¦ä¹ ï¼‰
é›†æˆå­¦ä¹ é€šè¿‡ç»„åˆå¤šä¸ªåŸºå­¦ä¹ å™¨çš„é¢„æµ‹ç»“æœæ¥æé«˜æ•´ä½“æ€§èƒ½ã€‚

**å®šç† 4.2**ï¼ˆé›†æˆå­¦ä¹ è¯¯å·®ç•Œï¼‰
å¯¹äº $T$ ä¸ªåŸºå­¦ä¹ å™¨ï¼Œé›†æˆåçš„è¯¯å·®æ»¡è¶³ï¼š
$E_{ensemble} \leq \frac{1}{T}\sum_{t=1}^T E_t + \frac{T-1}{T}\rho$

å…¶ä¸­ $E_t$ æ˜¯ç¬¬ $t$ ä¸ªåŸºå­¦ä¹ å™¨çš„è¯¯å·®ï¼Œ$\rho$ æ˜¯åŸºå­¦ä¹ å™¨é—´çš„ç›¸å…³æ€§ã€‚

### 4.3 èšç±»ç®—æ³•ç†è®º

**å®šä¹‰ 4.3**ï¼ˆK-meansèšç±»ï¼‰
K-meansç®—æ³•é€šè¿‡æœ€å°åŒ–ç°‡å†…å¹³æ–¹è¯¯å·®æ¥å°†æ•°æ®ç‚¹åˆ†ç»„ã€‚

**ç®—æ³• 4.1**ï¼ˆK-meansç®—æ³•ï¼‰

1. éšæœºåˆå§‹åŒ– $K$ ä¸ªèšç±»ä¸­å¿ƒ
2. å°†æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…ç»™æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
3. é‡æ–°è®¡ç®—èšç±»ä¸­å¿ƒ
4. é‡å¤æ­¥éª¤2-3ç›´åˆ°æ”¶æ•›

### 4.4 æ¨¡å‹è¯„ä¼°ç†è®º

**å®šä¹‰ 4.4**ï¼ˆäº¤å‰éªŒè¯ï¼‰
äº¤å‰éªŒè¯æ˜¯ä¸€ç§æ¨¡å‹è¯„ä¼°æŠ€æœ¯ï¼Œé€šè¿‡å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**å®šç† 4.3**ï¼ˆäº¤å‰éªŒè¯è¯¯å·®ç•Œï¼‰
å¯¹äº $k$ æŠ˜äº¤å‰éªŒè¯ï¼ŒçœŸå®è¯¯å·®ä¸äº¤å‰éªŒè¯è¯¯å·®çš„å…³ç³»ä¸ºï¼š
$E_{true} \leq E_{cv} + \sqrt{\frac{\log(k)}{2n}}$

### 4.5 ä¼˜åŒ–ç†è®º

**å®šä¹‰ 4.5**ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¸€é˜¶ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡æ²¿ç€ç›®æ ‡å‡½æ•°æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚

**ç®—æ³• 4.2**ï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰

1. åˆå§‹åŒ–å‚æ•° $\theta$
2. å¯¹äºæ¯ä¸ªæ‰¹æ¬¡ï¼š
   - è®¡ç®—æ¢¯åº¦ $\nabla_\theta L(\theta)$
   - æ›´æ–°å‚æ•° $\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta)$
3. é‡å¤ç›´åˆ°æ”¶æ•›

**å®šç† 4.4**ï¼ˆæ”¶æ•›æ€§å®šç†ï¼‰
å¯¹äºå‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™ä»¥ $O(1/t)$ çš„é€Ÿç‡æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚

### 4.6 è”é‚¦å­¦ä¹ ç†è®º

**å®šä¹‰ 4.6**ï¼ˆè”é‚¦å­¦ä¹ ï¼‰
è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œå…è®¸å¤šä¸ªå‚ä¸è€…åœ¨ä¿æŠ¤æ•°æ®éšç§çš„å‰æä¸‹åä½œè®­ç»ƒæ¨¡å‹ã€‚

**å®šç† 4.5**ï¼ˆè”é‚¦å­¦ä¹ æ”¶æ•›æ€§ï¼‰
åœ¨è”é‚¦å¹³å‡ç®—æ³•ä¸‹ï¼Œå¯¹äºå¼ºå‡¸å‡½æ•°ï¼Œç®—æ³•ä»¥ $O(1/T)$ çš„é€Ÿç‡æ”¶æ•›ï¼Œå…¶ä¸­ $T$ æ˜¯é€šä¿¡è½®æ•°ã€‚

**ç®—æ³• 4.3**ï¼ˆè”é‚¦å¹³å‡ç®—æ³•ï¼‰

1. åˆå§‹åŒ–å…¨å±€æ¨¡å‹å‚æ•° $w_0$
2. å¯¹äºæ¯è½® $t$ï¼š
   - æ¯ä¸ªå®¢æˆ·ç«¯ $k$ ä½¿ç”¨æœ¬åœ°æ•°æ®è®­ç»ƒæ¨¡å‹
   - è®¡ç®—æœ¬åœ°å‚æ•°æ›´æ–° $\Delta w_k^t$
   - æœåŠ¡å™¨èšåˆå‚æ•°ï¼š$w_{t+1} = w_t + \frac{1}{K}\sum_{k=1}^K \Delta w_k^t$
3. é‡å¤ç›´åˆ°æ”¶æ•›

### 4.7 å› æœæ¨ç†ç†è®º

**å®šä¹‰ 4.7**ï¼ˆå› æœå›¾ï¼‰
å› æœå›¾æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ $G = (V, E)$ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºå˜é‡ï¼Œè¾¹è¡¨ç¤ºå› æœå…³ç³»ã€‚

**å®šç† 4.6**ï¼ˆåé—¨å‡†åˆ™ï¼‰
ç»™å®šå› æœå›¾ $G$ å’Œå˜é‡é›† $X, Y, Z$ï¼Œå¦‚æœ $Z$ æ»¡è¶³åé—¨å‡†åˆ™ï¼Œåˆ™ï¼š
$P(Y|do(X)) = \sum_z P(Y|X, Z=z)P(Z=z)$

**ç®—æ³• 4.4**ï¼ˆå› æœå‘ç°ç®—æ³•ï¼‰

1. æ„å»ºå®Œå…¨æ— å‘å›¾
2. å¯¹äºæ¯å¯¹å˜é‡ $(X, Y)$ï¼š
   - æµ‹è¯•æ¡ä»¶ç‹¬ç«‹æ€§
   - å¦‚æœç‹¬ç«‹ï¼Œåˆ é™¤è¾¹ $X-Y$
3. ç¡®å®šè¾¹çš„æ–¹å‘
4. è¾“å‡ºå› æœå›¾

## 5. Rustä»£ç å®ç°

### 5.1 çº¿æ€§å›å½’å®ç°

```rust
use std::collections::HashMap;
use std::f64;

#[derive(Debug, Clone)]
pub struct LinearRegression {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub learning_rate: f64,
    pub max_iterations: usize,
}

impl LinearRegression {
    pub fn new(input_dim: usize, learning_rate: f64) -> Self {
        Self {
            weights: vec![0.0; input_dim],
            bias: 0.0,
            learning_rate,
            max_iterations: 1000,
        }
    }
    
    pub fn fit(&mut self, X: &[Vec<f64>], y: &[f64]) {
        let n_samples = X.len();
        let n_features = X[0].len();
        
        for _ in 0..self.max_iterations {
            let mut gradients_w = vec![0.0; n_features];
            let mut gradient_b = 0.0;
            
            // è®¡ç®—æ¢¯åº¦
            for i in 0..n_samples {
                let prediction = self.predict(&X[i]);
                let error = prediction - y[i];
                
                for j in 0..n_features {
                    gradients_w[j] += error * X[i][j];
                }
                gradient_b += error;
            }
            
            // æ›´æ–°å‚æ•°
            for j in 0..n_features {
                self.weights[j] -= self.learning_rate * gradients_w[j] / n_samples as f64;
            }
            self.bias -= self.learning_rate * gradient_b / n_samples as f64;
        }
    }
    
    pub fn predict(&self, x: &[f64]) -> f64 {
        let mut result = self.bias;
        for (i, &weight) in self.weights.iter().enumerate() {
            result += weight * x[i];
        }
        result
    }
    
    pub fn score(&self, X: &[Vec<f64>], y: &[f64]) -> f64 {
        let mut total_error = 0.0;
        for (i, x) in X.iter().enumerate() {
            let prediction = self.predict(x);
            total_error += (prediction - y[i]).powi(2);
        }
        1.0 - total_error / y.len() as f64
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_linear_regression() {
        let mut model = LinearRegression::new(2, 0.01);
        
        // ç®€å•çš„çº¿æ€§å…³ç³»: y = 2*x1 + 3*x2 + 1
        let X = vec![
            vec![1.0, 2.0],
            vec![2.0, 3.0],
            vec![3.0, 4.0],
            vec![4.0, 5.0],
        ];
        let y = vec![9.0, 13.0, 17.0, 21.0];
        
        model.fit(&X, &y);
        
        // æµ‹è¯•é¢„æµ‹
        let test_x = vec![5.0, 6.0];
        let prediction = model.predict(&test_x);
        let expected = 2.0 * 5.0 + 3.0 * 6.0 + 1.0;
        
        assert!((prediction - expected).abs() < 1.0);
    }
}

#[derive(Debug, Clone)]
pub struct Dataset {
    pub features: Vec<Vec<f64>>,
    pub targets: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct TrainingResult {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub loss_history: Vec<f64>,
    pub iterations: usize,
}

impl LinearRegression {
    pub fn new(feature_count: usize, learning_rate: f64, max_iterations: usize) -> Self {
        LinearRegression {
            weights: vec![0.0; feature_count],
            bias: 0.0,
            learning_rate,
            max_iterations,
        }
    }

    pub fn fit(&mut self, dataset: &Dataset) -> TrainingResult {
        let mut loss_history = Vec::new();
        
        for iteration in 0..self.max_iterations {
            let (gradient_weights, gradient_bias) = self.compute_gradients(dataset);
            
            // æ›´æ–°æƒé‡
            for i in 0..self.weights.len() {
                self.weights[i] -= self.learning_rate * gradient_weights[i];
            }
            self.bias -= self.learning_rate * gradient_bias;
            
            // è®¡ç®—æŸå¤±
            let loss = self.compute_loss(dataset);
            loss_history.push(loss);
            
            // æ£€æŸ¥æ”¶æ•›
            if iteration > 0 && (loss_history[iteration - 1] - loss).abs() < 1e-6 {
                break;
            }
        }
        
        TrainingResult {
            weights: self.weights.clone(),
            bias: self.bias,
            loss_history,
            iterations: self.max_iterations,
        }
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        let mut prediction = self.bias;
        for (i, &feature) in features.iter().enumerate() {
            prediction += self.weights[i] * feature;
        }
        prediction
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
    
    fn compute_gradients(&self, dataset: &Dataset) -> (Vec<f64>, f64) {
        let mut gradient_weights = vec![0.0; self.weights.len()];
        let mut gradient_bias = 0.0;
        let n = dataset.features.len() as f64;
        
        for (features, target) in dataset.features.iter().zip(dataset.targets.iter()) {
            let prediction = self.predict(features);
            let error = prediction - target;
            
            // è®¡ç®—æ¢¯åº¦
            for (i, &feature) in features.iter().enumerate() {
                gradient_weights[i] += (2.0 / n) * error * feature;
            }
            gradient_bias += (2.0 / n) * error;
        }
        
        (gradient_weights, gradient_bias)
    }
    
    fn compute_loss(&self, dataset: &Dataset) -> f64 {
        let mut total_loss = 0.0;
        let n = dataset.features.len() as f64;
        
        for (features, target) in dataset.features.iter().zip(dataset.targets.iter()) {
            let prediction = self.predict(features);
            let error = prediction - target;
            total_loss += error * error;
        }
        
        total_loss / n
    }
    
    pub fn r_squared(&self, dataset: &Dataset) -> f64 {
        let predictions = self.predict_batch(&dataset.features);
        let mean_target = dataset.targets.iter().sum::<f64>() / dataset.targets.len() as f64;
        
        let mut ss_res = 0.0;
        let mut ss_tot = 0.0;
        
        for (prediction, target) in predictions.iter().zip(dataset.targets.iter()) {
            ss_res += (prediction - target).powi(2);
            ss_tot += (target - mean_target).powi(2);
        }
        
        1.0 - (ss_res / ss_tot)
    }
}

impl Dataset {
    pub fn new(features: Vec<Vec<f64>>, targets: Vec<f64>) -> Self {
        Dataset { features, targets }
    }

    pub fn normalize(&self) -> (Dataset, Vec<f64>, Vec<f64>) {
        let feature_count = self.features[0].len();
        let mut means = vec![0.0; feature_count];
        let mut stds = vec![0.0; feature_count];
        
        // è®¡ç®—å‡å€¼
        for features in &self.features {
            for (i, &feature) in features.iter().enumerate() {
                means[i] += feature;
            }
        }
        for mean in &mut means {
            *mean /= self.features.len() as f64;
        }
        
        // è®¡ç®—æ ‡å‡†å·®
        for features in &self.features {
            for (i, &feature) in features.iter().enumerate() {
                stds[i] += (feature - means[i]).powi(2);
            }
        }
        for std in &mut stds {
            *std = (*std / self.features.len() as f64).sqrt();
        }
        
        // æ ‡å‡†åŒ–ç‰¹å¾
        let mut normalized_features = Vec::new();
        for features in &self.features {
            let mut normalized = Vec::new();
            for (i, &feature) in features.iter().enumerate() {
                normalized.push((feature - means[i]) / stds[i]);
            }
            normalized_features.push(normalized);
        }
        
        (Dataset::new(normalized_features, self.targets.clone()), means, stds)
    }
    
    pub fn split(&self, train_ratio: f64) -> (Dataset, Dataset) {
        let split_index = (self.features.len() as f64 * train_ratio) as usize;
        
        let train_features = self.features[..split_index].to_vec();
        let train_targets = self.targets[..split_index].to_vec();
        let test_features = self.features[split_index..].to_vec();
        let test_targets = self.targets[split_index..].to_vec();
        
        (Dataset::new(train_features, train_targets), Dataset::new(test_features, test_targets))
    }
}

#[derive(Debug, Clone)]
pub struct DecisionTree {
    pub root: Option<Box<TreeNode>>,
    pub max_depth: usize,
    pub min_samples_split: usize,
}

#[derive(Debug, Clone)]
pub enum TreeNode {
    Leaf {
        prediction: f64,
        samples: usize,
    },
    Split {
        feature_index: usize,
        threshold: f64,
        left: Box<TreeNode>,
        right: Box<TreeNode>,
        samples: usize,
    },
}

impl DecisionTree {
    pub fn new(max_depth: usize, min_samples_split: usize) -> Self {
        DecisionTree {
            root: None,
            max_depth,
            min_samples_split,
        }
    }
    
    pub fn fit(&mut self, dataset: &Dataset) {
        self.root = Some(Box::new(self.build_tree(dataset, 0)));
    }
    
    fn build_tree(&self, dataset: &Dataset, depth: usize) -> TreeNode {
        let samples = dataset.features.len();
        
        // æ£€æŸ¥åœæ­¢æ¡ä»¶
        if depth >= self.max_depth || samples < self.min_samples_split {
            return TreeNode::Leaf {
                prediction: self.calculate_leaf_prediction(dataset),
                samples,
            };
        }
        
        // å¯»æ‰¾æœ€ä½³åˆ†å‰²
        if let Some((best_feature, best_threshold, best_gain)) = self.find_best_split(dataset) {
            if best_gain > 0.0 {
                let (left_dataset, right_dataset) = self.split_dataset(dataset, best_feature, best_threshold);
                
                let left_node = self.build_tree(&left_dataset, depth + 1);
                let right_node = self.build_tree(&right_dataset, depth + 1);
                
                return TreeNode::Split {
                    feature_index: best_feature,
                    threshold: best_threshold,
                    left: Box::new(left_node),
                    right: Box::new(right_node),
                    samples,
                };
            }
        }
        
        // æ— æ³•åˆ†å‰²ï¼Œåˆ›å»ºå¶å­èŠ‚ç‚¹
        TreeNode::Leaf {
            prediction: self.calculate_leaf_prediction(dataset),
            samples,
        }
    }
    
    fn find_best_split(&self, dataset: &Dataset) -> Option<(usize, f64, f64)> {
        let mut best_gain = 0.0;
        let mut best_feature = 0;
        let mut best_threshold = 0.0;
        
        let parent_entropy = self.calculate_entropy(&dataset.targets);
        
        for feature_index in 0..dataset.features[0].len() {
            let mut unique_values: Vec<f64> = dataset.features.iter()
                .map(|f| f[feature_index])
                .collect();
            unique_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
            unique_values.dedup();
            
            for &threshold in &unique_values {
                let (left_dataset, right_dataset) = self.split_dataset(dataset, feature_index, threshold);
                
                if left_dataset.features.is_empty() || right_dataset.features.is_empty() {
                    continue;
                }
                
                let left_entropy = self.calculate_entropy(&left_dataset.targets);
                let right_entropy = self.calculate_entropy(&right_dataset.targets);
                
                let left_weight = left_dataset.features.len() as f64 / dataset.features.len() as f64;
                let right_weight = right_dataset.features.len() as f64 / dataset.features.len() as f64;
                
                let information_gain = parent_entropy - (left_weight * left_entropy + right_weight * right_entropy);
                
                if information_gain > best_gain {
                    best_gain = information_gain;
                    best_feature = feature_index;
                    best_threshold = threshold;
                }
            }
        }
        
        if best_gain > 0.0 {
            Some((best_feature, best_threshold, best_gain))
        } else {
            None
        }
    }
    
    fn calculate_entropy(&self, targets: &[f64]) -> f64 {
        let n = targets.len() as f64;
        let mean = targets.iter().sum::<f64>() / n;
        let variance = targets.iter().map(|&t| (t - mean).powi(2)).sum::<f64>() / n;
        
        if variance == 0.0 {
            0.0
        } else {
            0.5 * (1.0 + (2.0 * std::f64::consts::PI * variance).ln())
        }
    }
    
    fn split_dataset(&self, dataset: &Dataset, feature_index: usize, threshold: f64) -> (Dataset, Dataset) {
        let mut left_features = Vec::new();
        let mut left_targets = Vec::new();
        let mut right_features = Vec::new();
        let mut right_targets = Vec::new();
        
        for (i, features) in dataset.features.iter().enumerate() {
            if features[feature_index] <= threshold {
                left_features.push(features.clone());
                left_targets.push(dataset.targets[i]);
            } else {
                right_features.push(features.clone());
                right_targets.push(dataset.targets[i]);
            }
        }
        
        (Dataset { features: left_features, targets: left_targets },
         Dataset { features: right_features, targets: right_targets })
    }
    
    fn calculate_leaf_prediction(&self, dataset: &Dataset) -> f64 {
        dataset.targets.iter().sum::<f64>() / dataset.targets.len() as f64
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        if let Some(ref root) = self.root {
            self.predict_node(root, features)
        } else {
            0.0
        }
    }
    
    fn predict_node(&self, node: &TreeNode, features: &[f64]) -> f64 {
        match node {
            TreeNode::Leaf { prediction, .. } => *prediction,
            TreeNode::Split { feature_index, threshold, left, right, .. } => {
                if features[*feature_index] <= *threshold {
                    self.predict_node(left, features)
                } else {
                    self.predict_node(right, features)
                }
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_decision_tree() {
        let mut tree = DecisionTree::new(3, 2);
        
        // ç®€å•çš„åˆ†ç±»é—®é¢˜
        let dataset = Dataset {
            features: vec![
                vec![1.0, 2.0],
                vec![2.0, 3.0],
                vec![3.0, 4.0],
                vec![4.0, 5.0],
            ],
            targets: vec![0.0, 0.0, 1.0, 1.0],
        };
        
        tree.fit(&dataset);
        
        // æµ‹è¯•é¢„æµ‹
        let test_features = vec![2.5, 3.5];
        let prediction = tree.predict(&test_features);
        
        assert!(prediction >= 0.0 && prediction <= 1.0);
    }
}

#[derive(Debug, Clone)]
pub struct NeuralNetwork {
    pub layers: Vec<Layer>,
    pub learning_rate: f64,
    pub batch_size: usize,
    pub epochs: usize,
}

#[derive(Debug, Clone)]
pub struct Layer {
    pub neurons: Vec<Neuron>,
    pub activation: ActivationFunction,
}

#[derive(Debug, Clone)]
pub struct Neuron {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub delta: f64,
}

#[derive(Debug, Clone)]
pub enum ActivationFunction {
    Sigmoid,
    ReLU,
    Tanh,
    Linear,
}

impl NeuralNetwork {
    pub fn new(architecture: Vec<usize>, learning_rate: f64, batch_size: usize, epochs: usize) -> Self {
        let mut layers = Vec::new();
        
        for i in 0..architecture.len() - 1 {
            let layer_size = architecture[i + 1];
            let input_size = architecture[i];
            
            let mut neurons = Vec::new();
            for _ in 0..layer_size {
                let weights = (0..input_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect();
                neurons.push(Neuron {
                    weights,
                    bias: rand::random::<f64>() * 2.0 - 1.0,
                    delta: 0.0,
                });
            }
            
            let activation = if i == architecture.len() - 2 {
                ActivationFunction::Linear // è¾“å‡ºå±‚ä½¿ç”¨çº¿æ€§æ¿€æ´»
            } else {
                ActivationFunction::ReLU // éšè—å±‚ä½¿ç”¨ReLU
            };
            
            layers.push(Layer { neurons, activation });
        }
        
        NeuralNetwork {
            layers,
            learning_rate,
            batch_size,
            epochs,
        }
    }
    
    pub fn train(&mut self, dataset: &Dataset) -> Vec<f64> {
        let mut loss_history = Vec::new();
        
        for epoch in 0..self.epochs {
            let mut epoch_loss = 0.0;
            let batch_count = (dataset.features.len() + self.batch_size - 1) / self.batch_size;
            
            for batch in 0..batch_count {
                let start = batch * self.batch_size;
                let end = std::cmp::min(start + self.batch_size, dataset.features.len());
                
                let batch_features = &dataset.features[start..end];
                let batch_targets = &dataset.targets[start..end];
                
                let batch_loss = self.train_batch(batch_features, batch_targets);
                epoch_loss += batch_loss;
            }
            
            epoch_loss /= batch_count as f64;
            loss_history.push(epoch_loss);
            
            if epoch % 100 == 0 {
                println!("Epoch {}, Loss: {:.6}", epoch, epoch_loss);
            }
        }
        
        loss_history
    }
    
    fn train_batch(&mut self, features: &[Vec<f64>], targets: &[f64]) -> f64 {
        let mut total_loss = 0.0;
        
        // å‰å‘ä¼ æ’­
        for (feature, target) in features.iter().zip(targets.iter()) {
            let prediction = self.forward_pass(feature);
            let loss = 0.5 * (prediction - target).powi(2);
            total_loss += loss;
            
            // åå‘ä¼ æ’­
            self.backward_pass(feature, target);
        }
        
        // æ›´æ–°æƒé‡
        self.update_weights();
        
        total_loss / features.len() as f64
    }
    
    fn forward_pass(&mut self, input: &[f64]) -> f64 {
        let mut current_input = input.to_vec();
        
        for layer in &mut self.layers {
            let mut layer_output = Vec::new();
            
            for neuron in &mut layer.neurons {
                let mut sum = neuron.bias;
                for (i, &input_val) in current_input.iter().enumerate() {
                    sum += neuron.weights[i] * input_val;
                }
                
                let output = self.activate(sum, &layer.activation);
                layer_output.push(output);
            }
            
            current_input = layer_output;
        }
        
        current_input[0] // å‡è®¾è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªç¥ç»å…ƒ
    }
    
    fn backward_pass(&mut self, input: &[f64], target: &f64) {
        // è®¡ç®—è¾“å‡ºå±‚çš„è¯¯å·®
        let mut current_input = input.to_vec();
        let mut layer_outputs = vec![current_input.clone()];
        
        // å‰å‘ä¼ æ’­å¹¶ä¿å­˜ä¸­é—´ç»“æœ
        for layer in &mut self.layers {
            let mut layer_output = Vec::new();
            
            for neuron in &mut layer.neurons {
                let mut sum = neuron.bias;
                for (i, &input_val) in current_input.iter().enumerate() {
                    sum += neuron.weights[i] * input_val;
                }
                
                let output = self.activate(sum, &layer.activation);
                layer_output.push(output);
            }
            
            current_input = layer_output.clone();
            layer_outputs.push(layer_output);
        }
        
        // åå‘ä¼ æ’­è¯¯å·®
        let prediction = current_input[0];
        let output_error = prediction - target;
        
        for (layer_index, layer) in self.layers.iter_mut().enumerate().rev() {
            let layer_output = &layer_outputs[layer_index + 1];
            let prev_layer_output = &layer_outputs[layer_index];
            
            for (neuron_index, neuron) in layer.neurons.iter_mut().enumerate() {
                let output = layer_output[neuron_index];
                let derivative = self.activate_derivative(output, &layer.activation);
                
                if layer_index == self.layers.len() - 1 {
                    // è¾“å‡ºå±‚
                    neuron.delta = output_error * derivative;
                } else {
                    // éšè—å±‚
                    let mut error = 0.0;
                    for next_neuron in &self.layers[layer_index + 1].neurons {
                        error += next_neuron.delta * next_neuron.weights[neuron_index];
                    }
                    neuron.delta = error * derivative;
                }
                
                // æ›´æ–°æƒé‡æ¢¯åº¦
                for (weight_index, &input_val) in prev_layer_output.iter().enumerate() {
                    neuron.weights[weight_index] -= self.learning_rate * neuron.delta * input_val;
                }
                neuron.bias -= self.learning_rate * neuron.delta;
            }
        }
    }
    
    fn update_weights(&mut self) {
        // æƒé‡æ›´æ–°å·²åœ¨åå‘ä¼ æ’­ä¸­å®Œæˆ
    }
    
    fn activate(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => 1.0 / (1.0 + (-x).exp()),
            ActivationFunction::ReLU => x.max(0.0),
            ActivationFunction::Tanh => x.tanh(),
            ActivationFunction::Linear => x,
        }
    }
    
    fn activate_derivative(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => x * (1.0 - x),
            ActivationFunction::ReLU => if x > 0.0 { 1.0 } else { 0.0 },
            ActivationFunction::Tanh => 1.0 - x.powi(2),
            ActivationFunction::Linear => 1.0,
        }
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        self.forward_pass(features)
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
    
    pub fn evaluate(&self, dataset: &Dataset) -> f64 {
        let predictions = self.predict_batch(&dataset.features);
        let mut mse = 0.0;
        
        for (prediction, target) in predictions.iter().zip(dataset.targets.iter()) {
            mse += (prediction - target).powi(2);
        }
        
        mse / dataset.features.len() as f64
    }
}
```

### 5.4 æ”¯æŒå‘é‡æœºå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct SupportVectorMachine {
    pub support_vectors: Vec<Vec<f64>>,
    pub support_vector_labels: Vec<f64>,
    pub alphas: Vec<f64>,
    pub bias: f64,
    pub kernel: KernelFunction,
    pub c: f64,
}

#[derive(Debug, Clone)]
pub enum KernelFunction {
    Linear,
    RBF { gamma: f64 },
    Polynomial { degree: usize, coef0: f64 },
}

impl SupportVectorMachine {
    pub fn new(kernel: KernelFunction, c: f64) -> Self {
        SupportVectorMachine {
            support_vectors: Vec::new(),
            support_vector_labels: Vec::new(),
            alphas: Vec::new(),
            bias: 0.0,
            kernel,
            c,
        }
    }
    
    pub fn fit(&mut self, dataset: &Dataset) {
        let n_samples = dataset.features.len();
        let mut alphas = vec![0.0; n_samples];
        let mut bias = 0.0;
        
        // ç®€åŒ–çš„SMOç®—æ³•å®ç°
        for iteration in 0..100 {
            let mut num_changed = 0;
            
            for i in 0..n_samples {
                let error_i = self.decision_function(&dataset.features[i], &dataset.features, &dataset.targets, &alphas, bias) - dataset.targets[i];
                
                let r_i = dataset.targets[i] * error_i;
                
                if (r_i < -1e-3 && alphas[i] < self.c) || (r_i > 1e-3 && alphas[i] > 0.0) {
                    // é€‰æ‹©ç¬¬äºŒä¸ªalpha
                    let j = (i + 1) % n_samples;
                    let error_j = self.decision_function(&dataset.features[j], &dataset.features, &dataset.targets, &alphas, bias) - dataset.targets[j];
                    
                    let old_alpha_i = alphas[i];
                    let old_alpha_j = alphas[j];
                    
                    let eta = 2.0 * self.kernel_value(&dataset.features[i], &dataset.features[j]) 
                             - self.kernel_value(&dataset.features[i], &dataset.features[i])
                             - self.kernel_value(&dataset.features[j], &dataset.features[j]);
                    
                    if eta.abs() > 1e-8 {
                        alphas[j] = old_alpha_j + dataset.targets[j] * (error_i - error_j) / eta;
                        alphas[j] = alphas[j].max(0.0).min(self.c);
                        
                        if (alphas[j] - old_alpha_j).abs() > 1e-5 {
                            alphas[i] = old_alpha_i + dataset.targets[i] * dataset.targets[j] * (old_alpha_j - alphas[j]);
                            
                            // æ›´æ–°bias
                            let b1 = bias - error_i - dataset.targets[i] * (alphas[i] - old_alpha_i) * self.kernel_value(&dataset.features[i], &dataset.features[i])
                                     - dataset.targets[j] * (alphas[j] - old_alpha_j) * self.kernel_value(&dataset.features[i], &dataset.features[j]);
                            let b2 = bias - error_j - dataset.targets[i] * (alphas[i] - old_alpha_i) * self.kernel_value(&dataset.features[i], &dataset.features[j])
                                     - dataset.targets[j] * (alphas[j] - old_alpha_j) * self.kernel_value(&dataset.features[j], &dataset.features[j]);
                            bias = (b1 + b2) / 2.0;
                            
                            num_changed += 1;
                        }
                    }
                }
            }
            
            if num_changed == 0 {
                break;
            }
        }
        
        // ä¿å­˜æ”¯æŒå‘é‡
        for (i, &alpha) in alphas.iter().enumerate() {
            if alpha > 1e-5 {
                self.support_vectors.push(dataset.features[i].clone());
                self.support_vector_labels.push(dataset.targets[i]);
                self.alphas.push(alpha);
            }
        }
        
        self.bias = bias;
    }
    
    fn decision_function(&self, x: &[f64], X: &[Vec<f64>], y: &[f64], alphas: &[f64], bias: f64) -> f64 {
        let mut result = bias;
        for (i, alpha) in alphas.iter().enumerate() {
            if *alpha > 1e-5 {
                result += alpha * y[i] * self.kernel_value(x, &X[i]);
            }
        }
        result
    }
    
    fn kernel_value(&self, x1: &[f64], x2: &[f64]) -> f64 {
        match &self.kernel {
            KernelFunction::Linear => {
                x1.iter().zip(x2.iter()).map(|(a, b)| a * b).sum()
            },
            KernelFunction::RBF { gamma } => {
                let distance_squared: f64 = x1.iter().zip(x2.iter())
                    .map(|(a, b)| (a - b).powi(2))
                    .sum();
                (-gamma * distance_squared).exp()
            },
            KernelFunction::Polynomial { degree, coef0 } => {
                let dot_product: f64 = x1.iter().zip(x2.iter()).map(|(a, b)| a * b).sum();
                (dot_product + coef0).powi(*degree as i32)
            }
        }
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        let mut result = self.bias;
        for (i, alpha) in self.alphas.iter().enumerate() {
            result += alpha * self.support_vector_labels[i] * self.kernel_value(features, &self.support_vectors[i]);
        }
        result.signum()
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
    
    pub fn score(&self, dataset: &Dataset) -> f64 {
        let predictions = self.predict_batch(&dataset.features);
        let mut correct = 0;
        
        for (prediction, target) in predictions.iter().zip(dataset.targets.iter()) {
            if prediction.signum() == target.signum() {
                correct += 1;
            }
        }
        
        correct as f64 / dataset.features.len() as f64
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_svm() {
        let kernel = KernelFunction::Linear;
        let mut svm = SupportVectorMachine::new(kernel, 1.0);
        
        // ç®€å•çš„çº¿æ€§å¯åˆ†æ•°æ®
        let dataset = Dataset {
            features: vec![
                vec![1.0, 1.0],
                vec![2.0, 2.0],
                vec![3.0, 3.0],
                vec![1.0, 3.0],
                vec![2.0, 4.0],
                vec![3.0, 5.0],
            ],
            targets: vec![1.0, 1.0, 1.0, -1.0, -1.0, -1.0],
        };
        
        svm.fit(&dataset);
        
        // æµ‹è¯•é¢„æµ‹
        let test_features = vec![2.0, 2.5];
        let prediction = svm.predict(&test_features);
        
        assert!(prediction == 1.0 || prediction == -1.0);
    }
}
```

### 5.5 æ¨¡å‹è¯„ä¼°å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct ModelEvaluator {
    pub metrics: HashMap<String, f64>,
}

#[derive(Debug, Clone)]
pub struct CrossValidation {
    pub k_folds: usize,
    pub results: Vec<f64>,
}

impl ModelEvaluator {
    pub fn new() -> Self {
        ModelEvaluator {
            metrics: HashMap::new(),
        }
    }
    
    pub fn evaluate_regression(&mut self, predictions: &[f64], targets: &[f64]) {
        let mse = self.mean_squared_error(predictions, targets);
        let mae = self.mean_absolute_error(predictions, targets);
        let r2 = self.r_squared(predictions, targets);
        
        self.metrics.insert("MSE".to_string(), mse);
        self.metrics.insert("MAE".to_string(), mae);
        self.metrics.insert("R2".to_string(), r2);
    }
    
    pub fn evaluate_classification(&mut self, predictions: &[f64], targets: &[f64]) {
        let accuracy = self.accuracy(predictions, targets);
        let precision = self.precision(predictions, targets);
        let recall = self.recall(predictions, targets);
        let f1 = self.f1_score(predictions, targets);
        
        self.metrics.insert("Accuracy".to_string(), accuracy);
        self.metrics.insert("Precision".to_string(), precision);
        self.metrics.insert("Recall".to_string(), recall);
        self.metrics.insert("F1-Score".to_string(), f1);
    }
    
    fn mean_squared_error(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        predictions.iter().zip(targets.iter())
            .map(|(p, t)| (p - t).powi(2))
            .sum::<f64>() / predictions.len() as f64
    }
    
    fn mean_absolute_error(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        predictions.iter().zip(targets.iter())
            .map(|(p, t)| (p - t).abs())
            .sum::<f64>() / predictions.len() as f64
    }
    
    fn r_squared(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let mean_target = targets.iter().sum::<f64>() / targets.len() as f64;
        let ss_res: f64 = predictions.iter().zip(targets.iter())
            .map(|(p, t)| (p - t).powi(2))
            .sum();
        let ss_tot: f64 = targets.iter()
            .map(|t| (t - mean_target).powi(2))
            .sum();
        
        1.0 - (ss_res / ss_tot)
    }
    
    fn accuracy(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let mut correct = 0;
        for (p, t) in predictions.iter().zip(targets.iter()) {
            if p.signum() == t.signum() {
                correct += 1;
            }
        }
        correct as f64 / predictions.len() as f64
    }
    
    fn precision(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let mut true_positives = 0;
        let mut false_positives = 0;
        
        for (p, t) in predictions.iter().zip(targets.iter()) {
            if p.signum() > 0.0 {
                if t.signum() > 0.0 {
                    true_positives += 1;
                } else {
                    false_positives += 1;
                }
            }
        }
        
        if true_positives + false_positives == 0 {
            0.0
        } else {
            true_positives as f64 / (true_positives + false_positives) as f64
        }
    }
    
    fn recall(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let mut true_positives = 0;
        let mut false_negatives = 0;
        
        for (p, t) in predictions.iter().zip(targets.iter()) {
            if t.signum() > 0.0 {
                if p.signum() > 0.0 {
                    true_positives += 1;
                } else {
                    false_negatives += 1;
                }
            }
        }
        
        if true_positives + false_negatives == 0 {
            0.0
        } else {
            true_positives as f64 / (true_positives + false_negatives) as f64
        }
    }
    
    fn f1_score(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let precision = self.precision(predictions, targets);
        let recall = self.recall(predictions, targets);
        
        if precision + recall == 0.0 {
            0.0
        } else {
            2.0 * precision * recall / (precision + recall)
        }
    }
}

impl CrossValidation {
    pub fn new(k_folds: usize) -> Self {
        CrossValidation {
            k_folds,
            results: Vec::new(),
        }
    }
    
    pub fn cross_validate<F>(&mut self, dataset: &Dataset, model_factory: F) -> f64 
    where F: Fn() -> Box<dyn Model>
    {
        let fold_size = dataset.features.len() / self.k_folds;
        let mut scores = Vec::new();
        
        for fold in 0..self.k_folds {
            let start_idx = fold * fold_size;
            let end_idx = if fold == self.k_folds - 1 {
                dataset.features.len()
            } else {
                (fold + 1) * fold_size
            };
            
            // åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            let mut train_features = Vec::new();
            let mut train_targets = Vec::new();
            let mut val_features = Vec::new();
            let mut val_targets = Vec::new();
            
            for i in 0..dataset.features.len() {
                if i >= start_idx && i < end_idx {
                    val_features.push(dataset.features[i].clone());
                    val_targets.push(dataset.targets[i]);
                } else {
                    train_features.push(dataset.features[i].clone());
                    train_targets.push(dataset.targets[i]);
                }
            }
            
            let train_dataset = Dataset::new(train_features, train_targets);
            let val_dataset = Dataset::new(val_features, val_targets);
            
            // è®­ç»ƒæ¨¡å‹
            let mut model = model_factory();
            model.fit(&train_dataset);
            
            // è¯„ä¼°æ¨¡å‹
            let predictions = model.predict_batch(&val_dataset.features);
            let mut evaluator = ModelEvaluator::new();
            evaluator.evaluate_regression(&predictions, &val_dataset.targets);
            
            scores.push(evaluator.metrics["R2"].unwrap_or(0.0));
        }
        
        let mean_score = scores.iter().sum::<f64>() / scores.len() as f64;
        self.results = scores;
        mean_score
    }
}

pub trait Model {
    fn fit(&mut self, dataset: &Dataset);
    fn predict(&self, features: &[f64]) -> f64;
    fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
}

impl Model for LinearRegression {
    fn fit(&mut self, dataset: &Dataset) {
        self.fit(dataset);
    }
    
    fn predict(&self, features: &[f64]) -> f64 {
        self.predict(features)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_model_evaluation() {
        let mut evaluator = ModelEvaluator::new();
        
        let predictions = vec![1.0, 2.0, 3.0, 4.0];
        let targets = vec![1.1, 2.1, 2.9, 4.1];
        
        evaluator.evaluate_regression(&predictions, &targets);
        
        assert!(evaluator.metrics.contains_key("MSE"));
        assert!(evaluator.metrics.contains_key("R2"));
    }
    
    #[test]
    fn test_cross_validation() {
        let dataset = Dataset {
            features: vec![
                vec![1.0], vec![2.0], vec![3.0], vec![4.0],
                vec![5.0], vec![6.0], vec![7.0], vec![8.0],
            ],
            targets: vec![2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0],
        };
        
        let mut cv = CrossValidation::new(4);
        let score = cv.cross_validate(&dataset, || {
            Box::new(LinearRegression::new(1, 0.01, 100))
        });
        
        assert!(score > 0.0);
    }
}
```

## 6. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æ·±åº¦å­¦ä¹ ç†è®º](../02_Deep_Learning/01_Deep_Learning_Theory.md)
- [å¼ºåŒ–å­¦ä¹ ç†è®º](../03_Reinforcement_Learning/01_Reinforcement_Learning_Theory.md)
- [è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º](../04_Natural_Language_Processing/01_Natural_Language_Processing_Theory.md)

## 7. å‚è€ƒæ–‡çŒ®

1. Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v2.0  
**çŠ¶æ€**: å·²å®Œæˆæ ¸å¿ƒç†è®ºæ¡†æ¶å’ŒRustå®ç°ï¼ŒåŒ…å«çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œã€æ”¯æŒå‘é‡æœºå’Œæ¨¡å‹è¯„ä¼°ç­‰å®Œæ•´å®ç°

**æ›´æ–°æ—¥å¿—**:

- v2.0 (2024-12-21): æ·»åŠ æ”¯æŒå‘é‡æœºç†è®ºã€æ¨¡å‹è¯„ä¼°ç†è®ºã€ä¼˜åŒ–ç†è®ºï¼Œå®Œå–„Rustä»£ç å®ç°ï¼Œå¢å¼ºæ‰¹åˆ¤æ€§åˆ†æ
- v1.0 (2024-12-20): åˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«åŸºæœ¬æ¦‚å¿µã€å®šç†è¯æ˜å’ŒåŸºç¡€ç®—æ³•å®ç°

## æ‰¹åˆ¤æ€§åˆ†æ

### ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†

æœºå™¨å­¦ä¹ ç†è®ºä½œä¸ºäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œä¸»è¦å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š

1. **å­¦ä¹ é—®é¢˜çš„å½¢å¼åŒ–**ï¼šå°†å­¦ä¹ è¿‡ç¨‹æŠ½è±¡ä¸ºä»æ•°æ®ä¸­å¯»æ‰¾æœ€ä¼˜æ˜ å°„å‡½æ•°çš„é—®é¢˜
2. **æ³›åŒ–èƒ½åŠ›ç†è®º**ï¼šç ”ç©¶æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„è¡¨ç°èƒ½åŠ›
3. **ç®—æ³•è®¾è®¡ä¸ä¼˜åŒ–**ï¼šå¼€å‘é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ç®—æ³•
4. **æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©**ï¼šå»ºç«‹ç§‘å­¦çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°ä½“ç³»

### ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ

**ä¼˜ç‚¹**ï¼š

- **ç†è®ºåŸºç¡€æ‰å®**ï¼šåŸºäºç»Ÿè®¡å­¦ã€ä¼˜åŒ–ç†è®ºç­‰æ•°å­¦åŸºç¡€ï¼Œå…·æœ‰åšå®çš„ç†è®ºæ”¯æ’‘
- **ç®—æ³•ä¸°å¯Œå¤šæ ·**ï¼šæ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰å¤šç§èŒƒå¼
- **åº”ç”¨å¹¿æ³›**ï¼šåœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨
- **æŒç»­åˆ›æ–°**ï¼šæ·±åº¦å­¦ä¹ ã€è”é‚¦å­¦ä¹ ç­‰æ–°å…´æŠ€æœ¯ä¸æ–­æ¶Œç°

**ç¼ºç‚¹**ï¼š

- **å¯è§£é‡Šæ€§ä¸è¶³**ï¼šç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå†³ç­–è¿‡ç¨‹éš¾ä»¥è§£é‡Š
- **æ•°æ®ä¾èµ–æ€§**ï¼šæ¨¡å‹æ€§èƒ½ä¸¥é‡ä¾èµ–è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ•°é‡
- **æ³›åŒ–èƒ½åŠ›æœ‰é™**ï¼šåœ¨åˆ†å¸ƒåç§»æˆ–å¯¹æŠ—æ ·æœ¬é¢å‰è¡¨ç°ä¸ä½³
- **è®¡ç®—èµ„æºæ¶ˆè€—å¤§**ï¼šå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æº

### ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ

**ä¸æ•°å­¦åŸºç¡€çš„èåˆ**ï¼š

- **ç»Ÿè®¡å­¦**ï¼šä¸ºæ¨¡å‹è¯„ä¼°ã€å‡è®¾æ£€éªŒæä¾›ç†è®ºåŸºç¡€
- **ä¼˜åŒ–ç†è®º**ï¼šä¸ºå‚æ•°å­¦ä¹ æä¾›ç®—æ³•æ”¯æ’‘
- **ä¿¡æ¯è®º**ï¼šä¸ºç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹å¤æ‚åº¦æ§åˆ¶æä¾›æŒ‡å¯¼

**ä¸ç±»å‹ç†è®ºçš„äº¤å‰**ï¼š

- **ç±»å‹å®‰å…¨**ï¼šåœ¨æ¨¡å‹æ¥å£è®¾è®¡ä¸­åº”ç”¨ç±»å‹ç³»ç»Ÿç¡®ä¿å®‰å…¨æ€§
- **æŠ½è±¡å±‚æ¬¡**ï¼šé€šè¿‡ç±»å‹æŠ½è±¡å®ç°æ¨¡å‹ç»„ä»¶çš„æ¨¡å—åŒ–è®¾è®¡
- **å½¢å¼åŒ–éªŒè¯**ï¼šåˆ©ç”¨ç±»å‹ç†è®ºè¿›è¡Œæ¨¡å‹æ­£ç¡®æ€§éªŒè¯

**ä¸äººå·¥æ™ºèƒ½ç†è®ºçš„èåˆ**ï¼š

- **è®¤çŸ¥å»ºæ¨¡**ï¼šå€Ÿé‰´äººç±»å­¦ä¹ æœºåˆ¶è®¾è®¡æ›´æ™ºèƒ½çš„ç®—æ³•
- **çŸ¥è¯†è¡¨ç¤º**ï¼šå°†é¢†åŸŸçŸ¥è¯†èå…¥æœºå™¨å­¦ä¹ æ¨¡å‹
- **æ¨ç†æœºåˆ¶**ï¼šç»“åˆé€»è¾‘æ¨ç†å’Œç»Ÿè®¡å­¦ä¹ 

**ä¸æ§åˆ¶è®ºçš„äº’è¡¥**ï¼š

- **åé¦ˆæœºåˆ¶**ï¼šåœ¨çº¿å­¦ä¹ ä¸­çš„è‡ªé€‚åº”è°ƒæ•´
- **ç¨³å®šæ€§ç†è®º**ï¼šæ¨¡å‹è®­ç»ƒçš„æ”¶æ•›æ€§åˆ†æ
- **é²æ£’æ€§è®¾è®¡**ï¼šå¯¹æŠ—ç¯å¢ƒä¸‹çš„æ¨¡å‹ç¨³å®šæ€§

### åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›

**ç†è®ºåˆ›æ–°æ–¹å‘**ï¼š

1. **å› æœæ¨ç†**ï¼šä»ç›¸å…³æ€§å­¦ä¹ è½¬å‘å› æœæ€§å­¦ä¹ ï¼Œæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›
2. **å…ƒå­¦ä¹ **ï¼šå­¦ä¹ å¦‚ä½•å­¦ä¹ ï¼Œå®ç°å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡çš„èƒ½åŠ›
3. **è”é‚¦å­¦ä¹ **ï¼šåœ¨ä¿æŠ¤éšç§çš„å‰æä¸‹å®ç°åˆ†å¸ƒå¼å­¦ä¹ 
4. **ç¥ç»ç¬¦å·å­¦ä¹ **ï¼šç»“åˆç¥ç»ç½‘ç»œå’Œç¬¦å·æ¨ç†çš„ä¼˜åŠ¿

**æŠ€æœ¯å‘å±•è¶‹åŠ¿**ï¼š

1. **è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ **ï¼šå‡å°‘äººå·¥å¹²é¢„ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æ¨¡å‹è®¾è®¡
2. **ç»¿è‰²AI**ï¼šé™ä½æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„èƒ½è€—
3. **è¾¹ç¼˜è®¡ç®—**ï¼šå°†æœºå™¨å­¦ä¹ éƒ¨ç½²åˆ°èµ„æºå—é™çš„è®¾å¤‡ä¸Š
4. **é‡å­æœºå™¨å­¦ä¹ **ï¼šåˆ©ç”¨é‡å­è®¡ç®—çš„ä¼˜åŠ¿åŠ é€Ÿç‰¹å®šç®—æ³•

**ç¤¾ä¼šå½±å“è€ƒé‡**ï¼š

1. **å…¬å¹³æ€§**ï¼šç¡®ä¿ç®—æ³•å¯¹ä¸åŒç¾¤ä½“çš„å…¬å¹³æ€§
2. **é€æ˜åº¦**ï¼šæé«˜æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§
3. **è´£ä»»æ€§**ï¼šå»ºç«‹AIç³»ç»Ÿçš„è´£ä»»è¿½ç©¶æœºåˆ¶
4. **æ•™è‚²æ™®åŠ**ï¼šæé«˜å…¬ä¼—å¯¹æœºå™¨å­¦ä¹ çš„ç†è§£å’Œå‚ä¸åº¦

### å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»

**ç»å…¸æ•™æ**ï¼š

- Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
- Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.

**å‰æ²¿ç ”ç©¶**ï¼š

- Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.

**äº¤å‰å­¦ç§‘æ–‡çŒ®**ï¼š

- Pearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.
- Russell, S. J. (2019). Human Compatible: Artificial Intelligence and the Problem of Control. Viking.

**ç›¸å…³ç†è®ºé“¾æ¥**ï¼š

- [æ·±åº¦å­¦ä¹ ç†è®º](../02_Deep_Learning/01_Deep_Learning_Theory.md)
- [å¼ºåŒ–å­¦ä¹ ç†è®º](../03_Reinforcement_Learning/01_Reinforcement_Learning_Theory.md)
- [å› æœæ¨ç†ç†è®º](../05_Causal_Reasoning/01_Causal_Reasoning_Theory.md)
- [è”é‚¦å­¦ä¹ ç†è®º](../06_Federated_Learning/01_Federated_Learning_Theory.md)
