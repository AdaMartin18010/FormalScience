# 13.1.1 æœºå™¨å­¦ä¹ ç†è®º

## ç›®å½•

- [13.1.1 æœºå™¨å­¦ä¹ ç†è®º](#1311-æœºå™¨å­¦ä¹ ç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [1. åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
    - [1.1 æœºå™¨å­¦ä¹ å®šä¹‰](#11-æœºå™¨å­¦ä¹ å®šä¹‰)
    - [1.2 å­¦ä¹ ç±»å‹åˆ†ç±»](#12-å­¦ä¹ ç±»å‹åˆ†ç±»)
  - [2. å½¢å¼åŒ–å®šä¹‰](#2-å½¢å¼åŒ–å®šä¹‰)
    - [2.1 å­¦ä¹ é—®é¢˜](#21-å­¦ä¹ é—®é¢˜)
    - [2.2 å‡è®¾ç©ºé—´](#22-å‡è®¾ç©ºé—´)
    - [2.3 ç»éªŒé£é™©](#23-ç»éªŒé£é™©)
  - [3. å®šç†ä¸è¯æ˜](#3-å®šç†ä¸è¯æ˜)
    - [3.1 æ²¡æœ‰å…è´¹åˆé¤å®šç†](#31-æ²¡æœ‰å…è´¹åˆé¤å®šç†)
    - [3.2 æ³›åŒ–ç•Œå®šç†](#32-æ³›åŒ–ç•Œå®šç†)
  - [4. Rustä»£ç å®ç°](#4-rustä»£ç å®ç°)
    - [4.1 çº¿æ€§å›å½’å®ç°](#41-çº¿æ€§å›å½’å®ç°)
    - [4.2 å†³ç­–æ ‘å®ç°](#42-å†³ç­–æ ‘å®ç°)
    - [4.3 ç¥ç»ç½‘ç»œå®ç°](#43-ç¥ç»ç½‘ç»œå®ç°)
  - [5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨](#5-ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨)
  - [6. å‚è€ƒæ–‡çŒ®](#6-å‚è€ƒæ–‡çŒ®)
  - [æ‰¹åˆ¤æ€§åˆ†æ](#æ‰¹åˆ¤æ€§åˆ†æ)
    - [ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†](#ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†)
    - [ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ](#ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ)
    - [ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ](#ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ)
    - [åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›](#åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›)
    - [å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»](#å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»)

## ğŸ“‹ æ¦‚è¿°

æœºå™¨å­¦ä¹ ç†è®ºç ”ç©¶å¦‚ä½•è®©è®¡ç®—æœºç³»ç»Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ã€‚è¯¥ç†è®ºæ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºäººå·¥æ™ºèƒ½ç³»ç»Ÿæ„å»ºæä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 æœºå™¨å­¦ä¹ å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆæœºå™¨å­¦ä¹ ï¼‰
æœºå™¨å­¦ä¹ æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚

### 1.2 å­¦ä¹ ç±»å‹åˆ†ç±»

| å­¦ä¹ ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹ç®—æ³•         |
|--------------|------------------|------------------------------|------------------|
| ç›‘ç£å­¦ä¹      | Supervised       | ä»æ ‡è®°æ•°æ®ä¸­å­¦ä¹              | çº¿æ€§å›å½’, SVM    |
| æ— ç›‘ç£å­¦ä¹    | Unsupervised     | ä»æ— æ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼       | K-means, PCA     |
| å¼ºåŒ–å­¦ä¹      | Reinforcement    | é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ ç­–ç•¥       | Q-learning, DQN  |
| åŠç›‘ç£å­¦ä¹    | Semi-supervised  | ç»“åˆæ ‡è®°å’Œæœªæ ‡è®°æ•°æ®         | è‡ªè®­ç»ƒ, å›¾å­¦ä¹    |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 å­¦ä¹ é—®é¢˜

**å®šä¹‰ 2.1**ï¼ˆå­¦ä¹ é—®é¢˜ï¼‰
å­¦ä¹ é—®é¢˜æ˜¯ç»™å®šè¾“å…¥ç©ºé—´ $X$ å’Œè¾“å‡ºç©ºé—´ $Y$ï¼Œå¯»æ‰¾æ˜ å°„ $f: X \rightarrow Y$ çš„è¿‡ç¨‹ã€‚

### 2.2 å‡è®¾ç©ºé—´

**å®šä¹‰ 2.2**ï¼ˆå‡è®¾ç©ºé—´ï¼‰
å‡è®¾ç©ºé—´æ˜¯æ‰€æœ‰å¯èƒ½å‡è®¾çš„é›†åˆï¼Œè®°ä½œ $H = \{h: X \rightarrow Y\}$ã€‚

### 2.3 ç»éªŒé£é™©

**å®šä¹‰ 2.3**ï¼ˆç»éªŒé£é™©ï¼‰
ç»éªŒé£é™©æ˜¯å‡è®¾ $h$ åœ¨è®­ç»ƒé›† $S$ ä¸Šçš„å¹³å‡æŸå¤±ï¼š
$R_{emp}(h) = \frac{1}{n}\sum_{i=1}^{n} L(h(x_i), y_i)$

## 3. å®šç†ä¸è¯æ˜

### 3.1 æ²¡æœ‰å…è´¹åˆé¤å®šç†

**å®šç† 3.1**ï¼ˆæ²¡æœ‰å…è´¹åˆé¤å®šç†ï¼‰
åœ¨æ‰€æœ‰å¯èƒ½çš„é—®é¢˜ä¸Šï¼Œä»»ä½•å­¦ä¹ ç®—æ³•çš„å¹³å‡æ€§èƒ½éƒ½æ˜¯ç›¸åŒçš„ã€‚

**è¯æ˜**ï¼š
å¯¹äºä»»æ„ä¸¤ä¸ªç®—æ³• $A$ å’Œ $B$ï¼Œåœ¨æ‰€æœ‰å¯èƒ½çš„ç›®æ ‡å‡½æ•°ä¸Šï¼Œå®ƒä»¬çš„æœŸæœ›æ€§èƒ½ç›¸ç­‰ã€‚â–¡

### 3.2 æ³›åŒ–ç•Œå®šç†

**å®šç† 3.2**ï¼ˆæ³›åŒ–ç•Œï¼‰
å¯¹äºå‡è®¾ç©ºé—´ $H$ å’Œè®­ç»ƒé›† $S$ï¼Œä»¥æ¦‚ç‡ $1-\delta$ æœ‰ï¼š
$R(h) \leq R_{emp}(h) + \sqrt{\frac{\log|H| + \log(1/\delta)}{2n}}$

**è¯æ˜**ï¼š
ä½¿ç”¨Hoeffdingä¸ç­‰å¼å’Œè”åˆç•Œï¼Œè¯æ˜çœŸå®é£é™©ä¸ç»éªŒé£é™©çš„å·®è·ã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 çº¿æ€§å›å½’å®ç°

```rust
use std::collections::HashMap;
use std::f64;

#[derive(Debug, Clone)]
pub struct LinearRegression {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub learning_rate: f64,
    pub max_iterations: usize,
}

#[derive(Debug, Clone)]
pub struct Dataset {
    pub features: Vec<Vec<f64>>,
    pub targets: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct TrainingResult {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub loss_history: Vec<f64>,
    pub iterations: usize,
}

impl LinearRegression {
    pub fn new(feature_count: usize, learning_rate: f64, max_iterations: usize) -> Self {
        LinearRegression {
            weights: vec![0.0; feature_count],
            bias: 0.0,
            learning_rate,
            max_iterations,
        }
    }
    
    pub fn fit(&mut self, dataset: &Dataset) -> TrainingResult {
        let mut loss_history = Vec::new();
        
        for iteration in 0..self.max_iterations {
            let (gradient_weights, gradient_bias) = self.compute_gradients(dataset);
            
            // æ›´æ–°æƒé‡
            for i in 0..self.weights.len() {
                self.weights[i] -= self.learning_rate * gradient_weights[i];
            }
            self.bias -= self.learning_rate * gradient_bias;
            
            // è®¡ç®—æŸå¤±
            let loss = self.compute_loss(dataset);
            loss_history.push(loss);
            
            // æ£€æŸ¥æ”¶æ•›
            if iteration > 0 && (loss_history[iteration - 1] - loss).abs() < 1e-6 {
                break;
            }
        }
        
        TrainingResult {
            weights: self.weights.clone(),
            bias: self.bias,
            loss_history,
            iterations: self.max_iterations,
        }
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        let mut prediction = self.bias;
        for (i, &feature) in features.iter().enumerate() {
            prediction += self.weights[i] * feature;
        }
        prediction
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
    
    fn compute_gradients(&self, dataset: &Dataset) -> (Vec<f64>, f64) {
        let mut gradient_weights = vec![0.0; self.weights.len()];
        let mut gradient_bias = 0.0;
        let n = dataset.features.len() as f64;
        
        for (features, target) in dataset.features.iter().zip(dataset.targets.iter()) {
            let prediction = self.predict(features);
            let error = prediction - target;
            
            // è®¡ç®—æ¢¯åº¦
            for (i, &feature) in features.iter().enumerate() {
                gradient_weights[i] += (2.0 / n) * error * feature;
            }
            gradient_bias += (2.0 / n) * error;
        }
        
        (gradient_weights, gradient_bias)
    }
    
    fn compute_loss(&self, dataset: &Dataset) -> f64 {
        let mut total_loss = 0.0;
        let n = dataset.features.len() as f64;
        
        for (features, target) in dataset.features.iter().zip(dataset.targets.iter()) {
            let prediction = self.predict(features);
            let error = prediction - target;
            total_loss += error * error;
        }
        
        total_loss / n
    }
    
    pub fn r_squared(&self, dataset: &Dataset) -> f64 {
        let predictions = self.predict_batch(&dataset.features);
        let mean_target = dataset.targets.iter().sum::<f64>() / dataset.targets.len() as f64;
        
        let mut ss_res = 0.0;
        let mut ss_tot = 0.0;
        
        for (prediction, target) in predictions.iter().zip(dataset.targets.iter()) {
            ss_res += (prediction - target).powi(2);
            ss_tot += (target - mean_target).powi(2);
        }
        
        1.0 - (ss_res / ss_tot)
    }
}

impl Dataset {
    pub fn new(features: Vec<Vec<f64>>, targets: Vec<f64>) -> Self {
        Dataset { features, targets }
    }
    
    pub fn normalize(&self) -> (Dataset, Vec<f64>, Vec<f64>) {
        let feature_count = self.features[0].len();
        let mut means = vec![0.0; feature_count];
        let mut stds = vec![0.0; feature_count];
        
        // è®¡ç®—å‡å€¼
        for features in &self.features {
            for (i, &feature) in features.iter().enumerate() {
                means[i] += feature;
            }
        }
        for mean in &mut means {
            *mean /= self.features.len() as f64;
        }
        
        // è®¡ç®—æ ‡å‡†å·®
        for features in &self.features {
            for (i, &feature) in features.iter().enumerate() {
                stds[i] += (feature - means[i]).powi(2);
            }
        }
        for std in &mut stds {
            *std = (*std / self.features.len() as f64).sqrt();
        }
        
        // æ ‡å‡†åŒ–ç‰¹å¾
        let mut normalized_features = Vec::new();
        for features in &self.features {
            let mut normalized = Vec::new();
            for (i, &feature) in features.iter().enumerate() {
                normalized.push((feature - means[i]) / stds[i]);
            }
            normalized_features.push(normalized);
        }
        
        (Dataset::new(normalized_features, self.targets.clone()), means, stds)
    }
    
    pub fn split(&self, train_ratio: f64) -> (Dataset, Dataset) {
        let split_index = (self.features.len() as f64 * train_ratio) as usize;
        
        let train_features = self.features[..split_index].to_vec();
        let train_targets = self.targets[..split_index].to_vec();
        let test_features = self.features[split_index..].to_vec();
        let test_targets = self.targets[split_index..].to_vec();
        
        (Dataset::new(train_features, train_targets), Dataset::new(test_features, test_targets))
    }
}
```

### 4.2 å†³ç­–æ ‘å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct DecisionTree {
    pub root: Option<Box<TreeNode>>,
    pub max_depth: usize,
    pub min_samples_split: usize,
}

#[derive(Debug, Clone)]
pub enum TreeNode {
    Leaf {
        prediction: f64,
        samples: usize,
    },
    Split {
        feature_index: usize,
        threshold: f64,
        left: Box<TreeNode>,
        right: Box<TreeNode>,
        samples: usize,
    },
}

impl DecisionTree {
    pub fn new(max_depth: usize, min_samples_split: usize) -> Self {
        DecisionTree {
            root: None,
            max_depth,
            min_samples_split,
        }
    }
    
    pub fn fit(&mut self, dataset: &Dataset) {
        self.root = Some(Box::new(self.build_tree(dataset, 0)));
    }
    
    fn build_tree(&self, dataset: &Dataset, depth: usize) -> TreeNode {
        let samples = dataset.features.len();
        
        // æ£€æŸ¥åœæ­¢æ¡ä»¶
        if depth >= self.max_depth || samples < self.min_samples_split {
            return TreeNode::Leaf {
                prediction: self.calculate_leaf_prediction(dataset),
                samples,
            };
        }
        
        // å¯»æ‰¾æœ€ä½³åˆ†å‰²
        if let Some((best_feature, best_threshold, best_gain)) = self.find_best_split(dataset) {
            if best_gain > 0.0 {
                let (left_dataset, right_dataset) = self.split_dataset(dataset, best_feature, best_threshold);
                
                let left_node = self.build_tree(&left_dataset, depth + 1);
                let right_node = self.build_tree(&right_dataset, depth + 1);
                
                return TreeNode::Split {
                    feature_index: best_feature,
                    threshold: best_threshold,
                    left: Box::new(left_node),
                    right: Box::new(right_node),
                    samples,
                };
            }
        }
        
        // æ— æ³•åˆ†å‰²ï¼Œåˆ›å»ºå¶å­èŠ‚ç‚¹
        TreeNode::Leaf {
            prediction: self.calculate_leaf_prediction(dataset),
            samples,
        }
    }
    
    fn find_best_split(&self, dataset: &Dataset) -> Option<(usize, f64, f64)> {
        let mut best_gain = 0.0;
        let mut best_feature = 0;
        let mut best_threshold = 0.0;
        
        let parent_entropy = self.calculate_entropy(&dataset.targets);
        
        for feature_index in 0..dataset.features[0].len() {
            let mut unique_values: Vec<f64> = dataset.features.iter()
                .map(|f| f[feature_index])
                .collect();
            unique_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
            unique_values.dedup();
            
            for &threshold in &unique_values {
                let (left_dataset, right_dataset) = self.split_dataset(dataset, feature_index, threshold);
                
                if left_dataset.features.is_empty() || right_dataset.features.is_empty() {
                    continue;
                }
                
                let left_entropy = self.calculate_entropy(&left_dataset.targets);
                let right_entropy = self.calculate_entropy(&right_dataset.targets);
                
                let left_weight = left_dataset.features.len() as f64 / dataset.features.len() as f64;
                let right_weight = right_dataset.features.len() as f64 / dataset.features.len() as f64;
                
                let information_gain = parent_entropy - (left_weight * left_entropy + right_weight * right_entropy);
                
                if information_gain > best_gain {
                    best_gain = information_gain;
                    best_feature = feature_index;
                    best_threshold = threshold;
                }
            }
        }
        
        if best_gain > 0.0 {
            Some((best_feature, best_threshold, best_gain))
        } else {
            None
        }
    }
    
    fn split_dataset(&self, dataset: &Dataset, feature_index: usize, threshold: f64) -> (Dataset, Dataset) {
        let mut left_features = Vec::new();
        let mut left_targets = Vec::new();
        let mut right_features = Vec::new();
        let mut right_targets = Vec::new();
        
        for (features, target) in dataset.features.iter().zip(dataset.targets.iter()) {
            if features[feature_index] <= threshold {
                left_features.push(features.clone());
                left_targets.push(*target);
            } else {
                right_features.push(features.clone());
                right_targets.push(*target);
            }
        }
        
        (Dataset::new(left_features, left_targets), Dataset::new(right_features, right_targets))
    }
    
    fn calculate_entropy(&self, targets: &[f64]) -> f64 {
        let mut class_counts: HashMap<i64, usize> = HashMap::new();
        
        for &target in targets {
            let class = target.round() as i64;
            *class_counts.entry(class).or_insert(0) += 1;
        }
        
        let mut entropy = 0.0;
        let total = targets.len() as f64;
        
        for count in class_counts.values() {
            let probability = *count as f64 / total;
            if probability > 0.0 {
                entropy -= probability * probability.log2();
            }
        }
        
        entropy
    }
    
    fn calculate_leaf_prediction(&self, dataset: &Dataset) -> f64 {
        // å¯¹äºå›å½’é—®é¢˜ï¼Œè¿”å›å¹³å‡å€¼
        dataset.targets.iter().sum::<f64>() / dataset.targets.len() as f64
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        if let Some(ref root) = self.root {
            self.predict_node(root, features)
        } else {
            0.0
        }
    }
    
    fn predict_node(&self, node: &TreeNode, features: &[f64]) -> f64 {
        match node {
            TreeNode::Leaf { prediction, .. } => *prediction,
            TreeNode::Split { feature_index, threshold, left, right, .. } => {
                if features[*feature_index] <= *threshold {
                    self.predict_node(left, features)
                } else {
                    self.predict_node(right, features)
                }
            }
        }
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
}
```

### 4.3 ç¥ç»ç½‘ç»œå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct NeuralNetwork {
    pub layers: Vec<Layer>,
    pub learning_rate: f64,
    pub batch_size: usize,
    pub epochs: usize,
}

#[derive(Debug, Clone)]
pub struct Layer {
    pub neurons: Vec<Neuron>,
    pub activation: ActivationFunction,
}

#[derive(Debug, Clone)]
pub struct Neuron {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub delta: f64,
}

#[derive(Debug, Clone)]
pub enum ActivationFunction {
    Sigmoid,
    ReLU,
    Tanh,
    Linear,
}

impl NeuralNetwork {
    pub fn new(architecture: Vec<usize>, learning_rate: f64, batch_size: usize, epochs: usize) -> Self {
        let mut layers = Vec::new();
        
        for i in 0..architecture.len() - 1 {
            let layer_size = architecture[i + 1];
            let input_size = architecture[i];
            
            let mut neurons = Vec::new();
            for _ in 0..layer_size {
                let weights = (0..input_size).map(|_| rand::random::<f64>() * 2.0 - 1.0).collect();
                neurons.push(Neuron {
                    weights,
                    bias: rand::random::<f64>() * 2.0 - 1.0,
                    delta: 0.0,
                });
            }
            
            let activation = if i == architecture.len() - 2 {
                ActivationFunction::Linear // è¾“å‡ºå±‚ä½¿ç”¨çº¿æ€§æ¿€æ´»
            } else {
                ActivationFunction::ReLU // éšè—å±‚ä½¿ç”¨ReLU
            };
            
            layers.push(Layer { neurons, activation });
        }
        
        NeuralNetwork {
            layers,
            learning_rate,
            batch_size,
            epochs,
        }
    }
    
    pub fn train(&mut self, dataset: &Dataset) -> Vec<f64> {
        let mut loss_history = Vec::new();
        
        for epoch in 0..self.epochs {
            let mut epoch_loss = 0.0;
            let batch_count = (dataset.features.len() + self.batch_size - 1) / self.batch_size;
            
            for batch in 0..batch_count {
                let start = batch * self.batch_size;
                let end = std::cmp::min(start + self.batch_size, dataset.features.len());
                
                let batch_features = &dataset.features[start..end];
                let batch_targets = &dataset.targets[start..end];
                
                let batch_loss = self.train_batch(batch_features, batch_targets);
                epoch_loss += batch_loss;
            }
            
            epoch_loss /= batch_count as f64;
            loss_history.push(epoch_loss);
            
            if epoch % 100 == 0 {
                println!("Epoch {}, Loss: {:.6}", epoch, epoch_loss);
            }
        }
        
        loss_history
    }
    
    fn train_batch(&mut self, features: &[Vec<f64>], targets: &[f64]) -> f64 {
        let mut total_loss = 0.0;
        
        // å‰å‘ä¼ æ’­
        for (feature, target) in features.iter().zip(targets.iter()) {
            let prediction = self.forward_pass(feature);
            let loss = 0.5 * (prediction - target).powi(2);
            total_loss += loss;
            
            // åå‘ä¼ æ’­
            self.backward_pass(feature, target);
        }
        
        // æ›´æ–°æƒé‡
        self.update_weights();
        
        total_loss / features.len() as f64
    }
    
    fn forward_pass(&mut self, input: &[f64]) -> f64 {
        let mut current_input = input.to_vec();
        
        for layer in &mut self.layers {
            let mut layer_output = Vec::new();
            
            for neuron in &mut layer.neurons {
                let mut sum = neuron.bias;
                for (i, &input_val) in current_input.iter().enumerate() {
                    sum += neuron.weights[i] * input_val;
                }
                
                let output = self.activate(sum, &layer.activation);
                layer_output.push(output);
            }
            
            current_input = layer_output;
        }
        
        current_input[0] // å‡è®¾è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªç¥ç»å…ƒ
    }
    
    fn backward_pass(&mut self, input: &[f64], target: &f64) {
        // è®¡ç®—è¾“å‡ºå±‚çš„è¯¯å·®
        let mut current_input = input.to_vec();
        let mut layer_outputs = vec![current_input.clone()];
        
        // å‰å‘ä¼ æ’­å¹¶ä¿å­˜ä¸­é—´ç»“æœ
        for layer in &mut self.layers {
            let mut layer_output = Vec::new();
            
            for neuron in &mut layer.neurons {
                let mut sum = neuron.bias;
                for (i, &input_val) in current_input.iter().enumerate() {
                    sum += neuron.weights[i] * input_val;
                }
                
                let output = self.activate(sum, &layer.activation);
                layer_output.push(output);
            }
            
            current_input = layer_output.clone();
            layer_outputs.push(layer_output);
        }
        
        // åå‘ä¼ æ’­è¯¯å·®
        let prediction = current_input[0];
        let output_error = prediction - target;
        
        for (layer_index, layer) in self.layers.iter_mut().enumerate().rev() {
            let layer_output = &layer_outputs[layer_index + 1];
            let prev_layer_output = &layer_outputs[layer_index];
            
            for (neuron_index, neuron) in layer.neurons.iter_mut().enumerate() {
                let output = layer_output[neuron_index];
                let derivative = self.activate_derivative(output, &layer.activation);
                
                if layer_index == self.layers.len() - 1 {
                    // è¾“å‡ºå±‚
                    neuron.delta = output_error * derivative;
                } else {
                    // éšè—å±‚
                    let mut error = 0.0;
                    for next_neuron in &self.layers[layer_index + 1].neurons {
                        error += next_neuron.delta * next_neuron.weights[neuron_index];
                    }
                    neuron.delta = error * derivative;
                }
                
                // æ›´æ–°æƒé‡æ¢¯åº¦
                for (weight_index, &input_val) in prev_layer_output.iter().enumerate() {
                    neuron.weights[weight_index] -= self.learning_rate * neuron.delta * input_val;
                }
                neuron.bias -= self.learning_rate * neuron.delta;
            }
        }
    }
    
    fn update_weights(&mut self) {
        // æƒé‡æ›´æ–°å·²åœ¨åå‘ä¼ æ’­ä¸­å®Œæˆ
    }
    
    fn activate(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => 1.0 / (1.0 + (-x).exp()),
            ActivationFunction::ReLU => x.max(0.0),
            ActivationFunction::Tanh => x.tanh(),
            ActivationFunction::Linear => x,
        }
    }
    
    fn activate_derivative(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => x * (1.0 - x),
            ActivationFunction::ReLU => if x > 0.0 { 1.0 } else { 0.0 },
            ActivationFunction::Tanh => 1.0 - x.powi(2),
            ActivationFunction::Linear => 1.0,
        }
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        self.forward_pass(features)
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
    
    pub fn evaluate(&self, dataset: &Dataset) -> f64 {
        let predictions = self.predict_batch(&dataset.features);
        let mut mse = 0.0;
        
        for (prediction, target) in predictions.iter().zip(dataset.targets.iter()) {
            mse += (prediction - target).powi(2);
        }
        
        mse / dataset.features.len() as f64
    }
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æ·±åº¦å­¦ä¹ ç†è®º](../02_Deep_Learning/01_Deep_Learning_Theory.md)
- [å¼ºåŒ–å­¦ä¹ ç†è®º](../03_Reinforcement_Learning/01_Reinforcement_Learning_Theory.md)
- [è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º](../04_Natural_Language_Processing/01_Natural_Language_Processing_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
3. Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0

## æ‰¹åˆ¤æ€§åˆ†æ

### ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†

æœºå™¨å­¦ä¹ ç†è®ºå…³æ³¨ç®—æ³•è®¾è®¡ã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹ä¼˜åŒ–ï¼Œæ˜¯äººå·¥æ™ºèƒ½å’Œæ•°æ®åˆ†æçš„é‡è¦åŸºç¡€ã€‚

### ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ

ä¼˜ç‚¹ï¼šæä¾›äº†ç³»ç»ŸåŒ–çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ”¯æŒå¤æ‚æ™ºèƒ½ç³»ç»Ÿçš„æ„å»ºã€‚
ç¼ºç‚¹ï¼šæ¨¡å‹å¤æ‚æ€§çš„å¢åŠ ï¼Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ï¼Œå¯¹æ–°å…´å­¦ä¹ æŠ€æœ¯çš„é€‚åº”æ€§éœ€è¦æŒç»­æ”¹è¿›ã€‚

### ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ

- ä¸æ•°å­¦åŸºç¡€åœ¨ç®—æ³•å»ºæ¨¡ã€ä¼˜åŒ–ç†è®ºç­‰é¢†åŸŸæœ‰åº”ç”¨ã€‚
- ä¸ç±»å‹ç†è®ºåœ¨æ¨¡å‹æŠ½è±¡ã€æ¥å£è®¾è®¡ç­‰æ–¹é¢æœ‰åˆ›æ–°åº”ç”¨ã€‚
- ä¸äººå·¥æ™ºèƒ½ç†è®ºåœ¨æ™ºèƒ½å­¦ä¹ ã€è‡ªé€‚åº”ä¼˜åŒ–ç­‰æ–¹é¢æœ‰æ–°å…´èåˆã€‚
- ä¸æ§åˆ¶è®ºåœ¨å­¦ä¹ æ§åˆ¶ã€åé¦ˆæœºåˆ¶ç­‰æ–¹é¢äº’è¡¥ã€‚

### åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›

æœªæ¥æœºå™¨å­¦ä¹ ç†è®ºéœ€åŠ å¼ºä¸æ•°å­¦åŸºç¡€ã€ç±»å‹ç†è®ºã€äººå·¥æ™ºèƒ½ç†è®ºã€æ§åˆ¶è®ºç­‰é¢†åŸŸçš„èåˆï¼Œæ¨åŠ¨æ™ºèƒ½åŒ–ã€è‡ªé€‚åº”çš„æœºå™¨å­¦ä¹ ä½“ç³»ã€‚

### å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»

- äº¤å‰ç´¢å¼•.md
- Meta/æ‰¹åˆ¤æ€§åˆ†ææ¨¡æ¿.md
