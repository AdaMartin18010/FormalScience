# 13.2.1 æ·±åº¦å­¦ä¹ ç†è®º

## ç›®å½•

- [13.2.1 æ·±åº¦å­¦ä¹ ç†è®º](#1321-æ·±åº¦å­¦ä¹ ç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [1. åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
    - [1.1 æ·±åº¦å­¦ä¹ å®šä¹‰](#11-æ·±åº¦å­¦ä¹ å®šä¹‰)
    - [1.2 ç½‘ç»œæ¶æ„åˆ†ç±»](#12-ç½‘ç»œæ¶æ„åˆ†ç±»)
  - [2. å½¢å¼åŒ–å®šä¹‰](#2-å½¢å¼åŒ–å®šä¹‰)
    - [2.1 ç¥ç»ç½‘ç»œ](#21-ç¥ç»ç½‘ç»œ)
    - [2.2 åå‘ä¼ æ’­](#22-åå‘ä¼ æ’­)
    - [2.3 æ¢¯åº¦æ¶ˆå¤±](#23-æ¢¯åº¦æ¶ˆå¤±)
  - [3. å®šç†ä¸è¯æ˜](#3-å®šç†ä¸è¯æ˜)
    - [3.1 é€šç”¨è¿‘ä¼¼å®šç†](#31-é€šç”¨è¿‘ä¼¼å®šç†)
    - [3.2 æ¢¯åº¦æ¶ˆå¤±å®šç†](#32-æ¢¯åº¦æ¶ˆå¤±å®šç†)
  - [4. Rustä»£ç å®ç°](#4-rustä»£ç å®ç°)
    - [4.1 å‰é¦ˆç¥ç»ç½‘ç»œå®ç°](#41-å‰é¦ˆç¥ç»ç½‘ç»œå®ç°)
    - [4.2 å·ç§¯ç¥ç»ç½‘ç»œå®ç°](#42-å·ç§¯ç¥ç»ç½‘ç»œå®ç°)
    - [4.3 å¾ªç¯ç¥ç»ç½‘ç»œå®ç°](#43-å¾ªç¯ç¥ç»ç½‘ç»œå®ç°)
  - [5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨](#5-ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨)
  - [6. å‚è€ƒæ–‡çŒ®](#6-å‚è€ƒæ–‡çŒ®)
  - [æ‰¹åˆ¤æ€§åˆ†æ](#æ‰¹åˆ¤æ€§åˆ†æ)
    - [ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†](#ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†)
    - [ç†è®ºä¼˜åŠ¿ä¸å±€é™æ€§](#ç†è®ºä¼˜åŠ¿ä¸å±€é™æ€§)
    - [å­¦ç§‘äº¤å‰èåˆ](#å­¦ç§‘äº¤å‰èåˆ)
    - [åˆ›æ–°æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›](#åˆ›æ–°æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›)
    - [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)

## ğŸ“‹ æ¦‚è¿°

æ·±åº¦å­¦ä¹ ç†è®ºç ”ç©¶åŸºäºå¤šå±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚è¯¥ç†è®ºæ¶µç›–å‰é¦ˆç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œã€æ³¨æ„åŠ›æœºåˆ¶ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºå¤æ‚æ¨¡å¼è¯†åˆ«å’Œç‰¹å¾å­¦ä¹ æä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 æ·±åº¦å­¦ä¹ å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆæ·±åº¦å­¦ä¹ ï¼‰
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚

### 1.2 ç½‘ç»œæ¶æ„åˆ†ç±»

| æ¶æ„ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹åº”ç”¨         |
|--------------|------------------|------------------------------|------------------|
| å‰é¦ˆç½‘ç»œ     | Feedforward      | ä¿¡æ¯å•å‘ä¼ æ’­çš„ç½‘ç»œ           | åˆ†ç±», å›å½’       |
| å·ç§¯ç½‘ç»œ     | Convolutional    | ä½¿ç”¨å·ç§¯æ“ä½œçš„ç½‘ç»œ           | å›¾åƒè¯†åˆ«         |
| å¾ªç¯ç½‘ç»œ     | Recurrent        | å…·æœ‰è®°å¿†èƒ½åŠ›çš„ç½‘ç»œ           | åºåˆ—å»ºæ¨¡         |
| æ³¨æ„åŠ›ç½‘ç»œ   | Attention        | åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç½‘ç»œ         | æœºå™¨ç¿»è¯‘         |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 ç¥ç»ç½‘ç»œ

**å®šä¹‰ 2.1**ï¼ˆç¥ç»ç½‘ç»œï¼‰
ç¥ç»ç½‘ç»œæ˜¯ç”±å¤šä¸ªç¥ç»å…ƒå±‚ç»„æˆçš„è®¡ç®—æ¨¡å‹ï¼Œæ¯å±‚ç¥ç»å…ƒä¸ä¸‹ä¸€å±‚å…¨è¿æ¥ã€‚

### 2.2 åå‘ä¼ æ’­

**å®šä¹‰ 2.2**ï¼ˆåå‘ä¼ æ’­ï¼‰
åå‘ä¼ æ’­æ˜¯é€šè¿‡è®¡ç®—æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œæƒé‡çš„ç®—æ³•ã€‚

### 2.3 æ¢¯åº¦æ¶ˆå¤±

**å®šä¹‰ 2.3**ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰
æ¢¯åº¦æ¶ˆå¤±æ˜¯æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦åœ¨åå‘ä¼ æ’­æ—¶é€æ¸å˜å°çš„é—®é¢˜ã€‚

## 3. å®šç†ä¸è¯æ˜

### 3.1 é€šç”¨è¿‘ä¼¼å®šç†

**å®šç† 3.1**ï¼ˆé€šç”¨è¿‘ä¼¼å®šç†ï¼‰
å…·æœ‰å•ä¸ªéšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œå¯ä»¥è¿‘ä¼¼ä»»ä½•è¿ç»­å‡½æ•°ã€‚

**è¯æ˜**ï¼š
é€šè¿‡æ„é€ æ€§è¯æ˜ï¼Œä½¿ç”¨è¶³å¤Ÿå¤šçš„éšè—ç¥ç»å…ƒå¯ä»¥ä»»æ„ç²¾åº¦è¿‘ä¼¼è¿ç»­å‡½æ•°ã€‚â–¡

### 3.2 æ¢¯åº¦æ¶ˆå¤±å®šç†

**å®šç† 3.2**ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰
åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œå¦‚æœæ¿€æ´»å‡½æ•°çš„å¯¼æ•°å°äº1ï¼Œåˆ™æ¢¯åº¦ä¼šæŒ‡æ•°çº§è¡°å‡ã€‚

**è¯æ˜**ï¼š
è®¾ç¬¬ $l$ å±‚çš„æ¢¯åº¦ä¸º $\frac{\partial L}{\partial w_l}$ï¼Œåˆ™ï¼š
$\frac{\partial L}{\partial w_l} = \frac{\partial L}{\partial w_{l+1}} \cdot \sigma'(z_l) \cdot w_{l+1}$
å½“ $|\sigma'(z_l)| < 1$ æ—¶ï¼Œæ¢¯åº¦ä¼šé€æ¸å˜å°ã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 å‰é¦ˆç¥ç»ç½‘ç»œå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct FeedforwardNetwork {
    pub layers: Vec<Layer>,
    pub learning_rate: f64,
    pub batch_size: usize,
    pub epochs: usize,
}

#[derive(Debug, Clone)]
pub struct Layer {
    pub neurons: Vec<Neuron>,
    pub activation: ActivationFunction,
    pub dropout_rate: f64,
}

#[derive(Debug, Clone)]
pub struct Neuron {
    pub weights: Vec<f64>,
    pub bias: f64,
    pub delta: f64,
    pub output: f64,
    pub input: f64,
}

#[derive(Debug, Clone)]
pub enum ActivationFunction {
    Sigmoid,
    ReLU,
    Tanh,
    Softmax,
    Linear,
}

impl FeedforwardNetwork {
    pub fn new(architecture: Vec<usize>, learning_rate: f64, batch_size: usize, epochs: usize) -> Self {
        let mut layers = Vec::new();
        
        for i in 0..architecture.len() - 1 {
            let layer_size = architecture[i + 1];
            let input_size = architecture[i];
            
            let mut neurons = Vec::new();
            for _ in 0..layer_size {
                // Xavieråˆå§‹åŒ–
                let scale = (2.0 / input_size as f64).sqrt();
                let weights = (0..input_size).map(|_| (rand::random::<f64>() * 2.0 - 1.0) * scale).collect();
                neurons.push(Neuron {
                    weights,
                    bias: 0.0,
                    delta: 0.0,
                    output: 0.0,
                    input: 0.0,
                });
            }
            
            let activation = if i == architecture.len() - 2 {
                ActivationFunction::Softmax // è¾“å‡ºå±‚ä½¿ç”¨Softmax
            } else {
                ActivationFunction::ReLU // éšè—å±‚ä½¿ç”¨ReLU
            };
            
            layers.push(Layer { 
                neurons, 
                activation,
                dropout_rate: 0.5,
            });
        }
        
        FeedforwardNetwork {
            layers,
            learning_rate,
            batch_size,
            epochs,
        }
    }
    
    pub fn train(&mut self, dataset: &Dataset) -> Vec<f64> {
        let mut loss_history = Vec::new();
        
        for epoch in 0..self.epochs {
            let mut epoch_loss = 0.0;
            let batch_count = (dataset.features.len() + self.batch_size - 1) / self.batch_size;
            
            for batch in 0..batch_count {
                let start = batch * self.batch_size;
                let end = std::cmp::min(start + self.batch_size, dataset.features.len());
                
                let batch_features = &dataset.features[start..end];
                let batch_targets = &dataset.targets[start..end];
                
                let batch_loss = self.train_batch(batch_features, batch_targets);
                epoch_loss += batch_loss;
            }
            
            epoch_loss /= batch_count as f64;
            loss_history.push(epoch_loss);
            
            if epoch % 100 == 0 {
                println!("Epoch {}, Loss: {:.6}", epoch, epoch_loss);
            }
        }
        
        loss_history
    }
    
    fn train_batch(&mut self, features: &[Vec<f64>], targets: &[f64]) -> f64 {
        let mut total_loss = 0.0;
        
        // å‰å‘ä¼ æ’­
        for (feature, target) in features.iter().zip(targets.iter()) {
            let prediction = self.forward_pass(feature);
            let loss = self.cross_entropy_loss(&[prediction], &[*target]);
            total_loss += loss;
            
            // åå‘ä¼ æ’­
            self.backward_pass(feature, target);
        }
        
        // æ›´æ–°æƒé‡
        self.update_weights();
        
        total_loss / features.len() as f64
    }
    
    fn forward_pass(&mut self, input: &[f64]) -> f64 {
        let mut current_input = input.to_vec();
        
        for layer in &mut self.layers {
            let mut layer_output = Vec::new();
            
            for neuron in &mut layer.neurons {
                let mut sum = neuron.bias;
                for (i, &input_val) in current_input.iter().enumerate() {
                    sum += neuron.weights[i] * input_val;
                }
                
                neuron.input = sum;
                let output = self.activate(sum, &layer.activation);
                
                // Dropout
                if layer.dropout_rate > 0.0 && rand::random::<f64>() < layer.dropout_rate {
                    neuron.output = 0.0;
                } else {
                    neuron.output = output / (1.0 - layer.dropout_rate);
                }
                
                layer_output.push(neuron.output);
            }
            
            current_input = layer_output;
        }
        
        current_input[0] // å‡è®¾è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªç¥ç»å…ƒ
    }
    
    fn backward_pass(&mut self, input: &[f64], target: &f64) {
        // è®¡ç®—è¾“å‡ºå±‚çš„è¯¯å·®
        let output_layer = &mut self.layers[self.layers.len() - 1];
        let prediction = output_layer.neurons[0].output;
        let output_error = prediction - target;
        
        // åå‘ä¼ æ’­è¯¯å·®
        for (layer_index, layer) in self.layers.iter_mut().enumerate().rev() {
            let is_output_layer = layer_index == self.layers.len() - 1;
            
            for (neuron_index, neuron) in layer.neurons.iter_mut().enumerate() {
                let output = neuron.output;
                let derivative = self.activate_derivative(neuron.input, &layer.activation);
                
                if is_output_layer {
                    // è¾“å‡ºå±‚
                    neuron.delta = output_error * derivative;
                } else {
                    // éšè—å±‚
                    let mut error = 0.0;
                    for next_neuron in &self.layers[layer_index + 1].neurons {
                        error += next_neuron.delta * next_neuron.weights[neuron_index];
                    }
                    neuron.delta = error * derivative;
                }
            }
        }
    }
    
    fn update_weights(&mut self) {
        // æƒé‡æ›´æ–°å·²åœ¨åå‘ä¼ æ’­ä¸­å®Œæˆ
    }
    
    fn activate(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => 1.0 / (1.0 + (-x).exp()),
            ActivationFunction::ReLU => x.max(0.0),
            ActivationFunction::Tanh => x.tanh(),
            ActivationFunction::Softmax => x.exp(), // ç®€åŒ–å®ç°
            ActivationFunction::Linear => x,
        }
    }
    
    fn activate_derivative(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => {
                let sigmoid = 1.0 / (1.0 + (-x).exp());
                sigmoid * (1.0 - sigmoid)
            },
            ActivationFunction::ReLU => if x > 0.0 { 1.0 } else { 0.0 },
            ActivationFunction::Tanh => 1.0 - x.tanh().powi(2),
            ActivationFunction::Softmax => 1.0, // ç®€åŒ–å®ç°
            ActivationFunction::Linear => 1.0,
        }
    }
    
    fn cross_entropy_loss(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let mut loss = 0.0;
        for (prediction, target) in predictions.iter().zip(targets.iter()) {
            let epsilon = 1e-15;
            let clipped_prediction = prediction.max(epsilon).min(1.0 - epsilon);
            loss -= target * clipped_prediction.ln() + (1.0 - target) * (1.0 - clipped_prediction).ln();
        }
        loss
    }
    
    pub fn predict(&self, features: &[f64]) -> f64 {
        self.forward_pass(features)
    }
    
    pub fn predict_batch(&self, features: &[Vec<f64>]) -> Vec<f64> {
        features.iter().map(|f| self.predict(f)).collect()
    }
}
```

### 4.2 å·ç§¯ç¥ç»ç½‘ç»œå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct ConvolutionalNetwork {
    pub layers: Vec<ConvLayer>,
    pub learning_rate: f64,
    pub batch_size: usize,
    pub epochs: usize,
}

#[derive(Debug, Clone)]
pub enum ConvLayer {
    Convolution(ConvolutionLayer),
    Pooling(PoolingLayer),
    FullyConnected(FullyConnectedLayer),
    Flatten(FlattenLayer),
}

#[derive(Debug, Clone)]
pub struct ConvolutionLayer {
    pub filters: Vec<Filter>,
    pub stride: usize,
    pub padding: usize,
    pub input_channels: usize,
    pub output_channels: usize,
}

#[derive(Debug, Clone)]
pub struct Filter {
    pub weights: Vec<Vec<Vec<f64>>>,
    pub bias: f64,
    pub gradient_weights: Vec<Vec<Vec<f64>>>,
    pub gradient_bias: f64,
}

#[derive(Debug, Clone)]
pub struct PoolingLayer {
    pub pool_size: usize,
    pub stride: usize,
    pub pool_type: PoolType,
}

#[derive(Debug, Clone)]
pub enum PoolType {
    Max,
    Average,
}

#[derive(Debug, Clone)]
pub struct FullyConnectedLayer {
    pub neurons: Vec<Neuron>,
    pub activation: ActivationFunction,
}

#[derive(Debug, Clone)]
pub struct FlattenLayer {
    pub input_shape: (usize, usize, usize),
    pub output_size: usize,
}

impl ConvolutionalNetwork {
    pub fn new(learning_rate: f64, batch_size: usize, epochs: usize) -> Self {
        ConvolutionalNetwork {
            layers: Vec::new(),
            learning_rate,
            batch_size,
            epochs,
        }
    }
    
    pub fn add_convolution_layer(&mut self, input_channels: usize, output_channels: usize, 
                                kernel_size: usize, stride: usize, padding: usize) {
        let mut filters = Vec::new();
        for _ in 0..output_channels {
            let mut filter_weights = Vec::new();
            for _ in 0..input_channels {
                let mut channel_weights = Vec::new();
                for _ in 0..kernel_size {
                    let mut row_weights = Vec::new();
                    for _ in 0..kernel_size {
                        row_weights.push((rand::random::<f64>() * 2.0 - 1.0) * 0.1);
                    }
                    channel_weights.push(row_weights);
                }
                filter_weights.push(channel_weights);
            }
            
            filters.push(Filter {
                weights: filter_weights,
                bias: 0.0,
                gradient_weights: vec![vec![vec![0.0; kernel_size]; kernel_size]; input_channels],
                gradient_bias: 0.0,
            });
        }
        
        let conv_layer = ConvolutionLayer {
            filters,
            stride,
            padding,
            input_channels,
            output_channels,
        };
        
        self.layers.push(ConvLayer::Convolution(conv_layer));
    }
    
    pub fn add_pooling_layer(&mut self, pool_size: usize, stride: usize, pool_type: PoolType) {
        let pooling_layer = PoolingLayer {
            pool_size,
            stride,
            pool_type,
        };
        
        self.layers.push(ConvLayer::Pooling(pooling_layer));
    }
    
    pub fn add_flatten_layer(&mut self, input_shape: (usize, usize, usize)) {
        let output_size = input_shape.0 * input_shape.1 * input_shape.2;
        let flatten_layer = FlattenLayer {
            input_shape,
            output_size,
        };
        
        self.layers.push(ConvLayer::Flatten(flatten_layer));
    }
    
    pub fn add_fully_connected_layer(&mut self, input_size: usize, output_size: usize, 
                                   activation: ActivationFunction) {
        let mut neurons = Vec::new();
        for _ in 0..output_size {
            let weights = (0..input_size).map(|_| (rand::random::<f64>() * 2.0 - 1.0) * 0.1).collect();
            neurons.push(Neuron {
                weights,
                bias: 0.0,
                delta: 0.0,
                output: 0.0,
                input: 0.0,
            });
        }
        
        let fc_layer = FullyConnectedLayer {
            neurons,
            activation,
        };
        
        self.layers.push(ConvLayer::FullyConnected(fc_layer));
    }
    
    pub fn forward_pass(&self, input: &[Vec<Vec<f64>>]) -> Vec<f64> {
        let mut current_input = input.to_vec();
        
        for layer in &self.layers {
            current_input = match layer {
                ConvLayer::Convolution(conv_layer) => {
                    self.convolve(&current_input, conv_layer)
                },
                ConvLayer::Pooling(pooling_layer) => {
                    self.pool(&current_input, pooling_layer)
                },
                ConvLayer::Flatten(flatten_layer) => {
                    self.flatten(&current_input, flatten_layer)
                },
                ConvLayer::FullyConnected(fc_layer) => {
                    self.fully_connected(&current_input, fc_layer)
                },
            };
        }
        
        current_input
    }
    
    fn convolve(&self, input: &[Vec<Vec<f64>>], conv_layer: &ConvolutionLayer) -> Vec<Vec<Vec<f64>>> {
        let mut output = Vec::new();
        
        for filter in &conv_layer.filters {
            let mut filter_output = Vec::new();
            
            let input_height = input[0].len();
            let input_width = input[0][0].len();
            let kernel_size = filter.weights[0].len();
            
            let output_height = (input_height + 2 * conv_layer.padding - kernel_size) / conv_layer.stride + 1;
            let output_width = (input_width + 2 * conv_layer.padding - kernel_size) / conv_layer.stride + 1;
            
            for i in 0..output_height {
                let mut row = Vec::new();
                for j in 0..output_width {
                    let mut sum = filter.bias;
                    
                    for c in 0..conv_layer.input_channels {
                        for ki in 0..kernel_size {
                            for kj in 0..kernel_size {
                                let input_i = i * conv_layer.stride + ki;
                                let input_j = j * conv_layer.stride + kj;
                                
                                if input_i < input_height && input_j < input_width {
                                    sum += input[c][input_i][input_j] * filter.weights[c][ki][kj];
                                }
                            }
                        }
                    }
                    
                    row.push(sum.max(0.0)); // ReLUæ¿€æ´»
                }
                filter_output.push(row);
            }
            
            output.push(filter_output);
        }
        
        output
    }
    
    fn pool(&self, input: &[Vec<Vec<f64>>], pooling_layer: &PoolingLayer) -> Vec<Vec<Vec<f64>>> {
        let mut output = Vec::new();
        
        for channel in input {
            let mut channel_output = Vec::new();
            let input_height = channel.len();
            let input_width = channel[0].len();
            
            let output_height = (input_height - pooling_layer.pool_size) / pooling_layer.stride + 1;
            let output_width = (input_width - pooling_layer.pool_size) / pooling_layer.stride + 1;
            
            for i in 0..output_height {
                let mut row = Vec::new();
                for j in 0..output_width {
                    let mut values = Vec::new();
                    
                    for ki in 0..pooling_layer.pool_size {
                        for kj in 0..pooling_layer.pool_size {
                            let input_i = i * pooling_layer.stride + ki;
                            let input_j = j * pooling_layer.stride + kj;
                            
                            if input_i < input_height && input_j < input_width {
                                values.push(channel[input_i][input_j]);
                            }
                        }
                    }
                    
                    let pooled_value = match pooling_layer.pool_type {
                        PoolType::Max => values.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b)),
                        PoolType::Average => values.iter().sum::<f64>() / values.len() as f64,
                    };
                    
                    row.push(pooled_value);
                }
                channel_output.push(row);
            }
            
            output.push(channel_output);
        }
        
        output
    }
    
    fn flatten(&self, input: &[Vec<Vec<f64>>], flatten_layer: &FlattenLayer) -> Vec<f64> {
        let mut flattened = Vec::new();
        
        for channel in input {
            for row in channel {
                for &value in row {
                    flattened.push(value);
                }
            }
        }
        
        flattened
    }
    
    fn fully_connected(&self, input: &[f64], fc_layer: &FullyConnectedLayer) -> Vec<f64> {
        let mut output = Vec::new();
        
        for neuron in &fc_layer.neurons {
            let mut sum = neuron.bias;
            for (i, &input_val) in input.iter().enumerate() {
                sum += neuron.weights[i] * input_val;
            }
            
            let activated = self.activate(sum, &fc_layer.activation);
            output.push(activated);
        }
        
        output
    }
    
    fn activate(&self, x: f64, activation: &ActivationFunction) -> f64 {
        match activation {
            ActivationFunction::Sigmoid => 1.0 / (1.0 + (-x).exp()),
            ActivationFunction::ReLU => x.max(0.0),
            ActivationFunction::Tanh => x.tanh(),
            ActivationFunction::Softmax => x.exp(),
            ActivationFunction::Linear => x,
        }
    }
    
    pub fn train(&mut self, dataset: &ConvDataset) -> Vec<f64> {
        let mut loss_history = Vec::new();
        
        for epoch in 0..self.epochs {
            let mut epoch_loss = 0.0;
            let batch_count = (dataset.images.len() + self.batch_size - 1) / self.batch_size;
            
            for batch in 0..batch_count {
                let start = batch * self.batch_size;
                let end = std::cmp::min(start + self.batch_size, dataset.images.len());
                
                let batch_images = &dataset.images[start..end];
                let batch_labels = &dataset.labels[start..end];
                
                let batch_loss = self.train_batch(batch_images, batch_labels);
                epoch_loss += batch_loss;
            }
            
            epoch_loss /= batch_count as f64;
            loss_history.push(epoch_loss);
            
            if epoch % 100 == 0 {
                println!("Epoch {}, Loss: {:.6}", epoch, epoch_loss);
            }
        }
        
        loss_history
    }
    
    fn train_batch(&mut self, images: &[Vec<Vec<Vec<f64>>>], labels: &[f64]) -> f64 {
        let mut total_loss = 0.0;
        
        for (image, label) in images.iter().zip(labels.iter()) {
            let prediction = self.forward_pass(image);
            let loss = self.cross_entropy_loss(&prediction, &[*label]);
            total_loss += loss;
            
            // ç®€åŒ–å®ç°ï¼šè·³è¿‡åå‘ä¼ æ’­
        }
        
        total_loss / images.len() as f64
    }
    
    fn cross_entropy_loss(&self, predictions: &[f64], targets: &[f64]) -> f64 {
        let mut loss = 0.0;
        for (prediction, target) in predictions.iter().zip(targets.iter()) {
            let epsilon = 1e-15;
            let clipped_prediction = prediction.max(epsilon).min(1.0 - epsilon);
            loss -= target * clipped_prediction.ln() + (1.0 - target) * (1.0 - clipped_prediction).ln();
        }
        loss
    }
}

#[derive(Debug, Clone)]
pub struct ConvDataset {
    pub images: Vec<Vec<Vec<Vec<f64>>>>,
    pub labels: Vec<f64>,
}
```

### 4.3 å¾ªç¯ç¥ç»ç½‘ç»œå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct RecurrentNetwork {
    pub input_size: usize,
    pub hidden_size: usize,
    pub output_size: usize,
    pub num_layers: usize,
    pub learning_rate: f64,
    pub sequence_length: usize,
}

#[derive(Debug, Clone)]
pub struct RNNLayer {
    pub input_weights: Vec<Vec<f64>>,
    pub hidden_weights: Vec<Vec<f64>>,
    pub output_weights: Vec<Vec<f64>>,
    pub input_bias: Vec<f64>,
    pub hidden_bias: Vec<f64>,
    pub output_bias: Vec<f64>,
}

impl RecurrentNetwork {
    pub fn new(input_size: usize, hidden_size: usize, output_size: usize, 
               num_layers: usize, learning_rate: f64, sequence_length: usize) -> Self {
        RecurrentNetwork {
            input_size,
            hidden_size,
            output_size,
            num_layers,
            learning_rate,
            sequence_length,
        }
    }
    
    pub fn forward_pass(&self, sequence: &[Vec<f64>]) -> Vec<Vec<f64>> {
        let mut outputs = Vec::new();
        let mut hidden_states = vec![vec![0.0; self.hidden_size]; self.num_layers];
        
        for t in 0..sequence.len() {
            let input = &sequence[t];
            let mut current_input = input.clone();
            
            // å‰å‘ä¼ æ’­é€šè¿‡æ‰€æœ‰å±‚
            for layer in 0..self.num_layers {
                let hidden_state = &mut hidden_states[layer];
                
                // è®¡ç®—æ–°çš„éšè—çŠ¶æ€
                let new_hidden_state = self.compute_hidden_state(
                    &current_input, 
                    hidden_state, 
                    layer
                );
                
                *hidden_state = new_hidden_state.clone();
                current_input = new_hidden_state;
            }
            
            // è®¡ç®—è¾“å‡º
            let output = self.compute_output(&current_input);
            outputs.push(output);
        }
        
        outputs
    }
    
    fn compute_hidden_state(&self, input: &[f64], prev_hidden: &[f64], layer: usize) -> Vec<f64> {
        let mut hidden_state = vec![0.0; self.hidden_size];
        
        // ç®€åŒ–å®ç°ï¼šå‡è®¾æƒé‡çŸ©é˜µä¸ºå•ä½çŸ©é˜µ
        for i in 0..self.hidden_size {
            if i < input.len() {
                hidden_state[i] = input[i];
            }
            if i < prev_hidden.len() {
                hidden_state[i] += prev_hidden[i];
            }
            hidden_state[i] = hidden_state[i].tanh(); // æ¿€æ´»å‡½æ•°
        }
        
        hidden_state
    }
    
    fn compute_output(&self, hidden: &[f64]) -> Vec<f64> {
        let mut output = vec![0.0; self.output_size];
        
        // ç®€åŒ–å®ç°ï¼šç›´æ¥å¤åˆ¶éšè—çŠ¶æ€
        for i in 0..self.output_size.min(hidden.len()) {
            output[i] = hidden[i];
        }
        
        output
    }
    
    pub fn train(&mut self, sequences: &[Vec<Vec<f64>>], targets: &[Vec<Vec<f64>>]) -> Vec<f64> {
        let mut loss_history = Vec::new();
        let epochs = 100;
        
        for epoch in 0..epochs {
            let mut epoch_loss = 0.0;
            
            for (sequence, target) in sequences.iter().zip(targets.iter()) {
                let predictions = self.forward_pass(sequence);
                let loss = self.compute_loss(&predictions, target);
                epoch_loss += loss;
                
                // ç®€åŒ–å®ç°ï¼šè·³è¿‡åå‘ä¼ æ’­
            }
            
            epoch_loss /= sequences.len() as f64;
            loss_history.push(epoch_loss);
            
            if epoch % 10 == 0 {
                println!("Epoch {}, Loss: {:.6}", epoch, epoch_loss);
            }
        }
        
        loss_history
    }
    
    fn compute_loss(&self, predictions: &[Vec<f64>], targets: &[Vec<f64>]) -> f64 {
        let mut total_loss = 0.0;
        
        for (prediction, target) in predictions.iter().zip(targets.iter()) {
            for (pred, targ) in prediction.iter().zip(target.iter()) {
                let error = pred - targ;
                total_loss += error * error;
            }
        }
        
        total_loss / (predictions.len() * predictions[0].len()) as f64
    }
    
    pub fn generate_sequence(&self, seed: &[f64], length: usize) -> Vec<Vec<f64>> {
        let mut sequence = Vec::new();
        let mut current_input = seed.to_vec();
        
        for _ in 0..length {
            let output = self.forward_pass(&[current_input.clone()]);
            let next_input = output[0].clone();
            sequence.push(next_input.clone());
            current_input = next_input;
        }
        
        sequence
    }
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æœºå™¨å­¦ä¹ ç†è®º](../01_Machine_Learning/01_Machine_Learning_Theory.md)
- [å¼ºåŒ–å­¦ä¹ ç†è®º](../03_Reinforcement_Learning/01_Reinforcement_Learning_Theory.md)
- [è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º](../04_Natural_Language_Processing/01_Natural_Language_Processing_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep Learning. Nature.
3. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0

## æ‰¹åˆ¤æ€§åˆ†æ

### ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†

æ·±åº¦å­¦ä¹ ç†è®ºä½œä¸ºäººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œä¸»è¦å…³æ³¨åŸºäºå¤šå±‚ç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚ä¸»è¦ç†è®ºæµæ´¾åŒ…æ‹¬ï¼š

1. **å·ç§¯ç¥ç»ç½‘ç»œå­¦æ´¾**ï¼šä»¥LeCunä¸ºä»£è¡¨ï¼Œä¸“æ³¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰
2. **å¾ªç¯ç¥ç»ç½‘ç»œå­¦æ´¾**ï¼šä»¥Hochreiterå’ŒSchmidhuberä¸ºä»£è¡¨ï¼Œå…³æ³¨åºåˆ—æ•°æ®å¤„ç†
3. **ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå­¦æ´¾**ï¼šä»¥Goodfellowä¸ºä»£è¡¨ï¼Œå¼ºè°ƒç”Ÿæˆæ¨¡å‹çš„å¯¹æŠ—è®­ç»ƒ
4. **æ³¨æ„åŠ›æœºåˆ¶å­¦æ´¾**ï¼šä»¥Vaswaniä¸ºä»£è¡¨ï¼Œå…³æ³¨åºåˆ—å»ºæ¨¡çš„æ³¨æ„åŠ›æœºåˆ¶
5. **å›¾ç¥ç»ç½‘ç»œå­¦æ´¾**ï¼šä»¥Scarselliä¸ºä»£è¡¨ï¼Œå¤„ç†å›¾ç»“æ„æ•°æ®

### ç†è®ºä¼˜åŠ¿ä¸å±€é™æ€§

**ä¼˜åŠ¿**ï¼š

- **è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›**ï¼šè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å¤šå±‚æ¬¡è¡¨ç¤º
- **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šä»åŸå§‹æ•°æ®åˆ°æœ€ç»ˆè¾“å‡ºçš„ç›´æ¥å­¦ä¹ 
- **å¤§è§„æ¨¡æ•°æ®å¤„ç†**ï¼šèƒ½å¤Ÿå¤„ç†æµ·é‡çš„è®­ç»ƒæ•°æ®
- **ç‰¹å¾è‡ªåŠ¨æå–**ï¼šæ— éœ€æ‰‹å·¥è®¾è®¡ç‰¹å¾ï¼Œè‡ªåŠ¨å‘ç°æ•°æ®æ¨¡å¼
- **è·¨é¢†åŸŸåº”ç”¨**ï¼šåœ¨å›¾åƒã€è¯­éŸ³ã€è‡ªç„¶è¯­è¨€ç­‰é¢†åŸŸéƒ½æœ‰æˆåŠŸåº”ç”¨

**å±€é™æ€§**ï¼š

- **é»‘ç›’é—®é¢˜**ï¼šæ¨¡å‹å†³ç­–è¿‡ç¨‹éš¾ä»¥è§£é‡Šå’Œç†è§£
- **æ•°æ®ä¾èµ–**ï¼šéœ€è¦å¤§é‡é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®
- **è®¡ç®—èµ„æºéœ€æ±‚**ï¼šè®­ç»ƒè¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æº
- **è¿‡æ‹Ÿåˆé£é™©**ï¼šåœ¨æœ‰é™æ•°æ®ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆ
- **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™

### å­¦ç§‘äº¤å‰èåˆ

**ä¸æ•°å­¦ç†è®ºçš„èåˆ**ï¼š

- **ä¼˜åŒ–ç†è®º**ï¼šæ¢¯åº¦ä¸‹é™å’Œéšæœºä¼˜åŒ–çš„æ•°å­¦åŸºç¡€
- **æ¦‚ç‡è®º**ï¼šè´å¶æ–¯æ¨æ–­å’Œä¸ç¡®å®šæ€§å»ºæ¨¡
- **çº¿æ€§ä»£æ•°**ï¼šçŸ©é˜µè¿ç®—å’Œç‰¹å¾åˆ†è§£
- **ä¿¡æ¯è®º**ï¼šäº’ä¿¡æ¯å’Œç†µåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

**ä¸ç±»å‹ç†è®ºçš„ç»“åˆ**ï¼š

- **ç¥ç»ç½‘ç»œç±»å‹**ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å½¢å¼åŒ–ç±»å‹ç³»ç»Ÿ
- **æ¢¯åº¦ç±»å‹**ï¼šè‡ªåŠ¨å¾®åˆ†çš„ç±»å‹å®‰å…¨å®ç°
- **å¼ é‡ç±»å‹**ï¼šå¤šç»´æ•°æ®çš„ç±»å‹æŠ½è±¡
- **è®¡ç®—å›¾ç±»å‹**ï¼šè®¡ç®—å›¾çš„ç±»å‹ç³»ç»Ÿè¡¨ç¤º

**ä¸äººå·¥æ™ºèƒ½çš„äº¤å‰**ï¼š

- **å…ƒå­¦ä¹ **ï¼šå­¦ä¹ å¦‚ä½•å­¦ä¹ çš„æ–°å…´é¢†åŸŸ
- **ç¥ç»æ¶æ„æœç´¢**ï¼šè‡ªåŠ¨è®¾è®¡ç¥ç»ç½‘ç»œæ¶æ„
- **è”é‚¦å­¦ä¹ **ï¼šåˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„åä½œå­¦ä¹ 
- **å¼ºåŒ–å­¦ä¹ ç»“åˆ**ï¼šæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å‘å±•

**ä¸æ§åˆ¶è®ºçš„èåˆ**ï¼š

- **è‡ªé€‚åº”æ§åˆ¶**ï¼šæ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„è‡ªé€‚åº”è°ƒèŠ‚
- **ç¨³å®šæ€§åˆ†æ**ï¼šç¥ç»ç½‘ç»œè®­ç»ƒçš„ç¨³å®šæ€§ä¿è¯
- **åé¦ˆæœºåˆ¶**ï¼šåœ¨çº¿å­¦ä¹ å’Œå¢é‡å­¦ä¹ çš„åé¦ˆæ§åˆ¶
- **é²æ£’æ€§æ§åˆ¶**ï¼šå¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ä¿è¯

### åˆ›æ–°æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›

**ç†è®ºåˆ›æ–°æ–¹å‘**ï¼š

1. **ç¥ç»ç¬¦å·ç»“åˆ**ï¼šç¬¦å·æ¨ç†ä¸ç¥ç»ç½‘ç»œçš„èåˆ
2. **å› æœæ¨ç†**ï¼šä»ç›¸å…³æ€§å­¦ä¹ åˆ°å› æœæ€§ç†è§£
3. **å°æ ·æœ¬å­¦ä¹ **ï¼šåœ¨æœ‰é™æ•°æ®ä¸Šçš„æœ‰æ•ˆå­¦ä¹ 
4. **å¯è§£é‡ŠAI**ï¼šæé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯è§£é‡Šæ€§

**åº”ç”¨å‰æ™¯åˆ†æ**ï¼š

- **è‡ªåŠ¨é©¾é©¶**ï¼šè§†è§‰æ„ŸçŸ¥å’Œå†³ç­–ç³»ç»Ÿ
- **åŒ»ç–—è¯Šæ–­**ï¼šåŒ»å­¦å›¾åƒåˆ†æå’Œç–¾ç—…é¢„æµ‹
- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šæœºå™¨ç¿»è¯‘å’Œå¯¹è¯ç³»ç»Ÿ
- **æ¨èç³»ç»Ÿ**ï¼šä¸ªæ€§åŒ–æ¨èå’Œå†…å®¹ç”Ÿæˆ

**æŒ‘æˆ˜ä¸æœºé‡**ï¼š

- **å¯è§£é‡Šæ€§**ï¼šæé«˜æ¨¡å‹å†³ç­–çš„é€æ˜åº¦å’Œå¯ç†è§£æ€§
- **é²æ£’æ€§**ï¼šå¢å¼ºæ¨¡å‹å¯¹å¯¹æŠ—æ”»å‡»å’Œåˆ†å¸ƒåç§»çš„é²æ£’æ€§
- **æ•ˆç‡ä¼˜åŒ–**ï¼šé™ä½è®¡ç®—æˆæœ¬å’Œèƒ½æºæ¶ˆè€—
- **å…¬å¹³æ€§**ï¼šç¡®ä¿æ¨¡å‹å†³ç­–çš„å…¬å¹³æ€§å’Œæ— åè§

### å‚è€ƒæ–‡çŒ®

1. Goodfellow, I., Bengio, Y., & Courville, A. *Deep Learning*. MIT Press, 2016.
2. LeCun, Y., Bengio, Y., & Hinton, G. "Deep Learning." *Nature*, 2015.
3. Bishop, C. M. *Pattern Recognition and Machine Learning*. Springer, 2006.
4. Vaswani, A., et al. "Attention is all you need." *NIPS*, 2017.
5. Goodfellow, I., et al. "Generative adversarial nets." *NIPS*, 2014.
