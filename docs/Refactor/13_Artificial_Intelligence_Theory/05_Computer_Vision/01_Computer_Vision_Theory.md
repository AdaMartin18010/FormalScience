# 13.5.1 è®¡ç®—æœºè§†è§‰ç†è®º

## ğŸ“‹ æ¦‚è¿°

è®¡ç®—æœºè§†è§‰ç†è®ºç ”ç©¶è®¡ç®—æœºå¦‚ä½•ä»å›¾åƒæˆ–è§†é¢‘ä¸­è·å–ã€å¤„ç†ã€åˆ†æå’Œç†è§£ä¿¡æ¯ã€‚è¯¥ç†è®ºæ¶µç›–å›¾åƒå¤„ç†ã€ç‰¹å¾æå–ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºè§†è§‰æ™ºèƒ½ç³»ç»Ÿæä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 è®¡ç®—æœºè§†è§‰å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰
è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œç ”ç©¶è®¡ç®—æœºå¦‚ä½•ä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è‡ªåŠ¨æå–ã€åˆ†æå’Œç†è§£ä¿¡æ¯ã€‚

### 1.2 è®¡ç®—æœºè§†è§‰ä»»åŠ¡åˆ†ç±»

| ä»»åŠ¡ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹åº”ç”¨         |
|--------------|------------------|------------------------------|------------------|
| å›¾åƒåˆ†ç±»     | Image Classification| å°†å›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰ç±»åˆ«     | ç‰©ä½“è¯†åˆ«         |
| ç›®æ ‡æ£€æµ‹     | Object Detection | å®šä½å’Œè¯†åˆ«å›¾åƒä¸­çš„ç›®æ ‡     | è‡ªåŠ¨é©¾é©¶         |
| å›¾åƒåˆ†å‰²     | Image Segmentation| å°†å›¾åƒåˆ†å‰²ä¸ºè¯­ä¹‰åŒºåŸŸ       | åŒ»å­¦å½±åƒ         |
| ç‰¹å¾æå–     | Feature Extraction| æå–å›¾åƒçš„æ˜¾è‘—ç‰¹å¾         | å›¾åƒåŒ¹é…         |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 å›¾åƒè¡¨ç¤º

**å®šä¹‰ 2.1**ï¼ˆæ•°å­—å›¾åƒï¼‰
æ•°å­—å›¾åƒæ˜¯äºŒç»´å‡½æ•° $I(x,y)$ï¼Œå…¶ä¸­ $(x,y)$ æ˜¯åƒç´ åæ ‡ï¼Œ$I(x,y)$ æ˜¯åƒç´ å¼ºåº¦å€¼ã€‚

### 2.2 å·ç§¯æ“ä½œ

**å®šä¹‰ 2.2**ï¼ˆå·ç§¯ï¼‰
å·ç§¯æ“ä½œå®šä¹‰ä¸ºï¼š$(I * K)(x,y) = \sum_{i,j} I(x-i, y-j) \cdot K(i,j)$

### 2.3 ç‰¹å¾æ˜ å°„

**å®šä¹‰ 2.3**ï¼ˆç‰¹å¾æ˜ å°„ï¼‰
ç‰¹å¾æ˜ å°„æ˜¯å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºç‰¹å¾è¡¨ç¤ºçš„å‡½æ•°ï¼š$f: \mathbb{R}^{H \times W \times C} \rightarrow \mathbb{R}^{H' \times W' \times C'}$

## 3. å®šç†ä¸è¯æ˜

### 3.1 å·ç§¯å®šç†

**å®šç† 3.1**ï¼ˆå·ç§¯å®šç†ï¼‰
æ—¶åŸŸå·ç§¯ç­‰ä»·äºé¢‘åŸŸä¹˜ç§¯ï¼š$\mathcal{F}\{f * g\} = \mathcal{F}\{f\} \cdot \mathcal{F}\{g\}$

**è¯æ˜**ï¼š
é€šè¿‡å‚…é‡Œå¶å˜æ¢çš„å®šä¹‰å’Œå·ç§¯çš„å®šä¹‰ï¼Œç›´æ¥è®¡ç®—å³å¯å¾—åˆ°å·ç§¯å®šç†ã€‚â–¡

### 3.2 å°ºåº¦ä¸å˜æ€§å®šç†

**å®šç† 3.2**ï¼ˆå°ºåº¦ä¸å˜æ€§ï¼‰
SIFTç‰¹å¾å…·æœ‰å°ºåº¦ä¸å˜æ€§ï¼Œå³åœ¨ä¸åŒå°ºåº¦ä¸‹æå–çš„ç‰¹å¾å…·æœ‰ä¸€è‡´æ€§ã€‚

**è¯æ˜**ï¼š
é€šè¿‡é«˜æ–¯é‡‘å­—å¡”å’Œå·®åˆ†é«˜æ–¯é‡‘å­—å¡”çš„æ„å»ºï¼Œå¯ä»¥æ£€æµ‹åˆ°å°ºåº¦ä¸å˜çš„å…³é”®ç‚¹ã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 å›¾åƒå¤„ç†åŸºç¡€å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct Image {
    pub width: usize,
    pub height: usize,
    pub channels: usize,
    pub data: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct Kernel {
    pub width: usize,
    pub height: usize,
    pub data: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct FeaturePoint {
    pub x: f64,
    pub y: f64,
    pub scale: f64,
    pub orientation: f64,
    pub descriptor: Vec<f64>,
}

impl Image {
    pub fn new(width: usize, height: usize, channels: usize) -> Self {
        Image {
            width,
            height,
            channels,
            data: vec![0.0; width * height * channels],
        }
    }
    
    pub fn from_data(width: usize, height: usize, channels: usize, data: Vec<f64>) -> Self {
        Image {
            width,
            height,
            channels,
            data,
        }
    }
    
    pub fn get_pixel(&self, x: usize, y: usize, channel: usize) -> f64 {
        let index = (y * self.width + x) * self.channels + channel;
        self.data[index]
    }
    
    pub fn set_pixel(&mut self, x: usize, y: usize, channel: usize, value: f64) {
        let index = (y * self.width + x) * self.channels + channel;
        self.data[index] = value;
    }
    
    pub fn convolve(&self, kernel: &Kernel) -> Image {
        let mut result = Image::new(self.width, self.height, self.channels);
        
        let kernel_center_x = kernel.width / 2;
        let kernel_center_y = kernel.height / 2;
        
        for y in 0..self.height {
            for x in 0..self.width {
                for c in 0..self.channels {
                    let mut sum = 0.0;
                    
                    for ky in 0..kernel.height {
                        for kx in 0..kernel.width {
                            let image_x = x as i32 + kx as i32 - kernel_center_x as i32;
                            let image_y = y as i32 + ky as i32 - kernel_center_y as i32;
                            
                            if image_x >= 0 && image_x < self.width as i32 &&
                               image_y >= 0 && image_y < self.height as i32 {
                                let pixel_value = self.get_pixel(
                                    image_x as usize, 
                                    image_y as usize, 
                                    c
                                );
                                let kernel_value = kernel.data[ky * kernel.width + kx];
                                sum += pixel_value * kernel_value;
                            }
                        }
                    }
                    
                    result.set_pixel(x, y, c, sum);
                }
            }
        }
        
        result
    }
    
    pub fn gaussian_blur(&self, sigma: f64) -> Image {
        let kernel_size = (6.0 * sigma).ceil() as usize;
        if kernel_size % 2 == 0 {
            kernel_size += 1;
        }
        
        let kernel = Kernel::gaussian(kernel_size, sigma);
        self.convolve(&kernel)
    }
    
    pub fn sobel_edge_detection(&self) -> (Image, Image) {
        let sobel_x = Kernel::sobel_x();
        let sobel_y = Kernel::sobel_y();
        
        let grad_x = self.convolve(&sobel_x);
        let grad_y = self.convolve(&sobel_y);
        
        (grad_x, grad_y)
    }
    
    pub fn magnitude_and_direction(&self, grad_x: &Image, grad_y: &Image) -> (Image, Image) {
        let mut magnitude = Image::new(self.width, self.height, self.channels);
        let mut direction = Image::new(self.width, self.height, self.channels);
        
        for y in 0..self.height {
            for x in 0..self.width {
                for c in 0..self.channels {
                    let gx = grad_x.get_pixel(x, y, c);
                    let gy = grad_y.get_pixel(x, y, c);
                    
                    let mag = (gx * gx + gy * gy).sqrt();
                    let dir = gy.atan2(gx);
                    
                    magnitude.set_pixel(x, y, c, mag);
                    direction.set_pixel(x, y, c, dir);
                }
            }
        }
        
        (magnitude, direction)
    }
    
    pub fn threshold(&self, threshold: f64) -> Image {
        let mut result = Image::new(self.width, self.height, self.channels);
        
        for i in 0..self.data.len() {
            result.data[i] = if self.data[i] > threshold { 255.0 } else { 0.0 };
        }
        
        result
    }
    
    pub fn normalize(&mut self) {
        let min_val = self.data.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max_val = self.data.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
        
        if max_val > min_val {
            for pixel in &mut self.data {
                *pixel = (*pixel - min_val) / (max_val - min_val) * 255.0;
            }
        }
    }
}

impl Kernel {
    pub fn new(width: usize, height: usize) -> Self {
        Kernel {
            width,
            height,
            data: vec![0.0; width * height],
        }
    }
    
    pub fn gaussian(size: usize, sigma: f64) -> Self {
        let mut kernel = Kernel::new(size, size);
        let center = size / 2;
        
        for y in 0..size {
            for x in 0..size {
                let dx = x as f64 - center as f64;
                let dy = y as f64 - center as f64;
                let value = (-(dx * dx + dy * dy) / (2.0 * sigma * sigma)).exp();
                kernel.data[y * size + x] = value;
            }
        }
        
        // å½’ä¸€åŒ–
        let sum: f64 = kernel.data.iter().sum();
        for value in &mut kernel.data {
            *value /= sum;
        }
        
        kernel
    }
    
    pub fn sobel_x() -> Self {
        Kernel {
            width: 3,
            height: 3,
            data: vec![
                -1.0, 0.0, 1.0,
                -2.0, 0.0, 2.0,
                -1.0, 0.0, 1.0,
            ],
        }
    }
    
    pub fn sobel_y() -> Self {
        Kernel {
            width: 3,
            height: 3,
            data: vec![
                -1.0, -2.0, -1.0,
                 0.0,  0.0,  0.0,
                 1.0,  2.0,  1.0,
            ],
        }
    }
    
    pub fn laplacian() -> Self {
        Kernel {
            width: 3,
            height: 3,
            data: vec![
                0.0,  1.0, 0.0,
                1.0, -4.0, 1.0,
                0.0,  1.0, 0.0,
            ],
        }
    }
}
```

### 4.2 ç‰¹å¾æå–å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct SIFTDetector {
    pub octaves: usize,
    pub scales_per_octave: usize,
    pub sigma: f64,
    pub contrast_threshold: f64,
    pub edge_threshold: f64,
}

#[derive(Debug, Clone)]
pub struct GaussianPyramid {
    pub octaves: Vec<Octave>,
}

#[derive(Debug, Clone)]
pub struct Octave {
    pub images: Vec<Image>,
    pub scales: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct DoGPyramid {
    pub octaves: Vec<DoGOctave>,
}

#[derive(Debug, Clone)]
pub struct DoGOctave {
    pub images: Vec<Image>,
    pub scales: Vec<f64>,
}

impl SIFTDetector {
    pub fn new() -> Self {
        SIFTDetector {
            octaves: 4,
            scales_per_octave: 3,
            sigma: 1.6,
            contrast_threshold: 0.04,
            edge_threshold: 10.0,
        }
    }
    
    pub fn detect_keypoints(&self, image: &Image) -> Vec<FeaturePoint> {
        // æ„å»ºé«˜æ–¯é‡‘å­—å¡”
        let gaussian_pyramid = self.build_gaussian_pyramid(image);
        
        // æ„å»ºDoGé‡‘å­—å¡”
        let dog_pyramid = self.build_dog_pyramid(&gaussian_pyramid);
        
        // æ£€æµ‹å…³é”®ç‚¹
        let mut keypoints = Vec::new();
        
        for (octave_idx, dog_octave) in dog_pyramid.octaves.iter().enumerate() {
            for (scale_idx, _) in dog_octave.images.iter().enumerate().skip(1).take(dog_octave.images.len() - 2) {
                let keypoints_in_scale = self.detect_keypoints_in_scale(
                    &dog_pyramid, 
                    octave_idx, 
                    scale_idx
                );
                keypoints.extend(keypoints_in_scale);
            }
        }
        
        // è®¡ç®—å…³é”®ç‚¹æè¿°ç¬¦
        for keypoint in &mut keypoints {
            keypoint.descriptor = self.compute_descriptor(&gaussian_pyramid, keypoint);
        }
        
        keypoints
    }
    
    fn build_gaussian_pyramid(&self, image: &Image) -> GaussianPyramid {
        let mut pyramid = GaussianPyramid { octaves: Vec::new() };
        let mut current_image = image.clone();
        
        for octave in 0..self.octaves {
            let mut octave_images = Vec::new();
            let mut octave_scales = Vec::new();
            
            for scale in 0..self.scales_per_octave + 3 {
                let sigma = self.sigma * (2.0_f64.powf(scale as f64 / self.scales_per_octave as f64));
                let blurred = current_image.gaussian_blur(sigma);
                
                octave_images.push(blurred);
                octave_scales.push(sigma);
            }
            
            pyramid.octaves.push(Octave {
                images: octave_images,
                scales: octave_scales,
            });
            
            // ä¸‹é‡‡æ ·å›¾åƒç”¨äºä¸‹ä¸€å±‚
            if octave < self.octaves - 1 {
                current_image = self.downsample(&current_image);
            }
        }
        
        pyramid
    }
    
    fn build_dog_pyramid(&self, gaussian_pyramid: &GaussianPyramid) -> DoGPyramid {
        let mut dog_pyramid = DoGPyramid { octaves: Vec::new() };
        
        for octave in &gaussian_pyramid.octaves {
            let mut dog_images = Vec::new();
            let mut dog_scales = Vec::new();
            
            for i in 0..octave.images.len() - 1 {
                let dog_image = self.subtract_images(&octave.images[i + 1], &octave.images[i]);
                dog_images.push(dog_image);
                dog_scales.push(octave.scales[i]);
            }
            
            dog_pyramid.octaves.push(DoGOctave {
                images: dog_images,
                scales: dog_scales,
            });
        }
        
        dog_pyramid
    }
    
    fn detect_keypoints_in_scale(&self, dog_pyramid: &DoGPyramid, octave_idx: usize, scale_idx: usize) -> Vec<FeaturePoint> {
        let mut keypoints = Vec::new();
        let octave = &dog_pyramid.octaves[octave_idx];
        
        if scale_idx == 0 || scale_idx >= octave.images.len() - 1 {
            return keypoints;
        }
        
        let current_image = &octave.images[scale_idx];
        let prev_image = &octave.images[scale_idx - 1];
        let next_image = &octave.images[scale_idx + 1];
        
        for y in 1..current_image.height - 1 {
            for x in 1..current_image.width - 1 {
                let center_value = current_image.get_pixel(x, y, 0);
                
                // æ£€æŸ¥æ˜¯å¦ä¸ºå±€éƒ¨æå€¼
                if self.is_local_extremum(prev_image, current_image, next_image, x, y) {
                    // æ£€æŸ¥å¯¹æ¯”åº¦
                    if center_value.abs() > self.contrast_threshold {
                        // æ£€æŸ¥è¾¹ç¼˜å“åº”
                        if !self.is_edge_response(current_image, x, y) {
                            let scale = octave.scales[scale_idx];
                            let keypoint = FeaturePoint {
                                x: x as f64 * (1 << octave_idx) as f64,
                                y: y as f64 * (1 << octave_idx) as f64,
                                scale,
                                orientation: 0.0,
                                descriptor: Vec::new(),
                            };
                            keypoints.push(keypoint);
                        }
                    }
                }
            }
        }
        
        keypoints
    }
    
    fn is_local_extremum(&self, prev: &Image, current: &Image, next: &Image, x: usize, y: usize) -> bool {
        let center_value = current.get_pixel(x, y, 0);
        
        // æ£€æŸ¥3x3x3é‚»åŸŸ
        for dy in -1..=1 {
            for dx in -1..=1 {
                for dz in -1..=1 {
                    if dx == 0 && dy == 0 && dz == 0 {
                        continue;
                    }
                    
                    let nx = x as i32 + dx;
                    let ny = y as i32 + dy;
                    
                    if nx < 0 || nx >= current.width as i32 || ny < 0 || ny >= current.height as i32 {
                        continue;
                    }
                    
                    let neighbor_value = if dz == -1 {
                        prev.get_pixel(nx as usize, ny as usize, 0)
                    } else if dz == 0 {
                        current.get_pixel(nx as usize, ny as usize, 0)
                    } else {
                        next.get_pixel(nx as usize, ny as usize, 0)
                    };
                    
                    if center_value <= neighbor_value {
                        return false;
                    }
                }
            }
        }
        
        true
    }
    
    fn is_edge_response(&self, image: &Image, x: usize, y: usize) -> bool {
        let center = image.get_pixel(x, y, 0);
        let dxx = image.get_pixel(x + 1, y, 0) + image.get_pixel(x - 1, y, 0) - 2.0 * center;
        let dyy = image.get_pixel(x, y + 1, 0) + image.get_pixel(x, y - 1, 0) - 2.0 * center;
        let dxy = (image.get_pixel(x + 1, y + 1, 0) + image.get_pixel(x - 1, y - 1, 0) -
                   image.get_pixel(x + 1, y - 1, 0) - image.get_pixel(x - 1, y + 1, 0)) / 4.0;
        
        let trace = dxx + dyy;
        let det = dxx * dyy - dxy * dxy;
        
        if det <= 0.0 {
            return true;
        }
        
        let ratio = trace * trace / det;
        ratio > self.edge_threshold
    }
    
    fn compute_descriptor(&self, gaussian_pyramid: &GaussianPyramid, keypoint: &FeaturePoint) -> Vec<f64> {
        // ç®€åŒ–å®ç°ï¼šè¿”å›éšæœºæè¿°ç¬¦
        let mut descriptor = Vec::new();
        for _ in 0..128 {
            descriptor.push(rand::random::<f64>());
        }
        descriptor
    }
    
    fn subtract_images(&self, img1: &Image, img2: &Image) -> Image {
        let mut result = Image::new(img1.width, img1.height, img1.channels);
        
        for i in 0..img1.data.len() {
            result.data[i] = img1.data[i] - img2.data[i];
        }
        
        result
    }
    
    fn downsample(&self, image: &Image) -> Image {
        let new_width = image.width / 2;
        let new_height = image.height / 2;
        let mut result = Image::new(new_width, new_height, image.channels);
        
        for y in 0..new_height {
            for x in 0..new_width {
                for c in 0..image.channels {
                    let value = image.get_pixel(x * 2, y * 2, c);
                    result.set_pixel(x, y, c, value);
                }
            }
        }
        
        result
    }
}
```

### 4.3 ç›®æ ‡æ£€æµ‹å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct ObjectDetector {
    pub model: DetectionModel,
    pub confidence_threshold: f64,
    pub nms_threshold: f64,
}

#[derive(Debug, Clone)]
pub struct DetectionModel {
    pub backbone: BackboneNetwork,
    pub detection_head: DetectionHead,
}

#[derive(Debug, Clone)]
pub struct BackboneNetwork {
    pub layers: Vec<ConvLayer>,
    pub feature_maps: Vec<Image>,
}

#[derive(Debug, Clone)]
pub struct DetectionHead {
    pub classification_layers: Vec<DenseLayer>,
    pub regression_layers: Vec<DenseLayer>,
}

#[derive(Debug, Clone)]
pub struct ConvLayer {
    pub filters: usize,
    pub kernel_size: usize,
    pub stride: usize,
    pub padding: usize,
    pub weights: Vec<Vec<Vec<Vec<f64>>>>, // [out_channels, in_channels, height, width]
    pub biases: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct DenseLayer {
    pub input_size: usize,
    pub output_size: usize,
    pub weights: Vec<Vec<f64>>,
    pub biases: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct Detection {
    pub bbox: BoundingBox,
    pub class_id: usize,
    pub confidence: f64,
}

#[derive(Debug, Clone)]
pub struct BoundingBox {
    pub x: f64,
    pub y: f64,
    pub width: f64,
    pub height: f64,
}

impl ObjectDetector {
    pub fn new() -> Self {
        ObjectDetector {
            model: DetectionModel::new(),
            confidence_threshold: 0.5,
            nms_threshold: 0.4,
        }
    }
    
    pub fn detect(&self, image: &Image) -> Vec<Detection> {
        // æå–ç‰¹å¾
        let features = self.model.backbone.forward(image);
        
        // ç”Ÿæˆå€™é€‰åŒºåŸŸ
        let proposals = self.generate_proposals(&features);
        
        // åˆ†ç±»å’Œå›å½’
        let mut detections = Vec::new();
        
        for proposal in proposals {
            let (class_scores, bbox_deltas) = self.model.detection_head.forward(&proposal);
            
            // æ‰¾åˆ°æœ€é«˜ç½®ä¿¡åº¦çš„ç±»åˆ«
            let (class_id, confidence) = self.get_best_class(&class_scores);
            
            if confidence > self.confidence_threshold {
                // åº”ç”¨è¾¹ç•Œæ¡†å›å½’
                let bbox = self.apply_bbox_regression(&proposal, &bbox_deltas[class_id * 4..(class_id + 1) * 4]);
                
                detections.push(Detection {
                    bbox,
                    class_id,
                    confidence,
                });
            }
        }
        
        // éæå¤§å€¼æŠ‘åˆ¶
        self.non_maximum_suppression(detections)
    }
    
    fn generate_proposals(&self, features: &[Image]) -> Vec<BoundingBox> {
        let mut proposals = Vec::new();
        
        // ç®€åŒ–å®ç°ï¼šç”Ÿæˆæ»‘åŠ¨çª—å£
        let feature_map = &features[0];
        let stride = 16; // å‡è®¾ç‰¹å¾å›¾ç›¸å¯¹äºåŸå›¾çš„æ­¥é•¿
        
        for y in 0..feature_map.height {
            for x in 0..feature_map.width {
                let bbox = BoundingBox {
                    x: x as f64 * stride as f64,
                    y: y as f64 * stride as f64,
                    width: 64.0,
                    height: 64.0,
                };
                proposals.push(bbox);
            }
        }
        
        proposals
    }
    
    fn get_best_class(&self, class_scores: &[f64]) -> (usize, f64) {
        let mut best_class = 0;
        let mut best_score = class_scores[0];
        
        for (i, &score) in class_scores.iter().enumerate() {
            if score > best_score {
                best_score = score;
                best_class = i;
            }
        }
        
        (best_class, best_score)
    }
    
    fn apply_bbox_regression(&self, proposal: &BoundingBox, deltas: &[f64]) -> BoundingBox {
        let dx = deltas[0];
        let dy = deltas[1];
        let dw = deltas[2];
        let dh = deltas[3];
        
        let new_x = proposal.x + dx * proposal.width;
        let new_y = proposal.y + dy * proposal.height;
        let new_width = proposal.width * dw.exp();
        let new_height = proposal.height * dh.exp();
        
        BoundingBox {
            x: new_x,
            y: new_y,
            width: new_width,
            height: new_height,
        }
    }
    
    fn non_maximum_suppression(&self, mut detections: Vec<Detection>) -> Vec<Detection> {
        // æŒ‰ç±»åˆ«åˆ†ç»„
        let mut grouped_detections: HashMap<usize, Vec<Detection>> = HashMap::new();
        
        for detection in detections {
            grouped_detections.entry(detection.class_id)
                .or_insert_with(Vec::new)
                .push(detection);
        }
        
        let mut final_detections = Vec::new();
        
        for (_, class_detections) in grouped_detections {
            let mut remaining = class_detections;
            
            while !remaining.is_empty() {
                // æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹
                let best_idx = remaining.iter()
                    .enumerate()
                    .max_by(|a, b| a.1.confidence.partial_cmp(&b.1.confidence).unwrap())
                    .unwrap().0;
                
                let best_detection = remaining.remove(best_idx);
                final_detections.push(best_detection.clone());
                
                // ç§»é™¤ä¸æœ€ä½³æ£€æµ‹é‡å è¿‡å¤šçš„æ£€æµ‹
                remaining.retain(|detection| {
                    let iou = self.calculate_iou(&best_detection.bbox, &detection.bbox);
                    iou < self.nms_threshold
                });
            }
        }
        
        final_detections
    }
    
    fn calculate_iou(&self, bbox1: &BoundingBox, bbox2: &BoundingBox) -> f64 {
        let x1 = bbox1.x.max(bbox2.x);
        let y1 = bbox1.y.max(bbox2.y);
        let x2 = (bbox1.x + bbox1.width).min(bbox2.x + bbox2.width);
        let y2 = (bbox1.y + bbox1.height).min(bbox2.y + bbox2.height);
        
        if x2 <= x1 || y2 <= y1 {
            return 0.0;
        }
        
        let intersection = (x2 - x1) * (y2 - y1);
        let area1 = bbox1.width * bbox1.height;
        let area2 = bbox2.width * bbox2.height;
        let union = area1 + area2 - intersection;
        
        intersection / union
    }
}

impl DetectionModel {
    pub fn new() -> Self {
        DetectionModel {
            backbone: BackboneNetwork::new(),
            detection_head: DetectionHead::new(),
        }
    }
}

impl BackboneNetwork {
    pub fn new() -> Self {
        BackboneNetwork {
            layers: Vec::new(),
            feature_maps: Vec::new(),
        }
    }
    
    pub fn forward(&self, image: &Image) -> Vec<Image> {
        // ç®€åŒ–å®ç°ï¼šè¿”å›åŸå§‹å›¾åƒä½œä¸ºç‰¹å¾
        vec![image.clone()]
    }
}

impl DetectionHead {
    pub fn new() -> Self {
        DetectionHead {
            classification_layers: Vec::new(),
            regression_layers: Vec::new(),
        }
    }
    
    pub fn forward(&self, _proposal: &BoundingBox) -> (Vec<f64>, Vec<f64>) {
        // ç®€åŒ–å®ç°ï¼šè¿”å›éšæœºåˆ†ç±»åˆ†æ•°å’Œå›å½’åç§»
        let class_scores = vec![0.1, 0.8, 0.1]; // 3ä¸ªç±»åˆ«
        let bbox_deltas = vec![0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]; // 3ä¸ªç±»åˆ« * 4ä¸ªåç§»
        
        (class_scores, bbox_deltas)
    }
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æœºå™¨å­¦ä¹ ç†è®º](../01_Machine_Learning/01_Machine_Learning_Theory.md)
- [æ·±åº¦å­¦ä¹ ç†è®º](../02_Deep_Learning/01_Deep_Learning_Theory.md)
- [è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º](../04_Natural_Language_Processing/01_Natural_Language_Processing_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Szeliski, R. (2010). Computer Vision: Algorithms and Applications. Springer.
2. Lowe, D. G. (2004). Distinctive Image Features from Scale-Invariant Keypoints. IJCV.
3. Girshick, R. (2015). Fast R-CNN. ICCV.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0

## æ‰¹åˆ¤æ€§åˆ†æ

### ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†

è®¡ç®—æœºè§†è§‰ç†è®ºä½œä¸ºäººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œä¸»è¦å…³æ³¨è®¡ç®—æœºç†è§£å’Œåˆ†æè§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸»è¦ç†è®ºæµæ´¾åŒ…æ‹¬ï¼š

1. **å‡ ä½•å­¦æ´¾**ï¼šä»¥Marrä¸ºä»£è¡¨ï¼Œå¼ºè°ƒè§†è§‰çš„å‡ ä½•å’Œä¸‰ç»´é‡å»º
2. **ç‰¹å¾å­¦æ´¾**ï¼šä»¥Loweä¸ºä»£è¡¨ï¼Œå…³æ³¨å›¾åƒç‰¹å¾çš„æå–å’ŒåŒ¹é…
3. **æ·±åº¦å­¦ä¹ å­¦æ´¾**ï¼šä»¥LeCunä¸ºä»£è¡¨ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†è§†è§‰
4. **æ³¨æ„åŠ›å­¦æ´¾**ï¼šä»¥Vaswaniä¸ºä»£è¡¨ï¼Œå…³æ³¨è§†è§‰æ³¨æ„åŠ›æœºåˆ¶
5. **å¤šæ¨¡æ€å­¦æ´¾**ï¼šç»“åˆè¯­è¨€ã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€çš„è§†è§‰ç†è§£

### ç†è®ºä¼˜åŠ¿ä¸å±€é™æ€§

**ä¼˜åŠ¿**ï¼š

- **è§†è§‰ç†è§£èƒ½åŠ›**ï¼šèƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒå’Œè§†é¢‘å†…å®¹
- **å¤§è§„æ¨¡æ•°æ®å¤„ç†**ï¼šå¤„ç†æµ·é‡è§†è§‰æ•°æ®çš„èƒ½åŠ›
- **å®æ—¶å¤„ç†**ï¼šèƒ½å¤Ÿè¿›è¡Œå®æ—¶çš„è§†è§‰åˆ†æ
- **å¤šå°ºåº¦åˆ†æ**ï¼šä»åƒç´ çº§åˆ°è¯­ä¹‰çº§çš„è§†è§‰ç†è§£
- **é²æ£’æ€§**ï¼šå¯¹å…‰ç…§ã€è§’åº¦ç­‰å˜åŒ–çš„é€‚åº”èƒ½åŠ›

**å±€é™æ€§**ï¼š

- **æ•°æ®ä¾èµ–**ï¼šéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒ
- **å¯¹æŠ—æ”»å‡»**ï¼šå¯¹ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ ·æœ¬æ•æ„Ÿ
- **å¯è§£é‡Šæ€§å·®**ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹å†³ç­–è¿‡ç¨‹éš¾ä»¥è§£é‡Š
- **æ³›åŒ–èƒ½åŠ›**ï¼šåœ¨æœªè§åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›æœ‰é™
- **è®¡ç®—å¤æ‚åº¦**ï¼šå¤æ‚æ¨¡å‹éœ€è¦å¤§é‡è®¡ç®—èµ„æº

### å­¦ç§‘äº¤å‰èåˆ

**ä¸æ•°å­¦ç†è®ºçš„èåˆ**ï¼š

- **å‡ ä½•å­¦**ï¼šä¸‰ç»´é‡å»ºå’Œå‡ ä½•å˜æ¢
- **çº¿æ€§ä»£æ•°**ï¼šå›¾åƒå˜æ¢å’Œç‰¹å¾è¡¨ç¤º
- **æ¦‚ç‡è®º**ï¼šè§†è§‰æ¨¡å‹çš„æ¦‚ç‡åŸºç¡€
- **ä¼˜åŒ–ç†è®º**ï¼šè§†è§‰ç®—æ³•çš„ä¼˜åŒ–æ–¹æ³•

**ä¸ç±»å‹ç†è®ºçš„ç»“åˆ**ï¼š

- **å›¾åƒç±»å‹**ï¼šè§†è§‰æ•°æ®çš„ç±»å‹å®‰å…¨è¡¨ç¤º
- **ç‰¹å¾ç±»å‹**ï¼šè§†è§‰ç‰¹å¾çš„ç±»å‹ç³»ç»ŸæŠ½è±¡
- **è¯­ä¹‰ç±»å‹**ï¼šè§†è§‰è¯­ä¹‰çš„ç±»å‹å®‰å…¨å»ºæ¨¡
- **ä»»åŠ¡ç±»å‹**ï¼šè§†è§‰ä»»åŠ¡çš„ç±»å‹å®‰å…¨è®¾è®¡

**ä¸äººå·¥æ™ºèƒ½çš„äº¤å‰**ï¼š

- **å¤šæ¨¡æ€å­¦ä¹ **ï¼šç»“åˆè¯­è¨€ã€éŸ³é¢‘çš„è§†è§‰ç†è§£
- **å¼ºåŒ–å­¦ä¹ **ï¼šè§†è§‰å¯¼èˆªå’Œå†³ç­–
- **ç”Ÿæˆæ¨¡å‹**ï¼šå›¾åƒå’Œè§†é¢‘çš„ç”Ÿæˆ
- **å…ƒå­¦ä¹ **ï¼šå¿«é€Ÿé€‚åº”æ–°çš„è§†è§‰ä»»åŠ¡

**ä¸å½¢å¼è¯­è¨€ç†è®ºçš„èåˆ**ï¼š

- **è§†è§‰è¯­æ³•**ï¼šè§†è§‰ç»“æ„çš„å½¢å¼åŒ–è¡¨ç¤º
- **è¯­ä¹‰è¡¨ç¤º**ï¼šè§†è§‰è¯­ä¹‰çš„å½¢å¼åŒ–å»ºæ¨¡
- **åœºæ™¯ç†è§£**ï¼šå¤æ‚åœºæ™¯çš„ç»“æ„åŒ–åˆ†æ
- **è§†è§‰æ¨ç†**ï¼šåŸºäºè§†è§‰çš„é€»è¾‘æ¨ç†

### åˆ›æ–°æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›

**ç†è®ºåˆ›æ–°æ–¹å‘**ï¼š

1. **è‡ªç›‘ç£å­¦ä¹ **ï¼šå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–
2. **å¤šæ¨¡æ€èåˆ**ï¼šç»“åˆå¤šç§æ¨¡æ€çš„è§†è§‰ç†è§£
3. **å› æœæ¨ç†**ï¼šæé«˜æ¨¡å‹çš„å› æœæ¨ç†èƒ½åŠ›
4. **å¯è§£é‡Šæ€§**ï¼šæé«˜æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§

**åº”ç”¨å‰æ™¯åˆ†æ**ï¼š

- **è‡ªåŠ¨é©¾é©¶**ï¼šé“è·¯åœºæ™¯ç†è§£å’Œå†³ç­–
- **åŒ»ç–—å½±åƒ**ï¼šåŒ»å­¦å›¾åƒçš„åˆ†æå’Œè¯Šæ–­
- **å®‰é˜²ç›‘æ§**ï¼šè§†é¢‘ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹
- **å¢å¼ºç°å®**ï¼šè™šæ‹Ÿå’Œç°å®ä¸–ç•Œçš„èåˆ

**æŒ‘æˆ˜ä¸æœºé‡**ï¼š

- **æ•°æ®æ•ˆç‡**ï¼šå‡å°‘è®­ç»ƒæ•°æ®çš„éœ€æ±‚
- **é²æ£’æ€§æå‡**ï¼šæé«˜å¯¹å™ªå£°å’Œå¯¹æŠ—æ ·æœ¬çš„æŠµæŠ—èƒ½åŠ›
- **å®æ—¶æ€§**ï¼šæé«˜è§†è§‰å¤„ç†çš„å®æ—¶æ€§èƒ½
- **å®‰å…¨æ€§**ï¼šç¡®ä¿è§†è§‰ç³»ç»Ÿçš„å®‰å…¨ä½¿ç”¨

### å‚è€ƒæ–‡çŒ®

1. Szeliski, R. *Computer Vision: Algorithms and Applications*. Springer, 2010.
2. Lowe, D. G. "Distinctive Image Features from Scale-Invariant Keypoints." *IJCV*, 2004.
3. Krizhevsky, A., et al. "ImageNet Classification with Deep Convolutional Neural Networks." *NeurIPS*, 2012.
4. He, K., et al. "Deep Residual Learning for Image Recognition." *CVPR*, 2016.
5. Vaswani, A., et al. "Attention is All you Need." *NeurIPS*, 2017.
