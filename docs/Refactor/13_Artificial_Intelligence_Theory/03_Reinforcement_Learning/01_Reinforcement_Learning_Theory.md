# 13.3.1 å¼ºåŒ–å­¦ä¹ ç†è®º

## ğŸ“‹ æ¦‚è¿°

å¼ºåŒ–å­¦ä¹ ç†è®ºç ”ç©¶æ™ºèƒ½ä½“å¦‚ä½•é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚è¯¥ç†è®ºæ¶µç›–é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ã€ä»·å€¼å‡½æ•°ã€ç­–ç•¥æ¢¯åº¦ã€Qå­¦ä¹ ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºè‡ªä¸»å†³ç­–ç³»ç»Ÿæä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 å¼ºåŒ–å­¦ä¹ å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰
å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚

### 1.2 å¼ºåŒ–å­¦ä¹ è¦ç´ 

| è¦ç´          | è‹±æ–‡åç§°         | æè¿°                         | ä½œç”¨             |
|--------------|------------------|------------------------------|------------------|
| æ™ºèƒ½ä½“       | Agent            | æ‰§è¡ŒåŠ¨ä½œçš„å®ä½“               | å†³ç­–ä¸»ä½“         |
| ç¯å¢ƒ         | Environment      | æ™ºèƒ½ä½“äº¤äº’çš„å¤–éƒ¨ä¸–ç•Œ         | çŠ¶æ€è½¬æ¢         |
| çŠ¶æ€         | State            | ç¯å¢ƒçš„å½“å‰çŠ¶å†µ               | ä¿¡æ¯è¡¨ç¤º         |
| åŠ¨ä½œ         | Action           | æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œ           | è¡Œä¸ºé€‰æ‹©         |
| å¥–åŠ±         | Reward           | ç¯å¢ƒå¯¹åŠ¨ä½œçš„åé¦ˆ             | å­¦ä¹ ä¿¡å·         |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

**å®šä¹‰ 2.1**ï¼ˆé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼‰
é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹æ˜¯ä¸€ä¸ªäº”å…ƒç»„ $(S, A, P, R, \gamma)$ï¼Œå…¶ä¸­ï¼š

- $S$ æ˜¯çŠ¶æ€ç©ºé—´
- $A$ æ˜¯åŠ¨ä½œç©ºé—´  
- $P$ æ˜¯çŠ¶æ€è½¬ç§»æ¦‚ç‡
- $R$ æ˜¯å¥–åŠ±å‡½æ•°
- $\gamma$ æ˜¯æŠ˜æ‰£å› å­

### 2.2 ä»·å€¼å‡½æ•°

**å®šä¹‰ 2.2**ï¼ˆä»·å€¼å‡½æ•°ï¼‰
ä»·å€¼å‡½æ•° $V^\pi(s)$ è¡¨ç¤ºåœ¨ç­–ç•¥ $\pi$ ä¸‹ä»çŠ¶æ€ $s$ å¼€å§‹çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±ã€‚

### 2.3 ç­–ç•¥

**å®šä¹‰ 2.3**ï¼ˆç­–ç•¥ï¼‰
ç­–ç•¥ $\pi$ æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œ$\pi: S \rightarrow A$ã€‚

## 3. å®šç†ä¸è¯æ˜

### 3.1 è´å°”æ›¼æ–¹ç¨‹

**å®šç† 3.1**ï¼ˆè´å°”æ›¼æ–¹ç¨‹ï¼‰
ä»·å€¼å‡½æ•°æ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼š
$V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s'} P[s'|s,a](R(s,a,s') + \gamma V^\pi(s'))$

**è¯æ˜**ï¼š
æ ¹æ®ä»·å€¼å‡½æ•°çš„å®šä¹‰å’ŒæœŸæœ›çš„çº¿æ€§æ€§è´¨ï¼Œç›´æ¥å±•å¼€å³å¯å¾—åˆ°è´å°”æ›¼æ–¹ç¨‹ã€‚â–¡

### 3.2 æœ€ä¼˜ç­–ç•¥å­˜åœ¨æ€§å®šç†

**å®šç† 3.2**ï¼ˆæœ€ä¼˜ç­–ç•¥å­˜åœ¨æ€§ï¼‰
å¯¹äºæœ‰é™MDPï¼Œå­˜åœ¨ç¡®å®šæ€§çš„æœ€ä¼˜ç­–ç•¥ã€‚

**è¯æ˜**ï¼š
é€šè¿‡å€¼è¿­ä»£ç®—æ³•å¯ä»¥æ”¶æ•›åˆ°æœ€ä¼˜ä»·å€¼å‡½æ•°ï¼Œè¿›è€Œæ„é€ æœ€ä¼˜ç­–ç•¥ã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 Qå­¦ä¹ ç®—æ³•å®ç°

```rust
use std::collections::HashMap;
use std::f64;

#[derive(Debug, Clone)]
pub struct QLearning {
    pub q_table: HashMap<(usize, usize), f64>,
    pub learning_rate: f64,
    pub discount_factor: f64,
    pub epsilon: f64,
    pub state_space: usize,
    pub action_space: usize,
}

#[derive(Debug, Clone)]
pub struct Environment {
    pub states: Vec<State>,
    pub actions: Vec<Action>,
    pub transition_function: HashMap<(usize, usize), Vec<(usize, f64, f64)>>, // (state, action) -> [(next_state, probability, reward)]
}

#[derive(Debug, Clone)]
pub struct State {
    pub id: usize,
    pub features: Vec<f64>,
    pub is_terminal: bool,
}

#[derive(Debug, Clone)]
pub struct Action {
    pub id: usize,
    pub name: String,
}

#[derive(Debug, Clone)]
pub struct Episode {
    pub steps: Vec<Step>,
    pub total_reward: f64,
    pub length: usize,
}

#[derive(Debug, Clone)]
pub struct Step {
    pub state: usize,
    pub action: usize,
    pub reward: f64,
    pub next_state: usize,
    pub done: bool,
}

impl QLearning {
    pub fn new(state_space: usize, action_space: usize, learning_rate: f64, 
               discount_factor: f64, epsilon: f64) -> Self {
        QLearning {
            q_table: HashMap::new(),
            learning_rate,
            discount_factor,
            epsilon,
            state_space,
            action_space,
        }
    }
    
    pub fn get_q_value(&self, state: usize, action: usize) -> f64 {
        *self.q_table.get(&(state, action)).unwrap_or(&0.0)
    }
    
    pub fn set_q_value(&mut self, state: usize, action: usize, value: f64) {
        self.q_table.insert((state, action), value);
    }
    
    pub fn choose_action(&self, state: usize, available_actions: &[usize]) -> usize {
        if rand::random::<f64>() < self.epsilon {
            // æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            available_actions[rand::random::<usize>() % available_actions.len()]
        } else {
            // åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            let mut best_action = available_actions[0];
            let mut best_q_value = self.get_q_value(state, best_action);
            
            for &action in available_actions.iter().skip(1) {
                let q_value = self.get_q_value(state, action);
                if q_value > best_q_value {
                    best_q_value = q_value;
                    best_action = action;
                }
            }
            
            best_action
        }
    }
    
    pub fn update(&mut self, state: usize, action: usize, reward: f64, 
                  next_state: usize, available_actions: &[usize]) {
        let current_q = self.get_q_value(state, action);
        
        // è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
        let mut max_next_q = f64::NEG_INFINITY;
        for &next_action in available_actions {
            let next_q = self.get_q_value(next_state, next_action);
            max_next_q = max_next_q.max(next_q);
        }
        
        // Qå­¦ä¹ æ›´æ–°å…¬å¼
        let new_q = current_q + self.learning_rate * 
                   (reward + self.discount_factor * max_next_q - current_q);
        
        self.set_q_value(state, action, new_q);
    }
    
    pub fn train(&mut self, environment: &Environment, episodes: usize) -> Vec<f64> {
        let mut episode_rewards = Vec::new();
        
        for episode in 0..episodes {
            let episode_result = self.run_episode(environment);
            episode_rewards.push(episode_result.total_reward);
            
            // è¡°å‡æ¢ç´¢ç‡
            self.epsilon = self.epsilon * 0.995;
            
            if episode % 100 == 0 {
                println!("Episode {}, Total Reward: {:.2}, Epsilon: {:.3}", 
                        episode, episode_result.total_reward, self.epsilon);
            }
        }
        
        episode_rewards
    }
    
    fn run_episode(&mut self, environment: &Environment) -> Episode {
        let mut steps = Vec::new();
        let mut current_state = 0; // å‡è®¾ä»çŠ¶æ€0å¼€å§‹
        let mut total_reward = 0.0;
        
        loop {
            // è·å–å¯ç”¨åŠ¨ä½œ
            let available_actions = self.get_available_actions(current_state, environment);
            
            // é€‰æ‹©åŠ¨ä½œ
            let action = self.choose_action(current_state, &available_actions);
            
            // æ‰§è¡ŒåŠ¨ä½œ
            let (next_state, reward, done) = self.take_action(current_state, action, environment);
            
            // æ›´æ–°Qå€¼
            let next_available_actions = self.get_available_actions(next_state, environment);
            self.update(current_state, action, reward, next_state, &next_available_actions);
            
            // è®°å½•æ­¥éª¤
            steps.push(Step {
                state: current_state,
                action,
                reward,
                next_state,
                done,
            });
            
            total_reward += reward;
            current_state = next_state;
            
            if done {
                break;
            }
        }
        
        Episode {
            steps,
            total_reward,
            length: steps.len(),
        }
    }
    
    fn get_available_actions(&self, state: usize, environment: &Environment) -> Vec<usize> {
        // ç®€åŒ–å®ç°ï¼šè¿”å›æ‰€æœ‰åŠ¨ä½œ
        (0..self.action_space).collect()
    }
    
    fn take_action(&self, state: usize, action: usize, environment: &Environment) -> (usize, f64, bool) {
        // ç®€åŒ–å®ç°ï¼šæ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
        let key = (state, action);
        if let Some(transitions) = environment.transition_function.get(&key) {
            if let Some(&(next_state, _, reward)) = transitions.first() {
                let done = environment.states[next_state].is_terminal;
                return (next_state, reward, done);
            }
        }
        
        // é»˜è®¤è½¬æ¢
        (state, -1.0, false)
    }
    
    pub fn get_policy(&self) -> HashMap<usize, usize> {
        let mut policy = HashMap::new();
        
        for state in 0..self.state_space {
            let mut best_action = 0;
            let mut best_q_value = self.get_q_value(state, 0);
            
            for action in 1..self.action_space {
                let q_value = self.get_q_value(state, action);
                if q_value > best_q_value {
                    best_q_value = q_value;
                    best_action = action;
                }
            }
            
            policy.insert(state, best_action);
        }
        
        policy
    }
    
    pub fn evaluate_policy(&self, environment: &Environment, episodes: usize) -> f64 {
        let mut total_reward = 0.0;
        
        for _ in 0..episodes {
            let mut current_state = 0;
            let mut episode_reward = 0.0;
            
            loop {
                let available_actions = self.get_available_actions(current_state, environment);
                let action = self.choose_action(current_state, &available_actions);
                
                let (next_state, reward, done) = self.take_action(current_state, action, environment);
                episode_reward += reward;
                current_state = next_state;
                
                if done {
                    break;
                }
            }
            
            total_reward += episode_reward;
        }
        
        total_reward / episodes as f64
    }
}
```

### 4.2 ç­–ç•¥æ¢¯åº¦ç®—æ³•å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct PolicyGradient {
    pub policy_network: PolicyNetwork,
    pub learning_rate: f64,
    pub gamma: f64,
}

#[derive(Debug, Clone)]
pub struct PolicyNetwork {
    pub weights: Vec<Vec<f64>>,
    pub biases: Vec<f64>,
    pub input_size: usize,
    pub hidden_size: usize,
    pub output_size: usize,
}

#[derive(Debug, Clone)]
pub struct Trajectory {
    pub states: Vec<Vec<f64>>,
    pub actions: Vec<usize>,
    pub rewards: Vec<f64>,
    pub log_probs: Vec<f64>,
}

impl PolicyGradient {
    pub fn new(input_size: usize, hidden_size: usize, output_size: usize, 
               learning_rate: f64, gamma: f64) -> Self {
        PolicyGradient {
            policy_network: PolicyNetwork::new(input_size, hidden_size, output_size),
            learning_rate,
            gamma,
        }
    }
    
    pub fn select_action(&self, state: &[f64]) -> (usize, f64) {
        let action_probs = self.policy_network.forward(state);
        
        // é‡‡æ ·åŠ¨ä½œ
        let mut cumulative_prob = 0.0;
        let random_value = rand::random::<f64>();
        
        for (action, &prob) in action_probs.iter().enumerate() {
            cumulative_prob += prob;
            if random_value <= cumulative_prob {
                return (action, prob.ln());
            }
        }
        
        (action_probs.len() - 1, action_probs.last().unwrap().ln())
    }
    
    pub fn train(&mut self, trajectories: &[Trajectory]) {
        let mut policy_gradients = vec![vec![0.0; self.policy_network.weights[0].len()]; self.policy_network.weights.len()];
        let mut bias_gradients = vec![0.0; self.policy_network.biases.len()];
        
        for trajectory in trajectories {
            let returns = self.compute_returns(&trajectory.rewards);
            
            for (step, (state, action, log_prob, return_val)) in 
                trajectory.states.iter().zip(trajectory.actions.iter())
                .zip(trajectory.log_probs.iter()).zip(returns.iter()).enumerate() {
                
                // è®¡ç®—ç­–ç•¥æ¢¯åº¦
                let gradients = self.compute_policy_gradients(state, *action, *log_prob, *return_val);
                
                // ç´¯ç§¯æ¢¯åº¦
                for (i, row) in gradients.weights.iter().enumerate() {
                    for (j, &grad) in row.iter().enumerate() {
                        policy_gradients[i][j] += grad;
                    }
                }
                
                for (i, &grad) in gradients.biases.iter().enumerate() {
                    bias_gradients[i] += grad;
                }
            }
        }
        
        // æ›´æ–°ç½‘ç»œå‚æ•°
        let batch_size = trajectories.len() as f64;
        for (i, row) in policy_gradients.iter().enumerate() {
            for (j, &grad) in row.iter().enumerate() {
                self.policy_network.weights[i][j] += self.learning_rate * grad / batch_size;
            }
        }
        
        for (i, &grad) in bias_gradients.iter().enumerate() {
            self.policy_network.biases[i] += self.learning_rate * grad / batch_size;
        }
    }
    
    fn compute_returns(&self, rewards: &[f64]) -> Vec<f64> {
        let mut returns = vec![0.0; rewards.len()];
        let mut running_return = 0.0;
        
        for (i, &reward) in rewards.iter().enumerate().rev() {
            running_return = reward + self.gamma * running_return;
            returns[i] = running_return;
        }
        
        returns
    }
    
    fn compute_policy_gradients(&self, state: &[f64], action: usize, log_prob: f64, 
                               return_val: f64) -> PolicyNetwork {
        // ç®€åŒ–å®ç°ï¼šè®¡ç®—ç­–ç•¥æ¢¯åº¦
        let mut gradients = PolicyNetwork::new(
            self.policy_network.input_size,
            self.policy_network.hidden_size,
            self.policy_network.output_size
        );
        
        // è®¡ç®—æ¢¯åº¦ï¼ˆç®€åŒ–å®ç°ï¼‰
        let gradient_scale = return_val * log_prob;
        
        for i in 0..gradients.weights.len() {
            for j in 0..gradients.weights[i].len() {
                gradients.weights[i][j] = gradient_scale * 0.1; // ç®€åŒ–æ¢¯åº¦è®¡ç®—
            }
        }
        
        for i in 0..gradients.biases.len() {
            gradients.biases[i] = gradient_scale * 0.1;
        }
        
        gradients
    }
    
    pub fn collect_trajectory(&self, environment: &Environment, max_steps: usize) -> Trajectory {
        let mut states = Vec::new();
        let mut actions = Vec::new();
        let mut rewards = Vec::new();
        let mut log_probs = Vec::new();
        
        let mut current_state = vec![0.0; self.policy_network.input_size]; // åˆå§‹çŠ¶æ€
        let mut step_count = 0;
        
        while step_count < max_steps {
            states.push(current_state.clone());
            
            let (action, log_prob) = self.select_action(&current_state);
            actions.push(action);
            log_probs.push(log_prob);
            
            // æ‰§è¡ŒåŠ¨ä½œï¼ˆç®€åŒ–å®ç°ï¼‰
            let (next_state, reward, done) = self.simulate_action(&current_state, action, environment);
            
            rewards.push(reward);
            current_state = next_state;
            step_count += 1;
            
            if done {
                break;
            }
        }
        
        Trajectory {
            states,
            actions,
            rewards,
            log_probs,
        }
    }
    
    fn simulate_action(&self, state: &[f64], action: usize, environment: &Environment) -> (Vec<f64>, f64, bool) {
        // ç®€åŒ–å®ç°ï¼šæ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
        let next_state = state.clone(); // ç®€åŒ–çŠ¶æ€è½¬æ¢
        let reward = if action == 0 { 1.0 } else { -0.1 }; // ç®€åŒ–å¥–åŠ±å‡½æ•°
        let done = step_count > 100; // ç®€åŒ–ç»ˆæ­¢æ¡ä»¶
        
        (next_state, reward, done)
    }
}

impl PolicyNetwork {
    pub fn new(input_size: usize, hidden_size: usize, output_size: usize) -> Self {
        let mut weights = Vec::new();
        let mut biases = Vec::new();
        
        // è¾“å…¥å±‚åˆ°éšè—å±‚
        let input_weights = (0..hidden_size).map(|_| {
            (0..input_size).map(|_| (rand::random::<f64>() * 2.0 - 1.0) * 0.1).collect()
        }).collect();
        weights.push(input_weights);
        biases.push(0.0);
        
        // éšè—å±‚åˆ°è¾“å‡ºå±‚
        let output_weights = (0..output_size).map(|_| {
            (0..hidden_size).map(|_| (rand::random::<f64>() * 2.0 - 1.0) * 0.1).collect()
        }).collect();
        weights.push(output_weights);
        biases.push(0.0);
        
        PolicyNetwork {
            weights,
            biases,
            input_size,
            hidden_size,
            output_size,
        }
    }
    
    pub fn forward(&self, input: &[f64]) -> Vec<f64> {
        let mut current = input.to_vec();
        
        for (layer_idx, (weights, bias)) in self.weights.iter().zip(self.biases.iter()).enumerate() {
            let mut next = Vec::new();
            
            for (neuron_weights, &neuron_bias) in weights.iter().zip(std::iter::repeat(bias)) {
                let mut sum = neuron_bias;
                for (input_val, weight) in current.iter().zip(neuron_weights.iter()) {
                    sum += input_val * weight;
                }
                
                let output = if layer_idx == self.weights.len() - 1 {
                    // è¾“å‡ºå±‚ä½¿ç”¨softmax
                    sum.exp()
                } else {
                    // éšè—å±‚ä½¿ç”¨ReLU
                    sum.max(0.0)
                };
                
                next.push(output);
            }
            
            current = next;
        }
        
        // Softmaxå½’ä¸€åŒ–
        let sum: f64 = current.iter().sum();
        current.iter().map(|&x| x / sum).collect()
    }
}
```

### 4.3 æ·±åº¦Qç½‘ç»œå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct DeepQNetwork {
    pub q_network: QNetwork,
    pub target_network: QNetwork,
    pub learning_rate: f64,
    pub gamma: f64,
    pub epsilon: f64,
    pub epsilon_decay: f64,
    pub epsilon_min: f64,
    pub memory: ReplayBuffer,
    pub batch_size: usize,
    pub update_frequency: usize,
    pub step_count: usize,
}

#[derive(Debug, Clone)]
pub struct QNetwork {
    pub layers: Vec<Layer>,
    pub input_size: usize,
    pub output_size: usize,
}

#[derive(Debug, Clone)]
pub struct Layer {
    pub weights: Vec<Vec<f64>>,
    pub biases: Vec<f64>,
    pub activation: ActivationFunction,
}

#[derive(Debug, Clone)]
pub enum ActivationFunction {
    ReLU,
    Linear,
}

#[derive(Debug, Clone)]
pub struct ReplayBuffer {
    pub experiences: Vec<Experience>,
    pub capacity: usize,
}

#[derive(Debug, Clone)]
pub struct Experience {
    pub state: Vec<f64>,
    pub action: usize,
    pub reward: f64,
    pub next_state: Vec<f64>,
    pub done: bool,
}

impl DeepQNetwork {
    pub fn new(input_size: usize, hidden_size: usize, output_size: usize, 
               learning_rate: f64, gamma: f64, memory_capacity: usize) -> Self {
        let q_network = QNetwork::new(input_size, hidden_size, output_size);
        let target_network = QNetwork::new(input_size, hidden_size, output_size);
        
        DeepQNetwork {
            q_network,
            target_network,
            learning_rate,
            gamma,
            epsilon: 1.0,
            epsilon_decay: 0.995,
            epsilon_min: 0.01,
            memory: ReplayBuffer::new(memory_capacity),
            batch_size: 32,
            update_frequency: 100,
            step_count: 0,
        }
    }
    
    pub fn select_action(&mut self, state: &[f64], available_actions: &[usize]) -> usize {
        if rand::random::<f64>() < self.epsilon {
            // æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            available_actions[rand::random::<usize>() % available_actions.len()]
        } else {
            // åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            let q_values = self.q_network.forward(state);
            let mut best_action = available_actions[0];
            let mut best_q_value = q_values[best_action];
            
            for &action in available_actions.iter().skip(1) {
                let q_value = q_values[action];
                if q_value > best_q_value {
                    best_q_value = q_value;
                    best_action = action;
                }
            }
            
            best_action
        }
    }
    
    pub fn store_experience(&mut self, state: Vec<f64>, action: usize, reward: f64, 
                           next_state: Vec<f64>, done: bool) {
        let experience = Experience {
            state,
            action,
            reward,
            next_state,
            done,
        };
        
        self.memory.add(experience);
    }
    
    pub fn train(&mut self) -> Option<f64> {
        if self.memory.size() < self.batch_size {
            return None;
        }
        
        let batch = self.memory.sample(self.batch_size);
        let loss = self.update_network(&batch);
        
        self.step_count += 1;
        
        // æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if self.step_count % self.update_frequency == 0 {
            self.update_target_network();
        }
        
        // è¡°å‡æ¢ç´¢ç‡
        self.epsilon = (self.epsilon * self.epsilon_decay).max(self.epsilon_min);
        
        Some(loss)
    }
    
    fn update_network(&mut self, batch: &[Experience]) -> f64 {
        let mut total_loss = 0.0;
        
        for experience in batch {
            let current_q_values = self.q_network.forward(&experience.state);
            let next_q_values = self.target_network.forward(&experience.next_state);
            
            let target_q = if experience.done {
                experience.reward
            } else {
                experience.reward + self.gamma * next_q_values.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b))
            };
            
            let current_q = current_q_values[experience.action];
            let loss = 0.5 * (target_q - current_q).powi(2);
            total_loss += loss;
            
            // è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œï¼ˆç®€åŒ–å®ç°ï¼‰
            self.update_gradients(&experience.state, experience.action, target_q);
        }
        
        total_loss / batch.len() as f64
    }
    
    fn update_gradients(&mut self, state: &[f64], action: usize, target_q: f64) {
        // ç®€åŒ–å®ç°ï¼šä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ›´æ–°æƒé‡
        let current_q_values = self.q_network.forward(state);
        let current_q = current_q_values[action];
        let error = target_q - current_q;
        
        // æ›´æ–°ç½‘ç»œæƒé‡ï¼ˆç®€åŒ–å®ç°ï¼‰
        for layer in &mut self.q_network.layers {
            for row in &mut layer.weights {
                for weight in row {
                    *weight += self.learning_rate * error * 0.1; // ç®€åŒ–æ¢¯åº¦
                }
            }
            
            for bias in &mut layer.biases {
                *bias += self.learning_rate * error * 0.1;
            }
        }
    }
    
    fn update_target_network(&mut self) {
        // å¤åˆ¶ä¸»ç½‘ç»œæƒé‡åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_network = self.q_network.clone();
    }
    
    pub fn evaluate(&self, environment: &Environment, episodes: usize) -> f64 {
        let mut total_reward = 0.0;
        
        for _ in 0..episodes {
            let mut state = vec![0.0; self.q_network.input_size];
            let mut episode_reward = 0.0;
            let mut step_count = 0;
            
            loop {
                let available_actions = (0..self.q_network.output_size).collect::<Vec<_>>();
                let q_values = self.q_network.forward(&state);
                
                let mut best_action = 0;
                let mut best_q_value = q_values[0];
                
                for (action, &q_value) in q_values.iter().enumerate() {
                    if q_value > best_q_value {
                        best_q_value = q_value;
                        best_action = action;
                    }
                }
                
                let (next_state, reward, done) = self.simulate_action(&state, best_action, environment);
                episode_reward += reward;
                state = next_state;
                step_count += 1;
                
                if done || step_count > 1000 {
                    break;
                }
            }
            
            total_reward += episode_reward;
        }
        
        total_reward / episodes as f64
    }
    
    fn simulate_action(&self, state: &[f64], action: usize, environment: &Environment) -> (Vec<f64>, f64, bool) {
        // ç®€åŒ–å®ç°ï¼šæ¨¡æ‹Ÿç¯å¢ƒäº¤äº’
        let next_state = state.clone();
        let reward = if action == 0 { 1.0 } else { -0.1 };
        let done = false;
        
        (next_state, reward, done)
    }
}

impl QNetwork {
    pub fn new(input_size: usize, hidden_size: usize, output_size: usize) -> Self {
        let mut layers = Vec::new();
        
        // è¾“å…¥å±‚åˆ°éšè—å±‚
        let input_weights = (0..hidden_size).map(|_| {
            (0..input_size).map(|_| (rand::random::<f64>() * 2.0 - 1.0) * 0.1).collect()
        }).collect();
        layers.push(Layer {
            weights: input_weights,
            biases: vec![0.0; hidden_size],
            activation: ActivationFunction::ReLU,
        });
        
        // éšè—å±‚åˆ°è¾“å‡ºå±‚
        let output_weights = (0..output_size).map(|_| {
            (0..hidden_size).map(|_| (rand::random::<f64>() * 2.0 - 1.0) * 0.1).collect()
        }).collect();
        layers.push(Layer {
            weights: output_weights,
            biases: vec![0.0; output_size],
            activation: ActivationFunction::Linear,
        });
        
        QNetwork {
            layers,
            input_size,
            output_size,
        }
    }
    
    pub fn forward(&self, input: &[f64]) -> Vec<f64> {
        let mut current = input.to_vec();
        
        for layer in &self.layers {
            let mut next = Vec::new();
            
            for (neuron_weights, &bias) in layer.weights.iter().zip(layer.biases.iter()) {
                let mut sum = bias;
                for (input_val, weight) in current.iter().zip(neuron_weights.iter()) {
                    sum += input_val * weight;
                }
                
                let output = match layer.activation {
                    ActivationFunction::ReLU => sum.max(0.0),
                    ActivationFunction::Linear => sum,
                };
                
                next.push(output);
            }
            
            current = next;
        }
        
        current
    }
}

impl ReplayBuffer {
    pub fn new(capacity: usize) -> Self {
        ReplayBuffer {
            experiences: Vec::new(),
            capacity,
        }
    }
    
    pub fn add(&mut self, experience: Experience) {
        if self.experiences.len() >= self.capacity {
            self.experiences.remove(0);
        }
        self.experiences.push(experience);
    }
    
    pub fn sample(&self, batch_size: usize) -> Vec<Experience> {
        let mut batch = Vec::new();
        let size = self.experiences.len();
        
        for _ in 0..batch_size {
            let index = rand::random::<usize>() % size;
            batch.push(self.experiences[index].clone());
        }
        
        batch
    }
    
    pub fn size(&self) -> usize {
        self.experiences.len()
    }
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æœºå™¨å­¦ä¹ ç†è®º](../01_Machine_Learning/01_Machine_Learning_Theory.md)
- [æ·±åº¦å­¦ä¹ ç†è®º](../02_Deep_Learning/01_Deep_Learning_Theory.md)
- [è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º](../04_Natural_Language_Processing/01_Natural_Language_Processing_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.
2. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. Nature.
3. Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0


## æ‰¹åˆ¤æ€§åˆ†æ

- æœ¬èŠ‚å†…å®¹å¾…è¡¥å……ï¼šè¯·ä»å¤šå…ƒç†è®ºè§†è§’ã€å±€é™æ€§ã€äº‰è®®ç‚¹ã€åº”ç”¨å‰æ™¯ç­‰æ–¹é¢è¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æã€‚
