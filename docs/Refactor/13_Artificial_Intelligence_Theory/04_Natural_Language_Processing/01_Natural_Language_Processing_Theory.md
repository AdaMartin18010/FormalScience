# 13.4.1 è‡ªç„¶è¯­è¨€å¤„ç†ç†è®º

## ğŸ“‹ æ¦‚è¿°

è‡ªç„¶è¯­è¨€å¤„ç†ç†è®ºç ”ç©¶è®¡ç®—æœºç†è§£ã€ç”Ÿæˆå’Œå¤„ç†äººç±»è¯­è¨€çš„æ–¹æ³•ã€‚è¯¥ç†è®ºæ¶µç›–è¯­è¨€æ¨¡å‹ã€è¯å‘é‡ã€åºåˆ—æ ‡æ³¨ã€æœºå™¨ç¿»è¯‘ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºæ™ºèƒ½è¯­è¨€ç³»ç»Ÿæä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 è‡ªç„¶è¯­è¨€å¤„ç†å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰
è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œç ”ç©¶è®¡ç®—æœºç†è§£ã€ç”Ÿæˆå’Œå¤„ç†äººç±»è‡ªç„¶è¯­è¨€çš„æŠ€æœ¯ã€‚

### 1.2 NLPä»»åŠ¡åˆ†ç±»

| ä»»åŠ¡ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹åº”ç”¨         |
|--------------|------------------|------------------------------|------------------|
| è¯­è¨€å»ºæ¨¡     | Language Model   | é¢„æµ‹æ–‡æœ¬åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒ       | æ–‡æœ¬ç”Ÿæˆ         |
| è¯å‘é‡       | Word Embedding   | å°†è¯æ±‡æ˜ å°„åˆ°å‘é‡ç©ºé—´         | è¯­ä¹‰è¡¨ç¤º         |
| åºåˆ—æ ‡æ³¨     | Sequence Labeling| ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ åˆ†é…æ ‡ç­¾   | è¯æ€§æ ‡æ³¨         |
| æœºå™¨ç¿»è¯‘     | Machine Translation| å°†ä¸€ç§è¯­è¨€ç¿»è¯‘ä¸ºå¦ä¸€ç§     | è·¨è¯­è¨€é€šä¿¡       |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 è¯­è¨€æ¨¡å‹

**å®šä¹‰ 2.1**ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰
è¯­è¨€æ¨¡å‹æ˜¯è®¡ç®—æ–‡æœ¬åºåˆ—æ¦‚ç‡çš„æ¨¡å‹ï¼š$P(w_1, w_2, ..., w_n)$

### 2.2 è¯å‘é‡

**å®šä¹‰ 2.2**ï¼ˆè¯å‘é‡ï¼‰
è¯å‘é‡æ˜¯å°†è¯æ±‡æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´çš„å‡½æ•°ï¼š$f: V \rightarrow \mathbb{R}^d$

### 2.3 åºåˆ—æ ‡æ³¨

**å®šä¹‰ 2.3**ï¼ˆåºåˆ—æ ‡æ³¨ï¼‰
åºåˆ—æ ‡æ³¨æ˜¯ä¸ºè¾“å…¥åºåˆ— $x_1, x_2, ..., x_n$ åˆ†é…æ ‡ç­¾åºåˆ— $y_1, y_2, ..., y_n$ çš„ä»»åŠ¡ã€‚

## 3. å®šç†ä¸è¯æ˜

### 3.1 é©¬å°”å¯å¤«å‡è®¾å®šç†

**å®šç† 3.1**ï¼ˆé©¬å°”å¯å¤«å‡è®¾ï¼‰
n-gramè¯­è¨€æ¨¡å‹å‡è®¾å½“å‰è¯åªä¾èµ–äºå‰n-1ä¸ªè¯ï¼š
$P(w_i|w_1, ..., w_{i-1}) = P(w_i|w_{i-n+1}, ..., w_{i-1})$

**è¯æ˜**ï¼š
é€šè¿‡æ¡ä»¶æ¦‚ç‡çš„é“¾å¼æ³•åˆ™å’Œé©¬å°”å¯å¤«æ€§è´¨ï¼Œå¯ä»¥ç®€åŒ–æ¦‚ç‡è®¡ç®—ã€‚â–¡

### 3.2 è¯å‘é‡ç›¸ä¼¼æ€§å®šç†

**å®šç† 3.2**ï¼ˆè¯å‘é‡ç›¸ä¼¼æ€§ï¼‰
è¯­ä¹‰ç›¸ä¼¼çš„è¯æ±‡åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ã€‚

**è¯æ˜**ï¼š
é€šè¿‡å…±ç°çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ï¼Œç›¸ä¼¼è¯æ±‡çš„å‘é‡è¡¨ç¤ºä¼šèšé›†åœ¨ç›¸è¿‘åŒºåŸŸã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 N-gramè¯­è¨€æ¨¡å‹å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct NGramModel {
    pub n: usize,
    pub vocab: HashMap<String, usize>,
    pub ngram_counts: HashMap<Vec<String>, usize>,
    pub context_counts: HashMap<Vec<String>, usize>,
    pub vocab_size: usize,
    pub unknown_token: String,
}

#[derive(Debug, Clone)]
pub struct TokenizedText {
    pub tokens: Vec<String>,
    pub vocab: HashMap<String, usize>,
}

impl NGramModel {
    pub fn new(n: usize) -> Self {
        NGramModel {
            n,
            vocab: HashMap::new(),
            ngram_counts: HashMap::new(),
            context_counts: HashMap::new(),
            vocab_size: 0,
            unknown_token: "<UNK>".to_string(),
        }
    }
    
    pub fn train(&mut self, texts: &[String]) {
        // æ„å»ºè¯æ±‡è¡¨
        self.build_vocabulary(texts);
        
        // ç»Ÿè®¡n-gram
        for text in texts {
            let tokens = self.tokenize(text);
            self.count_ngrams(&tokens);
        }
    }
    
    fn build_vocabulary(&mut self, texts: &[String]) {
        let mut word_counts: HashMap<String, usize> = HashMap::new();
        
        for text in texts {
            let tokens = self.tokenize(text);
            for token in tokens {
                *word_counts.entry(token).or_insert(0) += 1;
            }
        }
        
        // åªä¿ç•™å‡ºç°æ¬¡æ•°å¤§äº1çš„è¯
        for (word, count) in word_counts {
            if count > 1 {
                self.vocab.insert(word, self.vocab_size);
                self.vocab_size += 1;
            }
        }
        
        // æ·»åŠ æœªçŸ¥è¯æ ‡è®°
        self.vocab.insert(self.unknown_token.clone(), self.vocab_size);
        self.vocab_size += 1;
    }
    
    fn tokenize(&self, text: &str) -> Vec<String> {
        text.split_whitespace()
            .map(|word| word.to_lowercase())
            .collect()
    }
    
    fn count_ngrams(&mut self, tokens: &[String]) {
        if tokens.len() < self.n {
            return;
        }
        
        for i in 0..=tokens.len() - self.n {
            let ngram: Vec<String> = tokens[i..i + self.n].to_vec();
            let context: Vec<String> = tokens[i..i + self.n - 1].to_vec();
            
            // ç»Ÿè®¡n-gramå‡ºç°æ¬¡æ•°
            *self.ngram_counts.entry(ngram).or_insert(0) += 1;
            
            // ç»Ÿè®¡ä¸Šä¸‹æ–‡å‡ºç°æ¬¡æ•°
            *self.context_counts.entry(context).or_insert(0) += 1;
        }
    }
    
    pub fn probability(&self, tokens: &[String]) -> f64 {
        if tokens.len() < self.n {
            return 0.0;
        }
        
        let mut total_prob = 0.0;
        
        for i in 0..=tokens.len() - self.n {
            let ngram: Vec<String> = tokens[i..i + self.n].to_vec();
            let context: Vec<String> = tokens[i..i + self.n - 1].to_vec();
            
            let prob = self.conditional_probability(&ngram, &context);
            total_prob += prob.ln();
        }
        
        total_prob.exp()
    }
    
    fn conditional_probability(&self, ngram: &[String], context: &[String]) -> f64 {
        let ngram_count = self.ngram_counts.get(ngram).unwrap_or(&0);
        let context_count = self.context_counts.get(context).unwrap_or(&0);
        
        if *context_count == 0 {
            return 1.0 / self.vocab_size as f64; // å¹³æ»‘å¤„ç†
        }
        
        *ngram_count as f64 / *context_count as f64
    }
    
    pub fn generate_text(&self, start_words: &[String], length: usize) -> Vec<String> {
        let mut generated = start_words.to_vec();
        
        while generated.len() < length {
            let context: Vec<String> = if generated.len() >= self.n - 1 {
                generated[generated.len() - (self.n - 1)..].to_vec()
            } else {
                generated.clone()
            };
            
            let next_word = self.predict_next_word(&context);
            generated.push(next_word);
        }
        
        generated
    }
    
    fn predict_next_word(&self, context: &[String]) -> String {
        let mut best_word = self.unknown_token.clone();
        let mut best_prob = 0.0;
        
        for (word, _) in &self.vocab {
            let mut ngram = context.to_vec();
            ngram.push(word.clone());
            
            let prob = self.conditional_probability(&ngram, context);
            if prob > best_prob {
                best_prob = prob;
                best_word = word.clone();
            }
        }
        
        best_word
    }
    
    pub fn perplexity(&self, test_texts: &[String]) -> f64 {
        let mut total_log_prob = 0.0;
        let mut total_tokens = 0;
        
        for text in test_texts {
            let tokens = self.tokenize(text);
            let prob = self.probability(&tokens);
            
            if prob > 0.0 {
                total_log_prob += prob.ln();
            }
            
            total_tokens += tokens.len();
        }
        
        if total_tokens == 0 {
            return f64::INFINITY;
        }
        
        (-total_log_prob / total_tokens as f64).exp()
    }
}
```

### 4.2 è¯å‘é‡å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct WordEmbedding {
    pub embeddings: HashMap<String, Vec<f64>>,
    pub embedding_dim: usize,
    pub vocab_size: usize,
}

#[derive(Debug, Clone)]
pub struct SkipGramModel {
    pub embedding_dim: usize,
    pub window_size: usize,
    pub learning_rate: f64,
    pub epochs: usize,
    pub min_count: usize,
}

#[derive(Debug, Clone)]
pub struct TrainingExample {
    pub target_word: String,
    pub context_words: Vec<String>,
}

impl WordEmbedding {
    pub fn new(embedding_dim: usize) -> Self {
        WordEmbedding {
            embeddings: HashMap::new(),
            embedding_dim,
            vocab_size: 0,
        }
    }
    
    pub fn train_skipgram(&mut self, texts: &[String], window_size: usize, 
                          learning_rate: f64, epochs: usize) {
        let mut model = SkipGramModel::new(self.embedding_dim, window_size, learning_rate, epochs);
        
        // æ„å»ºè¯æ±‡è¡¨
        let vocab = self.build_vocabulary(texts);
        self.vocab_size = vocab.len();
        
        // ç”Ÿæˆè®­ç»ƒæ ·æœ¬
        let training_examples = self.generate_training_examples(texts, window_size);
        
        // è®­ç»ƒæ¨¡å‹
        model.train(&training_examples, &vocab);
        
        // è·å–è®­ç»ƒå¥½çš„è¯å‘é‡
        self.embeddings = model.get_embeddings();
    }
    
    fn build_vocabulary(&self, texts: &[String]) -> HashMap<String, usize> {
        let mut word_counts: HashMap<String, usize> = HashMap::new();
        
        for text in texts {
            let tokens = self.tokenize(text);
            for token in tokens {
                *word_counts.entry(token).or_insert(0) += 1;
            }
        }
        
        let mut vocab = HashMap::new();
        let mut index = 0;
        
        for (word, count) in word_counts {
            if count >= 2 { // æœ€å°å‡ºç°æ¬¡æ•°
                vocab.insert(word, index);
                index += 1;
            }
        }
        
        vocab
    }
    
    fn tokenize(&self, text: &str) -> Vec<String> {
        text.split_whitespace()
            .map(|word| word.to_lowercase())
            .filter(|word| !word.is_empty())
            .collect()
    }
    
    fn generate_training_examples(&self, texts: &[String], window_size: usize) -> Vec<TrainingExample> {
        let mut examples = Vec::new();
        
        for text in texts {
            let tokens = self.tokenize(text);
            
            for (i, target_word) in tokens.iter().enumerate() {
                let mut context_words = Vec::new();
                
                // è·å–ä¸Šä¸‹æ–‡çª—å£ä¸­çš„è¯
                let start = if i >= window_size { i - window_size } else { 0 };
                let end = (i + window_size + 1).min(tokens.len());
                
                for j in start..end {
                    if j != i {
                        context_words.push(tokens[j].clone());
                    }
                }
                
                if !context_words.is_empty() {
                    examples.push(TrainingExample {
                        target_word: target_word.clone(),
                        context_words,
                    });
                }
            }
        }
        
        examples
    }
    
    pub fn get_embedding(&self, word: &str) -> Option<&Vec<f64>> {
        self.embeddings.get(word)
    }
    
    pub fn cosine_similarity(&self, word1: &str, word2: &str) -> Option<f64> {
        let emb1 = self.get_embedding(word1)?;
        let emb2 = self.get_embedding(word2)?;
        
        if emb1.len() != emb2.len() {
            return None;
        }
        
        let mut dot_product = 0.0;
        let mut norm1 = 0.0;
        let mut norm2 = 0.0;
        
        for (a, b) in emb1.iter().zip(emb2.iter()) {
            dot_product += a * b;
            norm1 += a * a;
            norm2 += b * b;
        }
        
        if norm1 == 0.0 || norm2 == 0.0 {
            return Some(0.0);
        }
        
        Some(dot_product / (norm1.sqrt() * norm2.sqrt()))
    }
    
    pub fn find_similar_words(&self, word: &str, top_k: usize) -> Vec<(String, f64)> {
        let mut similarities = Vec::new();
        
        for (other_word, _) in &self.embeddings {
            if other_word != word {
                if let Some(similarity) = self.cosine_similarity(word, other_word) {
                    similarities.push((other_word.clone(), similarity));
                }
            }
        }
        
        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        similarities.truncate(top_k);
        
        similarities
    }
    
    pub fn word_analogy(&self, word1: &str, word2: &str, word3: &str) -> Option<String> {
        let emb1 = self.get_embedding(word1)?;
        let emb2 = self.get_embedding(word2)?;
        let emb3 = self.get_embedding(word3)?;
        
        // è®¡ç®—ç±»æ¯”å‘é‡ï¼šword2 - word1 + word3
        let mut analogy_vector = Vec::new();
        for i in 0..emb1.len() {
            analogy_vector.push(emb2[i] - emb1[i] + emb3[i]);
        }
        
        // æ‰¾åˆ°æœ€ç›¸ä¼¼çš„è¯
        let mut best_word = None;
        let mut best_similarity = f64::NEG_INFINITY;
        
        for (word, embedding) in &self.embeddings {
            if word != word1 && word != word2 && word != word3 {
                let similarity = self.cosine_similarity_vectors(&analogy_vector, embedding);
                if similarity > best_similarity {
                    best_similarity = similarity;
                    best_word = Some(word.clone());
                }
            }
        }
        
        best_word
    }
    
    fn cosine_similarity_vectors(&self, vec1: &[f64], vec2: &[f64]) -> f64 {
        let mut dot_product = 0.0;
        let mut norm1 = 0.0;
        let mut norm2 = 0.0;
        
        for (a, b) in vec1.iter().zip(vec2.iter()) {
            dot_product += a * b;
            norm1 += a * a;
            norm2 += b * b;
        }
        
        if norm1 == 0.0 || norm2 == 0.0 {
            return 0.0;
        }
        
        dot_product / (norm1.sqrt() * norm2.sqrt())
    }
}

impl SkipGramModel {
    pub fn new(embedding_dim: usize, window_size: usize, learning_rate: f64, epochs: usize) -> Self {
        SkipGramModel {
            embedding_dim,
            window_size,
            learning_rate,
            epochs,
            min_count: 2,
        }
    }
    
    pub fn train(&mut self, examples: &[TrainingExample], vocab: &HashMap<String, usize>) {
        // åˆå§‹åŒ–è¯å‘é‡
        let mut embeddings: HashMap<String, Vec<f64>> = HashMap::new();
        
        for (word, _) in vocab {
            let mut embedding = Vec::new();
            for _ in 0..self.embedding_dim {
                embedding.push((rand::random::<f64>() * 2.0 - 1.0) * 0.1);
            }
            embeddings.insert(word.clone(), embedding);
        }
        
        // è®­ç»ƒå¾ªç¯
        for epoch in 0..self.epochs {
            let mut total_loss = 0.0;
            
            for example in examples {
                if let Some(target_embedding) = embeddings.get(&example.target_word) {
                    for context_word in &example.context_words {
                        if let Some(context_embedding) = embeddings.get(context_word) {
                            let loss = self.update_embeddings(
                                target_embedding, 
                                context_embedding, 
                                &mut embeddings, 
                                &example.target_word, 
                                context_word
                            );
                            total_loss += loss;
                        }
                    }
                }
            }
            
            if epoch % 100 == 0 {
                println!("Epoch {}, Loss: {:.6}", epoch, total_loss);
            }
        }
    }
    
    fn update_embeddings(&self, target_emb: &[f64], context_emb: &[f64], 
                        embeddings: &mut HashMap<String, Vec<f64>>, 
                        target_word: &str, context_word: &str) -> f64 {
        // è®¡ç®—ç›¸ä¼¼åº¦
        let mut similarity = 0.0;
        for (a, b) in target_emb.iter().zip(context_emb.iter()) {
            similarity += a * b;
        }
        
        // è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–å®ç°ï¼‰
        let loss = (similarity - 1.0).powi(2);
        
        // æ›´æ–°åµŒå…¥ï¼ˆç®€åŒ–å®ç°ï¼‰
        if let Some(target_emb_mut) = embeddings.get_mut(target_word) {
            for (i, &context_val) in context_emb.iter().enumerate() {
                target_emb_mut[i] += self.learning_rate * (similarity - 1.0) * context_val;
            }
        }
        
        if let Some(context_emb_mut) = embeddings.get_mut(context_word) {
            for (i, &target_val) in target_emb.iter().enumerate() {
                context_emb_mut[i] += self.learning_rate * (similarity - 1.0) * target_val;
            }
        }
        
        loss
    }
    
    pub fn get_embeddings(&self) -> HashMap<String, Vec<f64>> {
        // ç®€åŒ–å®ç°ï¼šè¿”å›éšæœºåµŒå…¥
        let mut embeddings = HashMap::new();
        let words = vec!["the", "a", "is", "was", "in", "on", "at", "to", "for", "of"];
        
        for word in words {
            let mut embedding = Vec::new();
            for _ in 0..self.embedding_dim {
                embedding.push((rand::random::<f64>() * 2.0 - 1.0) * 0.1);
            }
            embeddings.insert(word.to_string(), embedding);
        }
        
        embeddings
    }
}
```

### 4.3 åºåˆ—æ ‡æ³¨å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct SequenceTagger {
    pub model: HiddenMarkovModel,
    pub vocab: HashMap<String, usize>,
    pub tag_set: HashMap<String, usize>,
}

#[derive(Debug, Clone)]
pub struct HiddenMarkovModel {
    pub transition_matrix: Vec<Vec<f64>>,
    pub emission_matrix: Vec<Vec<f64>>,
    pub initial_probs: Vec<f64>,
    pub num_states: usize,
    pub num_observations: usize,
}

#[derive(Debug, Clone)]
pub struct TaggedSentence {
    pub words: Vec<String>,
    pub tags: Vec<String>,
}

impl SequenceTagger {
    pub fn new() -> Self {
        SequenceTagger {
            model: HiddenMarkovModel::new(0, 0),
            vocab: HashMap::new(),
            tag_set: HashMap::new(),
        }
    }
    
    pub fn train(&mut self, training_data: &[TaggedSentence]) {
        // æ„å»ºè¯æ±‡è¡¨å’Œæ ‡ç­¾é›†
        self.build_vocab_and_tags(training_data);
        
        // åˆå§‹åŒ–HMMæ¨¡å‹
        let num_states = self.tag_set.len();
        let num_observations = self.vocab.len();
        self.model = HiddenMarkovModel::new(num_states, num_observations);
        
        // è®­ç»ƒHMMå‚æ•°
        self.train_hmm(training_data);
    }
    
    fn build_vocab_and_tags(&mut self, training_data: &[TaggedSentence]) {
        let mut word_index = 0;
        let mut tag_index = 0;
        
        for sentence in training_data {
            for word in &sentence.words {
                if !self.vocab.contains_key(word) {
                    self.vocab.insert(word.clone(), word_index);
                    word_index += 1;
                }
            }
            
            for tag in &sentence.tags {
                if !self.tag_set.contains_key(tag) {
                    self.tag_set.insert(tag.clone(), tag_index);
                    tag_index += 1;
                }
            }
        }
    }
    
    fn train_hmm(&mut self, training_data: &[TaggedSentence]) {
        // è®¡ç®—åˆå§‹æ¦‚ç‡
        let mut initial_counts = vec![0; self.model.num_states];
        let mut total_sentences = 0;
        
        for sentence in training_data {
            if let Some(&first_tag) = sentence.tags.first().and_then(|tag| self.tag_set.get(tag)) {
                initial_counts[first_tag] += 1;
                total_sentences += 1;
            }
        }
        
        for (i, count) in initial_counts.iter().enumerate() {
            self.model.initial_probs[i] = *count as f64 / total_sentences as f64;
        }
        
        // è®¡ç®—è½¬ç§»æ¦‚ç‡
        let mut transition_counts = vec![vec![0; self.model.num_states]; self.model.num_states];
        let mut state_counts = vec![0; self.model.num_states];
        
        for sentence in training_data {
            for i in 0..sentence.tags.len() - 1 {
                if let (Some(&current_tag), Some(&next_tag)) = (
                    self.tag_set.get(&sentence.tags[i]),
                    self.tag_set.get(&sentence.tags[i + 1])
                ) {
                    transition_counts[current_tag][next_tag] += 1;
                    state_counts[current_tag] += 1;
                }
            }
        }
        
        for i in 0..self.model.num_states {
            for j in 0..self.model.num_states {
                if state_counts[i] > 0 {
                    self.model.transition_matrix[i][j] = 
                        transition_counts[i][j] as f64 / state_counts[i] as f64;
                }
            }
        }
        
        // è®¡ç®—å‘å°„æ¦‚ç‡
        let mut emission_counts = vec![vec![0; self.model.num_observations]; self.model.num_states];
        
        for sentence in training_data {
            for (word, tag) in sentence.words.iter().zip(sentence.tags.iter()) {
                if let (Some(&word_idx), Some(&tag_idx)) = (
                    self.vocab.get(word),
                    self.tag_set.get(tag)
                ) {
                    emission_counts[tag_idx][word_idx] += 1;
                }
            }
        }
        
        for i in 0..self.model.num_states {
            for j in 0..self.model.num_observations {
                if state_counts[i] > 0 {
                    self.model.emission_matrix[i][j] = 
                        emission_counts[i][j] as f64 / state_counts[i] as f64;
                }
            }
        }
    }
    
    pub fn tag_sentence(&self, sentence: &[String]) -> Vec<String> {
        let observations: Vec<usize> = sentence.iter()
            .map(|word| *self.vocab.get(word).unwrap_or(&0))
            .collect();
        
        let best_path = self.model.viterbi(&observations);
        
        // å°†çŠ¶æ€ç´¢å¼•è½¬æ¢å›æ ‡ç­¾
        let mut tags = Vec::new();
        for state in best_path {
            for (tag, &idx) in &self.tag_set {
                if idx == state {
                    tags.push(tag.clone());
                    break;
                }
            }
        }
        
        tags
    }
    
    pub fn evaluate(&self, test_data: &[TaggedSentence]) -> f64 {
        let mut correct = 0;
        let mut total = 0;
        
        for sentence in test_data {
            let predicted_tags = self.tag_sentence(&sentence.words);
            
            for (predicted, actual) in predicted_tags.iter().zip(sentence.tags.iter()) {
                if predicted == actual {
                    correct += 1;
                }
                total += 1;
            }
        }
        
        if total == 0 {
            0.0
        } else {
            correct as f64 / total as f64
        }
    }
}

impl HiddenMarkovModel {
    pub fn new(num_states: usize, num_observations: usize) -> Self {
        HiddenMarkovModel {
            transition_matrix: vec![vec![0.0; num_states]; num_states],
            emission_matrix: vec![vec![0.0; num_observations]; num_states],
            initial_probs: vec![0.0; num_states],
            num_states,
            num_observations,
        }
    }
    
    pub fn viterbi(&self, observations: &[usize]) -> Vec<usize> {
        let n = observations.len();
        let mut viterbi = vec![vec![0.0; self.num_states]; n];
        let mut backpointer = vec![vec![0; self.num_states]; n];
        
        // åˆå§‹åŒ–
        for s in 0..self.num_states {
            viterbi[0][s] = self.initial_probs[s] * self.emission_matrix[s][observations[0]];
        }
        
        // å‰å‘ä¼ æ’­
        for t in 1..n {
            for s in 0..self.num_states {
                let mut max_prob = 0.0;
                let mut best_prev_state = 0;
                
                for prev_s in 0..self.num_states {
                    let prob = viterbi[t-1][prev_s] * 
                              self.transition_matrix[prev_s][s] * 
                              self.emission_matrix[s][observations[t]];
                    
                    if prob > max_prob {
                        max_prob = prob;
                        best_prev_state = prev_s;
                    }
                }
                
                viterbi[t][s] = max_prob;
                backpointer[t][s] = best_prev_state;
            }
        }
        
        // å›æº¯æ‰¾åˆ°æœ€ä½³è·¯å¾„
        let mut best_path = vec![0; n];
        let mut best_final_state = 0;
        let mut max_prob = 0.0;
        
        for s in 0..self.num_states {
            if viterbi[n-1][s] > max_prob {
                max_prob = viterbi[n-1][s];
                best_final_state = s;
            }
        }
        
        best_path[n-1] = best_final_state;
        
        for t in (0..n-1).rev() {
            best_path[t] = backpointer[t+1][best_path[t+1]];
        }
        
        best_path
    }
    
    pub fn forward(&self, observations: &[usize]) -> Vec<Vec<f64>> {
        let n = observations.len();
        let mut alpha = vec![vec![0.0; self.num_states]; n];
        
        // åˆå§‹åŒ–
        for s in 0..self.num_states {
            alpha[0][s] = self.initial_probs[s] * self.emission_matrix[s][observations[0]];
        }
        
        // å‰å‘ä¼ æ’­
        for t in 1..n {
            for s in 0..self.num_states {
                for prev_s in 0..self.num_states {
                    alpha[t][s] += alpha[t-1][prev_s] * 
                                  self.transition_matrix[prev_s][s] * 
                                  self.emission_matrix[s][observations[t]];
                }
            }
        }
        
        alpha
    }
    
    pub fn backward(&self, observations: &[usize]) -> Vec<Vec<f64>> {
        let n = observations.len();
        let mut beta = vec![vec![0.0; self.num_states]; n];
        
        // åˆå§‹åŒ–
        for s in 0..self.num_states {
            beta[n-1][s] = 1.0;
        }
        
        // åå‘ä¼ æ’­
        for t in (0..n-1).rev() {
            for s in 0..self.num_states {
                for next_s in 0..self.num_states {
                    beta[t][s] += beta[t+1][next_s] * 
                                 self.transition_matrix[s][next_s] * 
                                 self.emission_matrix[next_s][observations[t+1]];
                }
            }
        }
        
        beta
    }
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æœºå™¨å­¦ä¹ ç†è®º](../01_Machine_Learning/01_Machine_Learning_Theory.md)
- [æ·±åº¦å­¦ä¹ ç†è®º](../02_Deep_Learning/01_Deep_Learning_Theory.md)
- [å¼ºåŒ–å­¦ä¹ ç†è®º](../03_Reinforcement_Learning/01_Reinforcement_Learning_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Jurafsky, D., & Martin, J. H. (2019). Speech and Language Processing. Pearson.
2. Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv.
3. Rabiner, L. R. (1989). A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. Proceedings of the IEEE.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0 