# 14.1.1 ç»Ÿè®¡åˆ†æç†è®º

## ç›®å½•

- [14.1.1 ç»Ÿè®¡åˆ†æç†è®º](#1411-ç»Ÿè®¡åˆ†æç†è®º)
  - [ç›®å½•](#ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
  - [1. åŸºæœ¬æ¦‚å¿µ](#1-åŸºæœ¬æ¦‚å¿µ)
    - [1.1 ç»Ÿè®¡åˆ†æå®šä¹‰](#11-ç»Ÿè®¡åˆ†æå®šä¹‰)
    - [1.2 ç»Ÿè®¡åˆ†ææ–¹æ³•åˆ†ç±»](#12-ç»Ÿè®¡åˆ†ææ–¹æ³•åˆ†ç±»)
  - [2. å½¢å¼åŒ–å®šä¹‰](#2-å½¢å¼åŒ–å®šä¹‰)
    - [2.1 éšæœºå˜é‡](#21-éšæœºå˜é‡)
    - [2.2 æ¦‚ç‡åˆ†å¸ƒ](#22-æ¦‚ç‡åˆ†å¸ƒ)
    - [2.3 æœŸæœ›å’Œæ–¹å·®](#23-æœŸæœ›å’Œæ–¹å·®)
  - [3. å®šç†ä¸è¯æ˜](#3-å®šç†ä¸è¯æ˜)
    - [3.1 å¤§æ•°å®šå¾‹](#31-å¤§æ•°å®šå¾‹)
    - [3.2 ä¸­å¿ƒæé™å®šç†](#32-ä¸­å¿ƒæé™å®šç†)
  - [4. Rustä»£ç å®ç°](#4-rustä»£ç å®ç°)
    - [4.1 æè¿°æ€§ç»Ÿè®¡å®ç°](#41-æè¿°æ€§ç»Ÿè®¡å®ç°)
    - [4.2 å‡è®¾æ£€éªŒå®ç°](#42-å‡è®¾æ£€éªŒå®ç°)
    - [4.3 å›å½’åˆ†æå®ç°](#43-å›å½’åˆ†æå®ç°)
  - [5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨](#5-ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨)
  - [6. å‚è€ƒæ–‡çŒ®](#6-å‚è€ƒæ–‡çŒ®)
  - [æ‰¹åˆ¤æ€§åˆ†æ](#æ‰¹åˆ¤æ€§åˆ†æ)
    - [ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†](#ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†)
    - [ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ](#ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ)
    - [ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ](#ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ)
    - [åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›](#åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›)
    - [å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»](#å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»)

## ğŸ“‹ æ¦‚è¿°

ç»Ÿè®¡åˆ†æç†è®ºç ”ç©¶æ•°æ®çš„æ”¶é›†ã€æ•´ç†ã€åˆ†æå’Œè§£é‡Šæ–¹æ³•ã€‚è¯¥ç†è®ºæ¶µç›–æè¿°æ€§ç»Ÿè®¡ã€æ¨æ–­æ€§ç»Ÿè®¡ã€å‡è®¾æ£€éªŒã€å›å½’åˆ†æç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºæ•°æ®é©±åŠ¨çš„å†³ç­–æä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 ç»Ÿè®¡åˆ†æå®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆç»Ÿè®¡åˆ†æï¼‰
ç»Ÿè®¡åˆ†ææ˜¯æ”¶é›†ã€æ•´ç†ã€åˆ†æå’Œè§£é‡Šæ•°æ®ä»¥å‘ç°æ¨¡å¼ã€è¶‹åŠ¿å’Œå…³ç³»çš„ç§‘å­¦æ–¹æ³•ã€‚

### 1.2 ç»Ÿè®¡åˆ†ææ–¹æ³•åˆ†ç±»

| æ–¹æ³•ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹åº”ç”¨         |
|--------------|------------------|------------------------------|------------------|
| æè¿°æ€§ç»Ÿè®¡   | Descriptive Statistics| æ€»ç»“å’Œæè¿°æ•°æ®ç‰¹å¾       | æ•°æ®æ¢ç´¢         |
| æ¨æ–­æ€§ç»Ÿè®¡   | Inferential Statistics| ä»æ ·æœ¬æ¨æ–­æ€»ä½“ç‰¹å¾     | å‡è®¾æ£€éªŒ         |
| å›å½’åˆ†æ     | Regression Analysis| å»ºç«‹å˜é‡é—´å…³ç³»æ¨¡å‹       | é¢„æµ‹å»ºæ¨¡         |
| æ—¶é—´åºåˆ—åˆ†æ | Time Series Analysis| åˆ†ææ—¶é—´ç›¸å…³æ•°æ®       | è¶‹åŠ¿é¢„æµ‹         |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 éšæœºå˜é‡

**å®šä¹‰ 2.1**ï¼ˆéšæœºå˜é‡ï¼‰
éšæœºå˜é‡æ˜¯ä»æ ·æœ¬ç©ºé—´åˆ°å®æ•°çš„å‡½æ•°ï¼š$X: \Omega \rightarrow \mathbb{R}$

### 2.2 æ¦‚ç‡åˆ†å¸ƒ

**å®šä¹‰ 2.2**ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰
æ¦‚ç‡åˆ†å¸ƒæ˜¯éšæœºå˜é‡å–å€¼çš„æ¦‚ç‡å‡½æ•°ï¼š$P(X = x)$

### 2.3 æœŸæœ›å’Œæ–¹å·®

**å®šä¹‰ 2.3**ï¼ˆæœŸæœ›ï¼‰
éšæœºå˜é‡Xçš„æœŸæœ›å®šä¹‰ä¸ºï¼š$E[X] = \sum_{x} x \cdot P(X = x)$

**å®šä¹‰ 2.4**ï¼ˆæ–¹å·®ï¼‰
éšæœºå˜é‡Xçš„æ–¹å·®å®šä¹‰ä¸ºï¼š$Var(X) = E[(X - E[X])^2]$

## 3. å®šç†ä¸è¯æ˜

### 3.1 å¤§æ•°å®šå¾‹

**å®šç† 3.1**ï¼ˆå¤§æ•°å®šå¾‹ï¼‰
å¯¹äºç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡åºåˆ— $X_1, X_2, ..., X_n$ï¼Œå½“ $n \rightarrow \infty$ æ—¶ï¼š
$\frac{1}{n}\sum_{i=1}^{n} X_i \rightarrow E[X_1]$ ï¼ˆä¾æ¦‚ç‡æ”¶æ•›ï¼‰

**è¯æ˜**ï¼š
é€šè¿‡åˆ‡æ¯”é›ªå¤«ä¸ç­‰å¼å’Œç‹¬ç«‹æ€§çš„æ€§è´¨ï¼Œå¯ä»¥è¯æ˜æ ·æœ¬å‡å€¼æ”¶æ•›åˆ°æœŸæœ›ã€‚â–¡

### 3.2 ä¸­å¿ƒæé™å®šç†

**å®šç† 3.2**ï¼ˆä¸­å¿ƒæé™å®šç†ï¼‰
å¯¹äºç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºå˜é‡åºåˆ— $X_1, X_2, ..., X_n$ï¼Œå½“ $n \rightarrow \infty$ æ—¶ï¼š
$\frac{\sum_{i=1}^{n} X_i - n\mu}{\sqrt{n}\sigma} \rightarrow N(0,1)$ ï¼ˆä¾åˆ†å¸ƒæ”¶æ•›ï¼‰

**è¯æ˜**ï¼š
é€šè¿‡ç‰¹å¾å‡½æ•°çš„æ€§è´¨å’Œæ³°å‹’å±•å¼€ï¼Œå¯ä»¥è¯æ˜æ ‡å‡†åŒ–å’Œæ”¶æ•›åˆ°æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 æè¿°æ€§ç»Ÿè®¡å®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct DescriptiveStatistics {
    pub data: Vec<f64>,
    pub n: usize,
}

#[derive(Debug, Clone)]
pub struct SummaryStatistics {
    pub mean: f64,
    pub median: f64,
    pub mode: f64,
    pub variance: f64,
    pub standard_deviation: f64,
    pub min: f64,
    pub max: f64,
    pub range: f64,
    pub quartiles: (f64, f64, f64),
}

impl DescriptiveStatistics {
    pub fn new(data: Vec<f64>) -> Self {
        let n = data.len();
        DescriptiveStatistics { data, n }
    }
    
    pub fn mean(&self) -> f64 {
        if self.n == 0 {
            return 0.0;
        }
        self.data.iter().sum::<f64>() / self.n as f64
    }
    
    pub fn median(&self) -> f64 {
        if self.n == 0 {
            return 0.0;
        }
        
        let mut sorted_data = self.data.clone();
        sorted_data.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        if self.n % 2 == 0 {
            let mid = self.n / 2;
            (sorted_data[mid - 1] + sorted_data[mid]) / 2.0
        } else {
            sorted_data[self.n / 2]
        }
    }
    
    pub fn mode(&self) -> f64 {
        if self.n == 0 {
            return 0.0;
        }
        
        let mut frequency_map: HashMap<f64, usize> = HashMap::new();
        
        for &value in &self.data {
            *frequency_map.entry(value).or_insert(0) += 1;
        }
        
        frequency_map.iter()
            .max_by(|a, b| a.1.cmp(b.1))
            .map(|(&value, _)| value)
            .unwrap_or(0.0)
    }
    
    pub fn variance(&self) -> f64 {
        if self.n <= 1 {
            return 0.0;
        }
        
        let mean = self.mean();
        let sum_squared_diff: f64 = self.data.iter()
            .map(|&x| (x - mean).powi(2))
            .sum();
        
        sum_squared_diff / (self.n - 1) as f64
    }
    
    pub fn standard_deviation(&self) -> f64 {
        self.variance().sqrt()
    }
    
    pub fn min(&self) -> f64 {
        self.data.iter().fold(f64::INFINITY, |a, &b| a.min(b))
    }
    
    pub fn max(&self) -> f64 {
        self.data.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b))
    }
    
    pub fn range(&self) -> f64 {
        self.max() - self.min()
    }
    
    pub fn quartiles(&self) -> (f64, f64, f64) {
        if self.n == 0 {
            return (0.0, 0.0, 0.0);
        }
        
        let mut sorted_data = self.data.clone();
        sorted_data.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let q1 = self.percentile(&sorted_data, 25.0);
        let q2 = self.percentile(&sorted_data, 50.0);
        let q3 = self.percentile(&sorted_data, 75.0);
        
        (q1, q2, q3)
    }
    
    fn percentile(&self, sorted_data: &[f64], p: f64) -> f64 {
        if sorted_data.is_empty() {
            return 0.0;
        }
        
        let index = (p / 100.0) * (sorted_data.len() - 1) as f64;
        let lower_index = index.floor() as usize;
        let upper_index = index.ceil() as usize;
        
        if lower_index == upper_index {
            sorted_data[lower_index]
        } else {
            let weight = index - lower_index as f64;
            sorted_data[lower_index] * (1.0 - weight) + sorted_data[upper_index] * weight
        }
    }
    
    pub fn summary(&self) -> SummaryStatistics {
        SummaryStatistics {
            mean: self.mean(),
            median: self.median(),
            mode: self.mode(),
            variance: self.variance(),
            standard_deviation: self.standard_deviation(),
            min: self.min(),
            max: self.max(),
            range: self.range(),
            quartiles: self.quartiles(),
        }
    }
    
    pub fn correlation(&self, other: &DescriptiveStatistics) -> f64 {
        if self.n != other.n || self.n == 0 {
            return 0.0;
        }
        
        let mean_x = self.mean();
        let mean_y = other.mean();
        
        let numerator: f64 = self.data.iter()
            .zip(other.data.iter())
            .map(|(&x, &y)| (x - mean_x) * (y - mean_y))
            .sum();
        
        let sum_sq_x: f64 = self.data.iter()
            .map(|&x| (x - mean_x).powi(2))
            .sum();
        
        let sum_sq_y: f64 = other.data.iter()
            .map(|&y| (y - mean_y).powi(2))
            .sum();
        
        let denominator = (sum_sq_x * sum_sq_y).sqrt();
        
        if denominator == 0.0 {
            0.0
        } else {
            numerator / denominator
        }
    }
    
    pub fn z_scores(&self) -> Vec<f64> {
        let mean = self.mean();
        let std_dev = self.standard_deviation();
        
        if std_dev == 0.0 {
            return vec![0.0; self.n];
        }
        
        self.data.iter()
            .map(|&x| (x - mean) / std_dev)
            .collect()
    }
    
    pub fn outliers(&self, threshold: f64) -> Vec<f64> {
        let z_scores = self.z_scores();
        
        z_scores.iter()
            .zip(self.data.iter())
            .filter(|(&z, _)| z.abs() > threshold)
            .map(|(_, &value)| value)
            .collect()
    }
}
```

### 4.2 å‡è®¾æ£€éªŒå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct HypothesisTest {
    pub null_hypothesis: String,
    pub alternative_hypothesis: String,
    pub test_statistic: f64,
    pub p_value: f64,
    pub significance_level: f64,
    pub decision: TestDecision,
}

#[derive(Debug, Clone)]
pub enum TestDecision {
    RejectNull,
    FailToRejectNull,
}

#[derive(Debug, Clone)]
pub struct TTest {
    pub sample1: Vec<f64>,
    pub sample2: Option<Vec<f64>>,
    pub test_type: TTestType,
}

#[derive(Debug, Clone)]
pub enum TTestType {
    OneSample,
    TwoSample,
    Paired,
}

impl HypothesisTest {
    pub fn new(null_hypothesis: String, alternative_hypothesis: String, 
               test_statistic: f64, p_value: f64, significance_level: f64) -> Self {
        let decision = if p_value < significance_level {
            TestDecision::RejectNull
        } else {
            TestDecision::FailToRejectNull
        };
        
        HypothesisTest {
            null_hypothesis,
            alternative_hypothesis,
            test_statistic,
            p_value,
            significance_level,
            decision,
        }
    }
    
    pub fn report(&self) -> String {
        format!(
            "Null Hypothesis: {}\nAlternative Hypothesis: {}\nTest Statistic: {:.4}\nP-value: {:.4}\nSignificance Level: {:.3}\nDecision: {:?}",
            self.null_hypothesis,
            self.alternative_hypothesis,
            self.test_statistic,
            self.p_value,
            self.significance_level,
            self.decision
        )
    }
}

impl TTest {
    pub fn new(sample1: Vec<f64>, sample2: Option<Vec<f64>>, test_type: TTestType) -> Self {
        TTest {
            sample1,
            sample2,
            test_type,
        }
    }
    
    pub fn one_sample_t_test(&self, hypothesized_mean: f64, significance_level: f64) -> HypothesisTest {
        let stats = DescriptiveStatistics::new(self.sample1.clone());
        let sample_mean = stats.mean();
        let sample_std = stats.standard_deviation();
        let n = self.sample1.len();
        
        if n == 0 || sample_std == 0.0 {
            return HypothesisTest::new(
                "Î¼ = Î¼â‚€".to_string(),
                "Î¼ â‰  Î¼â‚€".to_string(),
                0.0,
                1.0,
                significance_level,
            );
        }
        
        let t_statistic = (sample_mean - hypothesized_mean) / (sample_std / (n as f64).sqrt());
        let degrees_of_freedom = n - 1;
        let p_value = self.calculate_p_value(t_statistic, degrees_of_freedom);
        
        HypothesisTest::new(
            format!("Î¼ = {}", hypothesized_mean),
            format!("Î¼ â‰  {}", hypothesized_mean),
            t_statistic,
            p_value,
            significance_level,
        )
    }
    
    pub fn two_sample_t_test(&self, significance_level: f64) -> HypothesisTest {
        if self.sample2.is_none() {
            panic!("Two-sample t-test requires two samples");
        }
        
        let sample2 = self.sample2.as_ref().unwrap();
        let stats1 = DescriptiveStatistics::new(self.sample1.clone());
        let stats2 = DescriptiveStatistics::new(sample2.clone());
        
        let mean1 = stats1.mean();
        let mean2 = stats2.mean();
        let var1 = stats1.variance();
        let var2 = stats2.variance();
        let n1 = self.sample1.len();
        let n2 = sample2.len();
        
        if n1 == 0 || n2 == 0 {
            return HypothesisTest::new(
                "Î¼â‚ = Î¼â‚‚".to_string(),
                "Î¼â‚ â‰  Î¼â‚‚".to_string(),
                0.0,
                1.0,
                significance_level,
            );
        }
        
        // è®¡ç®—åˆå¹¶æ–¹å·®
        let pooled_variance = ((n1 - 1) as f64 * var1 + (n2 - 1) as f64 * var2) / 
                             ((n1 + n2 - 2) as f64);
        
        let standard_error = (pooled_variance * (1.0 / n1 as f64 + 1.0 / n2 as f64)).sqrt();
        let t_statistic = (mean1 - mean2) / standard_error;
        let degrees_of_freedom = n1 + n2 - 2;
        
        let p_value = self.calculate_p_value(t_statistic, degrees_of_freedom);
        
        HypothesisTest::new(
            "Î¼â‚ = Î¼â‚‚".to_string(),
            "Î¼â‚ â‰  Î¼â‚‚".to_string(),
            t_statistic,
            p_value,
            significance_level,
        )
    }
    
    fn calculate_p_value(&self, t_statistic: f64, degrees_of_freedom: usize) -> f64 {
        // ç®€åŒ–å®ç°ï¼šä½¿ç”¨è¿‘ä¼¼æ–¹æ³•è®¡ç®—på€¼
        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„tåˆ†å¸ƒè¡¨æˆ–æ•°å€¼æ–¹æ³•
        
        let t_abs = t_statistic.abs();
        
        // ä½¿ç”¨æ­£æ€åˆ†å¸ƒè¿‘ä¼¼ï¼ˆå½“è‡ªç”±åº¦è¾ƒå¤§æ—¶ï¼‰
        if degrees_of_freedom > 30 {
            // æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
            let z = t_abs / (1.0 + t_abs.powi(2) / degrees_of_freedom as f64).sqrt();
            let p_value = 2.0 * (1.0 - self.normal_cdf(z));
            p_value
        } else {
            // ç®€åŒ–å®ç°ï¼šä½¿ç”¨æŸ¥è¡¨æˆ–æ•°å€¼ç§¯åˆ†
            // è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„è¿‘ä¼¼
            let p_value = 2.0 * (-t_abs / (degrees_of_freedom as f64).sqrt()).exp();
            p_value.min(1.0)
        }
    }
    
    fn normal_cdf(&self, z: f64) -> f64 {
        // æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°è¿‘ä¼¼
        0.5 * (1.0 + (z / 2.0_f64.sqrt()).tanh())
    }
}
```

### 4.3 å›å½’åˆ†æå®ç°

```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
pub struct LinearRegression {
    pub coefficients: Vec<f64>,
    pub intercept: f64,
    pub r_squared: f64,
    pub adjusted_r_squared: f64,
    pub standard_error: f64,
    pub residuals: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct RegressionResult {
    pub model: LinearRegression,
    pub predictions: Vec<f64>,
    pub confidence_intervals: Vec<(f64, f64)>,
    pub prediction_intervals: Vec<(f64, f64)>,
}

#[derive(Debug, Clone)]
pub struct MultipleRegression {
    pub features: Vec<String>,
    pub coefficients: Vec<f64>,
    pub intercept: f64,
    pub r_squared: f64,
    pub adjusted_r_squared: f64,
    pub f_statistic: f64,
    pub p_value: f64,
}

impl LinearRegression {
    pub fn new() -> Self {
        LinearRegression {
            coefficients: Vec::new(),
            intercept: 0.0,
            r_squared: 0.0,
            adjusted_r_squared: 0.0,
            standard_error: 0.0,
            residuals: Vec::new(),
        }
    }
    
    pub fn fit(&mut self, x: &[f64], y: &[f64]) -> Result<(), String> {
        if x.len() != y.len() || x.is_empty() {
            return Err("Invalid input data".to_string());
        }
        
        let n = x.len() as f64;
        
        // è®¡ç®—å‡å€¼
        let mean_x: f64 = x.iter().sum::<f64>() / n;
        let mean_y: f64 = y.iter().sum::<f64>() / n;
        
        // è®¡ç®—æ–œç‡å’Œæˆªè·
        let numerator: f64 = x.iter()
            .zip(y.iter())
            .map(|(&xi, &yi)| (xi - mean_x) * (yi - mean_y))
            .sum();
        
        let denominator: f64 = x.iter()
            .map(|&xi| (xi - mean_x).powi(2))
            .sum();
        
        if denominator == 0.0 {
            return Err("Cannot fit regression line: zero variance in x".to_string());
        }
        
        self.coefficients = vec![numerator / denominator];
        self.intercept = mean_y - self.coefficients[0] * mean_x;
        
        // è®¡ç®—é¢„æµ‹å€¼å’Œæ®‹å·®
        let predictions: Vec<f64> = x.iter()
            .map(|&xi| self.predict(&[xi]))
            .collect();
        
        self.residuals = y.iter()
            .zip(predictions.iter())
            .map(|(&yi, &pred)| yi - pred)
            .collect();
        
        // è®¡ç®—RÂ²
        let ss_res: f64 = self.residuals.iter().map(|&r| r.powi(2)).sum();
        let ss_tot: f64 = y.iter()
            .map(|&yi| (yi - mean_y).powi(2))
            .sum();
        
        self.r_squared = if ss_tot == 0.0 {
            1.0
        } else {
            1.0 - ss_res / ss_tot
        };
        
        // è®¡ç®—è°ƒæ•´RÂ²
        self.adjusted_r_squared = 1.0 - (1.0 - self.r_squared) * (n - 1.0) / (n - 2.0);
        
        // è®¡ç®—æ ‡å‡†è¯¯å·®
        self.standard_error = (ss_res / (n - 2.0)).sqrt();
        
        Ok(())
    }
    
    pub fn predict(&self, x: &[f64]) -> f64 {
        if x.len() != self.coefficients.len() {
            panic!("Number of features does not match number of coefficients");
        }
        
        let mut prediction = self.intercept;
        for (xi, &coef) in x.iter().zip(self.coefficients.iter()) {
            prediction += xi * coef;
        }
        
        prediction
    }
    
    pub fn predict_multiple(&self, x_values: &[Vec<f64>]) -> Vec<f64> {
        x_values.iter()
            .map(|x| self.predict(x))
            .collect()
    }
    
    pub fn confidence_intervals(&self, x_values: &[f64], confidence_level: f64) -> Vec<(f64, f64)> {
        let n = x_values.len() as f64;
        let mean_x: f64 = x_values.iter().sum::<f64>() / n;
        
        // è®¡ç®—tåˆ†å¸ƒçš„ä¸´ç•Œå€¼ï¼ˆç®€åŒ–å®ç°ï¼‰
        let t_critical = 1.96; // 95%ç½®ä¿¡æ°´å¹³çš„è¿‘ä¼¼å€¼
        
        let mut intervals = Vec::new();
        
        for &x in x_values {
            let prediction = self.predict(&[x]);
            
            // è®¡ç®—é¢„æµ‹çš„æ ‡å‡†è¯¯å·®
            let se_pred = self.standard_error * 
                         (1.0 / n + (x - mean_x).powi(2) / 
                          x_values.iter().map(|&xi| (xi - mean_x).powi(2)).sum::<f64>()).sqrt();
            
            let margin = t_critical * se_pred;
            intervals.push((prediction - margin, prediction + margin));
        }
        
        intervals
    }
    
    pub fn prediction_intervals(&self, x_values: &[f64], confidence_level: f64) -> Vec<(f64, f64)> {
        let n = x_values.len() as f64;
        let mean_x: f64 = x_values.iter().sum::<f64>() / n;
        
        // è®¡ç®—tåˆ†å¸ƒçš„ä¸´ç•Œå€¼ï¼ˆç®€åŒ–å®ç°ï¼‰
        let t_critical = 1.96; // 95%ç½®ä¿¡æ°´å¹³çš„è¿‘ä¼¼å€¼
        
        let mut intervals = Vec::new();
        
        for &x in x_values {
            let prediction = self.predict(&[x]);
            
            // è®¡ç®—é¢„æµ‹çš„æ ‡å‡†è¯¯å·®
            let se_pred = self.standard_error * 
                         (1.0 + 1.0 / n + (x - mean_x).powi(2) / 
                          x_values.iter().map(|&xi| (xi - mean_x).powi(2)).sum::<f64>()).sqrt();
            
            let margin = t_critical * se_pred;
            intervals.push((prediction - margin, prediction + margin));
        }
        
        intervals
    }
    
    pub fn summary(&self) -> String {
        format!(
            "Linear Regression Summary:\n\
             Intercept: {:.4}\n\
             Coefficient: {:.4}\n\
             R-squared: {:.4}\n\
             Adjusted R-squared: {:.4}\n\
             Standard Error: {:.4}",
            self.intercept,
            self.coefficients[0],
            self.r_squared,
            self.adjusted_r_squared,
            self.standard_error
        )
    }
}

impl MultipleRegression {
    pub fn new(features: Vec<String>) -> Self {
        MultipleRegression {
            features,
            coefficients: Vec::new(),
            intercept: 0.0,
            r_squared: 0.0,
            adjusted_r_squared: 0.0,
            f_statistic: 0.0,
            p_value: 0.0,
        }
    }
    
    pub fn fit(&mut self, x: &[Vec<f64>], y: &[f64]) -> Result<(), String> {
        if x.is_empty() || y.is_empty() || x.len() != y.len() {
            return Err("Invalid input data".to_string());
        }
        
        let n = x.len() as f64;
        let p = self.features.len() as f64;
        
        // ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ±‚è§£
        let coefficients = self.least_squares(x, y)?;
        
        self.intercept = coefficients[0];
        self.coefficients = coefficients[1..].to_vec();
        
        // è®¡ç®—é¢„æµ‹å€¼
        let predictions: Vec<f64> = x.iter()
            .map(|xi| self.predict(xi))
            .collect();
        
        // è®¡ç®—RÂ²
        let mean_y: f64 = y.iter().sum::<f64>() / n;
        let ss_res: f64 = y.iter()
            .zip(predictions.iter())
            .map(|(&yi, &pred)| (yi - pred).powi(2))
            .sum();
        let ss_tot: f64 = y.iter()
            .map(|&yi| (yi - mean_y).powi(2))
            .sum();
        
        self.r_squared = if ss_tot == 0.0 {
            1.0
        } else {
            1.0 - ss_res / ss_tot
        };
        
        // è®¡ç®—è°ƒæ•´RÂ²
        self.adjusted_r_squared = 1.0 - (1.0 - self.r_squared) * (n - 1.0) / (n - p - 1.0);
        
        // è®¡ç®—Fç»Ÿè®¡é‡
        let ms_res = ss_res / (n - p - 1.0);
        let ms_reg = (ss_tot - ss_res) / p;
        self.f_statistic = if ms_res == 0.0 { 0.0 } else { ms_reg / ms_res };
        
        // è®¡ç®—på€¼ï¼ˆç®€åŒ–å®ç°ï¼‰
        self.p_value = self.calculate_f_p_value(self.f_statistic, p as usize, (n - p - 1.0) as usize);
        
        Ok(())
    }
    
    fn least_squares(&self, x: &[Vec<f64>], y: &[f64]) -> Result<Vec<f64>, String> {
        // æ„å»ºè®¾è®¡çŸ©é˜µ
        let mut design_matrix = Vec::new();
        for xi in x {
            let mut row = vec![1.0]; // æˆªè·é¡¹
            row.extend_from_slice(xi);
            design_matrix.push(row);
        }
        
        // ä½¿ç”¨é«˜æ–¯æ¶ˆå…ƒæ³•æ±‚è§£æ­£è§„æ–¹ç¨‹
        // è¿™é‡Œä½¿ç”¨ç®€åŒ–å®ç°
        let n_features = design_matrix[0].len();
        let mut coefficients = vec![0.0; n_features];
        
        // è®¡ç®—X'Xå’ŒX'y
        let mut xtx = vec![vec![0.0; n_features]; n_features];
        let mut xty = vec![0.0; n_features];
        
        for (i, row) in design_matrix.iter().enumerate() {
            for j in 0..n_features {
                for k in 0..n_features {
                    xtx[j][k] += row[j] * row[k];
                }
                xty[j] += row[j] * y[i];
            }
        }
        
        // æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ï¼ˆç®€åŒ–å®ç°ï¼‰
        coefficients[0] = xty[0] / xtx[0][0]; // æˆªè·
        for i in 1..n_features {
            coefficients[i] = xty[i] / xtx[i][i]; // ç®€åŒ–å‡è®¾
        }
        
        Ok(coefficients)
    }
    
    pub fn predict(&self, x: &[f64]) -> f64 {
        if x.len() != self.coefficients.len() {
            panic!("Number of features does not match number of coefficients");
        }
        
        let mut prediction = self.intercept;
        for (xi, &coef) in x.iter().zip(self.coefficients.iter()) {
            prediction += xi * coef;
        }
        
        prediction
    }
    
    fn calculate_f_p_value(&self, f_statistic: f64, df1: usize, df2: usize) -> f64 {
        // ç®€åŒ–å®ç°ï¼šFåˆ†å¸ƒçš„på€¼è®¡ç®—
        // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„æ•°å€¼æ–¹æ³•
        if f_statistic <= 0.0 {
            return 1.0;
        }
        
        // ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•
        let p_value = (-f_statistic / 2.0).exp();
        p_value.min(1.0)
    }
    
    pub fn summary(&self) -> String {
        format!(
            "Multiple Regression Summary:\n\
             Intercept: {:.4}\n\
             R-squared: {:.4}\n\
             Adjusted R-squared: {:.4}\n\
             F-statistic: {:.4}\n\
             P-value: {:.4}",
            self.intercept,
            self.r_squared,
            self.adjusted_r_squared,
            self.f_statistic,
            self.p_value
        )
    }
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [æœºå™¨å­¦ä¹ ç†è®º](../../13_Artificial_Intelligence_Theory/01_Machine_Learning/01_Machine_Learning_Theory.md)
- [æ·±åº¦å­¦ä¹ ç†è®º](../../13_Artificial_Intelligence_Theory/02_Deep_Learning/01_Deep_Learning_Theory.md)
- [æ•°æ®æŒ–æ˜ç†è®º](../02_Data_Mining/01_Data_Mining_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Casella, G., & Berger, R. L. (2002). Statistical Inference. Duxbury.
2. Montgomery, D. C., et al. (2012). Introduction to Linear Regression Analysis. Wiley.
3. Rice, J. A. (2006). Mathematical Statistics and Data Analysis. Cengage Learning.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0

## æ‰¹åˆ¤æ€§åˆ†æ

### ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†

ç»Ÿè®¡åˆ†æç†è®ºå…³æ³¨æ•°æ®å»ºæ¨¡ã€æ¨æ–­æ–¹æ³•å’Œé¢„æµ‹ä¼˜åŒ–ï¼Œæ˜¯æ•°æ®ç§‘å­¦å’Œç»Ÿè®¡å­¦ä¹ çš„é‡è¦åŸºç¡€ã€‚

### ä¸»æµè§‚ç‚¹çš„ä¼˜ç¼ºç‚¹åˆ†æ

ä¼˜ç‚¹ï¼šæä¾›äº†ç³»ç»ŸåŒ–çš„ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼Œæ”¯æŒå¤æ‚æ•°æ®ç³»ç»Ÿçš„æ„å»ºã€‚
ç¼ºç‚¹ï¼šç»Ÿè®¡å¤æ‚æ€§çš„å¢åŠ ï¼Œæ¨æ–­å‡†ç¡®æ€§çš„æŒ‘æˆ˜ï¼Œå¯¹æ–°å…´ç»Ÿè®¡æŠ€æœ¯çš„é€‚åº”æ€§éœ€è¦æŒç»­æ”¹è¿›ã€‚

### ä¸å…¶ä»–å­¦ç§‘çš„äº¤å‰ä¸èåˆ

- ä¸æ•°å­¦åŸºç¡€åœ¨ç»Ÿè®¡å»ºæ¨¡ã€æ¦‚ç‡ç†è®ºç­‰é¢†åŸŸæœ‰åº”ç”¨ã€‚
- ä¸ç±»å‹ç†è®ºåœ¨ç»Ÿè®¡æŠ½è±¡ã€æ¥å£è®¾è®¡ç­‰æ–¹é¢æœ‰åˆ›æ–°åº”ç”¨ã€‚
- ä¸äººå·¥æ™ºèƒ½ç†è®ºåœ¨æ™ºèƒ½ç»Ÿè®¡ã€è‡ªé€‚åº”æ¨æ–­ç­‰æ–¹é¢æœ‰æ–°å…´èåˆã€‚
- ä¸æ§åˆ¶è®ºåœ¨ç»Ÿè®¡æ§åˆ¶ã€åé¦ˆæœºåˆ¶ç­‰æ–¹é¢äº’è¡¥ã€‚

### åˆ›æ–°æ€§æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›

æœªæ¥ç»Ÿè®¡åˆ†æç†è®ºéœ€åŠ å¼ºä¸æ•°å­¦åŸºç¡€ã€ç±»å‹ç†è®ºã€äººå·¥æ™ºèƒ½ç†è®ºã€æ§åˆ¶è®ºç­‰é¢†åŸŸçš„èåˆï¼Œæ¨åŠ¨æ™ºèƒ½åŒ–ã€è‡ªé€‚åº”çš„ç»Ÿè®¡åˆ†æä½“ç³»ã€‚

### å‚è€ƒæ–‡çŒ®ä¸è¿›ä¸€æ­¥é˜…è¯»

- äº¤å‰ç´¢å¼•.md
- Meta/æ‰¹åˆ¤æ€§åˆ†ææ¨¡æ¿.md
