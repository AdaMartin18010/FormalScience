# 17.1 æœºå™¨å­¦ä¹ ç†è®º (Machine Learning Theory)

**åˆ›å»ºæ—¶é—´**: 2025-01-17  
**æœ€åæ›´æ–°**: 2025-01-17  
**æ–‡æ¡£çŠ¶æ€**: æ´»è·ƒ  
**å…³è”æ¨¡å—**: `17_Data_Science_Theory`

## ğŸ“ æ¦‚è¿°

æœºå™¨å­¦ä¹ ç†è®ºæ˜¯æ•°æ®ç§‘å­¦çš„æ ¸å¿ƒï¼Œç ”ç©¶å¦‚ä½•ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚æœ¬æ–‡æ¡£æ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä»¥åŠç®—æ³•å®ç°ã€æ€§èƒ½åˆ†æå’Œç†è®ºè¯æ˜ã€‚

## ğŸ”¬ ç†è®ºåŸºç¡€

### æœºå™¨å­¦ä¹ å½¢å¼åŒ–å®šä¹‰

**å®šä¹‰ 17.1.1** (å­¦ä¹ é—®é¢˜)
å­¦ä¹ é—®é¢˜æ˜¯å¯»æ‰¾å‡½æ•° $f: \mathcal{X} \rightarrow \mathcal{Y}$ ä½¿å¾— $f(x) \approx y$ï¼Œå…¶ä¸­ï¼š

- $\mathcal{X}$ æ˜¯è¾“å…¥ç©ºé—´
- $\mathcal{Y}$ æ˜¯è¾“å‡ºç©ºé—´
- $(x, y)$ æ˜¯è®­ç»ƒæ ·æœ¬

**å®šä¹‰ 17.1.2** (æŸå¤±å‡½æ•°)
æŸå¤±å‡½æ•° $L: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}^+$ è¡¡é‡é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®å¼‚ã€‚

**å®šä¹‰ 17.1.3** (æ³›åŒ–è¯¯å·®)
æ³›åŒ–è¯¯å·®æ˜¯æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„æœŸæœ›è¯¯å·®ï¼š
$E_{gen} = \mathbb{E}_{(x,y) \sim \mathcal{D}}[L(f(x), y)]$

**å®šç† 17.1.1** (VCç»´ç†è®º)
å¯¹äºæœ‰é™VCç»´ $d$ çš„å‡è®¾ç±» $\mathcal{H}$ï¼Œå­˜åœ¨æ³›åŒ–è¯¯å·®ä¸Šç•Œï¼š
$E_{gen} \leq E_{emp} + O(\sqrt{\frac{d \log n}{n}})$

å…¶ä¸­ $E_{emp}$ æ˜¯ç»éªŒè¯¯å·®ï¼Œ$n$ æ˜¯æ ·æœ¬æ•°é‡ã€‚

### ç›‘ç£å­¦ä¹ ç†è®º

**å®šä¹‰ 17.1.4** (ç›‘ç£å­¦ä¹ )
ç›‘ç£å­¦ä¹ ä»æ ‡è®°æ•°æ® $\{(x_i, y_i)\}_{i=1}^n$ å­¦ä¹ æ˜ å°„ $f: \mathcal{X} \rightarrow \mathcal{Y}$ã€‚

**å®šä¹‰ 17.1.5** (çº¿æ€§å›å½’)
çº¿æ€§å›å½’æ¨¡å‹ï¼š$f(x) = w^T x + b$ï¼Œå…¶ä¸­ $w$ æ˜¯æƒé‡å‘é‡ï¼Œ$b$ æ˜¯åç½®ã€‚

**å®šç† 17.1.2** (æœ€å°äºŒä¹˜æœ€ä¼˜æ€§)
å¯¹äºçº¿æ€§å›å½’ï¼Œæœ€å°äºŒä¹˜ä¼°è®¡ $\hat{w} = (X^T X)^{-1} X^T y$ æ˜¯æœ€ä¼˜æ— åä¼°è®¡ã€‚

### æ— ç›‘ç£å­¦ä¹ ç†è®º

**å®šä¹‰ 17.1.6** (èšç±»)
èšç±»å°†æ•°æ®ç‚¹åˆ†ç»„åˆ° $k$ ä¸ªç°‡ä¸­ï¼Œæœ€å°åŒ–ç°‡å†…è·ç¦»ï¼Œæœ€å¤§åŒ–ç°‡é—´è·ç¦»ã€‚

**å®šä¹‰ 17.1.7** (K-meansç›®æ ‡)
K-meansç›®æ ‡å‡½æ•°ï¼š$J = \sum_{i=1}^k \sum_{x \in C_i} \|x - \mu_i\|^2$

å…¶ä¸­ $\mu_i$ æ˜¯ç¬¬ $i$ ä¸ªç°‡çš„ä¸­å¿ƒã€‚

## ğŸ—ï¸ ç®—æ³•å®ç°

### Python æœºå™¨å­¦ä¹ æ¡†æ¶

```python
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

class LinearRegression:
    """çº¿æ€§å›å½’å®ç°"""
    
    def __init__(self, learning_rate=0.01, max_iter=1000):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.weights = None
        self.bias = None
        self.costs = []
    
    def fit(self, X, y):
        """è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹"""
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for _ in range(self.max_iter):
            # å‰å‘ä¼ æ’­
            y_pred = self.predict(X)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®°å½•æŸå¤±
            cost = mean_squared_error(y, y_pred)
            self.costs.append(cost)
    
    def predict(self, X):
        """é¢„æµ‹"""
        return np.dot(X, self.weights) + self.bias
    
    def score(self, X, y):
        """è®¡ç®—RÂ²åˆ†æ•°"""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)

class KMeans:
    """K-meansèšç±»å®ç°"""
    
    def __init__(self, n_clusters=3, max_iter=300):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.centroids = None
        self.labels = None
    
    def fit(self, X):
        """è®­ç»ƒK-meansæ¨¡å‹"""
        n_samples, n_features = X.shape
        
        # éšæœºåˆå§‹åŒ–ä¸­å¿ƒç‚¹
        idx = np.random.choice(n_samples, self.n_clusters, replace=False)
        self.centroids = X[idx]
        
        for _ in range(self.max_iter):
            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„ä¸­å¿ƒ
            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
            self.labels = np.argmin(distances, axis=0)
            
            # æ›´æ–°ä¸­å¿ƒç‚¹
            new_centroids = np.array([X[self.labels == k].mean(axis=0) 
                                     for k in range(self.n_clusters)])
            
            # æ£€æŸ¥æ”¶æ•›
            if np.all(self.centroids == new_centroids):
                break
                
            self.centroids = new_centroids
    
    def predict(self, X):
        """é¢„æµ‹èšç±»æ ‡ç­¾"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

class NeuralNetwork:
    """ç®€å•ç¥ç»ç½‘ç»œå®ç°"""
    
    def __init__(self, layers, learning_rate=0.01):
        self.layers = layers
        self.learning_rate = learning_rate
        self.weights = []
        self.biases = []
        self._initialize_parameters()
    
    def _initialize_parameters(self):
        """åˆå§‹åŒ–æƒé‡å’Œåç½®"""
        for i in range(len(self.layers) - 1):
            w = np.random.randn(self.layers[i+1], self.layers[i]) * 0.01
            b = np.zeros((self.layers[i+1], 1))
            self.weights.append(w)
            self.biases.append(b)
    
    def sigmoid(self, z):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z):
        """Sigmoidå¯¼æ•°"""
        return self.sigmoid(z) * (1 - self.sigmoid(z))
    
    def forward_propagation(self, X):
        """å‰å‘ä¼ æ’­"""
        A = X
        activations = [A]
        
        for i in range(len(self.weights)):
            Z = np.dot(self.weights[i], A) + self.biases[i]
            A = self.sigmoid(Z)
            activations.append(A)
        
        return activations
    
    def backward_propagation(self, X, y, activations):
        """åå‘ä¼ æ’­"""
        m = X.shape[1]
        delta = activations[-1] - y
        
        for i in range(len(self.weights) - 1, -1, -1):
            dW = np.dot(delta, activations[i].T) / m
            db = np.sum(delta, axis=1, keepdims=True) / m
            
            if i > 0:
                delta = np.dot(self.weights[i].T, delta) * self.sigmoid_derivative(
                    np.dot(self.weights[i-1], activations[i-1]) + self.biases[i-1]
                )
            
            self.weights[i] -= self.learning_rate * dW
            self.biases[i] -= self.learning_rate * db
    
    def fit(self, X, y, epochs=1000):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        for _ in range(epochs):
            activations = self.forward_propagation(X)
            self.backward_propagation(X, y, activations)
    
    def predict(self, X):
        """é¢„æµ‹"""
        activations = self.forward_propagation(X)
        return activations[-1]
```

### æ€§èƒ½åˆ†æå’Œè¯„ä¼°

```python
class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def evaluate_regression(self, model, X_test, y_test):
        """è¯„ä¼°å›å½’æ¨¡å‹"""
        y_pred = model.predict(X_test)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = np.mean(np.abs(y_test - y_pred))
        r2 = model.score(X_test, y_test)
        
        return {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'RÂ²': r2
        }
    
    def evaluate_classification(self, model, X_test, y_test):
        """è¯„ä¼°åˆ†ç±»æ¨¡å‹"""
        y_pred = model.predict(X_test)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        
        # æ··æ·†çŸ©é˜µ
        from sklearn.metrics import confusion_matrix, classification_report
        cm = confusion_matrix(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        
        return {
            'Accuracy': accuracy,
            'Confusion_Matrix': cm,
            'Classification_Report': report
        }
    
    def cross_validation(self, model, X, y, cv=5):
        """äº¤å‰éªŒè¯"""
        from sklearn.model_selection import cross_val_score
        
        scores = cross_val_score(model, X, y, cv=cv)
        return {
            'Mean_Score': scores.mean(),
            'Std_Score': scores.std(),
            'Scores': scores
        }
    
    def learning_curve(self, model, X, y):
        """å­¦ä¹ æ›²çº¿"""
        from sklearn.model_selection import learning_curve
        
        train_sizes, train_scores, val_scores = learning_curve(
            model, X, y, train_sizes=np.linspace(0.1, 1.0, 10)
        )
        
        return {
            'Train_Sizes': train_sizes,
            'Train_Scores': train_scores,
            'Val_Scores': val_scores
        }

# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X = np.random.randn(100, 2)
    y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(100) * 0.1
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # è®­ç»ƒçº¿æ€§å›å½’
    lr = LinearRegression(learning_rate=0.01, max_iter=1000)
    lr.fit(X_train, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    evaluator = ModelEvaluator()
    results = evaluator.evaluate_regression(lr, X_test, y_test)
    
    print("Linear Regression Results:")
    for metric, value in results.items():
        print(f"{metric}: {value:.4f}")
    
    # å¯è§†åŒ–å­¦ä¹ æ›²çº¿
    plt.plot(lr.costs)
    plt.title('Learning Curve')
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.show()
```

## ğŸ“Š å¤æ‚åº¦åˆ†æ

### ç®—æ³•å¤æ‚åº¦

**å®šç† 17.1.3** (çº¿æ€§å›å½’å¤æ‚åº¦)
çº¿æ€§å›å½’çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(nd + d^3)$ï¼Œå…¶ä¸­ $n$ æ˜¯æ ·æœ¬æ•°ï¼Œ$d$ æ˜¯ç‰¹å¾æ•°ã€‚

**è¯æ˜**:

- è®¡ç®— $X^T X$: $O(nd^2)$
- è®¡ç®— $(X^T X)^{-1}$: $O(d^3)$
- è®¡ç®— $(X^T X)^{-1} X^T y$: $O(d^2 + nd)$

**å®šç† 17.1.4** (K-meanså¤æ‚åº¦)
K-meansçš„æ—¶é—´å¤æ‚åº¦ä¸º $O(nkd)$ï¼Œå…¶ä¸­ $k$ æ˜¯ç°‡æ•°ã€‚

**è¯æ˜**:

- æ¯æ¬¡è¿­ä»£è®¡ç®—è·ç¦»: $O(nkd)$
- æ›´æ–°ä¸­å¿ƒç‚¹: $O(nkd)$
- æ€»è¿­ä»£æ¬¡æ•°é€šå¸¸ä¸ºå¸¸æ•°

**å®šç† 17.1.5** (ç¥ç»ç½‘ç»œå¤æ‚åº¦)
å‰å‘ä¼ æ’­çš„æ—¶é—´å¤æ‚åº¦ä¸º $O(\sum_{i=1}^{L} n_i n_{i-1})$ï¼Œå…¶ä¸­ $L$ æ˜¯å±‚æ•°ï¼Œ$n_i$ æ˜¯ç¬¬ $i$ å±‚çš„ç¥ç»å…ƒæ•°ã€‚

## ğŸ”— ä¸æ¨¡å—å†…ä¸»é¢˜çš„å…³ç³»

- **17.2 ç»Ÿè®¡åˆ†æç†è®º**: æä¾›ç»Ÿè®¡æ¨æ–­å’Œå‡è®¾æ£€éªŒåŸºç¡€
- **17.3 æ•°æ®æŒ–æ˜ç†è®º**: æ¨¡å¼å‘ç°å’Œå…³è”è§„åˆ™æŒ–æ˜
- **17.4 æ•°æ®å¯è§†åŒ–ç†è®º**: æ¨¡å‹ç»“æœçš„å¯è§†åŒ–å±•ç¤º
- **17.5 æ•°æ®ä¼¦ç†ç†è®º**: æœºå™¨å­¦ä¹ ä¸­çš„å…¬å¹³æ€§å’Œéšç§ä¿æŠ¤

## ğŸ§­ æ‰¹åˆ¤æ€§åˆ†æ

### å“²å­¦ç»´åº¦

- **æ•°æ®å“²å­¦**: æœºå™¨å­¦ä¹ ä½“ç°äº†"æ•°æ®å³çŸ¥è¯†"çš„å“²å­¦è§‚ç‚¹ï¼Œä½†æ•°æ®æœ¬èº«æ˜¯å¦ç­‰åŒäºçŸ¥è¯†å­˜åœ¨äº‰è®®
- **è®¤è¯†è®ºåŸºç¡€**: æœºå™¨å­¦ä¹ çš„å­¦ä¹ è¿‡ç¨‹åæ˜ äº†äººç±»è®¤çŸ¥çš„æ¨¡å¼è¯†åˆ«å’Œå½’çº³æ¨ç†
- **æœ¬ä½“è®ºåæ€**: æœºå™¨å­¦ä¹ æ¨¡å‹ä½œä¸ºæŠ½è±¡å®ä½“ï¼Œå…¶å­˜åœ¨å½¢å¼ä»‹äºæ•°å­¦å¯¹è±¡å’Œç‰©ç†å®ç°ä¹‹é—´

### æ–¹æ³•è®ºç»´åº¦

- **ç®—æ³•æ¯”è¾ƒ**: ä¸åŒæœºå™¨å­¦ä¹ ç®—æ³•å„æœ‰ä¼˜ç¼ºç‚¹ï¼Œéœ€è¦æ ¹æ®å…·ä½“é—®é¢˜é€‰æ‹©
- **æ¨¡å‹é€‰æ‹©**: æ¨¡å‹å¤æ‚åº¦å’Œæ³›åŒ–èƒ½åŠ›çš„æƒè¡¡æ˜¯æ ¸å¿ƒé—®é¢˜
- **è¯„ä¼°æ–¹æ³•**: äº¤å‰éªŒè¯ã€ç•™ä¸€æ³•ç­‰è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§éœ€è¦æ·±å…¥åˆ†æ

### å·¥ç¨‹ç»´åº¦

- **å®ç°å¤æ‚åº¦**: ç†è®ºç®—æ³•åˆ°å·¥ç¨‹å®ç°çš„è½¬æ¢å­˜åœ¨æ€§èƒ½æŸå¤±
- **å¯æ‰©å±•æ€§**: å¤§è§„æ¨¡æ•°æ®çš„å¤„ç†èƒ½åŠ›æ˜¯å®é™…åº”ç”¨çš„å…³é”®
- **é²æ£’æ€§**: æ¨¡å‹å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼çš„æ•æ„Ÿæ€§éœ€è¦ç‰¹åˆ«å…³æ³¨

### ç¤¾ä¼šæŠ€æœ¯ç»´åº¦

- **ç®—æ³•åè§**: è®­ç»ƒæ•°æ®ä¸­çš„åè§ä¼šå¯¼è‡´æ¨¡å‹æ­§è§†
- **éšç§ä¿æŠ¤**: æœºå™¨å­¦ä¹ å¯¹ä¸ªäººéšç§çš„æ½œåœ¨å¨èƒ
- **è´£ä»»å½’å±**: æœºå™¨å­¦ä¹ å†³ç­–çš„è´£ä»»å½’å±é—®é¢˜
- **å°±ä¸šå½±å“**: è‡ªåŠ¨åŒ–å¯¹å°±ä¸šå¸‚åœºçš„å½±å“

## ğŸ“š å‚è§

- [17.2 ç»Ÿè®¡åˆ†æç†è®º](../02_Statistical_Analysis_Theory.md)
- [17.3 æ•°æ®æŒ–æ˜ç†è®º](../17.1_Data_Mining_Theory.md)
- [ç»Ÿä¸€æœ¯è¯­è¡¨](../../04_Type_Theory/TERMINOLOGY_TABLE.md)

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
2. Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.
3. Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
4. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
5. Mitchell, T. M. (1997). *Machine Learning*. McGraw-Hill.
