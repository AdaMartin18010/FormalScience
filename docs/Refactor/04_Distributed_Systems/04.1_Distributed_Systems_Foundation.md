# 04.1 分布式系统基础

## 目录

```markdown
04.1 分布式系统基础
├── 1. 引言
│   ├── 1.1 分布式系统概述
│   ├── 1.2 历史发展
│   ├── 1.3 基本概念
│   └── 1.4 应用领域
├── 2. 理论基础
│   ├── 2.1 系统模型
│   ├── 2.2 一致性理论
│   ├── 2.3 容错理论
│   └── 2.4 并发控制
├── 3. 核心算法
│   ├── 3.1 共识算法
│   ├── 3.2 复制算法
│   ├── 3.3 路由算法
│   └── 3.4 同步算法
├── 4. 实现示例
│   ├── 4.1 Rust实现
│   ├── 4.2 网络通信
│   └── 4.3 状态管理
└── 5. 参考文献
```

## 1. 引言

### 1.1 分布式系统概述

分布式系统是由多个独立计算机组成的系统，这些计算机通过网络进行通信和协调，共同完成特定任务。

**定义 1.1.1** (分布式系统)
分布式系统是一个由多个独立节点组成的系统，节点间通过网络进行通信，共同提供某种服务。

### 1.2 历史发展

分布式系统的发展历程：
1. **早期网络系统** (1960s-1970s)
2. **分布式操作系统** (1980s-1990s)
3. **分布式数据库** (1990s-2000s)
4. **云计算和微服务** (2000s-至今)

### 1.3 基本概念

**定义 1.3.1** (节点)
节点是分布式系统中的独立计算单元。

**定义 1.3.2** (消息)
消息是节点间通信的基本单位。

**定义 1.3.3** (状态)
状态是系统在某一时刻的完整信息。

## 2. 理论基础

### 2.1 系统模型

**定义 2.1.1** (异步系统模型)
异步分布式系统模型：
- 消息传递延迟无界
- 节点处理时间无界
- 节点可能崩溃

**定义 2.1.2** (同步系统模型)
同步分布式系统模型：
- 消息传递延迟有界
- 节点处理时间有界
- 节点时钟同步

### 2.2 一致性理论

**定义 2.2.1** (强一致性)
强一致性要求所有节点看到相同的数据状态。

**定义 2.2.2** (最终一致性)
最终一致性允许暂时的不一致，但最终会收敛到一致状态。

**CAP定理**:
在分布式系统中，最多只能同时满足一致性(Consistency)、可用性(Availability)和分区容错性(Partition tolerance)中的两个。

### 2.3 容错理论

**定义 2.3.1** (故障模型)
- 崩溃故障：节点停止工作
- 拜占庭故障：节点可能发送错误信息
- 网络分区：网络连接中断

### 2.4 并发控制

**定义 2.4.1** (并发控制)
并发控制确保多个操作在分布式环境下的正确性。

## 3. 核心算法

### 3.1 共识算法

**Paxos算法**:
```rust
#[derive(Debug, Clone)]
enum Message {
    Prepare(Prepare),
    Promise(Promise),
    Accept(Accept),
    Accepted(Accepted),
}

#[derive(Debug, Clone)]
struct Prepare {
    proposal_number: u64,
}

#[derive(Debug, Clone)]
struct Promise {
    proposal_number: u64,
    accepted_proposal: Option<u64>,
    accepted_value: Option<String>,
}

#[derive(Debug, Clone)]
struct Accept {
    proposal_number: u64,
    value: String,
}

#[derive(Debug, Clone)]
struct Accepted {
    proposal_number: u64,
    value: String,
}

struct PaxosNode {
    id: u64,
    proposal_number: u64,
    accepted_proposal: Option<u64>,
    accepted_value: Option<String>,
    promises: Vec<Promise>,
    accepts: Vec<Accepted>,
}

impl PaxosNode {
    fn new(id: u64) -> Self {
        PaxosNode {
            id,
            proposal_number: 0,
            accepted_proposal: None,
            accepted_value: None,
            promises: Vec::new(),
            accepts: Vec::new(),
        }
    }
    
    fn propose(&mut self, value: String) -> Vec<Message> {
        self.proposal_number += 1;
        let prepare = Prepare {
            proposal_number: self.proposal_number,
        };
        vec![Message::Prepare(prepare)]
    }
    
    fn handle_prepare(&mut self, prepare: Prepare) -> Option<Message> {
        if prepare.proposal_number > self.proposal_number {
            self.proposal_number = prepare.proposal_number;
            Some(Message::Promise(Promise {
                proposal_number: prepare.proposal_number,
                accepted_proposal: self.accepted_proposal,
                accepted_value: self.accepted_value.clone(),
            }))
        } else {
            None
        }
    }
    
    fn handle_promise(&mut self, promise: Promise) -> Option<Message> {
        if promise.proposal_number == self.proposal_number {
            self.promises.push(promise.clone());
            
            // 如果收到多数派的promise
            if self.promises.len() >= 3 { // 假设5个节点，需要3个
                let value = self.select_value();
                Some(Message::Accept(Accept {
                    proposal_number: self.proposal_number,
                    value,
                }))
            } else {
                None
            }
        } else {
            None
        }
    }
    
    fn select_value(&self) -> String {
        // 选择最高编号的已接受值，或提议新值
        let mut highest_proposal = None;
        let mut selected_value = None;
        
        for promise in &self.promises {
            if let Some(proposal) = promise.accepted_proposal {
                if highest_proposal.is_none() || proposal > highest_proposal.unwrap() {
                    highest_proposal = Some(proposal);
                    selected_value = promise.accepted_value.clone();
                }
            }
        }
        
        selected_value.unwrap_or_else(|| "default".to_string())
    }
}
```

### 3.2 复制算法

**主从复制**:
```rust
#[derive(Debug, Clone)]
enum ReplicationMessage {
    Write(String, String), // key, value
    Read(String),
    Response(String, Option<String>), // request_id, value
    Heartbeat,
}

struct MasterNode {
    data: HashMap<String, String>,
    replicas: Vec<u64>,
    pending_writes: HashMap<String, Vec<u64>>,
}

impl MasterNode {
    fn new() -> Self {
        MasterNode {
            data: HashMap::new(),
            replicas: vec![1, 2, 3], // 副本节点ID
            pending_writes: HashMap::new(),
        }
    }
    
    fn write(&mut self, key: String, value: String) -> Vec<ReplicationMessage> {
        // 写入主节点
        self.data.insert(key.clone(), value.clone());
        
        // 发送到所有副本
        let mut messages = Vec::new();
        for replica_id in &self.replicas {
            messages.push(ReplicationMessage::Write(key.clone(), value.clone()));
        }
        messages
    }
    
    fn read(&self, key: String) -> Option<String> {
        self.data.get(&key).cloned()
    }
}

struct ReplicaNode {
    id: u64,
    data: HashMap<String, String>,
    master_id: u64,
}

impl ReplicaNode {
    fn new(id: u64, master_id: u64) -> Self {
        ReplicaNode {
            id,
            data: HashMap::new(),
            master_id,
        }
    }
    
    fn handle_write(&mut self, key: String, value: String) {
        self.data.insert(key, value);
    }
    
    fn handle_read(&self, key: String) -> Option<String> {
        self.data.get(&key).cloned()
    }
}
```

### 3.3 路由算法

**一致性哈希**:
```rust
use std::collections::BTreeMap;
use std::hash::{Hash, Hasher};

struct ConsistentHash {
    ring: BTreeMap<u64, String>,
    virtual_nodes: u32,
}

impl ConsistentHash {
    fn new(virtual_nodes: u32) -> Self {
        ConsistentHash {
            ring: BTreeMap::new(),
            virtual_nodes,
        }
    }
    
    fn add_node(&mut self, node: String) {
        for i in 0..self.virtual_nodes {
            let virtual_node = format!("{}#{}", node, i);
            let hash = self.hash(&virtual_node);
            self.ring.insert(hash, node.clone());
        }
    }
    
    fn remove_node(&mut self, node: &str) {
        let mut to_remove = Vec::new();
        for (hash, ring_node) in &self.ring {
            if ring_node == node {
                to_remove.push(*hash);
            }
        }
        for hash in to_remove {
            self.ring.remove(&hash);
        }
    }
    
    fn get_node(&self, key: &str) -> Option<&String> {
        if self.ring.is_empty() {
            return None;
        }
        
        let hash = self.hash(key);
        self.ring.range(hash..).next()
            .or_else(|| self.ring.iter().next())
            .map(|(_, node)| node)
    }
    
    fn hash(&self, key: &str) -> u64 {
        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        key.hash(&mut hasher);
        hasher.finish()
    }
}
```

### 3.4 同步算法

**向量时钟**:
```rust
use std::collections::HashMap;

#[derive(Debug, Clone)]
struct VectorClock {
    clocks: HashMap<u64, u64>,
}

impl VectorClock {
    fn new() -> Self {
        VectorClock {
            clocks: HashMap::new(),
        }
    }
    
    fn increment(&mut self, node_id: u64) {
        *self.clocks.entry(node_id).or_insert(0) += 1;
    }
    
    fn merge(&mut self, other: &VectorClock) {
        for (node_id, clock) in &other.clocks {
            let current = self.clocks.entry(*node_id).or_insert(0);
            *current = (*current).max(*clock);
        }
    }
    
    fn compare(&self, other: &VectorClock) -> ClockRelation {
        let mut less = false;
        let mut greater = false;
        
        let all_nodes: std::collections::HashSet<_> = 
            self.clocks.keys().chain(other.clocks.keys()).cloned().collect();
        
        for node_id in all_nodes {
            let self_clock = self.clocks.get(&node_id).unwrap_or(&0);
            let other_clock = other.clocks.get(&node_id).unwrap_or(&0);
            
            if self_clock < other_clock {
                less = true;
            } else if self_clock > other_clock {
                greater = true;
            }
        }
        
        match (less, greater) {
            (false, false) => ClockRelation::Equal,
            (true, false) => ClockRelation::Before,
            (false, true) => ClockRelation::After,
            (true, true) => ClockRelation::Concurrent,
        }
    }
}

#[derive(Debug, PartialEq)]
enum ClockRelation {
    Before,
    After,
    Equal,
    Concurrent,
}
```

## 4. 实现示例

### 4.1 Rust实现

```rust
use tokio::net::{TcpListener, TcpStream};
use tokio::io::{AsyncReadExt, AsyncWriteExt};
use serde::{Serialize, Deserialize};
use std::collections::HashMap;

#[derive(Serialize, Deserialize, Debug, Clone)]
enum Message {
    Heartbeat { node_id: u64, timestamp: u64 },
    Data { key: String, value: String },
    Request { id: u64, data: String },
    Response { id: u64, data: String },
}

struct DistributedNode {
    id: u64,
    peers: HashMap<u64, String>, // node_id -> address
    data: HashMap<String, String>,
    vector_clock: VectorClock,
}

impl DistributedNode {
    fn new(id: u64) -> Self {
        DistributedNode {
            id,
            peers: HashMap::new(),
            data: HashMap::new(),
            vector_clock: VectorClock::new(),
        }
    }
    
    fn add_peer(&mut self, node_id: u64, address: String) {
        self.peers.insert(node_id, address);
    }
    
    async fn start_server(&self, address: &str) -> Result<(), Box<dyn std::error::Error>> {
        let listener = TcpListener::bind(address).await?;
        println!("Node {} listening on {}", self.id, address);
        
        loop {
            let (mut socket, _) = listener.accept().await?;
            
            let mut buf = vec![0; 1024];
            let n = socket.read(&mut buf).await?;
            
            if n > 0 {
                let message: Message = serde_json::from_slice(&buf[..n])?;
                self.handle_message(message).await;
            }
        }
    }
    
    async fn handle_message(&self, message: Message) {
        match message {
            Message::Heartbeat { node_id, timestamp } => {
                println!("Received heartbeat from node {} at {}", node_id, timestamp);
            }
            Message::Data { key, value } => {
                println!("Received data: {} = {}", key, value);
            }
            Message::Request { id, data } => {
                println!("Received request {}: {}", id, data);
            }
            Message::Response { id, data } => {
                println!("Received response {}: {}", id, data);
            }
        }
    }
    
    async fn send_message(&self, peer_id: u64, message: Message) -> Result<(), Box<dyn std::error::Error>> {
        if let Some(address) = self.peers.get(&peer_id) {
            let mut stream = TcpStream::connect(address).await?;
            let data = serde_json::to_vec(&message)?;
            stream.write_all(&data).await?;
        }
        Ok(())
    }
    
    async fn broadcast(&self, message: Message) -> Result<(), Box<dyn std::error::Error>> {
        for peer_id in self.peers.keys() {
            if *peer_id != self.id {
                self.send_message(*peer_id, message.clone()).await?;
            }
        }
        Ok(())
    }
}
```

### 4.2 网络通信

```rust
use tokio::sync::mpsc;

#[derive(Debug, Clone)]
struct NetworkMessage {
    from: u64,
    to: u64,
    data: Vec<u8>,
}

struct NetworkLayer {
    node_id: u64,
    tx: mpsc::Sender<NetworkMessage>,
    rx: mpsc::Receiver<NetworkMessage>,
}

impl NetworkLayer {
    fn new(node_id: u64) -> Self {
        let (tx, rx) = mpsc::channel(100);
        NetworkLayer { node_id, tx, rx }
    }
    
    async fn send(&self, to: u64, data: Vec<u8>) -> Result<(), mpsc::error::SendError<NetworkMessage>> {
        let message = NetworkMessage {
            from: self.node_id,
            to,
            data,
        };
        self.tx.send(message).await
    }
    
    async fn receive(&mut self) -> Option<NetworkMessage> {
        self.rx.recv().await
    }
}
```

### 4.3 状态管理

```rust
use std::sync::{Arc, Mutex};
use tokio::sync::RwLock;

#[derive(Debug, Clone)]
struct State {
    data: HashMap<String, String>,
    version: u64,
}

struct StateManager {
    state: Arc<RwLock<State>>,
    replicas: Vec<u64>,
}

impl StateManager {
    fn new() -> Self {
        StateManager {
            state: Arc::new(RwLock::new(State {
                data: HashMap::new(),
                version: 0,
            })),
            replicas: Vec::new(),
        }
    }
    
    async fn update(&self, key: String, value: String) {
        let mut state = self.state.write().await;
        state.data.insert(key, value);
        state.version += 1;
    }
    
    async fn get(&self, key: &str) -> Option<String> {
        let state = self.state.read().await;
        state.data.get(key).cloned()
    }
    
    async fn get_version(&self) -> u64 {
        let state = self.state.read().await;
        state.version
    }
}
```

## 5. 参考文献

1. Tanenbaum, A. S., & Van Steen, M. (2007). *Distributed Systems: Principles and Paradigms*. Prentice Hall.
2. Coulouris, G., Dollimore, J., Kindberg, T., & Blair, G. (2011). *Distributed Systems: Concepts and Design*. Pearson.
3. Lamport, L. (1998). *The Part-Time Parliament*. ACM Transactions on Computer Systems.
4. Brewer, E. A. (2000). *Towards Robust Distributed Systems*. PODC.
5. Vogels, W. (2009). *Eventually Consistent*. Communications of the ACM.

---

**相关文档**:
- [04.2 分布式算法](04.2_Distributed_Algorithms.md)
- [04.3 共识理论](04.3_Consensus_Theory.md)
- [04.4 分布式一致性](04.4_Distributed_Consistency.md)
- [11.1 并发理论基础](../11_Concurrency_Theory/11.1_Concurrency_Theory_Foundation.md)
