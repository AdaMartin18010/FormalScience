# 02.9.3 微分学

## 📋 概述

微分学是数学分析的核心分支，研究函数的局部变化率和切线性质。本文档建立严格的微分学理论框架，包含形式化定义、定理证明和实际应用。

## 🎯 核心目标

1. 建立严格的微分概念和形式化定义
2. 证明微分学的基本定理
3. 提供完整的代码实现
4. 展示微分学在形式科学中的应用

## 📚 目录

1. [基本概念](#1-基本概念)
2. [形式化定义](#2-形式化定义)
3. [定理与证明](#3-定理与证明)
4. [代码实现](#4-代码实现)
5. [应用示例](#5-应用示例)
6. [相关理论](#6-相关理论)
7. [参考文献](#7-参考文献)

## 1. 基本概念

### 1.1 导数的直观理解

导数描述函数在某点的瞬时变化率，几何上表示切线的斜率。

**定义 1.1.1** (导数直观定义)
设函数 $f: \mathbb{R} \to \mathbb{R}$，点 $x_0 \in \mathbb{R}$，如果极限
$$\lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$
存在，则称该极限为 $f$ 在 $x_0$ 处的导数，记作 $f'(x_0)$。

### 1.2 可微性

**定义 1.2.1** (可微函数)
函数 $f$ 在点 $x_0$ 处可微，当且仅当存在线性函数 $L: \mathbb{R} \to \mathbb{R}$ 使得：
$$\lim_{h \to 0} \frac{f(x_0 + h) - f(x_0) - L(h)}{h} = 0$$

### 1.3 微分算子

**定义 1.3.1** (微分算子)
微分算子 $D$ 是一个从可微函数空间到函数空间的线性算子：
$$D: C^1(\mathbb{R}) \to C(\mathbb{R})$$
$$D(f) = f'$$

## 2. 形式化定义

### 2.1 严格导数定义

**定义 2.1.1** (ε-δ 导数定义)
函数 $f: \mathbb{R} \to \mathbb{R}$ 在点 $x_0$ 处可导，当且仅当存在实数 $L$，使得对任意 $\varepsilon > 0$，存在 $\delta > 0$，当 $0 < |h| < \delta$ 时：
$$\left|\frac{f(x_0 + h) - f(x_0)}{h} - L\right| < \varepsilon$$

此时 $L = f'(x_0)$。

### 2.2 高阶导数

**定义 2.2.1** (n阶导数)
函数 $f$ 的 n 阶导数递归定义为：
$$f^{(0)} = f$$
$$f^{(n)} = (f^{(n-1)})' \quad (n \geq 1)$$

### 2.3 偏导数

**定义 2.3.1** (偏导数)
设 $f: \mathbb{R}^n \to \mathbb{R}$，在点 $x = (x_1, \ldots, x_n)$ 处关于第 $i$ 个变量的偏导数定义为：
$$\frac{\partial f}{\partial x_i}(x) = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_n)}{h}$$

## 3. 定理与证明

### 3.1 基本求导法则

**定理 3.1.1** (线性性)
设 $f, g$ 在 $x_0$ 处可导，$\alpha, \beta \in \mathbb{R}$，则：
$$(\alpha f + \beta g)'(x_0) = \alpha f'(x_0) + \beta g'(x_0)$$

**证明**：
$$\begin{align}
(\alpha f + \beta g)'(x_0) &= \lim_{h \to 0} \frac{(\alpha f + \beta g)(x_0 + h) - (\alpha f + \beta g)(x_0)}{h} \\
&= \lim_{h \to 0} \frac{\alpha f(x_0 + h) + \beta g(x_0 + h) - \alpha f(x_0) - \beta g(x_0)}{h} \\
&= \alpha \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h} + \beta \lim_{h \to 0} \frac{g(x_0 + h) - g(x_0)}{h} \\
&= \alpha f'(x_0) + \beta g'(x_0)
\end{align}$$

**定理 3.1.2** (乘积法则)
设 $f, g$ 在 $x_0$ 处可导，则：
$$(fg)'(x_0) = f'(x_0)g(x_0) + f(x_0)g'(x_0)$$

**证明**：
$$\begin{align}
(fg)'(x_0) &= \lim_{h \to 0} \frac{f(x_0 + h)g(x_0 + h) - f(x_0)g(x_0)}{h} \\
&= \lim_{h \to 0} \frac{f(x_0 + h)g(x_0 + h) - f(x_0)g(x_0 + h) + f(x_0)g(x_0 + h) - f(x_0)g(x_0)}{h} \\
&= \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h} \cdot g(x_0 + h) + f(x_0) \cdot \frac{g(x_0 + h) - g(x_0)}{h} \\
&= f'(x_0)g(x_0) + f(x_0)g'(x_0)
\end{align}$$

### 3.2 链式法则

**定理 3.2.1** (链式法则)
设 $f$ 在 $g(x_0)$ 处可导，$g$ 在 $x_0$ 处可导，则：
$$(f \circ g)'(x_0) = f'(g(x_0)) \cdot g'(x_0)$$

**证明**：
设 $y_0 = g(x_0)$，则：
$$\begin{align}
(f \circ g)'(x_0) &= \lim_{h \to 0} \frac{f(g(x_0 + h)) - f(g(x_0))}{h} \\
&= \lim_{h \to 0} \frac{f(g(x_0 + h)) - f(g(x_0))}{g(x_0 + h) - g(x_0)} \cdot \frac{g(x_0 + h) - g(x_0)}{h} \\
&= f'(g(x_0)) \cdot g'(x_0)
\end{align}$$

### 3.3 中值定理

**定理 3.3.1** (拉格朗日中值定理)
设 $f$ 在闭区间 $[a, b]$ 上连续，在开区间 $(a, b)$ 上可导，则存在 $\xi \in (a, b)$ 使得：
$$f(b) - f(a) = f'(\xi)(b - a)$$

**证明**：
构造辅助函数：
$$F(x) = f(x) - f(a) - \frac{f(b) - f(a)}{b - a}(x - a)$$

则 $F(a) = F(b) = 0$，由罗尔定理，存在 $\xi \in (a, b)$ 使得 $F'(\xi) = 0$，即：
$$f'(\xi) - \frac{f(b) - f(a)}{b - a} = 0$$

因此：
$$f(b) - f(a) = f'(\xi)(b - a)$$

## 4. 代码实现

### 4.1 Rust 实现

```rust
use std::f64;

/// 数值微分实现
pub struct NumericalDifferentiation;

impl NumericalDifferentiation {
    /// 使用中心差分计算数值导数
    pub fn central_difference<F>(f: F, x: f64, h: f64) -> f64 
    where
        F: Fn(f64) -> f64,
    {
        (f(x + h) - f(x - h)) / (2.0 * h)
    }
    
    /// 使用前向差分计算数值导数
    pub fn forward_difference<F>(f: F, x: f64, h: f64) -> f64 
    where
        F: Fn(f64) -> f64,
    {
        (f(x + h) - f(x)) / h
    }
    
    /// 自适应步长的数值微分
    pub fn adaptive_differentiation<F>(f: F, x: f64, tolerance: f64) -> f64 
    where
        F: Fn(f64) -> f64,
    {
        let mut h = 1e-6;
        let mut prev_derivative = Self::central_difference(&f, x, h);
        
        loop {
            h /= 2.0;
            let current_derivative = Self::central_difference(&f, x, h);
            
            if (current_derivative - prev_derivative).abs() < tolerance {
                return current_derivative;
            }
            
            prev_derivative = current_derivative;
        }
    }
}

/// 符号微分实现
pub struct SymbolicDifferentiation;

impl SymbolicDifferentiation {
    /// 多项式求导
    pub fn differentiate_polynomial(coefficients: &[f64]) -> Vec<f64> {
        coefficients
            .iter()
            .enumerate()
            .skip(1)
            .map(|(i, &coeff)| coeff * i as f64)
            .collect()
    }
    
    /// 基本函数求导
    pub fn differentiate_basic_function(func_type: &str, x: f64) -> f64 {
        match func_type {
            "sin" => x.cos(),
            "cos" => -x.sin(),
            "exp" => x.exp(),
            "ln" => 1.0 / x,
            "power" => x.powf(x - 1.0),
            _ => panic!("Unknown function type"),
        }
    }
}

/// 自动微分实现
#[derive(Debug, Clone)]
pub struct DualNumber {
    pub real: f64,
    pub dual: f64,
}

impl DualNumber {
    pub fn new(real: f64, dual: f64) -> Self {
        Self { real, dual }
    }
    
    pub fn constant(x: f64) -> Self {
        Self::new(x, 0.0)
    }
    
    pub fn variable(x: f64) -> Self {
        Self::new(x, 1.0)
    }
}

impl std::ops::Add for DualNumber {
    type Output = Self;
    
    fn add(self, other: Self) -> Self {
        Self::new(self.real + other.real, self.dual + other.dual)
    }
}

impl std::ops::Mul for DualNumber {
    type Output = Self;
    
    fn mul(self, other: Self) -> Self {
        Self::new(
            self.real * other.real,
            self.real * other.dual + self.dual * other.real,
        )
    }
}

impl DualNumber {
    pub fn sin(self) -> Self {
        Self::new(self.real.sin(), self.dual * self.real.cos())
    }
    
    pub fn cos(self) -> Self {
        Self::new(self.real.cos(), -self.dual * self.real.sin())
    }
    
    pub fn exp(self) -> Self {
        let exp_real = self.real.exp();
        Self::new(exp_real, self.dual * exp_real)
    }
}

/// 自动微分函数
pub fn automatic_differentiation<F>(f: F, x: f64) -> f64 
where
    F: Fn(DualNumber) -> DualNumber,
{
    let dual_x = DualNumber::variable(x);
    let result = f(dual_x);
    result.dual
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_numerical_differentiation() {
        let f = |x: f64| x.powi(2);
        let derivative = NumericalDifferentiation::central_difference(f, 2.0, 1e-6);
        assert!((derivative - 4.0).abs() < 1e-5);
    }
    
    #[test]
    fn test_symbolic_differentiation() {
        let coefficients = vec![1.0, 2.0, 3.0]; // 1 + 2x + 3x²
        let derivative = SymbolicDifferentiation::differentiate_polynomial(&coefficients);
        assert_eq!(derivative, vec![2.0, 6.0]); // 2 + 6x
    }
    
    #[test]
    fn test_automatic_differentiation() {
        let f = |x: DualNumber| x * x; // x²
        let derivative = automatic_differentiation(f, 3.0);
        assert!((derivative - 6.0).abs() < 1e-10);
    }
}
```

### 4.2 Haskell 实现

```haskell
-- 数值微分
module Differentiation where

import Data.List

-- 中心差分
centralDifference :: (Double -> Double) -> Double -> Double -> Double
centralDifference f x h = (f (x + h) - f (x - h)) / (2 * h)

-- 前向差分
forwardDifference :: (Double -> Double) -> Double -> Double -> Double
forwardDifference f x h = (f (x + h) - f x) / h

-- 自适应步长微分
adaptiveDifferentiation :: (Double -> Double) -> Double -> Double -> Double
adaptiveDifferentiation f x tolerance = go 1e-6
  where
    go h = let
        current = centralDifference f x h
        next = centralDifference f x (h / 2)
        in if abs (next - current) < tolerance
           then next
           else go (h / 2)

-- 符号微分
data Expression = 
    Constant Double
  | Variable String
  | Add Expression Expression
  | Multiply Expression Expression
  | Power Expression Double
  | Sin Expression
  | Cos Expression
  | Exp Expression
  | Ln Expression

-- 符号求导
differentiate :: Expression -> Expression
differentiate (Constant _) = Constant 0
differentiate (Variable _) = Constant 1
differentiate (Add e1 e2) = Add (differentiate e1) (differentiate e2)
differentiate (Multiply e1 e2) = 
    Add (Multiply (differentiate e1) e2) (Multiply e1 (differentiate e2))
differentiate (Power e n) = 
    Multiply (Multiply (Constant n) (Power e (n - 1))) (differentiate e)
differentiate (Sin e) = Multiply (Cos e) (differentiate e)
differentiate (Cos e) = Multiply (Multiply (Constant (-1)) (Sin e)) (differentiate e)
differentiate (Exp e) = Multiply (Exp e) (differentiate e)
differentiate (Ln e) = Multiply (Power e (-1)) (differentiate e)

-- 自动微分
data Dual = Dual Double Double

instance Num Dual where
    (Dual a b) + (Dual c d) = Dual (a + c) (b + d)
    (Dual a b) * (Dual c d) = Dual (a * c) (a * d + b * c)
    negate (Dual a b) = Dual (-a) (-b)
    abs (Dual a b) = Dual (abs a) (if a >= 0 then b else -b)
    signum (Dual a _) = Dual (signum a) 0
    fromInteger n = Dual (fromInteger n) 0

instance Floating Dual where
    pi = Dual pi 0
    exp (Dual a b) = Dual (exp a) (b * exp a)
    log (Dual a b) = Dual (log a) (b / a)
    sin (Dual a b) = Dual (sin a) (b * cos a)
    cos (Dual a b) = Dual (cos a) (-b * sin a)
    asin (Dual a b) = Dual (asin a) (b / sqrt (1 - a^2))
    acos (Dual a b) = Dual (acos a) (-b / sqrt (1 - a^2))
    atan (Dual a b) = Dual (atan a) (b / (1 + a^2))
    sinh (Dual a b) = Dual (sinh a) (b * cosh a)
    cosh (Dual a b) = Dual (cosh a) (b * sinh a)
    asinh (Dual a b) = Dual (asinh a) (b / sqrt (1 + a^2))
    acosh (Dual a b) = Dual (acosh a) (b / sqrt (a^2 - 1))
    atanh (Dual a b) = Dual (atanh a) (b / (1 - a^2))

-- 自动微分函数
automaticDifferentiation :: (Dual -> Dual) -> Double -> Double
automaticDifferentiation f x = dual
  where
    Dual _ dual = f (Dual x 1)

-- 测试函数
testDifferentiation :: IO ()
testDifferentiation = do
    let f x = x^2
    putStrLn "数值微分测试:"
    print $ centralDifference f 2.0 1e-6
    putStrLn "自动微分测试:"
    print $ automaticDifferentiation (\x -> x * x) 2.0
```

## 5. 应用示例

### 5.1 优化算法中的应用

```rust
/// 梯度下降优化
pub struct GradientDescent {
    learning_rate: f64,
    tolerance: f64,
    max_iterations: usize,
}

impl GradientDescent {
    pub fn new(learning_rate: f64, tolerance: f64, max_iterations: usize) -> Self {
        Self {
            learning_rate,
            tolerance,
            max_iterations,
        }
    }
    
    pub fn optimize<F, G>(&self, f: F, grad_f: G, initial_x: f64) -> f64 
    where
        F: Fn(f64) -> f64,
        G: Fn(f64) -> f64,
    {
        let mut x = initial_x;
        
        for iteration in 0..self.max_iterations {
            let gradient = grad_f(x);
            
            if gradient.abs() < self.tolerance {
                println!("收敛于第 {} 次迭代", iteration);
                break;
            }
            
            x -= self.learning_rate * gradient;
        }
        
        x
    }
}

// 使用示例
fn main() {
    let optimizer = GradientDescent::new(0.01, 1e-6, 1000);
    
    // 最小化函数 f(x) = x² + 2x + 1
    let f = |x: f64| x.powi(2) + 2.0 * x + 1.0;
    let grad_f = |x: f64| 2.0 * x + 2.0;
    
    let minimum = optimizer.optimize(f, grad_f, 5.0);
    println!("最小值点: x = {}", minimum);
    println!("最小值: f(x) = {}", f(minimum));
}
```

### 5.2 物理模拟中的应用

```rust
/// 弹簧-质量系统模拟
pub struct SpringMassSystem {
    mass: f64,
    spring_constant: f64,
    damping_coefficient: f64,
}

impl SpringMassSystem {
    pub fn new(mass: f64, spring_constant: f64, damping_coefficient: f64) -> Self {
        Self {
            mass,
            spring_constant,
            damping_coefficient,
        }
    }
    
    /// 计算加速度 (使用牛顿第二定律)
    pub fn acceleration(&self, position: f64, velocity: f64) -> f64 {
        let spring_force = -self.spring_constant * position;
        let damping_force = -self.damping_coefficient * velocity;
        (spring_force + damping_force) / self.mass
    }
    
    /// 欧拉方法数值积分
    pub fn simulate(&self, initial_position: f64, initial_velocity: f64, 
                   time_step: f64, total_time: f64) -> Vec<(f64, f64, f64)> {
        let mut positions = Vec::new();
        let mut position = initial_position;
        let mut velocity = initial_velocity;
        let mut time = 0.0;
        
        while time <= total_time {
            let acceleration = self.acceleration(position, velocity);
            
            positions.push((time, position, velocity));
            
            // 欧拉积分
            position += velocity * time_step;
            velocity += acceleration * time_step;
            time += time_step;
        }
        
        positions
    }
}
```

## 6. 相关理论

### 6.1 与积分学的关系

微分学与积分学通过微积分基本定理紧密联系：

**定理 6.1.1** (微积分基本定理)
设 $f$ 在 $[a, b]$ 上连续，$F$ 是 $f$ 的原函数，则：
$$\int_a^b f(x) dx = F(b) - F(a)$$

### 6.2 与泰勒级数的关系

**定理 6.2.1** (泰勒定理)
设 $f$ 在 $x_0$ 的邻域内具有 $n+1$ 阶导数，则：
$$f(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x - x_0)^k + R_n(x)$$

其中余项 $R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x - x_0)^{n+1}$，$\xi$ 在 $x_0$ 与 $x$ 之间。

### 6.3 与微分方程的关系

微分学为微分方程理论提供基础：

**定义 6.3.1** (常微分方程)
形如 $F(x, y, y', y'', \ldots, y^{(n)}) = 0$ 的方程称为 n 阶常微分方程。

## 7. 参考文献

1. **Rudin, W.** (1976). *Principles of Mathematical Analysis*. McGraw-Hill.
2. **Apostol, T. M.** (1974). *Mathematical Analysis*. Addison-Wesley.
3. **Spivak, M.** (2008). *Calculus*. Publish or Perish.
4. **Courant, R., & John, F.** (1999). *Introduction to Calculus and Analysis*. Springer.
5. **Dieudonné, J.** (1969). *Foundations of Modern Analysis*. Academic Press.

---

**相关文档**:
- [02.9.1 极限理论](../02.9.1_极限理论.md)
- [02.9.2 连续性](../02.9.2_连续性.md)
- [02.9.4 积分学](../02.9.4_积分学.md)
- [02.4.1 函数概念](../02.4.1_函数概念.md)
- [02.4.2 函数性质](../02.4.2_函数性质.md) 