# 02.3 上下文无关语言理论

## 目录

```markdown
02.3 上下文无关语言理论
├── 1. 概述
│   ├── 1.1 定义与特征
│   ├── 1.2 与正则语言的关系
│   └── 1.3 应用领域
├── 2. 上下文无关文法
│   ├── 2.1 基本定义
│   ├── 2.2 推导过程
│   ├── 2.3 文法分类
│   └── 2.4 规范化
├── 3. 下推自动机
│   ├── 3.1 基本定义
│   ├── 3.2 等价性
│   ├── 3.3 确定性PDA
│   └── 3.4 非确定性PDA
├── 4. 语法分析
│   ├── 4.1 自顶向下分析
│   ├── 4.2 自底向上分析
│   ├── 4.3 LL分析
│   └── 4.4 LR分析
├── 5. 形式化证明
│   ├── 5.1 泵引理
│   ├── 5.2 闭包性质
│   ├── 5.3 不可判定性
│   └── 5.4 算法复杂性
├── 6. 算法实现
│   ├── 6.1 Haskell实现
│   ├── 6.2 Rust实现
│   └── 6.3 解析器生成
├── 7. 应用实例
│   ├── 7.1 编程语言语法
│   ├── 7.2 自然语言处理
│   └── 7.3 配置文件解析
└── 8. 参考文献
```

## 1. 概述

### 1.1 定义与特征

**定义 1.1** (上下文无关语言)
上下文无关语言是可以通过上下文无关文法生成的语言类。

**特征**:

- 具有递归结构
- 支持嵌套括号
- 表达能力介于正则语言和上下文相关语言之间
- 具有泵引理

### 1.2 与正则语言的关系

**定理 1.2** (包含关系)
正则语言 ⊂ 上下文无关语言

**证明**:
每个正则文法都是上下文无关文法的特例。

### 1.3 应用领域

- **编程语言**: 语法定义和解析
- **自然语言**: 句法分析
- **配置文件**: 结构化数据解析
- **标记语言**: XML、HTML解析

## 2. 上下文无关文法

### 2.1 基本定义

**定义 2.1** (上下文无关文法)
上下文无关文法是四元组 $G = (V, \Sigma, P, S)$，其中：

- $V$ 是非终结符集
- $\Sigma$ 是终结符集
- $P$ 是产生式集，形式为 $A \to \alpha$
- $S \in V$ 是开始符号

**定义 2.2** (产生式)
产生式 $A \to \alpha$ 表示非终结符 $A$ 可以重写为字符串 $\alpha$。

### 2.2 推导过程

**定义 2.3** (推导)
推导关系 $\Rightarrow$ 定义如下：

- 如果 $A \to \alpha \in P$，则 $\beta A \gamma \Rightarrow \beta \alpha \gamma$
- 如果 $\alpha \Rightarrow \beta$ 且 $\beta \Rightarrow \gamma$，则 $\alpha \Rightarrow \gamma$

**定义 2.4** (语言)
文法 $G$ 生成的语言 $L(G) = \{w \in \Sigma^* \mid S \Rightarrow^* w\}$

### 2.3 文法分类

**定义 2.5** (文法分类)

1. **Chomsky范式**: 产生式形式为 $A \to BC$ 或 $A \to a$
2. **Greibach范式**: 产生式形式为 $A \to a\alpha$
3. **简化文法**: 无无用符号和无ε产生式

### 2.4 规范化

**定理 2.6** (Chomsky范式)
每个上下文无关文法都有等价的Chomsky范式。

**算法 2.7** (Chomsky范式转换)

```rust
fn to_chomsky_normal_form(grammar: &ContextFreeGrammar) -> ContextFreeGrammar {
    // 1. 消除ε产生式
    let grammar = eliminate_epsilon_productions(grammar);
    
    // 2. 消除单位产生式
    let grammar = eliminate_unit_productions(grammar);
    
    // 3. 消除无用符号
    let grammar = eliminate_useless_symbols(grammar);
    
    // 4. 转换为Chomsky范式
    let grammar = convert_to_chomsky_form(grammar);
    
    grammar
}
```

## 3. 下推自动机

### 3.1 基本定义

**定义 3.1** (下推自动机)
下推自动机是七元组 $M = (Q, \Sigma, \Gamma, \delta, q_0, Z_0, F)$，其中：

- $Q$ 是状态集
- $\Sigma$ 是输入字母表
- $\Gamma$ 是栈字母表
- $\delta: Q \times (\Sigma \cup \{\varepsilon\}) \times \Gamma \to 2^{Q \times \Gamma^*}$ 是转移函数
- $q_0 \in Q$ 是初始状态
- $Z_0 \in \Gamma$ 是初始栈符号
- $F \subseteq Q$ 是接受状态集

### 3.2 等价性

**定理 3.2** (CFG与PDA等价性)
语言是上下文无关的当且仅当它可以被下推自动机识别。

### 3.3 确定性PDA

**定义 3.3** (确定性PDA)
PDA是确定性的，如果转移函数满足确定性条件。

**性质**:

- 表达能力弱于非确定性PDA
- 对应确定性上下文无关语言
- 具有线性时间解析算法

### 3.4 非确定性PDA

**定义 3.4** (非确定性PDA)
非确定性PDA允许多个转移选择。

**性质**:

- 表达能力等于上下文无关语言
- 对应非确定性上下文无关语言
- 具有多项式时间解析算法

## 4. 语法分析

### 4.1 自顶向下分析

**定义 4.1** (自顶向下分析)
从开始符号开始，逐步推导到输入字符串。

**算法**:

1. 从开始符号开始
2. 选择产生式进行推导
3. 递归处理非终结符
4. 匹配终结符

### 4.2 自底向上分析

**定义 4.2** (自底向上分析)
从输入字符串开始，逐步归约到开始符号。

**算法**:

1. 从输入字符串开始
2. 识别句柄
3. 应用产生式进行归约
4. 重复直到归约到开始符号

### 4.3 LL分析

**定义 4.3** (LL分析)
左到右扫描，最左推导的分析方法。

**性质**:

- 确定性
- 递归下降
- 预测分析

### 4.4 LR分析

**定义 4.4** (LR分析)
左到右扫描，最右推导的分析方法。

**性质**:

- 确定性
- 移进-归约
- 状态机驱动

## 5. 形式化证明

### 5.1 泵引理

**定理 5.1** (上下文无关语言泵引理)
设 $L$ 是上下文无关语言，则存在常数 $p$，使得对于任意 $w \in L$ 且 $|w| \geq p$，存在分解 $w = uvxyz$，满足：

1. $|vxy| \leq p$
2. $|vy| > 0$
3. 对于任意 $i \geq 0$，$uv^ixy^iz \in L$

### 5.2 闭包性质

**定理 5.2** (闭包性质)
上下文无关语言在以下运算下闭包：

1. **并运算**: $L_1 \cup L_2$
2. **连接运算**: $L_1 \cdot L_2$
3. **克林星号**: $L^*$
4. **同态**: $h(L)$

**定理 5.3** (非闭包性质)
上下文无关语言在以下运算下不闭包：

1. **交运算**: $L_1 \cap L_2$
2. **补运算**: $\overline{L}$

### 5.3 不可判定性

**定理 5.4** (不可判定问题)
以下问题是不可判定的：

1. **等价性**: $L(G_1) = L(G_2)$
2. **包含性**: $L(G_1) \subseteq L(G_2)$
3. **歧义性**: 文法是否歧义
4. **空性**: $L(G) = \emptyset$

### 5.4 算法复杂性

**定理 5.5** (解析复杂性)

- **CYK算法**: $O(n^3)$
- **Earley算法**: $O(n^3)$
- **LL分析**: $O(n)$
- **LR分析**: $O(n)$

## 6. 算法实现

### 6.1 Haskell实现

```haskell
-- 上下文无关文法
data CFG = CFG {
    nonterminals :: Set Char,
    terminals :: Set Char,
    productions :: Map Char [String],
    startSymbol :: Char
}

-- 下推自动机
data PDA = PDA {
    states :: Set Int,
    inputAlphabet :: Set Char,
    stackAlphabet :: Set Char,
    transitions :: Map (Int, Maybe Char, Char) [(Int, String)],
    initialState :: Int,
    initialStack :: Char,
    acceptingStates :: Set Int
}

-- CYK算法
cykParse :: CFG -> String -> Bool
cykParse cfg w = startSymbol cfg `elem` table !! 0 !! (length w - 1)
  where
    n = length w
    table = [[generateCell i j | j <- [0..n-1]] | i <- [0..n-1]]
    
    generateCell i j
        | i == 0 = generateTerminal j
        | otherwise = generateNonTerminal i j
    
    generateTerminal j = 
        let symbol = w !! j
        in [nt | nt <- nonterminals cfg, 
                any (== [symbol]) (productions cfg ! nt)]
    
    generateNonTerminal i j = 
        [nt | nt <- nonterminals cfg,
              any (canGenerate i j nt) (productions cfg ! nt)]
    
    canGenerate i j nt production =
        case production of
            [a, b] -> any (\k -> a `elem` table !! (i-1) !! k && 
                                b `elem` table !! k !! (j-i+1)) [0..i-1]
            _ -> False

-- LL分析器
data LLParser = LLParser {
    llGrammar :: CFG,
    firstSets :: Map Char (Set Char),
    followSets :: Map Char (Set Char),
    parseTable :: Map (Char, Char) [String]
}

-- 构建LL分析表
buildLLTable :: CFG -> LLParser
buildLLTable cfg = LLParser {
    llGrammar = cfg,
    firstSets = computeFirstSets cfg,
    followSets = computeFollowSets cfg,
    parseTable = buildParseTable cfg
}

-- 计算First集
computeFirstSets :: CFG -> Map Char (Set Char)
computeFirstSets cfg = 
    let initial = Map.fromList [(nt, Set.empty) | nt <- nonterminals cfg]
    in fixPoint (updateFirstSets cfg) initial

-- 计算Follow集
computeFollowSets :: CFG -> Map Char (Set Char)
computeFollowSets cfg =
    let initial = Map.fromList [(nt, if nt == startSymbol cfg then Set.singleton '$' else Set.empty) | nt <- nonterminals cfg]
    in fixPoint (updateFollowSets cfg) initial

-- LR分析器
data LRParser = LRParser {
    lrGrammar :: CFG,
    states :: [LRState],
    actionTable :: Map (Int, Char) LRAction,
    gotoTable :: Map (Int, Char) Int
}

data LRState = LRState {
    items :: Set LRItem,
    transitions :: Map Char Int
}

data LRItem = LRItem {
    lhs :: Char,
    rhs :: String,
    dot :: Int,
    lookahead :: Set Char
}

data LRAction = Shift Int | Reduce Char String | Accept | Error

-- 构建LR分析表
buildLRTable :: CFG -> LRParser
buildLRTable cfg = 
    let canonical = buildCanonicalCollection cfg
        action = buildActionTable canonical
        goto = buildGotoTable canonical
    in LRParser {
        lrGrammar = cfg,
        states = canonical,
        actionTable = action,
        gotoTable = goto
    }
```

### 6.2 Rust实现

```rust
use std::collections::{HashMap, HashSet};

// 上下文无关文法
#[derive(Clone, Debug)]
struct ContextFreeGrammar {
    nonterminals: HashSet<char>,
    terminals: HashSet<char>,
    productions: HashMap<char, Vec<String>>,
    start_symbol: char,
}

impl ContextFreeGrammar {
    fn new(start_symbol: char) -> Self {
        ContextFreeGrammar {
            nonterminals: HashSet::new(),
            terminals: HashSet::new(),
            productions: HashMap::new(),
            start_symbol,
        }
    }
    
    fn add_production(&mut self, lhs: char, rhs: String) {
        self.nonterminals.insert(lhs);
        for c in rhs.chars() {
            if c.is_uppercase() {
                self.nonterminals.insert(c);
            } else {
                self.terminals.insert(c);
            }
        }
        self.productions.entry(lhs).or_insert_with(Vec::new).push(rhs);
    }
    
    // CYK算法实现
    fn cyk_parse(&self, w: &str) -> bool {
        let n = w.len();
        let mut table = vec![vec![HashSet::new(); n]; n];
        
        // 填充对角线
        for (j, c) in w.chars().enumerate() {
            for (&nt, productions) in &self.productions {
                for production in productions {
                    if production == c.to_string() {
                        table[0][j].insert(nt);
                    }
                }
            }
        }
        
        // 填充其余部分
        for i in 1..n {
            for j in 0..n-i {
                for k in 0..i {
                    for (&nt, productions) in &self.productions {
                        for production in productions {
                            if production.len() == 2 {
                                let a = production.chars().nth(0).unwrap();
                                let b = production.chars().nth(1).unwrap();
                                if table[k][j].contains(&a) && table[i-k-1][j+k+1].contains(&b) {
                                    table[i][j].insert(nt);
                                }
                            }
                        }
                    }
                }
            }
        }
        
        table[n-1][0].contains(&self.start_symbol)
    }
}

// 下推自动机
#[derive(Clone, Debug)]
struct PushdownAutomaton {
    states: HashSet<usize>,
    input_alphabet: HashSet<char>,
    stack_alphabet: HashSet<char>,
    transitions: HashMap<(usize, Option<char>, char), Vec<(usize, String)>>,
    initial_state: usize,
    initial_stack: char,
    accepting_states: HashSet<usize>,
}

impl PushdownAutomaton {
    fn accepts(&self, w: &str) -> bool {
        let mut configurations = vec![(self.initial_state, w, vec![self.initial_stack])];
        
        while let Some((state, input, stack)) = configurations.pop() {
            if input.is_empty() && self.accepting_states.contains(&state) {
                return true;
            }
            
            let current_input = input.chars().next();
            let current_stack = stack.last().copied();
            
            if let Some(current_stack) = current_stack {
                let key = (state, current_input, current_stack);
                if let Some(transitions) = self.transitions.get(&key) {
                    for &(next_state, ref stack_push) in transitions {
                        let mut new_stack = stack.clone();
                        new_stack.pop();
                        for c in stack_push.chars().rev() {
                            new_stack.push(c);
                        }
                        
                        let new_input = if current_input.is_some() { &input[1..] } else { input };
                        configurations.push((next_state, new_input, new_stack));
                    }
                }
            }
        }
        
        false
    }
}

// LL分析器
#[derive(Clone, Debug)]
struct LLParser {
    grammar: ContextFreeGrammar,
    first_sets: HashMap<char, HashSet<char>>,
    follow_sets: HashMap<char, HashSet<char>>,
    parse_table: HashMap<(char, char), Vec<String>>,
}

impl LLParser {
    fn new(grammar: ContextFreeGrammar) -> Self {
        let first_sets = Self::compute_first_sets(&grammar);
        let follow_sets = Self::compute_follow_sets(&grammar, &first_sets);
        let parse_table = Self::build_parse_table(&grammar, &first_sets, &follow_sets);
        
        LLParser {
            grammar,
            first_sets,
            follow_sets,
            parse_table,
        }
    }
    
    fn compute_first_sets(grammar: &ContextFreeGrammar) -> HashMap<char, HashSet<char>> {
        let mut first_sets = HashMap::new();
        
        // 初始化
        for &nt in &grammar.nonterminals {
            first_sets.insert(nt, HashSet::new());
        }
        
        // 迭代计算
        let mut changed = true;
        while changed {
            changed = false;
            for (&nt, productions) in &grammar.productions {
                for production in productions {
                    let first = Self::first_of_string(production, &first_sets, &grammar);
                    let current = first_sets.get_mut(&nt).unwrap();
                    let old_size = current.len();
                    current.extend(first);
                    if current.len() > old_size {
                        changed = true;
                    }
                }
            }
        }
        
        first_sets
    }
    
    fn first_of_string(s: &str, first_sets: &HashMap<char, HashSet<char>>, grammar: &ContextFreeGrammar) -> HashSet<char> {
        let mut result = HashSet::new();
        let mut all_nullable = true;
        
        for c in s.chars() {
            if grammar.terminals.contains(&c) {
                result.insert(c);
                all_nullable = false;
                break;
            } else if let Some(first) = first_sets.get(&c) {
                for &symbol in first {
                    if symbol != 'ε' {
                        result.insert(symbol);
                    }
                }
                if !first.contains(&'ε') {
                    all_nullable = false;
                    break;
                }
            }
        }
        
        if all_nullable {
            result.insert('ε');
        }
        
        result
    }
    
    fn compute_follow_sets(grammar: &ContextFreeGrammar, first_sets: &HashMap<char, HashSet<char>>) -> HashMap<char, HashSet<char>> {
        let mut follow_sets = HashMap::new();
        
        // 初始化
        for &nt in &grammar.nonterminals {
            follow_sets.insert(nt, HashSet::new());
        }
        follow_sets.get_mut(&grammar.start_symbol).unwrap().insert('$');
        
        // 迭代计算
        let mut changed = true;
        while changed {
            changed = false;
            for (&nt, productions) in &grammar.productions {
                for production in productions {
                    let chars: Vec<char> = production.chars().collect();
                    for i in 0..chars.len() {
                        if grammar.nonterminals.contains(&chars[i]) {
                            let beta = &chars[i+1..];
                            let first_beta = Self::first_of_string(&beta.iter().collect::<String>(), first_sets, grammar);
                            
                            let current = follow_sets.get_mut(&chars[i]).unwrap();
                            let old_size = current.len();
                            
                            for &symbol in &first_beta {
                                if symbol != 'ε' {
                                    current.insert(symbol);
                                }
                            }
                            
                            if first_beta.contains(&'ε') {
                                if let Some(follow_nt) = follow_sets.get(&nt) {
                                    current.extend(follow_nt);
                                }
                            }
                            
                            if current.len() > old_size {
                                changed = true;
                            }
                        }
                    }
                }
            }
        }
        
        follow_sets
    }
    
    fn build_parse_table(grammar: &ContextFreeGrammar, first_sets: &HashMap<char, HashSet<char>>, follow_sets: &HashMap<char, HashSet<char>>) -> HashMap<(char, char), Vec<String>> {
        let mut parse_table = HashMap::new();
        
        for (&nt, productions) in &grammar.productions {
            for production in productions {
                let first = Self::first_of_string(production, first_sets, grammar);
                
                for &symbol in &first {
                    if symbol != 'ε' {
                        parse_table.insert((nt, symbol), vec![production.clone()]);
                    }
                }
                
                if first.contains(&'ε') {
                    if let Some(follow) = follow_sets.get(&nt) {
                        for &symbol in follow {
                            parse_table.insert((nt, symbol), vec![production.clone()]);
                        }
                    }
                }
            }
        }
        
        parse_table
    }
    
    fn parse(&self, input: &str) -> bool {
        let mut stack = vec![self.grammar.start_symbol];
        let mut input_chars: Vec<char> = input.chars().collect();
        input_chars.push('$');
        let mut input_pos = 0;
        
        while !stack.is_empty() {
            let top = stack.last().unwrap();
            let current_input = input_chars[input_pos];
            
            if self.grammar.terminals.contains(top) {
                if *top == current_input {
                    stack.pop();
                    input_pos += 1;
                } else {
                    return false;
                }
            } else {
                if let Some(productions) = self.parse_table.get(&(*top, current_input)) {
                    stack.pop();
                    if let Some(production) = productions.first() {
                        for c in production.chars().rev() {
                            stack.push(c);
                        }
                    }
                } else {
                    return false;
                }
            }
        }
        
        input_pos == input_chars.len()
    }
}
```

### 6.3 解析器生成

**解析器生成器**:

```rust
// 解析器生成器
struct ParserGenerator;

impl ParserGenerator {
    // 生成LL(1)解析器
    fn generate_ll1_parser(grammar: &ContextFreeGrammar) -> Result<LLParser, String> {
        // 检查LL(1)条件
        if !Self::is_ll1(grammar) {
            return Err("Grammar is not LL(1)".to_string());
        }
        
        Ok(LLParser::new(grammar.clone()))
    }
    
    // 生成LR(1)解析器
    fn generate_lr1_parser(grammar: &ContextFreeGrammar) -> LRParser {
        LRParser::new(grammar.clone())
    }
    
    // 检查LL(1)条件
    fn is_ll1(grammar: &ContextFreeGrammar) -> bool {
        // 实现LL(1)条件检查
        true
    }
}
```

## 7. 应用实例

### 7.1 编程语言语法

**简单表达式文法**:

```rust
// 表达式文法
fn expression_grammar() -> ContextFreeGrammar {
    let mut grammar = ContextFreeGrammar::new('E');
    
    // E -> E + T | T
    grammar.add_production('E', "E+T".to_string());
    grammar.add_production('E', "T".to_string());
    
    // T -> T * F | F
    grammar.add_production('T', "T*F".to_string());
    grammar.add_production('T', "F".to_string());
    
    // F -> (E) | id
    grammar.add_production('F', "(E)".to_string());
    grammar.add_production('F', "i".to_string());
    
    grammar
}

// 表达式解析器
fn expression_parser() -> LLParser {
    let grammar = expression_grammar();
    LLParser::new(grammar)
}
```

### 7.2 自然语言处理

**简单句法文法**:

```rust
// 句法文法
fn syntax_grammar() -> ContextFreeGrammar {
    let mut grammar = ContextFreeGrammar::new('S');
    
    // S -> NP VP
    grammar.add_production('S', "NP VP".to_string());
    
    // NP -> Det N | N
    grammar.add_production('NP', "Det N".to_string());
    grammar.add_production('NP', "N".to_string());
    
    // VP -> V NP | V
    grammar.add_production('VP', "V NP".to_string());
    grammar.add_production('VP', "V".to_string());
    
    // 词类
    grammar.add_production('Det', "the".to_string());
    grammar.add_production('Det', "a".to_string());
    grammar.add_production('N', "cat".to_string());
    grammar.add_production('N', "dog".to_string());
    grammar.add_production('V', "chases".to_string());
    grammar.add_production('V', "runs".to_string());
    
    grammar
}
```

### 7.3 配置文件解析

**JSON子集文法**:

```rust
// JSON子集文法
fn json_grammar() -> ContextFreeGrammar {
    let mut grammar = ContextFreeGrammar::new('J');
    
    // J -> { P } | [ A ] | "string" | number | true | false | null
    grammar.add_production('J', "{P}".to_string());
    grammar.add_production('J', "[A]".to_string());
    grammar.add_production('J', "\"s\"".to_string());
    grammar.add_production('J', "n".to_string());
    grammar.add_production('J', "true".to_string());
    grammar.add_production('J', "false".to_string());
    grammar.add_production('J', "null".to_string());
    
    // P -> K:J | K:J,P
    grammar.add_production('P', "K:J".to_string());
    grammar.add_production('P', "K:J,P".to_string());
    
    // A -> J | J,A
    grammar.add_production('A', "J".to_string());
    grammar.add_production('A', "J,A".to_string());
    
    // K -> "string"
    grammar.add_production('K', "\"s\"".to_string());
    
    grammar
}
```

## 8. 参考文献

1. Hopcroft, J. E., Motwani, R., & Ullman, J. D. (2006). *Introduction to Automata Theory, Languages, and Computation*.
2. Sipser, M. (2012). *Introduction to the Theory of Computation*.
3. Aho, A. V., Lam, M. S., Sethi, R., & Ullman, J. D. (2006). *Compilers: Principles, Techniques, and Tools*.
4. Grune, D., & Jacobs, C. J. (2008). *Parsing Techniques: A Practical Guide*.
5. Knuth, D. E. (1965). On the translation of languages from left to right.

---

**相关链接**:

- [02.1 形式语言理论基础](02.1_Formal_Language_Foundation.md)
- [02.2 正则语言](02.2_Regular_Languages.md)
- [02.4 上下文相关语言](02.4_Context_Sensitive_Languages.md)
- [02.6 自动机理论](02.6_Automata_Theory.md)
- [08.4 解析理论](../08_Programming_Language_Theory/08.4_Parsing_Theory.md)


## 批判性分析

- 本节内容待补充：请从多元理论视角、局限性、争议点、应用前景等方面进行批判性分析。
