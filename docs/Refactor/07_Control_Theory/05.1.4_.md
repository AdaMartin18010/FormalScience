# 05.1.4 系统辨识理论

## 📋 概述

系统辨识理论是控制理论的重要组成部分，研究如何从系统的输入输出数据中识别系统的数学模型。系统辨识为控制系统设计提供了必要的模型基础，是现代控制理论不可或缺的工具。

## 🎯 核心目标

1. **建立系统辨识的数学框架**
2. **研究参数辨识方法**
3. **发展状态估计理论**
4. **提供模型验证和选择方法**

## 📚 目录

1. [基本概念](#1-基本概念)
2. [参数辨识](#2-参数辨识)
3. [状态估计](#3-状态估计)
4. [模型结构辨识](#4-模型结构辨识)
5. [递归辨识](#5-递归辨识)
6. [鲁棒辨识](#6-鲁棒辨识)
7. [非线性系统辨识](#7-非线性系统辨识)
8. [模型验证](#8-模型验证)
9. [代码实现](#9-代码实现)
10. [应用示例](#10-应用示例)
11. [相关理论](#11-相关理论)
12. [参考文献](#12-参考文献)

## 1. 基本概念

### 1.1 系统辨识定义

**定义 1.1.1** (系统辨识)
系统辨识是从系统的输入输出数据中确定系统数学模型的过程，包括模型结构选择和参数估计。

### 1.2 辨识问题分类

**定义 1.1.2** (辨识问题分类)

1. **参数辨识**: 已知模型结构，估计模型参数
2. **结构辨识**: 确定模型的数学结构
3. **状态估计**: 从输出数据估计系统状态
4. **模型验证**: 验证辨识模型的准确性

### 1.3 辨识准则

**定义 1.1.3** (辨识准则)
辨识问题通常基于以下准则：

1. **最小二乘准则**: 最小化预测误差的平方和
2. **最大似然准则**: 最大化观测数据的似然函数
3. **贝叶斯准则**: 基于先验知识的后验概率最大化

## 2. 参数辨识

### 2.1 最小二乘辨识

**定义 2.1.1** (最小二乘辨识)
对于线性系统 $y(t) = \phi^T(t)\theta + e(t)$，最小二乘估计为：
$$\hat{\theta} = \arg\min_{\theta} \sum_{t=1}^N [y(t) - \phi^T(t)\theta]^2$$

**定理 2.1.1** (最小二乘解)
最小二乘估计的解析解为：
$$\hat{\theta} = \left[\sum_{t=1}^N \phi(t)\phi^T(t)\right]^{-1} \sum_{t=1}^N \phi(t)y(t)$$

**证明**：
设 $J(\theta) = \sum_{t=1}^N [y(t) - \phi^T(t)\theta]^2$，则：
$$\frac{\partial J}{\partial \theta} = -2\sum_{t=1}^N \phi[t](y(t) - \phi^T(t)\theta) = 0$$

整理得：
$$\sum_{t=1}^N \phi(t)\phi^T(t)\theta = \sum_{t=1}^N \phi(t)y(t)$$

因此：
$$\hat{\theta} = \left[\sum_{t=1}^N \phi(t)\phi^T(t)\right]^{-1} \sum_{t=1}^N \phi(t)y(t)$$

### 2.2 递推最小二乘

**算法 2.1.1** (递推最小二乘)
递推最小二乘算法为：
$$
\begin{align}
\hat{\theta}(t) &= \hat{\theta}(t-1) + K[t](y(t) - \phi^T(t)\hat{\theta}(t-1)) \\
K(t) &= P(t-1)\phi[t](\lambda + \phi^T(t)P(t-1)\phi(t))^{-1} \\
P(t) &= \frac{1}{\lambda}[I - K(t)\phi^T(t)]P(t-1)
\end{align}
$$

其中 $\lambda$ 是遗忘因子，$0 < \lambda \leq 1$。

### 2.3 最大似然辨识

**定义 2.1.2** (最大似然辨识)
对于系统 $y(t) = G(q^{-1})u(t) + H(q^{-1})e(t)$，最大似然估计为：
$$\hat{\theta} = \arg\max_{\theta} p(y^N|\theta)$$

其中 $p(y^N|\theta)$ 是观测数据的似然函数。

**定理 2.1.2** (高斯噪声下的最大似然)
在高斯噪声假设下，最大似然估计等价于最小化预测误差的方差。

## 3. 状态估计

### 3.1 卡尔曼滤波

**算法 3.1.1** (卡尔曼滤波)
对于系统：
$$\begin{align}
x(t+1) &= Ax(t) + Bu(t) + w(t) \\
y(t) &= Cx(t) + v(t)
\end{align}$$

卡尔曼滤波算法为：
$$\begin{align}
\hat{x}(t|t-1) &= A\hat{x}(t-1|t-1) + Bu(t-1) \\
P(t|t-1) &= AP(t-1|t-1)A^T + Q \\
K(t) &= P(t|t-1)C^T[CP(t|t-1)C^T + R]^{-1} \\
\hat{x}(t|t) &= \hat{x}(t|t-1) + K[t](y(t) - C\hat{x}(t|t-1)) \\
P(t|t) &= [I - K(t)C]P(t|t-1)
\end{align}$$

### 3.2 扩展卡尔曼滤波

**算法 3.1.2** (扩展卡尔曼滤波)
对于非线性系统：
$$\begin{align}
x(t+1) &= f(x(t), u(t)) + w(t) \\
y(t) &= h(x(t)) + v(t)
\end{align}$$

扩展卡尔曼滤波算法为：
$$\begin{align}
\hat{x}(t|t-1) &= f(\hat{x}(t-1|t-1), u(t-1)) \\
P(t|t-1) &= F(t-1)P(t-1|t-1)F^T(t-1) + Q \\
K(t) &= P(t|t-1)H^T[t](H(t)P(t|t-1)H^T(t) + R)^{-1} \\
\hat{x}(t|t) &= \hat{x}(t|t-1) + K[t](y(t) - h(\hat{x}(t|t-1))) \\
P(t|t) &= [I - K(t)H(t)]P(t|t-1)
\end{align}$$

其中 $F(t) = \frac{\partial f}{\partial x}|_{\hat{x}(t|t)}$，$H(t) = \frac{\partial h}{\partial x}|_{\hat{x}(t|t-1)}$。

### 3.3 粒子滤波

**算法 3.1.3** (粒子滤波)
粒子滤波算法为：
1. **初始化**: 生成 $N$ 个粒子 $x_0^{(i)} \sim p(x_0)$
2. **预测**: $x_t^{(i)} \sim p(x_t|x_{t-1}^{(i)}, u_t)$
3. **更新**: 计算权重 $w_t^{(i)} = p(y_t|x_t^{(i)})$
4. **重采样**: 根据权重重新采样粒子
5. **估计**: $\hat{x}_t = \sum_{i=1}^N w_t^{(i)}x_t^{(i)}$

## 4. 模型结构辨识

### 4.1 模型阶数辨识

**定义 4.1.1** (模型阶数辨识)
确定系统模型的最小阶数，使得模型能够充分描述系统动态特性。

**方法 4.1.1** (信息准则)
常用的信息准则包括：
1. **AIC准则**: $AIC = N\ln(\hat{\sigma}^2) + 2n$
2. **BIC准则**: $BIC = N\ln(\hat{\sigma}^2) + n\ln(N)$
3. **MDL准则**: $MDL = N\ln(\hat{\sigma}^2) + n\ln(N)/2$

其中 $N$ 是数据长度，$n$ 是参数个数，$\hat{\sigma}^2$ 是残差方差。

### 4.2 模型结构选择

**算法 4.1.1** (模型结构选择)
1. **候选模型生成**: 生成多个候选模型结构
2. **参数估计**: 对每个候选模型进行参数估计
3. **模型验证**: 验证每个模型的性能
4. **最优选择**: 根据信息准则选择最优模型

## 5. 递归辨识

### 5.1 递归最小二乘

**算法 5.1.1** (递归最小二乘)
递归最小二乘算法的矩阵形式为：
$$\begin{align}
\hat{\theta}(t) &= \hat{\theta}(t-1) + K(t)\varepsilon(t) \\
\varepsilon(t) &= y(t) - \phi^T(t)\hat{\theta}(t-1) \\
K(t) &= P(t-1)\phi[t](\lambda + \phi^T(t)P(t-1)\phi(t))^{-1} \\
P(t) &= \frac{1}{\lambda}[I - K(t)\phi^T(t)]P(t-1)
\end{align}$$

### 5.2 递归最大似然

**算法 5.1.2** (递归最大似然)
递归最大似然算法为：
$$\begin{align}
\hat{\theta}(t) &= \hat{\theta}(t-1) + K(t)\varepsilon(t) \\
\varepsilon(t) &= y(t) - \hat{y}(t|t-1) \\
K(t) &= P(t-1)\psi[t](\lambda + \psi^T(t)P(t-1)\psi(t))^{-1} \\
P(t) &= \frac{1}{\lambda}[I - K(t)\psi^T(t)]P(t-1)
\end{align}$$

其中 $\psi(t) = -\frac{\partial \varepsilon(t)}{\partial \theta}|_{\hat{\theta}(t-1)}$。

## 6. 鲁棒辨识

### 6.1 鲁棒辨识问题

**定义 6.1.1** (鲁棒辨识)
在存在模型不确定性和噪声的情况下，设计对扰动不敏感的辨识算法。

### 6.2 H∞辨识

**算法 6.1.1** (H∞辨识)
H∞辨识算法为：
$$\hat{\theta} = \arg\min_{\theta} \max_{w \in W} \frac{\|y - \hat{y}(\theta)\|_2}{\|w\|_2}$$

其中 $W$ 是扰动集合。

### 6.3 集员辨识

**定义 6.1.2** (集员辨识)
集员辨识的目标是确定参数的不确定性集合，而不是点估计。

**算法 6.1.2** (集员辨识)
集员辨识算法为：
$$\Theta(t) = \{\theta : |y(t) - \phi^T(t)\theta| \leq \gamma(t)\}$$

其中 $\gamma(t)$ 是噪声边界。

## 7. 非线性系统辨识

### 7.1 非线性模型结构

**定义 7.1.1** (非线性模型结构)
常用的非线性模型结构包括：
1. **NARX模型**: $y(t) = f(y(t-1), ..., y(t-n_y), u(t-1), ..., u(t-n_u))$
2. **神经网络模型**: $y(t) = NN(y(t-1), ..., y(t-n_y), u(t-1), ..., u(t-n_u))$
3. **模糊模型**: $y(t) = \sum_{i=1}^M w_i f_i(y(t-1), ..., y(t-n_y), u(t-1), ..., u(t-n_u))$

### 7.2 非线性参数辨识

**算法 7.1.1** (非线性最小二乘)
非线性最小二乘算法为：
$$\hat{\theta} = \arg\min_{\theta} \sum_{t=1}^N [y(t) - f(\phi(t), \theta)]^2$$

**算法 7.1.2** (梯度下降)
梯度下降算法为：
$$\theta(t+1) = \theta(t) - \mu \nabla J(\theta(t))$$

其中 $\mu$ 是学习率，$\nabla J(\theta)$ 是目标函数的梯度。

## 8. 模型验证

### 8.1 残差分析

**定义 8.1.1** (残差分析)
残差定义为：
$$\varepsilon(t) = y(t) - \hat{y}(t)$$

**方法 8.1.1** (残差检验)
1. **白噪声检验**: 检验残差是否为白噪声
2. **独立性检验**: 检验残差与输入是否独立
3. **正态性检验**: 检验残差是否服从正态分布

### 8.2 交叉验证

**方法 8.2.1** (交叉验证)
1. **数据分割**: 将数据分为训练集和验证集
2. **模型训练**: 在训练集上训练模型
3. **模型验证**: 在验证集上验证模型性能
4. **性能评估**: 计算验证误差

### 8.3 模型比较

**方法 8.3.1** (模型比较)
1. **拟合优度**: 比较模型对数据的拟合程度
2. **预测能力**: 比较模型的预测性能
3. **复杂度**: 比较模型的复杂度
4. **鲁棒性**: 比较模型的鲁棒性

## 9. 代码实现

### 9.1 系统辨识器

```rust
use nalgebra::{DMatrix, DVector, Matrix, Vector};
use rand::Rng;
use std::f64::consts::PI;

/// 系统辨识器
pub struct SystemIdentifier {
    pub theta: DVector<f64>,
    pub p: DMatrix<f64>,
    pub lambda: f64,
}

impl SystemIdentifier {
    /// 创建新的系统辨识器
    pub fn new(n_params: usize, lambda: f64) -> Self {
        Self {
            theta: DVector::zeros(n_params),
            p: DMatrix::identity(n_params, n_params) * 1000.0,
            lambda,
        }
    }

    /// 最小二乘辨识
    pub fn least_squares_identification(&mut self, phi: &DVector<f64>, y: f64) {
        let prediction = phi.dot(&self.theta);
        let error = y - prediction;

        let k = &self.p * phi / (self.lambda + phi.dot(&(&self.p * phi)));
        self.theta += &k * error;

        let i_k_phi = DMatrix::identity(self.theta.len(), self.theta.len()) - &k * phi.transpose();
        self.p = &i_k_phi * &self.p / self.lambda;
    }

    /// 批量最小二乘辨识
    pub fn batch_least_squares(&self, phi_matrix: &DMatrix<f64>, y_vector: &DVector<f64>) -> DVector<f64> {
        let phi_t_phi = phi_matrix.transpose() * phi_matrix;
        let phi_t_y = phi_matrix.transpose() * y_vector;

        phi_t_phi.try_inverse().unwrap() * phi_t_y
    }

    /// 获取参数估计
    pub fn get_parameters(&self) -> &DVector<f64> {
        &self.theta
    }

    /// 获取参数协方差矩阵
    pub fn get_covariance(&self) -> &DMatrix<f64> {
        &self.p
    }
}

/// 卡尔曼滤波器
pub struct KalmanFilter {
    pub x: DVector<f64>,
    pub p: DMatrix<f64>,
    pub a: DMatrix<f64>,
    pub b: DMatrix<f64>,
    pub c: DMatrix<f64>,
    pub q: DMatrix<f64>,
    pub r: DMatrix<f64>,
}

impl KalmanFilter {
    /// 创建新的卡尔曼滤波器
    pub fn new(a: DMatrix<f64>, b: DMatrix<f64>, c: DMatrix<f64>,
               q: DMatrix<f64>, r: DMatrix<f64>) -> Self {
        let n = a.nrows();
        Self {
            x: DVector::zeros(n),
            p: DMatrix::identity(n, n),
            a, b, c, q, r,
        }
    }

    /// 预测步骤
    pub fn predict(&mut self, u: &DVector<f64>) {
        // 状态预测
        self.x = &self.a * &self.x + &self.b * u;

        // 协方差预测
        self.p = &self.a * &self.p * &self.a.transpose() + &self.q;
    }

    /// 更新步骤
    pub fn update(&mut self, y: f64) {
        let y_pred = self.c.dot(&self.x);
        let error = y - y_pred;

        // 卡尔曼增益
        let s = &self.c * &self.p * &self.c.transpose() + self.r[(0, 0)];
        let k = &self.p * &self.c.transpose() / s;

        // 状态更新
        self.x += &k * error;

        // 协方差更新
        let i_kc = DMatrix::identity(self.x.len(), self.x.len()) - &k * &self.c;
        self.p = &i_kc * &self.p;
    }

    /// 获取状态估计
    pub fn get_state(&self) -> &DVector<f64> {
        &self.x
    }

    /// 获取状态协方差
    pub fn get_state_covariance(&self) -> &DMatrix<f64> {
        &self.p
    }
}

/// 扩展卡尔曼滤波器
pub struct ExtendedKalmanFilter {
    pub x: DVector<f64>,
    pub p: DMatrix<f64>,
    pub q: DMatrix<f64>,
    pub r: f64,
}

impl ExtendedKalmanFilter {
    /// 创建新的扩展卡尔曼滤波器
    pub fn new(n_states: usize, q: DMatrix<f64>, r: f64) -> Self {
        Self {
            x: DVector::zeros(n_states),
            p: DMatrix::identity(n_states, n_states),
            q, r,
        }
    }

    /// 预测步骤
    pub fn predict<F>(&mut self, u: &DVector<f64>, f: F)
    where F: Fn(&DVector<f64>, &DVector<f64>) -> DVector<f64> {
        // 状态预测
        self.x = f(&self.x, u);

        // 计算雅可比矩阵
        let f_jacobian = self.compute_jacobian_f(&self.x, u, &f);

        // 协方差预测
        self.p = &f_jacobian * &self.p * &f_jacobian.transpose() + &self.q;
    }

    /// 更新步骤
    pub fn update<H>(&mut self, y: f64, h: H)
    where H: Fn(&DVector<f64>) -> f64 {
        let y_pred = h(&self.x);
        let error = y - y_pred;

        // 计算观测雅可比矩阵
        let h_jacobian = self.compute_jacobian_h(&self.x, &h);

        // 卡尔曼增益
        let s = &h_jacobian * &self.p * &h_jacobian.transpose() + self.r;
        let k = &self.p * &h_jacobian.transpose() / s;

        // 状态更新
        self.x += &k * error;

        // 协方差更新
        let i_kh = DMatrix::identity(self.x.len(), self.x.len()) - &k * &h_jacobian;
        self.p = &i_kh * &self.p;
    }

    /// 计算状态转移雅可比矩阵
    fn compute_jacobian_f<F>(&self, x: &DVector<f64>, u: &DVector<f64>, f: &F) -> DMatrix<f64>
    where F: Fn(&DVector<f64>, &DVector<f64>) -> DVector<f64> {
        let n = x.len();
        let mut jacobian = DMatrix::zeros(n, n);
        let h = 1e-6;

        for i in 0..n {
            let mut x_plus = x.clone();
            x_plus[i] += h;
            let f_plus = f(&x_plus, u);
            let f_orig = f(x, u);

            for j in 0..n {
                jacobian[(j, i)] = (f_plus[j] - f_orig[j]) / h;
            }
        }

        jacobian
    }

    /// 计算观测雅可比矩阵
    fn compute_jacobian_h<H>(&self, x: &DVector<f64>, h: &H) -> DMatrix<f64>
    where H: Fn(&DVector<f64>) -> f64 {
        let n = x.len();
        let mut jacobian = DMatrix::zeros(1, n);
        let h_val = 1e-6;

        for i in 0..n {
            let mut x_plus = x.clone();
            x_plus[i] += h_val;
            let h_plus = h(&x_plus);
            let h_orig = h(x);

            jacobian[(0, i)] = (h_plus - h_orig) / h_val;
        }

        jacobian
    }
}

/// 模型验证器
pub struct ModelValidator;

impl ModelValidator {
    /// 计算残差
    pub fn compute_residuals(y_true: &[f64], y_pred: &[f64]) -> Vec<f64> {
        y_true.iter().zip(y_pred.iter())
            .map(|(&y_t, &y_p)| y_t - y_p)
            .collect()
    }

    /// 计算均方误差
    pub fn mean_squared_error(y_true: &[f64], y_pred: &[f64]) -> f64 {
        let n = y_true.len() as f64;
        y_true.iter().zip(y_pred.iter())
            .map(|(&y_t, &y_p)| (y_t - y_p).powi(2))
            .sum::<f64>() / n
    }

    /// 计算决定系数
    pub fn coefficient_of_determination(y_true: &[f64], y_pred: &[f64]) -> f64 {
        let y_mean = y_true.iter().sum::<f64>() / y_true.len() as f64;
        let ss_tot = y_true.iter().map(|&y| (y - y_mean).powi(2)).sum::<f64>();
        let ss_res = y_true.iter().zip(y_pred.iter())
            .map(|(&y_t, &y_p)| (y_t - y_p).powi(2))
            .sum::<f64>();

        1.0 - ss_res / ss_tot
    }

    /// 白噪声检验
    pub fn whiteness_test(residuals: &[f64], max_lag: usize) -> f64 {
        let n = residuals.len();
        let mut autocorr = vec![0.0; max_lag + 1];

        // 计算自相关函数
        for lag in 0..=max_lag {
            let mut sum = 0.0;
            for i in 0..n-lag {
                sum += residuals[i] * residuals[i + lag];
            }
            autocorr[lag] = sum / (n - lag) as f64;
        }

        // 计算Q统计量
        let q_stat = n as f64 * (n + 2.0) * autocorr.iter().skip(1)
            .map(|&r| r.powi(2) / (n - 1) as f64)
            .sum::<f64>();

        q_stat
    }

    /// AIC准则
    pub fn aic_criterion(residuals: &[f64], n_params: usize) -> f64 {
        let n = residuals.len() as f64;
        let mse = Self::mean_squared_error(residuals, &vec![0.0; residuals.len()]);
        n * mse.ln() + 2.0 * n_params as f64
    }

    /// BIC准则
    pub fn bic_criterion(residuals: &[f64], n_params: usize) -> f64 {
        let n = residuals.len() as f64;
        let mse = Self::mean_squared_error(residuals, &vec![0.0; residuals.len()]);
        n * mse.ln() + n_params as f64 * n.ln()
    }
}

/// 非线性系统辨识器
pub struct NonlinearSystemIdentifier {
    pub weights: Vec<f64>,
    pub learning_rate: f64,
}

impl NonlinearSystemIdentifier {
    /// 创建新的非线性系统辨识器
    pub fn new(n_weights: usize, learning_rate: f64) -> Self {
        let mut rng = rand::thread_rng();
        let weights = (0..n_weights).map(|_| rng.gen_range(-0.1..0.1)).collect();

        Self {
            weights,
            learning_rate,
        }
    }

    /// 神经网络前向传播
    pub fn forward(&self, inputs: &[f64]) -> f64 {
        let mut output = 0.0;
        for (i, &input) in inputs.iter().enumerate() {
            output += self.weights[i] * input;
        }
        output += self.weights[inputs.len()]; // 偏置项
        output.tanh() // 激活函数
    }

    /// 梯度下降训练
    pub fn train(&mut self, inputs: &[f64], target: f64) {
        let prediction = self.forward(inputs);
        let error = target - prediction;

        // 计算梯度
        let mut gradients = vec![0.0; self.weights.len()];
        let activation_derivative = 1.0 - prediction.powi(2); // tanh的导数

        for (i, &input) in inputs.iter().enumerate() {
            gradients[i] = -error * activation_derivative * input;
        }
        gradients[inputs.len()] = -error * activation_derivative; // 偏置梯度

        // 更新权重
        for (i, gradient) in gradients.iter().enumerate() {
            self.weights[i] -= self.learning_rate * gradient;
        }
    }

    /// 批量训练
    pub fn batch_train(&mut self, training_data: &[(&[f64], f64)], epochs: usize) {
        for _ in 0..epochs {
            for &(inputs, target) in training_data {
                self.train(inputs, target);
            }
        }
    }
}
```

### 9.2 测试代码

```rust
# [cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_least_squares_identification() {
        let mut identifier = SystemIdentifier::new(2, 0.95);

        // 模拟数据: y = 2*x1 + 3*x2 + noise
        let phi1 = DVector::from_vec(vec![1.0, 2.0]);
        let phi2 = DVector::from_vec(vec![2.0, 1.0]);
        let phi3 = DVector::from_vec(vec![3.0, 0.5]);

        identifier.least_squares_identification(&phi1, 8.0);
        identifier.least_squares_identification(&phi2, 7.0);
        identifier.least_squares_identification(&phi3, 7.5);

        let params = identifier.get_parameters();
        println!("估计参数: {:?}", params);

        // 验证参数估计的合理性
        assert!((params[0] - 2.0).abs() < 1.0);
        assert!((params[1] - 3.0).abs() < 1.0);
    }

    #[test]
    fn test_kalman_filter() {
        // 简单的一维系统
        let a = DMatrix::from_row_slice(1, 1, &[1.0]);
        let b = DMatrix::from_row_slice(1, 1, &[1.0]);
        let c = DMatrix::from_row_slice(1, 1, &[1.0]);
        let q = DMatrix::from_row_slice(1, 1, &[0.1]);
        let r = DMatrix::from_row_slice(1, 1, &[1.0]);

        let mut kf = KalmanFilter::new(a, b, c, q, r);

        let u = DVector::from_vec(vec![1.0]);
        let y = 5.0;

        kf.predict(&u);
        kf.update(y);

        let state = kf.get_state();
        println!("状态估计: {:?}", state);

        assert!(state[0].abs() > 0.0);
    }

    #[test]
    fn test_model_validation() {
        let y_true = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let y_pred = vec![1.1, 1.9, 3.1, 3.9, 5.1];

        let mse = ModelValidator::mean_squared_error(&y_true, &y_pred);
        let r2 = ModelValidator::coefficient_of_determination(&y_true, &y_pred);

        println!("MSE: {:.4}", mse);
        println!("R²: {:.4}", r2);

        assert!(mse > 0.0);
        assert!(r2 > 0.9);
    }

    #[test]
    fn test_nonlinear_identification() {
        let mut identifier = NonlinearSystemIdentifier::new(3, 0.01);

        // 训练数据: y = tanh(2*x1 + 3*x2 + 1)
        let training_data = vec![
            (&[1.0, 2.0], (2.0*1.0 + 3.0*2.0 + 1.0).tanh()),
            (&[2.0, 1.0], (2.0*2.0 + 3.0*1.0 + 1.0).tanh()),
            (&[0.5, 1.5], (2.0*0.5 + 3.0*1.5 + 1.0).tanh()),
        ];

        identifier.batch_train(&training_data, 1000);

        // 测试预测
        let test_input = [1.0, 1.0];
        let prediction = identifier.forward(&test_input);
        let expected = (2.0*1.0 + 3.0*1.0 + 1.0).tanh();

        println!("预测值: {:.4}, 期望值: {:.4}", prediction, expected);
        assert!((prediction - expected).abs() < 0.1);
    }
}
```

## 10. 应用示例

### 10.1 线性系统辨识

```rust
/// 线性系统辨识示例
pub fn linear_system_identification_example() {
    println!("线性系统辨识示例:");

    // 真实系统: y(t) = 2*u(t-1) + 3*u(t-2) + 0.5*y(t-1) + noise
    let mut identifier = SystemIdentifier::new(4, 0.95);

    let mut u_history = vec![0.0; 3];
    let mut y_history = vec![0.0; 3];

    // 生成训练数据
    let mut rng = rand::thread_rng();
    for t in 0..100 {
        let u = rng.gen_range(-1.0..1.0);
        let noise = rng.gen_range(-0.1..0.1);

        // 真实输出
        let y_true = 2.0 * u_history[1] + 3.0 * u_history[2] + 0.5 * y_history[1] + noise;

        // 回归向量
        let phi = DVector::from_vec(vec![
            u_history[1], u_history[2], y_history[1], 1.0
        ]);

        // 在线辨识
        identifier.least_squares_identification(&phi, y_true);

        // 更新历史
        u_history.rotate_right(1);
        u_history[0] = u;
        y_history.rotate_right(1);
        y_history[0] = y_true;

        if t % 20 == 0 {
            let params = identifier.get_parameters();
            println!("t={}: 参数估计: [{:.3}, {:.3}, {:.3}, {:.3}]",
                     t, params[0], params[1], params[2], params[3]);
        }
    }

    let final_params = identifier.get_parameters();
    println!("\n最终参数估计:");
    println!("a1 (真实值: 2.0): {:.3}", final_params[0]);
    println!("a2 (真实值: 3.0): {:.3}", final_params[1]);
    println!("b1 (真实值: 0.5): {:.3}", final_params[2]);
    println!("bias: {:.3}", final_params[3]);
}
```

### 10.2 非线性系统辨识

```rust
/// 非线性系统辨识示例
pub fn nonlinear_system_identification_example() {
    println!("非线性系统辨识示例:");

    // 真实系统: y(t) = tanh(2*u(t-1) + 3*u(t-2)) + noise
    let mut identifier = NonlinearSystemIdentifier::new(4, 0.01);

    let mut u_history = vec![0.0; 3];
    let mut y_history = vec![0.0; 3];

    // 生成训练数据
    let mut rng = rand::thread_rng();
    let mut training_data = Vec::new();

    for _ in 0..1000 {
        let u = rng.gen_range(-1.0..1.0);
        let noise = rng.gen_range(-0.05..0.05);

        // 真实输出
        let y_true = (2.0 * u_history[1] + 3.0 * u_history[2]).tanh() + noise;

        // 输入特征
        let inputs = vec![u_history[1], u_history[2], y_history[1], 1.0];
        training_data.push((inputs, y_true));

        // 更新历史
        u_history.rotate_right(1);
        u_history[0] = u;
        y_history.rotate_right(1);
        y_history[0] = y_true;
    }

    // 训练神经网络
    identifier.batch_train(&training_data.iter().map(|(i, t)| (i.as_slice(), *t)).collect::<Vec<_>>(), 100);

    println!("训练完成");
    println!("权重: {:?}", identifier.weights);

    // 测试预测
    let test_inputs = vec![
        vec![1.0, 0.5, 0.0, 1.0],
        vec![0.5, 1.0, 0.0, 1.0],
        vec![0.0, 0.0, 0.0, 1.0],
    ];

    println!("\n预测测试:");
    for (i, inputs) in test_inputs.iter().enumerate() {
        let prediction = identifier.forward(inputs);
        let expected = (2.0 * inputs[0] + 3.0 * inputs[1]).tanh();
        println!("测试{}: 预测={:.4}, 期望={:.4}, 误差={:.4}",
                 i+1, prediction, expected, (prediction - expected).abs());
    }
}
```

### 10.3 状态估计示例

```rust
/// 状态估计示例
pub fn state_estimation_example() {
    println!("状态估计示例:");

    // 简单的一维运动系统
    let a = DMatrix::from_row_slice(2, 2, &[1.0, 1.0, 0.0, 1.0]); // 位置和速度
    let b = DMatrix::from_row_slice(2, 1, &[0.5, 1.0]); // 加速度输入
    let c = DMatrix::from_row_slice(1, 2, &[1.0, 0.0]); // 只观测位置
    let q = DMatrix::from_row_slice(2, 2, &[0.1, 0.0, 0.0, 0.1]); // 过程噪声
    let r = DMatrix::from_row_slice(1, 1, &[1.0]); // 观测噪声

    let mut kf = KalmanFilter::new(a, b, c, q, r);

    // 真实状态
    let mut true_position = 0.0;
    let mut true_velocity = 0.0;

    let mut rng = rand::thread_rng();

    println!("时间 | 真实位置 | 观测位置 | 估计位置 | 估计速度");

    for t in 0..20 {
        // 真实系统
        let acceleration = 1.0; // 恒定加速度
        true_velocity += acceleration;
        true_position += true_velocity;

        // 添加过程噪声
        true_position += rng.gen_range(-0.1..0.1);
        true_velocity += rng.gen_range(-0.1..0.1);

        // 观测（带噪声）
        let observed_position = true_position + rng.gen_range(-0.5..0.5);

        // 卡尔曼滤波
        let u = DVector::from_vec(vec![acceleration]);
        kf.predict(&u);
        kf.update(observed_position);

        let state = kf.get_state();

        if t % 5 == 0 {
            println!("{:2} | {:8.3} | {:8.3} | {:8.3} | {:8.3}",
                     t, true_position, observed_position, state[0], state[1]);
        }
    }

    println!("\n最终状态估计:");
    let final_state = kf.get_state();
    println!("位置: {:.3} (真实: {:.3})", final_state[0], true_position);
    println!("速度: {:.3} (真实: {:.3})", final_state[1], true_velocity);
}
```

## 11. 相关理论

### 11.1 控制理论
- [05.1.1 基础控制理论](05.1.1_基础控制理论.md)
- [05.1.2 线性系统理论](05.1.2_线性系统理论.md)
- [05.1.3 系统稳定性理论](05.1.3_系统稳定性理论.md)

### 11.2 状态估计理论
- [05.2.4 状态估计理论](05.2.4_状态估计理论.md)
- [05.2.3 最优控制理论](05.2.3_最优控制理论.md)

### 11.3 自适应控制理论
- [05.5.1 自适应控制基础](05.5.1_自适应控制基础.md)
- [05.5.2 自校正控制](05.5.2_自校正控制.md)

### 11.4 鲁棒控制理论
- [05.4.1 鲁棒控制基础](05.4.1_鲁棒控制基础.md)
- [05.4.2 μ综合方法](05.4.2_μ综合方法.md)

## 12. 参考文献

1. **Ljung, L.** (1999). *System Identification: Theory for the User*. Prentice Hall.
2. **Söderström, T., & Stoica, P.** (1989). *System Identification*. Prentice Hall.
3. **Nelles, O.** (2001). *Nonlinear System Identification: From Classical Approaches to Neural Networks and Fuzzy Models*. Springer.
4. **Anderson, B. D. O., & Moore, J. B.** (1979). *Optimal Filtering*. Prentice Hall.
5. **Haykin, S.** (2001). *Kalman Filtering and Neural Networks*. Wiley.
6. **Doucet, A., de Freitas, N., & Gordon, N.** (2001). *Sequential Monte Carlo Methods in Practice*. Springer.
7. **Akaike, H.** (1974). *A New Look at the Statistical Model Identification*. IEEE Transactions on Automatic Control.
8. **Schwarz, G.** (1978). *Estimating the Dimension of a Model*. The Annals of Statistics.
9. **Rissanen, J.** (1978). *Modeling by Shortest Data Description*. Automatica.
10. **Zhou, K., & Doyle, J. C.** (1998). *Essentials of Robust Control*. Prentice Hall.

---

**关键词**: 系统辨识、参数辨识、状态估计、卡尔曼滤波、模型验证、最小二乘、最大似然

**相关文档**:
- [05.1.1 基础控制理论](05.1.1_基础控制理论.md)
- [05.1.2 线性系统理论](05.1.2_线性系统理论.md)
- [05.1.3 系统稳定性理论](05.1.3_系统稳定性理论.md)


## 批判性分析

- 本节内容待补充：请从多元理论视角、局限性、争议点、应用前景等方面进行批判性分析。
