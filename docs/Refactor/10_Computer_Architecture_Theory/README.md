# 10. è®¡ç®—æœºæ¶æ„ç†è®º (Computer Architecture Theory)

## ğŸ“‹ æ¨¡å—æ¦‚è¿°

è®¡ç®—æœºæ¶æ„ç†è®ºæ˜¯ç ”ç©¶è®¡ç®—æœºç³»ç»Ÿæ¦‚å¿µç»“æ„å’Œæ“ä½œè¡Œä¸ºçš„æ ¸å¿ƒç†è®ºæ¨¡å—ï¼Œä¸ºè½¯ä»¶å·¥ç¨‹ã€æ“ä½œç³»ç»Ÿã€å¹¶å‘ç†è®ºç­‰æä¾›ç¡¬ä»¶åŸºç¡€æ”¯æ’‘ã€‚æœ¬æ¨¡å—æ¶µç›–å¤„ç†å™¨æ¶æ„ã€å­˜å‚¨ç³»ç»Ÿã€å¹¶è¡Œè®¡ç®—ã€æ€§èƒ½ä¼˜åŒ–ç­‰å…³é”®é¢†åŸŸï¼Œç ”ç©¶æŒ‡ä»¤é›†æ¶æ„(ISA)çš„è®¾è®¡åŸç†å’Œå¾®æ¶æ„çš„é«˜æ•ˆå®ç°æ–¹æ³•ï¼Œä¸ºä¸Šå±‚è½¯ä»¶ç³»ç»Ÿæä¾›æ€§èƒ½ä¿éšœå’Œä¼˜åŒ–æŒ‡å¯¼ã€‚

## ğŸ—ï¸ ç›®å½•ç»“æ„

```text
10_Computer_Architecture_Theory/
â”œâ”€â”€ README.md                                    # æ¨¡å—æ€»è§ˆ
â”œâ”€â”€ 01_Processor_Architecture/                   # å¤„ç†å™¨æ¶æ„
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_Instruction_Set_Architecture.md      # æŒ‡ä»¤é›†æ¶æ„
â”‚   â”œâ”€â”€ 02_Microarchitecture_Design.md          # å¾®æ¶æ„è®¾è®¡
â”‚   â”œâ”€â”€ 03_Pipeline_Technology.md               # æµæ°´çº¿æŠ€æœ¯
â”‚   â””â”€â”€ 04_Branch_Prediction.md                 # åˆ†æ”¯é¢„æµ‹
â”œâ”€â”€ 02_Memory_Systems/                           # å­˜å‚¨ç³»ç»Ÿ
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_Memory_Hierarchy.md                  # å†…å­˜å±‚æ¬¡
â”‚   â”œâ”€â”€ 02_Virtual_Memory.md                    # è™šæ‹Ÿå†…å­˜
â”‚   â”œâ”€â”€ 03_Memory_Consistency.md                # å­˜å‚¨ä¸€è‡´æ€§
â”‚   â””â”€â”€ 04_Cache_Optimization.md                # ç¼“å­˜ä¼˜åŒ–
â”œâ”€â”€ 03_Parallel_Computing/                       # å¹¶è¡Œè®¡ç®—
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_Multicore_Architecture.md            # å¤šæ ¸æ¶æ„
â”‚   â”œâ”€â”€ 02_Vector_Processing.md                 # å‘é‡å¤„ç†
â”‚   â”œâ”€â”€ 03_GPU_Computing.md                     # GPUè®¡ç®—
â”‚   â””â”€â”€ 04_Distributed_Computing.md             # åˆ†å¸ƒå¼è®¡ç®—
â”œâ”€â”€ 04_Performance_Optimization/                 # æ€§èƒ½ä¼˜åŒ–
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01_Performance_Analysis.md              # æ€§èƒ½åˆ†æ
â”‚   â”œâ”€â”€ 02_Power_Management.md                  # åŠŸè€—ç®¡ç†
â”‚   â”œâ”€â”€ 03_Reliability_Design.md                # å¯é æ€§è®¾è®¡
â”‚   â””â”€â”€ 04_Scalability_Design.md                # å¯æ‰©å±•æ€§è®¾è®¡
â””â”€â”€ 09.1_Fundamentals/                          # åŸºç¡€ç†è®º
    â”œâ”€â”€ README.md
    â”œâ”€â”€ 01_Computer_Architecture_Foundations.md # è®¡ç®—æœºä½“ç³»ç»“æ„åŸºç¡€
    â”œâ”€â”€ 02_ISA_Theory.md                        # æŒ‡ä»¤é›†æ¶æ„ç†è®º
    â”œâ”€â”€ 03_Microarchitecture_Theory.md          # å¾®æ¶æ„è®¾è®¡ç†è®º
    â””â”€â”€ 04_Memory_Hierarchy_Theory.md           # å†…å­˜å±‚æ¬¡ç†è®º
```

## ğŸ”¬ æ ¸å¿ƒç†è®º

### 1. è®¡ç®—æœºæ¶æ„åŸºç¡€ç†è®º

**å®šä¹‰ 1.1** (è®¡ç®—æœºä½“ç³»ç»“æ„)
è®¡ç®—æœºä½“ç³»ç»“æ„æ˜¯è®¡ç®—æœºç³»ç»Ÿçš„æ¦‚å¿µç»“æ„å’ŒåŠŸèƒ½ç‰¹æ€§ï¼ŒåŒ…æ‹¬æŒ‡ä»¤é›†ã€æ•°æ®ç±»å‹ã€å¯»å€æ¨¡å¼ã€å¯„å­˜å™¨ç»„ç»‡ã€ä¸­æ–­æœºåˆ¶ç­‰ç¨‹åºå‘˜å¯è§çš„ç³»ç»Ÿå±æ€§ã€‚

**å®šä¹‰ 1.2** (æŒ‡ä»¤é›†æ¶æ„)
æŒ‡ä»¤é›†æ¶æ„(ISA)æ˜¯è®¡ç®—æœºç¡¬ä»¶ä¸è½¯ä»¶ä¹‹é—´çš„æ¥å£è§„èŒƒï¼Œå®šä¹‰ä¸º $ISA = (I, R, M, A)$ï¼Œå…¶ä¸­ï¼š

- $I$ æ˜¯æŒ‡ä»¤é›†
- $R$ æ˜¯å¯„å­˜å™¨é›†
- $M$ æ˜¯å†…å­˜æ¨¡å‹
- $A$ æ˜¯å¯»å€æ¨¡å¼

**å®šç† 1.1** (ISAå®Œå¤‡æ€§)
å¯¹äºä»»æ„è®¡ç®—é—®é¢˜ï¼Œå­˜åœ¨ä¸€ä¸ªå®Œå¤‡çš„ISAèƒ½å¤Ÿè§£å†³è¯¥é—®é¢˜ã€‚

### 2. å¤„ç†å™¨æ¶æ„ç†è®º

**å®šä¹‰ 2.1** (æµæ°´çº¿)
æµæ°´çº¿æ˜¯å°†æŒ‡ä»¤æ‰§è¡Œåˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µçš„æŠ€æœ¯ï¼š$T_{pipeline} = \max\{T_i\} + T_{overhead}$

**å®šä¹‰ 2.2** (CPI)
æ¯æ¡æŒ‡ä»¤çš„å¹³å‡æ—¶é’Ÿå‘¨æœŸæ•°ï¼š$CPI = \frac{T_{total}}{N_{instructions}}$

**å®šç† 2.1** (Amdahlå®šå¾‹)
å¹¶è¡ŒåŒ–åŠ é€Ÿæ¯”ï¼š$S = \frac{1}{(1-p) + \frac{p}{n}}$ï¼Œå…¶ä¸­ $p$ æ˜¯å¯å¹¶è¡ŒåŒ–éƒ¨åˆ†ï¼Œ$n$ æ˜¯å¤„ç†å™¨æ•°é‡ã€‚

### 3. å­˜å‚¨ç³»ç»Ÿç†è®º

**å®šä¹‰ 3.1** (å†…å­˜å±‚æ¬¡)
å†…å­˜å±‚æ¬¡æ˜¯ç”±ä¸åŒé€Ÿåº¦ã€å®¹é‡ã€æˆæœ¬çš„å­˜å‚¨è®¾å¤‡ç»„æˆçš„å±‚æ¬¡ç»“æ„ï¼š$H = \{L_1, L_2, \ldots, L_n\}$

**å®šä¹‰ 3.2** (ç¼“å­˜å‘½ä¸­ç‡)
ç¼“å­˜å‘½ä¸­ç‡ï¼š$H = \frac{N_{hit}}{N_{total}}$

**å®šç† 3.1** (å±€éƒ¨æ€§åŸç†)
ç¨‹åºè®¿é—®å…·æœ‰æ—¶é—´å±€éƒ¨æ€§å’Œç©ºé—´å±€éƒ¨æ€§ã€‚

## ğŸ’» Rustå®ç°

### å¤„ç†å™¨æ¶æ„æ¨¡æ‹Ÿ

```rust
use std::collections::HashMap;
use std::fmt;

/// æŒ‡ä»¤ç±»å‹
#[derive(Debug, Clone)]
pub enum Instruction {
    Add { rd: usize, rs1: usize, rs2: usize },
    Sub { rd: usize, rs1: usize, rs2: usize },
    Load { rd: usize, rs1: usize, offset: i32 },
    Store { rs1: usize, rs2: usize, offset: i32 },
    Branch { rs1: usize, rs2: usize, offset: i32 },
    Jump { target: u32 },
}

/// å¤„ç†å™¨çŠ¶æ€
#[derive(Debug)]
pub struct ProcessorState {
    pub registers: Vec<i32>,
    pub memory: HashMap<u32, u8>,
    pub pc: u32,
    pub cycle_count: u64,
}

impl ProcessorState {
    pub fn new() -> Self {
        ProcessorState {
            registers: vec![0; 32],
            memory: HashMap::new(),
            pc: 0,
            cycle_count: 0,
        }
    }
    
    /// è¯»å–å†…å­˜
    pub fn read_memory(&self, address: u32) -> u32 {
        let mut value = 0u32;
        for i in 0..4 {
            let byte = self.memory.get(&(address + i)).unwrap_or(&0);
            value |= (*byte as u32) << (i * 8);
        }
        value
    }
    
    /// å†™å…¥å†…å­˜
    pub fn write_memory(&mut self, address: u32, value: u32) {
        for i in 0..4 {
            let byte = ((value >> (i * 8)) & 0xFF) as u8;
            self.memory.insert(address + i, byte);
        }
    }
}

/// æµæ°´çº¿å¤„ç†å™¨
#[derive(Debug)]
pub struct PipelineProcessor {
    pub state: ProcessorState,
    pub pipeline_stages: Vec<Option<PipelineStage>>,
    pub branch_predictor: BranchPredictor,
}

#[derive(Debug, Clone)]
pub struct PipelineStage {
    pub instruction: Instruction,
    pub pc: u32,
    pub stage: PipelineStageType,
}

#[derive(Debug, Clone)]
pub enum PipelineStageType {
    Fetch,
    Decode,
    Execute,
    Memory,
    WriteBack,
}

impl PipelineProcessor {
    pub fn new() -> Self {
        PipelineProcessor {
            state: ProcessorState::new(),
            pipeline_stages: vec![None; 5],
            branch_predictor: BranchPredictor::new(),
        }
    }
    
    /// æ‰§è¡Œä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸ
    pub fn cycle(&mut self) {
        // ä»åå¾€å‰æ‰§è¡Œï¼Œé¿å…æ•°æ®å†’é™©
        self.write_back();
        self.memory();
        self.execute();
        self.decode();
        self.fetch();
        
        self.state.cycle_count += 1;
    }
    
    /// å–æŒ‡é˜¶æ®µ
    fn fetch(&mut self) {
        if self.pipeline_stages[0].is_none() {
            // æ¨¡æ‹Ÿä»å†…å­˜å–æŒ‡ä»¤
            let instruction = self.fetch_instruction(self.state.pc);
            if let Some(inst) = instruction {
                self.pipeline_stages[0] = Some(PipelineStage {
                    instruction: inst,
                    pc: self.state.pc,
                    stage: PipelineStageType::Fetch,
                });
                self.state.pc += 4;
            }
        }
    }
    
    /// è¯‘ç é˜¶æ®µ
    fn decode(&mut self) {
        if let Some(stage) = self.pipeline_stages[0].take() {
            self.pipeline_stages[1] = Some(PipelineStage {
                instruction: stage.instruction,
                pc: stage.pc,
                stage: PipelineStageType::Decode,
            });
        }
    }
    
    /// æ‰§è¡Œé˜¶æ®µ
    fn execute(&mut self) {
        if let Some(stage) = self.pipeline_stages[1].take() {
            match &stage.instruction {
                Instruction::Add { rd, rs1, rs2 } => {
                    let result = self.state.registers[*rs1] + self.state.registers[*rs2];
                    self.state.registers[*rd] = result;
                }
                Instruction::Sub { rd, rs1, rs2 } => {
                    let result = self.state.registers[*rs1] - self.state.registers[*rs2];
                    self.state.registers[*rd] = result;
                }
                Instruction::Branch { rs1, rs2, offset } => {
                    let rs1_val = self.state.registers[*rs1];
                    let rs2_val = self.state.registers[*rs2];
                    
                    // åˆ†æ”¯é¢„æµ‹
                    let predicted_taken = self.branch_predictor.predict(stage.pc);
                    if predicted_taken {
                        self.state.pc = (stage.pc as i32 + offset) as u32;
                    }
                    
                    // å®é™…åˆ†æ”¯ç»“æœ
                    let actual_taken = rs1_val == rs2_val;
                    self.branch_predictor.update(stage.pc, actual_taken);
                    
                    if actual_taken != predicted_taken {
                        // åˆ†æ”¯é¢„æµ‹é”™è¯¯ï¼Œæ¸…ç©ºæµæ°´çº¿
                        self.flush_pipeline();
                    }
                }
                _ => {}
            }
            
            self.pipeline_stages[2] = Some(PipelineStage {
                instruction: stage.instruction,
                pc: stage.pc,
                stage: PipelineStageType::Execute,
            });
        }
    }
    
    /// è®¿å­˜é˜¶æ®µ
    fn memory(&mut self) {
        if let Some(stage) = self.pipeline_stages[2].take() {
            match &stage.instruction {
                Instruction::Load { rd, rs1, offset } => {
                    let address = (self.state.registers[*rs1] as i32 + offset) as u32;
                    let value = self.state.read_memory(address);
                    self.state.registers[*rd] = value as i32;
                }
                Instruction::Store { rs1, rs2, offset } => {
                    let address = (self.state.registers[*rs1] as i32 + offset) as u32;
                    let value = self.state.registers[*rs2] as u32;
                    self.state.write_memory(address, value);
                }
                _ => {}
            }
            
            self.pipeline_stages[3] = Some(PipelineStage {
                instruction: stage.instruction,
                pc: stage.pc,
                stage: PipelineStageType::Memory,
            });
        }
    }
    
    /// å†™å›é˜¶æ®µ
    fn write_back(&mut self) {
        self.pipeline_stages[4] = self.pipeline_stages[3].take();
    }
    
    /// æ¸…ç©ºæµæ°´çº¿
    fn flush_pipeline(&mut self) {
        for stage in &mut self.pipeline_stages {
            *stage = None;
        }
    }
    
    /// æ¨¡æ‹Ÿå–æŒ‡ä»¤
    fn fetch_instruction(&self, pc: u32) -> Option<Instruction> {
        // ç®€åŒ–çš„æŒ‡ä»¤è·å–
        let instruction_data = self.state.read_memory(pc);
        
        // ç®€åŒ–çš„æŒ‡ä»¤è§£ç 
        match instruction_data & 0x7F {
            0x33 => Some(Instruction::Add {
                rd: ((instruction_data >> 7) & 0x1F) as usize,
                rs1: ((instruction_data >> 15) & 0x1F) as usize,
                rs2: ((instruction_data >> 20) & 0x1F) as usize,
            }),
            0x63 => Some(Instruction::Branch {
                rs1: ((instruction_data >> 15) & 0x1F) as usize,
                rs2: ((instruction_data >> 20) & 0x1F) as usize,
                offset: ((instruction_data >> 8) & 0xF) as i32,
            }),
            _ => None,
        }
    }
}

/// åˆ†æ”¯é¢„æµ‹å™¨
#[derive(Debug)]
pub struct BranchPredictor {
    pub prediction_table: HashMap<u32, u8>, // 2ä½é¥±å’Œè®¡æ•°å™¨
}

impl BranchPredictor {
    pub fn new() -> Self {
        BranchPredictor {
            prediction_table: HashMap::new(),
        }
    }
    
    /// é¢„æµ‹åˆ†æ”¯
    pub fn predict(&self, pc: u32) -> bool {
        let counter = self.prediction_table.get(&pc).unwrap_or(&1);
        *counter >= 2
    }
    
    /// æ›´æ–°é¢„æµ‹å™¨
    pub fn update(&mut self, pc: u32, taken: bool) {
        let counter = self.prediction_table.entry(pc).or_insert(1);
        
        if taken {
            *counter = (*counter + 1).min(3);
        } else {
            *counter = (*counter - 1).max(0);
        }
    }
}

/// ç¼“å­˜ç³»ç»Ÿ
#[derive(Debug)]
pub struct Cache {
    pub sets: Vec<Vec<CacheLine>>,
    pub set_size: usize,
    pub line_size: usize,
    pub access_count: u64,
    pub hit_count: u64,
}

#[derive(Debug, Clone)]
pub struct CacheLine {
    pub tag: u32,
    pub data: Vec<u8>,
    pub valid: bool,
    pub dirty: bool,
    pub lru_counter: u64,
}

impl Cache {
    pub fn new(sets: usize, set_size: usize, line_size: usize) -> Self {
        let mut cache_sets = Vec::new();
        for _ in 0..sets {
            let mut set = Vec::new();
            for _ in 0..set_size {
                set.push(CacheLine {
                    tag: 0,
                    data: vec![0; line_size],
                    valid: false,
                    dirty: false,
                    lru_counter: 0,
                });
            }
            cache_sets.push(set);
        }
        
        Cache {
            sets: cache_sets,
            set_size,
            line_size,
            access_count: 0,
            hit_count: 0,
        }
    }
    
    /// è¯»å–æ•°æ®
    pub fn read(&mut self, address: u32) -> Option<u8> {
        self.access_count += 1;
        
        let set_index = (address as usize / self.line_size) % self.sets.len();
        let tag = address >> (32 - (self.sets.len() - 1).leading_zeros());
        let offset = (address as usize) % self.line_size;
        
        let set = &mut self.sets[set_index];
        
        // æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜è¡Œ
        for line in set.iter_mut() {
            if line.valid && line.tag == tag {
                line.lru_counter = self.access_count;
                self.hit_count += 1;
                return Some(line.data[offset]);
            }
        }
        
        // ç¼“å­˜ç¼ºå¤±
        None
    }
    
    /// å†™å…¥æ•°æ®
    pub fn write(&mut self, address: u32, data: u8) {
        self.access_count += 1;
        
        let set_index = (address as usize / self.line_size) % self.sets.len();
        let tag = address >> (32 - (self.sets.len() - 1).leading_zeros());
        let offset = (address as usize) % self.line_size;
        
        let set = &mut self.sets[set_index];
        
        // æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜è¡Œ
        for line in set.iter_mut() {
            if line.valid && line.tag == tag {
                line.data[offset] = data;
                line.dirty = true;
                line.lru_counter = self.access_count;
                self.hit_count += 1;
                return;
            }
        }
        
        // ç¼“å­˜ç¼ºå¤±ï¼Œéœ€è¦æ›¿æ¢
        self.replace_line(set_index, tag, address, data, offset);
    }
    
    /// æ›¿æ¢ç¼“å­˜è¡Œ
    fn replace_line(&mut self, set_index: usize, tag: u32, address: u32, data: u8, offset: usize) {
        let set = &mut self.sets[set_index];
        
        // æ‰¾åˆ°LRUç¼“å­˜è¡Œ
        let mut lru_index = 0;
        let mut min_lru = u64::MAX;
        
        for (i, line) in set.iter().enumerate() {
            if !line.valid {
                lru_index = i;
                break;
            }
            if line.lru_counter < min_lru {
                min_lru = line.lru_counter;
                lru_index = i;
            }
        }
        
        // æ›¿æ¢ç¼“å­˜è¡Œ
        let line = &mut set[lru_index];
        line.tag = tag;
        line.data[offset] = data;
        line.valid = true;
        line.dirty = true;
        line.lru_counter = self.access_count;
    }
    
    /// è®¡ç®—å‘½ä¸­ç‡
    pub fn hit_rate(&self) -> f64 {
        if self.access_count == 0 {
            0.0
        } else {
            self.hit_count as f64 / self.access_count as f64
        }
    }
}

/// å†…å­˜å±‚æ¬¡ç³»ç»Ÿ
#[derive(Debug)]
pub struct MemoryHierarchy {
    pub l1_cache: Cache,
    pub l2_cache: Cache,
    pub main_memory: HashMap<u32, u8>,
    pub access_latency: HashMap<String, u64>,
}

impl MemoryHierarchy {
    pub fn new() -> Self {
        MemoryHierarchy {
            l1_cache: Cache::new(64, 4, 64),   // 16KB L1ç¼“å­˜
            l2_cache: Cache::new(512, 8, 64),   // 256KB L2ç¼“å­˜
            main_memory: HashMap::new(),
            access_latency: {
                let mut latencies = HashMap::new();
                latencies.insert("L1".to_string(), 1);
                latencies.insert("L2".to_string(), 10);
                latencies.insert("Memory".to_string(), 100);
                latencies
            },
        }
    }
    
    /// è¯»å–æ•°æ®
    pub fn read(&mut self, address: u32) -> (u8, u64) {
        let mut total_latency = 0;
        
        // å°è¯•L1ç¼“å­˜
        if let Some(data) = self.l1_cache.read(address) {
            total_latency += self.access_latency["L1"];
            return (data, total_latency);
        }
        
        // L1ç¼ºå¤±ï¼Œå°è¯•L2ç¼“å­˜
        total_latency += self.access_latency["L1"];
        if let Some(data) = self.l2_cache.read(address) {
            total_latency += self.access_latency["L2"];
            // å°†æ•°æ®å†™å›L1
            self.l1_cache.write(address, data);
            return (data, total_latency);
        }
        
        // L2ç¼ºå¤±ï¼Œè®¿é—®ä¸»å­˜
        total_latency += self.access_latency["L2"];
        let data = self.main_memory.get(&address).unwrap_or(&0);
        total_latency += self.access_latency["Memory"];
        
        // å°†æ•°æ®å†™å›L2å’ŒL1
        self.l2_cache.write(address, *data);
        self.l1_cache.write(address, *data);
        
        (*data, total_latency)
    }
    
    /// å†™å…¥æ•°æ®
    pub fn write(&mut self, address: u32, data: u8) -> u64 {
        let mut total_latency = 0;
        
        // å†™ç›´è¾¾ç­–ç•¥
        self.l1_cache.write(address, data);
        self.l2_cache.write(address, data);
        self.main_memory.insert(address, data);
        
        total_latency += self.access_latency["L1"];
        total_latency += self.access_latency["L2"];
        total_latency += self.access_latency["Memory"];
        
        total_latency
    }
}
```

### å¹¶è¡Œè®¡ç®—å®ç°

```rust
use std::sync::{Arc, Mutex};
use std::thread;
use std::time::Instant;

/// å¤šæ ¸å¤„ç†å™¨
#[derive(Debug)]
pub struct MulticoreProcessor {
    pub cores: Vec<Core>,
    pub shared_memory: Arc<Mutex<HashMap<u32, u8>>>,
    pub cache_coherence: CacheCoherenceProtocol,
}

#[derive(Debug)]
pub struct Core {
    pub id: usize,
    pub local_cache: Cache,
    pub processor: PipelineProcessor,
}

impl MulticoreProcessor {
    pub fn new(num_cores: usize) -> Self {
        let mut cores = Vec::new();
        let shared_memory = Arc::new(Mutex::new(HashMap::new()));
        
        for i in 0..num_cores {
            cores.push(Core {
                id: i,
                local_cache: Cache::new(32, 4, 64),
                processor: PipelineProcessor::new(),
            });
        }
        
        MulticoreProcessor {
            cores,
            shared_memory,
            cache_coherence: CacheCoherenceProtocol::new(),
        }
    }
    
    /// å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
    pub fn parallel_execute<F>(&mut self, tasks: Vec<F>) -> Vec<f64>
    where F: FnOnce() -> f64 + Send + 'static {
        let mut handles = Vec::new();
        let shared_memory = Arc::clone(&self.shared_memory);
        
        for (i, task) in tasks.into_iter().enumerate() {
            let memory_clone = Arc::clone(&shared_memory);
            let handle = thread::spawn(move || {
                let start = Instant::now();
                let result = task();
                let duration = start.elapsed().as_secs_f64();
                (i, result, duration)
            });
            handles.push(handle);
        }
        
        let mut results = Vec::new();
        for handle in handles {
            if let Ok((_, result, _)) = handle.join() {
                results.push(result);
            }
        }
        
        results
    }
    
    /// è®¡ç®—åŠ é€Ÿæ¯”
    pub fn calculate_speedup(&self, serial_time: f64, parallel_time: f64) -> f64 {
        serial_time / parallel_time
    }
    
    /// è®¡ç®—æ•ˆç‡
    pub fn calculate_efficiency(&self, speedup: f64, num_cores: usize) -> f64 {
        speedup / num_cores as f64
    }
}

/// ç¼“å­˜ä¸€è‡´æ€§åè®®
#[derive(Debug)]
pub struct CacheCoherenceProtocol {
    pub directory: HashMap<u32, CacheLineState>,
}

#[derive(Debug, Clone)]
pub enum CacheLineState {
    Invalid,
    Shared,
    Exclusive,
    Modified,
}

impl CacheCoherenceProtocol {
    pub fn new() -> Self {
        CacheCoherenceProtocol {
            directory: HashMap::new(),
        }
    }
    
    /// å¤„ç†è¯»è¯·æ±‚
    pub fn handle_read(&mut self, address: u32, core_id: usize) -> CacheLineState {
        let state = self.directory.entry(address).or_insert(CacheLineState::Invalid);
        
        match state {
            CacheLineState::Invalid => {
                *state = CacheLineState::Shared;
                CacheLineState::Shared
            }
            CacheLineState::Shared => {
                CacheLineState::Shared
            }
            CacheLineState::Exclusive => {
                *state = CacheLineState::Shared;
                CacheLineState::Shared
            }
            CacheLineState::Modified => {
                *state = CacheLineState::Shared;
                CacheLineState::Shared
            }
        }
    }
    
    /// å¤„ç†å†™è¯·æ±‚
    pub fn handle_write(&mut self, address: u32, core_id: usize) -> CacheLineState {
        let state = self.directory.entry(address).or_insert(CacheLineState::Invalid);
        
        match state {
            CacheLineState::Invalid => {
                *state = CacheLineState::Modified;
                CacheLineState::Modified
            }
            CacheLineState::Shared => {
                *state = CacheLineState::Modified;
                CacheLineState::Modified
            }
            CacheLineState::Exclusive => {
                *state = CacheLineState::Modified;
                CacheLineState::Modified
            }
            CacheLineState::Modified => {
                CacheLineState::Modified
            }
        }
    }
}

/// å‘é‡å¤„ç†å™¨
#[derive(Debug)]
pub struct VectorProcessor {
    pub vector_registers: Vec<Vec<f64>>,
    pub vector_length: usize,
    pub vector_unit: VectorUnit,
}

#[derive(Debug)]
pub struct VectorUnit {
    pub lanes: usize,
    pub pipeline_depth: usize,
}

impl VectorProcessor {
    pub fn new(vector_length: usize, lanes: usize) -> Self {
        VectorProcessor {
            vector_registers: vec![vec![0.0; vector_length]; 32],
            vector_length,
            vector_unit: VectorUnit {
                lanes,
                pipeline_depth: 4,
            },
        }
    }
    
    /// å‘é‡åŠ æ³•
    pub fn vector_add(&mut self, vd: usize, vs1: usize, vs2: usize) -> Vec<f64> {
        let mut result = vec![0.0; self.vector_length];
        
        for i in 0..self.vector_length {
            result[i] = self.vector_registers[vs1][i] + self.vector_registers[vs2][i];
        }
        
        self.vector_registers[vd] = result.clone();
        result
    }
    
    /// å‘é‡ä¹˜æ³•
    pub fn vector_mul(&mut self, vd: usize, vs1: usize, vs2: usize) -> Vec<f64> {
        let mut result = vec![0.0; self.vector_length];
        
        for i in 0..self.vector_length {
            result[i] = self.vector_registers[vs1][i] * self.vector_registers[vs2][i];
        }
        
        self.vector_registers[vd] = result.clone();
        result
    }
    
    /// å‘é‡ç‚¹ç§¯
    pub fn vector_dot_product(&mut self, vs1: usize, vs2: usize) -> f64 {
        let mut sum = 0.0;
        
        for i in 0..self.vector_length {
            sum += self.vector_registers[vs1][i] * self.vector_registers[vs2][i];
        }
        
        sum
    }
    
    /// å‘é‡åŒ–çŸ©é˜µä¹˜æ³•
    pub fn matrix_multiply_vectorized(&mut self, a: &[Vec<f64>], b: &[Vec<f64>]) -> Vec<Vec<f64>> {
        let n = a.len();
        let mut result = vec![vec![0.0; n]; n];
        
        for i in 0..n {
            for j in 0..n {
                // åŠ è½½è¡Œå‘é‡åˆ°å‘é‡å¯„å­˜å™¨
                self.vector_registers[0] = a[i].clone();
                self.vector_registers[1] = b[j].clone();
                
                // æ‰§è¡Œå‘é‡ç‚¹ç§¯
                result[i][j] = self.vector_dot_product(0, 1);
            }
        }
        
        result
    }
}
```

## ğŸ“Š åº”ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šæµæ°´çº¿æ€§èƒ½åˆ†æ

```rust
fn main() {
    let mut processor = PipelineProcessor::new();
    
    // æ¨¡æ‹Ÿç¨‹åºæ‰§è¡Œ
    for _ in 0..100 {
        processor.cycle();
    }
    
    println!("Processor state: {:?}", processor.state);
    println!("Branch predictor accuracy: {:.2}%", 
             processor.branch_predictor.prediction_table.len() as f64 / 100.0 * 100.0);
}
```

### ç¤ºä¾‹2ï¼šç¼“å­˜æ€§èƒ½åˆ†æ

```rust
fn main() {
    let mut cache = Cache::new(64, 4, 64);
    
    // æ¨¡æ‹Ÿå†…å­˜è®¿é—®æ¨¡å¼
    for i in 0..1000 {
        let address = (i * 4) as u32;
        cache.write(address, (i % 256) as u8);
    }
    
    // é‡å¤è®¿é—®ä»¥æµ‹è¯•ç¼“å­˜æ•ˆæœ
    for i in 0..1000 {
        let address = (i * 4) as u32;
        cache.read(address);
    }
    
    println!("Cache hit rate: {:.2}%", cache.hit_rate() * 100.0);
}
```

### ç¤ºä¾‹3ï¼šå¹¶è¡Œè®¡ç®—æ€§èƒ½åˆ†æ

```rust
fn main() {
    let mut multicore = MulticoreProcessor::new(4);
    
    // åˆ›å»ºå¹¶è¡Œä»»åŠ¡
    let tasks: Vec<Box<dyn FnOnce() -> f64 + Send>> = vec![
        Box::new(|| {
            let mut sum = 0.0;
            for i in 0..1000000 {
                sum += (i as f64).sqrt();
            }
            sum
        }),
        Box::new(|| {
            let mut product = 1.0;
            for i in 1..100000 {
                product *= i as f64;
            }
            product
        }),
        Box::new(|| {
            let mut max = 0.0;
            for i in 0..1000000 {
                max = max.max((i as f64).sin());
            }
            max
        }),
        Box::new(|| {
            let mut min = f64::INFINITY;
            for i in 0..1000000 {
                min = min.min((i as f64).cos());
            }
            min
        }),
    ];
    
    let start = std::time::Instant::now();
    let results = multicore.parallel_execute(tasks);
    let parallel_time = start.elapsed().as_secs_f64();
    
    println!("Parallel execution time: {:.4}s", parallel_time);
    println!("Results: {:?}", results);
    
    // è®¡ç®—åŠ é€Ÿæ¯”
    let serial_time = parallel_time * 4.0; // å‡è®¾ä¸²è¡Œæ—¶é—´æ˜¯å¹¶è¡Œæ—¶é—´çš„4å€
    let speedup = multicore.calculate_speedup(serial_time, parallel_time);
    let efficiency = multicore.calculate_efficiency(speedup, 4);
    
    println!("Speedup: {:.2}x", speedup);
    println!("Efficiency: {:.2}%", efficiency * 100.0);
}
```

## ğŸ”¬ ç†è®ºæ‰©å±•

### 1. é‡å­è®¡ç®—æ¶æ„

**å®šä¹‰ 4.1** (é‡å­æ¯”ç‰¹)
é‡å­æ¯”ç‰¹æ˜¯é‡å­è®¡ç®—çš„åŸºæœ¬å•ä½ï¼š$|\psi\rangle = \alpha|0\rangle + \beta|1\rangle$

**å®šç† 4.1** (é‡å­å¹¶è¡Œæ€§)
é‡å­è®¡ç®—æœºå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè®¡ç®—è·¯å¾„ã€‚

### 2. ç¥ç»å½¢æ€è®¡ç®—

**å®šä¹‰ 4.2** (ç¥ç»å½¢æ€å¤„ç†å™¨)
ç¥ç»å½¢æ€å¤„ç†å™¨æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç½‘ç»œçš„æ¶æ„å’ŒåŠŸèƒ½ã€‚

**å®šç† 4.2** (ç¥ç»å½¢æ€æ•ˆç‡)
ç¥ç»å½¢æ€å¤„ç†å™¨åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæ¯”ä¼ ç»Ÿå¤„ç†å™¨æ›´é«˜æ•ˆã€‚

### 3. å¯é‡æ„è®¡ç®—

**å®šä¹‰ 4.3** (å¯é‡æ„æ¶æ„)
å¯é‡æ„æ¶æ„å¯ä»¥æ ¹æ®åº”ç”¨éœ€æ±‚åŠ¨æ€æ”¹å˜ç¡¬ä»¶ç»“æ„ã€‚

**å®šç† 4.3** (å¯é‡æ„ä¼˜åŠ¿)
å¯é‡æ„æ¶æ„åœ¨çµæ´»æ€§å’Œæ€§èƒ½ä¹‹é—´æä¾›è‰¯å¥½å¹³è¡¡ã€‚

## ğŸ¯ æ‰¹åˆ¤æ€§åˆ†æ

### ä¸»è¦ç†è®ºè§‚ç‚¹æ¢³ç†

1. **å¤„ç†å™¨æ¶æ„è®¾è®¡**ï¼š
   - æŒ‡ä»¤é›†æ¶æ„ä¸ºè½¯ä»¶æä¾›æŠ½è±¡æ¥å£
   - å¾®æ¶æ„è®¾è®¡å¹³è¡¡æ€§èƒ½å’ŒåŠŸè€—
   - æµæ°´çº¿æŠ€æœ¯æé«˜æŒ‡ä»¤ååé‡

2. **å­˜å‚¨ç³»ç»Ÿè´¡çŒ®**ï¼š
   - å†…å­˜å±‚æ¬¡ä¼˜åŒ–è®¿é—®å»¶è¿Ÿ
   - ç¼“å­˜ä¸€è‡´æ€§ä¿è¯æ•°æ®æ­£ç¡®æ€§
   - è™šæ‹Ÿå†…å­˜æ‰©å±•åœ°å€ç©ºé—´

3. **å¹¶è¡Œè®¡ç®—ä»·å€¼**ï¼š
   - å¤šæ ¸æ¶æ„æé«˜ç³»ç»Ÿæ€§èƒ½
   - å‘é‡å¤„ç†åŠ é€Ÿæ•°å€¼è®¡ç®—
   - åˆ†å¸ƒå¼è®¡ç®—æ”¯æŒå¤§è§„æ¨¡åº”ç”¨

### ç†è®ºä¼˜åŠ¿ä¸å±€é™æ€§

**ä¼˜åŠ¿**ï¼š

- ç†è®ºåŸºç¡€æ‰å®ï¼Œæ•°å­¦å½¢å¼åŒ–ç¨‹åº¦é«˜
- å®é™…åº”ç”¨å¹¿æ³›ï¼ŒæŠ€æœ¯æˆç†Ÿ
- æ€§èƒ½ä¼˜åŒ–æ•ˆæœæ˜¾è‘—

**å±€é™æ€§**ï¼š

- åŠŸè€—å¢™å’Œå†…å­˜å¢™é™åˆ¶
- å¹¶è¡ŒåŒ–å¼€é”€å’ŒåŒæ­¥é—®é¢˜
- ç¼“å­˜ä¸€è‡´æ€§çš„å¤æ‚æ€§

### å­¦ç§‘äº¤å‰èåˆ

1. **ä¸æ“ä½œç³»ç»Ÿç†è®º**ï¼š
   - è¿›ç¨‹è°ƒåº¦å’Œå†…å­˜ç®¡ç†
   - è®¾å¤‡é©±åŠ¨å’Œä¸­æ–­å¤„ç†
   - è™šæ‹ŸåŒ–æŠ€æœ¯æ”¯æŒ

2. **ä¸å¹¶å‘ç†è®º**ï¼š
   - å¤šçº¿ç¨‹ç¼–ç¨‹æ¨¡å‹
   - åŒæ­¥åŸè¯­å®ç°
   - æ— é”æ•°æ®ç»“æ„

3. **ä¸è½¯ä»¶å·¥ç¨‹ç†è®º**ï¼š
   - ç¼–è¯‘å™¨ä¼˜åŒ–æŠ€æœ¯
   - æ€§èƒ½åˆ†æå’Œè°ƒä¼˜
   - å¹¶è¡Œç¼–ç¨‹æ¨¡å‹

### åˆ›æ–°æ‰¹åˆ¤ä¸æœªæ¥å±•æœ›

**å½“å‰æŒ‘æˆ˜**ï¼š

1. æ‘©å°”å®šå¾‹æ”¾ç¼“çš„åº”å¯¹
2. åŠŸè€—å’Œæ•£çƒ­é—®é¢˜
3. å†…å­˜å¸¦å®½ç“¶é¢ˆ

**æœªæ¥å‘å±•æ–¹å‘**ï¼š

1. ä¸“ç”¨åŠ é€Ÿå™¨è®¾è®¡
2. é‡å­è®¡ç®—æ¶æ„
3. ç¥ç»å½¢æ€è®¡ç®—
4. å¯é‡æ„è®¡ç®—

**ç¤¾ä¼šå½±å“åˆ†æ**ï¼š

- è®¡ç®—æœºæ¶æ„æ”¯æ’‘äº†ç°ä»£ä¿¡æ¯æŠ€æœ¯
- æ€§èƒ½æå‡æ¨åŠ¨åº”ç”¨åˆ›æ–°
- éœ€è¦å¹³è¡¡æ€§èƒ½ä¸èƒ½æ•ˆ

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Hennessy, J. L., Patterson, D. A. (2017). "Computer Architecture: A Quantitative Approach"
2. Patterson, D. A., Hennessy, J. L. (2013). "Computer Organization and Design"
3. Flynn, M. J. (1995). "Computer Architecture: Pipelined and Parallel Processor Design"
4. Hill, M. D., et al. (2017). "A Primer on Memory Consistency and Cache Coherence"
5. Culler, D. E., Singh, J. P. (1998). "Parallel Computer Architecture"

---

*æœ¬æ¨¡å—ä¸ºå½¢å¼ç§‘å­¦çŸ¥è¯†åº“çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸ºè®¡ç®—æœºç³»ç»Ÿè®¾è®¡æä¾›ç†è®ºåŸºç¡€ã€‚é€šè¿‡ä¸¥æ ¼çš„æ•°å­¦å½¢å¼åŒ–å’ŒRustä»£ç å®ç°ï¼Œç¡®ä¿ç†è®ºçš„å¯éªŒè¯æ€§å’Œå®ç”¨æ€§ã€‚*
