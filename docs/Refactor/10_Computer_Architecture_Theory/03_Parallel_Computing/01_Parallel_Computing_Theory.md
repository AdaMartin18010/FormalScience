# 09.3.1 å¹¶è¡Œè®¡ç®—ç†è®º

## ğŸ“‹ æ¦‚è¿°

å¹¶è¡Œè®¡ç®—ç†è®ºç ”ç©¶å¤šå¤„ç†å™¨ç³»ç»Ÿä¸­çš„è®¡ç®—æ¨¡å‹ã€å¹¶è¡Œç®—æ³•è®¾è®¡ä¸æ€§èƒ½åˆ†æã€‚è¯¥ç†è®ºæ¶µç›–å¹¶è¡Œæ¶æ„ã€åŒæ­¥æœºåˆ¶ã€è´Ÿè½½å‡è¡¡ã€å¯æ‰©å±•æ€§ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºé«˜æ€§èƒ½è®¡ç®—æä¾›ç†è®ºåŸºç¡€ã€‚

## 1. åŸºæœ¬æ¦‚å¿µ

### 1.1 å¹¶è¡Œè®¡ç®—å®šä¹‰

**å®šä¹‰ 1.1**ï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰
å¹¶è¡Œè®¡ç®—æ˜¯åŒæ—¶ä½¿ç”¨å¤šä¸ªè®¡ç®—èµ„æºè§£å†³åŒä¸€é—®é¢˜çš„è®¡ç®—æ¨¡å¼ã€‚

### 1.2 å¹¶è¡Œæ¶æ„åˆ†ç±»

| æ¶æ„ç±»å‹     | è‹±æ–‡åç§°         | æè¿°                         | å…¸å‹ä»£è¡¨         |
|--------------|------------------|------------------------------|------------------|
| SISD         | Single ISD       | å•æŒ‡ä»¤å•æ•°æ®æµ               | ä¼ ç»ŸCPU          |
| SIMD         | Single IMD       | å•æŒ‡ä»¤å¤šæ•°æ®æµ               | GPU, å‘é‡å¤„ç†å™¨  |
| MISD         | Multiple ISD     | å¤šæŒ‡ä»¤å•æ•°æ®æµ               | å®¹é”™ç³»ç»Ÿ         |
| MIMD         | Multiple IMD     | å¤šæŒ‡ä»¤å¤šæ•°æ®æµ               | å¤šæ ¸CPU, é›†ç¾¤    |

## 2. å½¢å¼åŒ–å®šä¹‰

### 2.1 å¹¶è¡Œè®¡ç®—æ¨¡å‹

**å®šä¹‰ 2.1**ï¼ˆPRAMæ¨¡å‹ï¼‰
å¹¶è¡Œéšæœºè®¿é—®æœºPRAMæ˜¯å¹¶è¡Œè®¡ç®—çš„ç†è®ºæ¨¡å‹ï¼ŒåŒ…å«å…±äº«å†…å­˜å’Œå¤šä¸ªå¤„ç†å™¨ã€‚

### 2.2 å¹¶è¡Œå¤æ‚åº¦

**å®šä¹‰ 2.2**ï¼ˆå¹¶è¡Œæ—¶é—´å¤æ‚åº¦ï¼‰
å¹¶è¡Œæ—¶é—´å¤æ‚åº¦æ˜¯å¹¶è¡Œç®—æ³•åœ¨æœ€åæƒ…å†µä¸‹çš„æ‰§è¡Œæ—¶é—´ã€‚

### 2.3 åŠ é€Ÿæ¯”

**å®šä¹‰ 2.3**ï¼ˆåŠ é€Ÿæ¯”ï¼‰
åŠ é€Ÿæ¯” $S_p = T_1 / T_p$ï¼Œå…¶ä¸­ $T_1$ æ˜¯ä¸²è¡Œæ—¶é—´ï¼Œ$T_p$ æ˜¯ $p$ ä¸ªå¤„ç†å™¨çš„å¹¶è¡Œæ—¶é—´ã€‚

## 3. å®šç†ä¸è¯æ˜

### 3.1 Amdahlå®šå¾‹

**å®šç† 3.1**ï¼ˆAmdahlå®šå¾‹ï¼‰
è‹¥ç¨‹åºä¸­æœ‰ $f$ æ¯”ä¾‹çš„éƒ¨åˆ†å¿…é¡»ä¸²è¡Œæ‰§è¡Œï¼Œåˆ™æœ€å¤§åŠ é€Ÿæ¯” $S_{max} = 1/f$ã€‚

**è¯æ˜**ï¼š
è®¾æ€»å·¥ä½œé‡ä¸º $W$ï¼Œä¸²è¡Œéƒ¨åˆ†ä¸º $fW$ï¼Œå¹¶è¡Œéƒ¨åˆ†ä¸º $(1-f)W$ã€‚
$T_1 = W$ï¼Œ$T_p = fW + (1-f)W/p$ã€‚
$S_p = T_1/T_p = W/(fW + (1-f)W/p) = 1/(f + (1-f)/p)$ã€‚
å½“ $p \to \infty$ æ—¶ï¼Œ$S_{max} = 1/f$ã€‚â–¡

### 3.2 Gustafsonå®šå¾‹

**å®šç† 3.2**ï¼ˆGustafsonå®šå¾‹ï¼‰
åœ¨å›ºå®šæ—¶é—´çº¦æŸä¸‹ï¼Œå¯æ‰©å±•çš„å¹¶è¡Œç¨‹åºåŠ é€Ÿæ¯” $S_p = p - \alpha(p-1)$ï¼Œå…¶ä¸­ $\alpha$ æ˜¯ä¸²è¡Œæ¯”ä¾‹ã€‚

**è¯æ˜**ï¼š
è®¾å›ºå®šæ—¶é—´ä¸º $T$ï¼Œä¸²è¡Œæ—¶é—´ä¸º $T_s$ï¼Œå¹¶è¡Œæ—¶é—´ä¸º $T_p$ã€‚
$T_s = \alpha T + (1-\alpha)pT$ï¼Œ$T_p = T$ã€‚
$S_p = T_s/T_p = \alpha + (1-\alpha)p = p - \alpha(p-1)$ã€‚â–¡

## 4. Rustä»£ç å®ç°

### 4.1 å¹¶è¡Œå½’çº¦ç®—æ³•

```rust
use std::sync::{Arc, Mutex};
use std::thread;

pub fn parallel_reduce<T, F>(data: &[T], op: F, num_threads: usize) -> T 
where 
    T: Copy + Send + Sync,
    F: Fn(T, T) -> T + Send + Sync,
{
    let data = Arc::new(data.to_vec());
    let result = Arc::new(Mutex::new(None));
    let mut handles = vec![];
    
    let chunk_size = (data.len() + num_threads - 1) / num_threads;
    
    for i in 0..num_threads {
        let data = Arc::clone(&data);
        let result = Arc::clone(&result);
        let op = op.clone();
        
        let handle = thread::spawn(move || {
            let start = i * chunk_size;
            let end = std::cmp::min(start + chunk_size, data.len());
            
            if start < data.len() {
                let mut local_result = data[start];
                for j in start + 1..end {
                    local_result = op(local_result, data[j]);
                }
                
                let mut global_result = result.lock().unwrap();
                if let Some(ref mut current) = *global_result {
                    *current = op(*current, local_result);
                } else {
                    *global_result = Some(local_result);
                }
            }
        });
        handles.push(handle);
    }
    
    for handle in handles {
        handle.join().unwrap();
    }
    
    result.lock().unwrap().unwrap()
}
```

### 4.2 å¹¶è¡Œæ’åºç®—æ³•

```rust
use std::sync::{Arc, Mutex};
use std::thread;

pub fn parallel_merge_sort<T>(data: &mut [T], num_threads: usize) 
where 
    T: Ord + Copy + Send + Sync,
{
    if data.len() <= 1 {
        return;
    }
    
    if num_threads <= 1 {
        sequential_merge_sort(data);
        return;
    }
    
    let mid = data.len() / 2;
    let (left, right) = data.split_at_mut(mid);
    
    let left_handle = {
        let left = left.to_vec();
        thread::spawn(move || {
            let mut left = left;
            parallel_merge_sort(&mut left, num_threads / 2);
            left
        })
    };
    
    let right_handle = {
        let right = right.to_vec();
        thread::spawn(move || {
            let mut right = right;
            parallel_merge_sort(&mut right, num_threads / 2);
            right
        })
    };
    
    let sorted_left = left_handle.join().unwrap();
    let sorted_right = right_handle.join().unwrap();
    
    merge(data, &sorted_left, &sorted_right);
}

fn sequential_merge_sort<T: Ord + Copy>(data: &mut [T]) {
    if data.len() <= 1 {
        return;
    }
    
    let mid = data.len() / 2;
    let (left, right) = data.split_at_mut(mid);
    
    sequential_merge_sort(left);
    sequential_merge_sort(right);
    
    let left = left.to_vec();
    let right = right.to_vec();
    merge(data, &left, &right);
}

fn merge<T: Ord + Copy>(result: &mut [T], left: &[T], right: &[T]) {
    let mut i = 0;
    let mut j = 0;
    let mut k = 0;
    
    while i < left.len() && j < right.len() {
        if left[i] <= right[j] {
            result[k] = left[i];
            i += 1;
        } else {
            result[k] = right[j];
            j += 1;
        }
        k += 1;
    }
    
    while i < left.len() {
        result[k] = left[i];
        i += 1;
        k += 1;
    }
    
    while j < right.len() {
        result[k] = right[j];
        j += 1;
        k += 1;
    }
}
```

### 4.3 å¹¶è¡ŒçŸ©é˜µä¹˜æ³•

```rust
use std::sync::{Arc, Mutex};
use std::thread;

pub fn parallel_matrix_multiply(
    a: &[Vec<f64>], 
    b: &[Vec<f64>], 
    num_threads: usize
) -> Vec<Vec<f64>> {
    let n = a.len();
    let m = b[0].len();
    let p = b.len();
    
    let mut result = vec![vec![0.0; m]; n];
    let result = Arc::new(Mutex::new(result));
    let mut handles = vec![];
    
    let rows_per_thread = (n + num_threads - 1) / num_threads;
    
    for thread_id in 0..num_threads {
        let a = a.to_vec();
        let b = b.to_vec();
        let result = Arc::clone(&result);
        
        let handle = thread::spawn(move || {
            let start_row = thread_id * rows_per_thread;
            let end_row = std::cmp::min(start_row + rows_per_thread, n);
            
            for i in start_row..end_row {
                for j in 0..m {
                    let mut sum = 0.0;
                    for k in 0..p {
                        sum += a[i][k] * b[k][j];
                    }
                    
                    let mut result = result.lock().unwrap();
                    result[i][j] = sum;
                }
            }
        });
        handles.push(handle);
    }
    
    for handle in handles {
        handle.join().unwrap();
    }
    
    Arc::try_unwrap(result).unwrap().into_inner().unwrap()
}
```

## 5. ç›¸å…³ç†è®ºä¸äº¤å‰å¼•ç”¨

- [å¤„ç†å™¨æ¶æ„ç†è®º](../01_Processor_Architecture/01_Processor_Architecture_Theory.md)
- [å†…å­˜ç³»ç»Ÿç†è®º](../02_Memory_Systems/01_Memory_Systems_Theory.md)
- [æ€§èƒ½ä¼˜åŒ–ç†è®º](../04_Performance_Optimization/01_Performance_Optimization_Theory.md)

## 6. å‚è€ƒæ–‡çŒ®

1. Kumar, V., Grama, A., Gupta, A., & Karypis, G. (1994). Introduction to Parallel Computing. Benjamin/Cummings.
2. Culler, D. E., Singh, J. P., & Gupta, A. (1998). Parallel Computer Architecture: A Hardware/Software Approach. Morgan Kaufmann.
3. Amdahl, G. M. (1967). Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities. AFIPS Conference Proceedings.

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ21æ—¥  
**ç»´æŠ¤è€…**: AIåŠ©æ‰‹  
**ç‰ˆæœ¬**: v1.0

## æ‰¹åˆ¤æ€§åˆ†æ

### å¤šå…ƒç†è®ºè§†è§’

- æ€§èƒ½è§†è§’ï¼šå¹¶è¡Œè®¡ç®—ç†è®ºå…³æ³¨å¤šå¤„ç†å™¨ç³»ç»Ÿçš„æ€§èƒ½æå‡å’Œæ•ˆç‡ä¼˜åŒ–ã€‚
- ç®—æ³•è§†è§’ï¼šå¹¶è¡Œè®¡ç®—ç†è®ºæ¶‰åŠå¹¶è¡Œç®—æ³•è®¾è®¡å’Œåˆ†æã€‚
- ç³»ç»Ÿè§†è§’ï¼šå¹¶è¡Œè®¡ç®—ç†è®ºå…³æ³¨å¤šå¤„ç†å™¨ç³»ç»Ÿçš„æ¶æ„å’Œè®¾è®¡ã€‚
- å¯æ‰©å±•æ€§è§†è§’ï¼šå¹¶è¡Œè®¡ç®—ç†è®ºå…³æ³¨ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œè´Ÿè½½å‡è¡¡ã€‚

### å±€é™æ€§åˆ†æ

- å¹¶è¡ŒåŒ–å¼€é”€ï¼šå¹¶è¡ŒåŒ–æœ¬èº«å¸¦æ¥çš„é€šä¿¡å’ŒåŒæ­¥å¼€é”€ã€‚
- å¯æ‰©å±•æ€§é™åˆ¶ï¼šAmdahlå®šå¾‹å¯¹å¹¶è¡ŒåŠ é€Ÿæ¯”çš„é™åˆ¶ã€‚
- ç¼–ç¨‹å¤æ‚æ€§ï¼šå¹¶è¡Œç¼–ç¨‹çš„å¤æ‚æ€§å’Œæ­£ç¡®æ€§éªŒè¯å›°éš¾ã€‚
- è´Ÿè½½å‡è¡¡ï¼šåŠ¨æ€è´Ÿè½½åˆ†é…å’Œè´Ÿè½½ä¸å‡è¡¡çš„æŒ‘æˆ˜ã€‚

### äº‰è®®ä¸åˆ†æ­§

- å¹¶è¡Œç¼–ç¨‹æ¨¡å‹ï¼šå…±äº«å†…å­˜vsåˆ†å¸ƒå¼å†…å­˜çš„ç¼–ç¨‹æ¨¡å‹é€‰æ‹©ã€‚
- åŒæ­¥ç­–ç•¥ï¼šåŒæ­¥vså¼‚æ­¥çš„å¹¶è¡Œç­–ç•¥æƒè¡¡ã€‚
- é€šä¿¡æ¨¡å¼ï¼šä¸åŒé€šä¿¡æ¨¡å¼çš„é€‰æ‹©å’Œä¼˜åŒ–ã€‚
- å¯æ‰©å±•æ€§ç­–ç•¥ï¼šä¸åŒå¯æ‰©å±•æ€§ç­–ç•¥çš„æ€§èƒ½å’Œå¤æ‚åº¦æƒè¡¡ã€‚

### åº”ç”¨å‰æ™¯

- é«˜æ€§èƒ½è®¡ç®—ï¼šç§‘å­¦è®¡ç®—å’Œå¤§è§„æ¨¡ä»¿çœŸã€‚
- å¤§æ•°æ®å¤„ç†ï¼šåˆ†å¸ƒå¼æ•°æ®å¤„ç†æ¡†æ¶ã€‚
- AIè®­ç»ƒï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¹¶è¡Œè®­ç»ƒã€‚
- äº‘è®¡ç®—ï¼šå¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿçš„å¹¶è¡Œå¤„ç†ã€‚

### æ”¹è¿›å»ºè®®

- å‘å±•æ™ºèƒ½åŒ–çš„å¹¶è¡Œè°ƒåº¦ç®—æ³•ï¼Œå‡å°‘äººå·¥è°ƒä¼˜æˆæœ¬ã€‚
- å»ºç«‹è‡ªåŠ¨åŒ–çš„å¹¶è¡ŒåŒ–æ–¹æ³•å’Œç®€åŒ–ç¼–ç¨‹æ¨¡å‹ã€‚
- åŠ å¼ºå¹¶è¡Œç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œè´Ÿè½½å‡è¡¡èƒ½åŠ›ã€‚
- é€‚åº”æ–°å…´åº”ç”¨åœºæ™¯çš„å¹¶è¡Œè®¡ç®—åˆ›æ–°ã€‚
