# AIä¼¦ç†å­¦ (AI Ethics)

## ğŸ“‹ 1. æ¦‚è¿°

AIä¼¦ç†å­¦æ˜¯åº”ç”¨ä¼¦ç†å­¦çš„æ–°å…´åˆ†æ”¯ï¼Œä¸“æ³¨äºç ”ç©¶äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®¾è®¡ã€å¼€å‘å’Œåº”ç”¨ä¸­çš„é“å¾·é—®é¢˜ã€‚éšç€AIæŠ€æœ¯çš„å¿«é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨ï¼Œå…¶äº§ç”Ÿçš„ä¼¦ç†æŒ‘æˆ˜æ—¥ç›Šå¤æ‚ã€‚æœ¬æ–‡æ¡£ä»å½¢å¼ç§‘å­¦çš„è§’åº¦ï¼Œç³»ç»Ÿæ€§åœ°åˆ†æAIä¼¦ç†å­¦çš„æ ¸å¿ƒé—®é¢˜ã€åŸåˆ™å’Œå®ç°æ–¹æ³•ï¼Œå¹¶æä¾›å½¢å¼åŒ–è¡¨ç¤ºä¸è®¡ç®—æ¨¡å‹ã€‚

## ğŸ¯ 2. AIä¼¦ç†å­¦çš„æ ¸å¿ƒé—®é¢˜

AIä¼¦ç†å­¦æ¢è®¨ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š

1. AIç³»ç»Ÿå¦‚ä½•ä½“ç°å’Œéµå¾ªé“å¾·ä»·å€¼ï¼Ÿ
2. å¦‚ä½•åœ¨AIç³»ç»Ÿä¸­å®ç°å…¬å¹³ã€é€æ˜å’Œé—®è´£ï¼Ÿ
3. äººç±»åº”å¦‚ä½•ä¿æŒå¯¹AIç³»ç»Ÿçš„æœ‰æ•ˆæ§åˆ¶ï¼Ÿ
4. AIç³»ç»Ÿçš„å†³ç­–è´£ä»»åº”å¦‚ä½•åˆ†é…ï¼Ÿ
5. å¦‚ä½•å¹³è¡¡AIåˆ›æ–°ä¸ä¼¦ç†é£é™©ï¼Ÿ

## ğŸ“š 3. AIä¼¦ç†åŸåˆ™å½¢å¼åŒ–

### 3.1 æ ¸å¿ƒAIä¼¦ç†åŸåˆ™

AIä¼¦ç†çš„æ ¸å¿ƒåŸåˆ™å¯å½¢å¼åŒ–ä¸ºï¼š

$$
AIEthics = \langle Beneficence, NonMaleficence, Autonomy, Justice, Explicability \rangle
$$

å…¶ä¸­ï¼š

- $Beneficence$ è¡¨ç¤ºæœ‰ç›ŠåŸåˆ™ï¼Œä¿ƒè¿›äººç±»ç¦ç¥‰
- $NonMaleficence$ è¡¨ç¤ºæ— å®³åŸåˆ™ï¼Œé¿å…ä¼¤å®³
- $Autonomy$ è¡¨ç¤ºè‡ªä¸»åŸåˆ™ï¼Œå°Šé‡äººç±»è‡ªä¸»æ€§
- $Justice$ è¡¨ç¤ºå…¬æ­£åŸåˆ™ï¼Œç¡®ä¿å…¬å¹³å’ŒåŒ…å®¹
- $Explicability$ è¡¨ç¤ºå¯è§£é‡Šæ€§åŸåˆ™ï¼Œç¡®ä¿é€æ˜åº¦å’Œé—®è´£åˆ¶

### 3.2 ä¼¦ç†åŸåˆ™å½¢å¼åŒ–è¡¨ç¤º

æ¯ä¸€ä¸ªä¼¦ç†åŸåˆ™å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªè¯„ä¼°å‡½æ•°ï¼š

$$
\begin{align}
Beneficence(AI, C) &= \sum_{i=1}^{n} w_i \times Benefit_i(AI, C) \\
NonMaleficence(AI, C) &= -\sum_{j=1}^{m} w_j \times Harm_j(AI, C) \\
Autonomy(AI, C) &= \sum_{k=1}^{p} w_k \times HumanControl_k(AI, C) \\
Justice(AI, C) &= \sum_{l=1}^{q} w_l \times Fairness_l(AI, C) \\
Explicability(AI, C) &= \sum_{o=1}^{r} w_o \times (Transparency_o(AI, C) + Accountability_o(AI, C))
\end{align}
$$

å…¶ä¸­ï¼š

- $AI$ è¡¨ç¤ºAIç³»ç»Ÿ
- $C$ è¡¨ç¤ºåº”ç”¨æƒ…å¢ƒ
- $w_i, w_j, w_k, w_l, w_o$ æ˜¯ç›¸åº”æŒ‡æ ‡çš„æƒé‡

### 3.3 AIä¼¦ç†è¯„ä¼°ç»¼åˆå‡½æ•°

AIç³»ç»Ÿçš„æ€»ä½“ä¼¦ç†è¯„åˆ†å¯è¡¨ç¤ºä¸ºï¼š

$$
EthicalScore(AI, C) = \alpha \cdot Beneficence(AI, C) + \beta \cdot NonMaleficence(AI, C) + \gamma \cdot Autonomy(AI, C) + \delta \cdot Justice(AI, C) + \epsilon \cdot Explicability(AI, C)
$$

å…¶ä¸­ $\alpha, \beta, \gamma, \delta, \epsilon$ æ˜¯æƒé‡ç³»æ•°ï¼Œä¸” $\alpha + \beta + \gamma + \delta + \epsilon = 1$ã€‚

## ğŸ” 4. AIä¼¦ç†ä¸»è¦é—®é¢˜å½¢å¼åŒ–

### 4.1 ç®—æ³•å…¬å¹³æ€§

ç®—æ³•å…¬å¹³æ€§å¯ä»¥é€šè¿‡å¤šç§æŒ‡æ ‡å½¢å¼åŒ–ï¼š

#### 4.1.1 ç»Ÿè®¡å…¬å¹³æ€§æŒ‡æ ‡

$$
\begin{align}
DemographicParity(A, S) &= |P(A(x)=1 | S=0) - P(A(x)=1 | S=1)| \\
EqualizedOdds(A, S, Y) &= |P(A(x)=1 | S=0, Y=y) - P(A(x)=1 | S=1, Y=y)| \text{ for } y \in \{0, 1\}
\end{align}
$$

å…¶ä¸­ï¼š

- $A$ æ˜¯ç®—æ³•
- $S$ æ˜¯æ•æ„Ÿå±æ€§ï¼ˆå¦‚æ€§åˆ«ã€ç§æ—ï¼‰
- $Y$ æ˜¯ç›®æ ‡å˜é‡

#### 4.1.2 ä¸ªä½“å…¬å¹³æ€§æŒ‡æ ‡

$$
IndividualFairness(A, d, D) = \max_{x, y \in \mathcal{X}} \frac{|A(x) - A(y)|}{d(x, y)}
$$

å…¶ä¸­ï¼š

- $d$ æ˜¯ä¸ªä½“é—´çš„ç›¸ä¼¼åº¦æŒ‡æ ‡
- $D$ æ˜¯å†³ç­–ç©ºé—´ä¸­çš„è·ç¦»åº¦é‡

#### 4.1.3 å¤šæ ‡å‡†å…¬å¹³æ€§å¹³è¡¡

ä¸åŒå…¬å¹³æ€§æ ‡å‡†é—´çš„æƒè¡¡ï¼š

$$
FairnessObjective = \lambda_1 \cdot DemographicParity + \lambda_2 \cdot EqualizedOdds + \lambda_3 \cdot IndividualFairness
$$

### 4.2 AIé€æ˜æ€§å’Œå¯è§£é‡Šæ€§

é€æ˜æ€§å’Œå¯è§£é‡Šæ€§å¯å½¢å¼åŒ–ä¸ºï¼š

$$
\begin{align}
Transparency(AI) &= \langle CodeAccess, DataDocumentation, ProcessVisibility \rangle \\
Explainability(AI, d) &= Complexity(AI)^{-1} \cdot Interpretability(AI, d)
\end{align}
$$

å…¶ä¸­ $d$ æ˜¯å†³ç­–ã€‚

### 4.3 AIç³»ç»Ÿçš„é—®è´£åˆ¶

AIç³»ç»Ÿçš„é—®è´£åˆ¶å¯å½¢å¼åŒ–ä¸ºï¼š

$$
Accountability(AI) = \langle Responsibility, Answerability, Sanctionability \rangle
$$

è´£ä»»åˆ†é…æ¨¡å‹ï¼š

$$
ResponsibilityAllocation(outcome) = \sum_{i=1}^{n} r_i \cdot Agent_i
$$

å…¶ä¸­ $r_i$ æ˜¯ä»£ç†äºº $i$ çš„è´£ä»»æ¯”ä¾‹ï¼Œä¸” $\sum_{i=1}^{n} r_i = 1$ã€‚

### 4.4 AIéšç§ä¿æŠ¤

éšç§ä¿æŠ¤ç¨‹åº¦å¯å½¢å¼åŒ–ä¸ºï¼š

$$
PrivacyProtection = \langle DataMinimization, Anonymization, AccessControl \rangle
$$

å·®åˆ†éšç§ä¿è¯ï¼š

$$
\forall S \subseteq Range(A), \forall D, D' \text{ such that } |D \Delta D'| = 1: P[A(D) \in S] \leq e^\epsilon \cdot P[A(D') \in S]
$$

### 4.5 äººå·¥æ™ºèƒ½å®‰å…¨

AIå®‰å…¨æ€§å¯å½¢å¼åŒ–ä¸ºï¼š

$$
AISafety = \langle Robustness, Security, Alignment, Containment \rangle
$$

é²æ£’æ€§åº¦é‡ï¼š

$$
Robustness(AI, \delta) = \inf_{||x'-x||<\delta} Loss(AI(x'), y)
$$

## ğŸ’» 5. AIä¼¦ç†è®¡ç®—å®ç°

ä»¥ä¸‹ä»£ç å®ç°äº†AIä¼¦ç†å­¦çš„æ ¸å¿ƒæ¡†æ¶ï¼š

```rust
// åŸºç¡€ç±»å‹å®šä¹‰
pub type AgentId = u64;
pub type SystemId = u64;
pub type ContextId = u64;
pub type AttributeId = u64;

// AIç³»ç»Ÿ
pub struct AISystem {
    pub id: SystemId,
    pub name: String,
    pub capabilities: Vec<AICapability>,
    pub decision_model: DecisionModel,
    pub transparency_level: f64,  // 0.0-1.0
    pub explainability_level: f64, // 0.0-1.0
    pub data_sources: Vec<DataSource>,
}

// AIèƒ½åŠ›
pub struct AICapability {
    pub name: String,
    pub risk_level: RiskLevel,
    pub autonomy_level: AutonomyLevel,
    pub human_oversight: HumanOversight,
}

// ä¼¦ç†è¯„ä¼°æ¡†æ¶
pub struct AIEthicsFramework {
    pub beneficence_weight: f64,
    pub non_maleficence_weight: f64,
    pub autonomy_weight: f64,
    pub justice_weight: f64,
    pub explicability_weight: f64,
    pub fairness_metrics: Vec<FairnessMetric>,
    pub privacy_requirements: PrivacyRequirements,
}

impl AIEthicsFramework {
    pub fn new_balanced() -> Self {
        Self {
            beneficence_weight: 0.2,
            non_maleficence_weight: 0.2,
            autonomy_weight: 0.2,
            justice_weight: 0.2,
            explicability_weight: 0.2,
            fairness_metrics: vec![
                FairnessMetric::DemographicParity,
                FairnessMetric::EqualizedOdds,
                FairnessMetric::EqualOpportunity,
            ],
            privacy_requirements: PrivacyRequirements {
                data_minimization: true,
                anonymization_required: true,
                consent_required: true,
                access_controls: vec![
                    AccessControl::Authentication,
                    AccessControl::Authorization,
                    AccessControl::Auditing,
                ],
            },
        }
    }
    
    pub fn evaluate_system(&self, system: &AISystem, context: &Context) -> AIEthicsEvaluation {
        // è®¡ç®—å—ç›Šæ€§è¯„åˆ†
        let beneficence_score = self.evaluate_beneficence(system, context);
        
        // è®¡ç®—æ— å®³æ€§è¯„åˆ†
        let non_maleficence_score = self.evaluate_non_maleficence(system, context);
        
        // è®¡ç®—è‡ªä¸»æ€§å°Šé‡è¯„åˆ†
        let autonomy_score = self.evaluate_autonomy(system, context);
        
        // è®¡ç®—å…¬æ­£æ€§è¯„åˆ†
        let justice_score = self.evaluate_justice(system, context);
        
        // è®¡ç®—å¯è§£é‡Šæ€§è¯„åˆ†
        let explicability_score = self.evaluate_explicability(system, context);
        
        // è®¡ç®—ç»¼åˆè¯„åˆ†
        let total_score = 
            self.beneficence_weight * beneficence_score +
            self.non_maleficence_weight * non_maleficence_score +
            self.autonomy_weight * autonomy_score +
            self.justice_weight * justice_score +
            self.explicability_weight * explicability_score;
            
        // è¯†åˆ«ä¼¦ç†é£é™©
        let risks = self.identify_ethical_risks(system, context);
        
        // ç”Ÿæˆæ”¹è¿›å»ºè®®
        let recommendations = self.generate_recommendations(system, context, &risks);
        
        AIEthicsEvaluation {
            system_id: system.id,
            context_id: context.id,
            beneficence_score,
            non_maleficence_score,
            autonomy_score,
            justice_score,
            explicability_score,
            total_score,
            ethical_risks: risks,
            recommendations,
            evaluation_time: chrono::Utc::now(),
        }
    }
    
    fn evaluate_beneficence(&self, system: &AISystem, context: &Context) -> f64 {
        // è¯„ä¼°ç³»ç»Ÿå¯¹äººç±»ç¦ç¥‰çš„è´¡çŒ®
        let mut positive_impacts = 0.0;
        
        // è¯„ä¼°ç»æµæ•ˆç›Š
        let economic_benefit = calculate_economic_benefit(system, context);
        
        // è¯„ä¼°ç¤¾ä¼šæ•ˆç›Š
        let social_benefit = calculate_social_benefit(system, context);
        
        // è¯„ä¼°ä¸ªäººæ•ˆç›Š
        let personal_benefit = calculate_personal_benefit(system, context);
        
        positive_impacts = 0.4 * economic_benefit + 0.3 * social_benefit + 0.3 * personal_benefit;
        
        // å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        positive_impacts.min(1.0).max(0.0)
    }
    
    fn evaluate_non_maleficence(&self, system: &AISystem, context: &Context) -> f64 {
        // è¯„ä¼°ç³»ç»Ÿé¿å…ä¼¤å®³çš„ç¨‹åº¦
        let potential_harm_mitigated = 1.0 - calculate_potential_harm(system, context);
        
        // è¯„ä¼°å®‰å…¨æœºåˆ¶
        let safety_score = evaluate_safety_mechanisms(system);
        
        // è¯„ä¼°é£é™©ç®¡ç†
        let risk_management = evaluate_risk_management(system, context);
        
        // ç»¼åˆè¯„åˆ†
        let harm_avoidance = 0.4 * potential_harm_mitigated + 0.3 * safety_score + 0.3 * risk_management;
        
        // å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        harm_avoidance.min(1.0).max(0.0)
    }
    
    fn evaluate_autonomy(&self, system: &AISystem, context: &Context) -> f64 {
        // è¯„ä¼°ç³»ç»Ÿå¯¹äººç±»è‡ªä¸»æ€§çš„å°Šé‡ç¨‹åº¦
        
        // è¯„ä¼°äººç±»æ§åˆ¶åº¦
        let human_control = evaluate_human_control(system);
        
        // è¯„ä¼°è‡ªä¸»å†³ç­–ç¨‹åº¦
        let autonomy_support = evaluate_autonomy_support(system, context);
        
        // è¯„ä¼°çŸ¥æƒ…åŒæ„å®æ–½
        let informed_consent = evaluate_informed_consent(system, context);
        
        // ç»¼åˆè¯„åˆ†
        let autonomy_respect = 0.4 * human_control + 0.3 * autonomy_support + 0.3 * informed_consent;
        
        // å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        autonomy_respect.min(1.0).max(0.0)
    }
    
    fn evaluate_justice(&self, system: &AISystem, context: &Context) -> f64 {
        // è¯„ä¼°ç³»ç»Ÿçš„å…¬æ­£æ€§
        
        // è®¡ç®—å„ç§å…¬å¹³æ€§æŒ‡æ ‡
        let fairness_scores = self.fairness_metrics.iter()
            .map(|metric| evaluate_fairness_metric(system, context, metric))
            .collect::<Vec<f64>>();
            
        // è®¡ç®—å¹³å‡å…¬å¹³æ€§è¯„åˆ†
        let avg_fairness = if fairness_scores.is_empty() {
            0.0
        } else {
            fairness_scores.iter().sum::<f64>() / fairness_scores.len() as f64
        };
        
        // è¯„ä¼°åŒ…å®¹æ€§
        let inclusiveness = evaluate_inclusiveness(system, context);
        
        // è¯„ä¼°èµ„æºåˆ†é…å…¬æ­£æ€§
        let resource_justice = evaluate_resource_justice(system, context);
        
        // ç»¼åˆè¯„åˆ†
        let justice_score = 0.5 * avg_fairness + 0.25 * inclusiveness + 0.25 * resource_justice;
        
        // å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        justice_score.min(1.0).max(0.0)
    }
    
    fn evaluate_explicability(&self, system: &AISystem, context: &Context) -> f64 {
        // è¯„ä¼°ç³»ç»Ÿçš„å¯è§£é‡Šæ€§å’Œé€æ˜åº¦
        
        // è¯„ä¼°é€æ˜åº¦
        let transparency = system.transparency_level;
        
        // è¯„ä¼°å¯è§£é‡Šæ€§
        let explainability = system.explainability_level;
        
        // è¯„ä¼°å†³ç­–å¯ç†è§£æ€§
        let decision_understandability = evaluate_decision_understandability(system, context);
        
        // è¯„ä¼°é—®è´£æœºåˆ¶
        let accountability = evaluate_accountability_mechanisms(system);
        
        // ç»¼åˆè¯„åˆ†
        let explicability_score = 0.25 * transparency + 0.25 * explainability + 
                                0.25 * decision_understandability + 0.25 * accountability;
        
        // å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        explicability_score.min(1.0).max(0.0)
    }
    
    fn identify_ethical_risks(&self, system: &AISystem, context: &Context) -> Vec<EthicalRisk> {
        // è¯†åˆ«ç³»ç»Ÿçš„ä¼¦ç†é£é™©
        let mut risks = Vec::new();
        
        // æ£€æŸ¥å…¬å¹³æ€§é£é™©
        for attribute in &context.sensitive_attributes {
            let bias_level = measure_bias_for_attribute(system, context, attribute);
            if bias_level > 0.2 { // å‡è®¾0.2ä¸ºé˜ˆå€¼
                risks.push(EthicalRisk {
                    risk_type: RiskType::Bias,
                    description: format!("å¯¹å±æ€§'{}''è¡¨ç°å‡º{}çº§åˆ«çš„åè§",
                                       attribute.name, categorize_bias_level(bias_level)),
                    severity: bias_level,
                    affected_principle: EthicalPrinciple::Justice,
                });
            }
        }
        
        // æ£€æŸ¥é€æ˜åº¦é£é™©
        if system.transparency_level < 0.5 {
            risks.push(EthicalRisk {
                risk_type: RiskType::LackOfTransparency,
                description: format!("ç³»ç»Ÿé€æ˜åº¦ä¸è¶³({})", system.transparency_level),
                severity: 1.0 - system.transparency_level,
                affected_principle: EthicalPrinciple::Explicability,
            });
        }
        
        // æ£€æŸ¥è‡ªä¸»æ€§é£é™©
        let autonomy_risk = assess_autonomy_risk(system, context);
        if autonomy_risk > 0.3 { // å‡è®¾0.3ä¸ºé˜ˆå€¼
            risks.push(EthicalRisk {
                risk_type: RiskType::AutonomyViolation,
                description: "ç³»ç»Ÿå¯èƒ½è¿‡åº¦é™åˆ¶ç”¨æˆ·è‡ªä¸»æ€§".to_string(),
                severity: autonomy_risk,
                affected_principle: EthicalPrinciple::Autonomy,
            });
        }
        
        // æ£€æŸ¥æ½œåœ¨ä¼¤å®³é£é™©
        let harm_risks = assess_potential_harms(system, context);
        for harm in harm_risks {
            if harm.probability * harm.impact > 0.25 { // é£é™©é˜ˆå€¼
                risks.push(EthicalRisk {
                    risk_type: RiskType::PotentialHarm,
                    description: format!("æ½œåœ¨ä¼¤å®³: {}", harm.description),
                    severity: harm.probability * harm.impact,
                    affected_principle: EthicalPrinciple::NonMaleficence,
                });
            }
        }
        
        risks
    }
    
    fn generate_recommendations(&self, system: &AISystem, context: &Context,
                               risks: &[EthicalRisk]) -> Vec<EthicalRecommendation> {
        // ç”Ÿæˆä¼¦ç†æ”¹è¿›å»ºè®®
        let mut recommendations = Vec::new();
        
        // åŸºäºè¯†åˆ«çš„é£é™©ç”Ÿæˆå»ºè®®
        for risk in risks {
            match risk.affected_principle {
                EthicalPrinciple::Justice => {
                    if risk.risk_type == RiskType::Bias {
                        recommendations.push(EthicalRecommendation {
                            principle: EthicalPrinciple::Justice,
                            description: format!("å®æ–½åè§ç¼“è§£ç­–ç•¥ï¼Œç‰¹åˆ«å…³æ³¨'{}'",
                                              extract_attribute_from_risk(risk)),
                            implementation_difficulty: calculate_difficulty(risk, system),
                            expected_improvement: estimate_improvement(risk, system),
                        });
                    }
                },
                EthicalPrinciple::Explicability => {
                    if risk.risk_type == RiskType::LackOfTransparency {
                        recommendations.push(EthicalRecommendation {
                            principle: EthicalPrinciple::Explicability,
                            description: "å¢å¼ºç³»ç»Ÿé€æ˜åº¦å’Œå¯è§£é‡Šæ€§æœºåˆ¶".to_string(),
                            implementation_difficulty: calculate_difficulty(risk, system),
                            expected_improvement: estimate_improvement(risk, system),
                        });
                    }
                },
                // å…¶ä»–åŸåˆ™çš„å»ºè®®...
                _ => {}
            }
        }
        
        // æ·»åŠ ä¸€èˆ¬æ€§æ”¹è¿›å»ºè®®
        recommendations.push(EthicalRecommendation {
            principle: EthicalPrinciple::Beneficence,
            description: "å®šæœŸè¯„ä¼°ç³»ç»Ÿå¯¹ç”¨æˆ·å’Œç¤¾ä¼šçš„ç§¯æå½±å“".to_string(),
            implementation_difficulty: 0.4,
            expected_improvement: 0.3,
        });
        
        recommendations.push(EthicalRecommendation {
            principle: EthicalPrinciple::NonMaleficence,
            description: "å®æ–½æŒç»­çš„é£é™©ç›‘æ§å’Œç¼“è§£ç­–ç•¥".to_string(),
            implementation_difficulty: 0.5,
            expected_improvement: 0.4,
        });
        
        recommendations
    }
}

// ç®—æ³•å…¬å¹³æ€§å®ç°
pub struct FairnessEvaluator {
    pub sensitive_attributes: Vec<AttributeId>,
    pub fairness_threshold: f64,
    pub metrics: Vec<FairnessMetric>,
}

impl FairnessEvaluator {
    pub fn evaluate_demographic_parity(&self, system: &AISystem, 
                                      data: &DataSet) -> HashMap<AttributeId, f64> {
        // è®¡ç®—å„æ•æ„Ÿå±æ€§çš„äººå£å¹³ä»·å·®å¼‚
        let mut results = HashMap::new();
        
        for attr_id in &self.sensitive_attributes {
            // è®¡ç®—ä¸åŒç»„ä¹‹é—´å†³ç­–ç‡çš„å·®å¼‚
            let max_disparity = calculate_max_disparity_for_attribute(system, data, *attr_id);
            results.insert(*attr_id, max_disparity);
        }
        
        results
    }
    
    pub fn evaluate_equalized_odds(&self, system: &AISystem, 
                                 data: &DataSet) -> HashMap<AttributeId, f64> {
        // è®¡ç®—å„æ•æ„Ÿå±æ€§çš„å‡ç­‰èµ”ç‡å·®å¼‚
        let mut results = HashMap::new();
        
        for attr_id in &self.sensitive_attributes {
            // è®¡ç®—ä¸åŒç»„ä¹‹é—´çš„çœŸé˜³æ€§ç‡å’Œå‡é˜³æ€§ç‡å·®å¼‚
            let max_odds_difference = calculate_max_odds_difference_for_attribute(system, data, *attr_id);
            results.insert(*attr_id, max_odds_difference);
        }
        
        results
    }
    
    pub fn is_fair(&self, disparities: &HashMap<AttributeId, f64>) -> bool {
        // åˆ¤æ–­ç³»ç»Ÿæ˜¯å¦å…¬å¹³
        for (_, disparity) in disparities {
            if *disparity > self.fairness_threshold {
                return false;
            }
        }
        
        true
    }
    
    pub fn fairness_mitigation_strategy(&self, system: &AISystem, 
                                       disparities: &HashMap<AttributeId, f64>) -> MitigationStrategy {
        // ç¡®å®šç¼“è§£åè§çš„ç­–ç•¥
        // å®é™…å®ç°ä¼šæ ¹æ®å…·ä½“æƒ…å†µï¼Œé€‰æ‹©é¢„å¤„ç†ã€å¤„ç†ä¸­æˆ–åå¤„ç†ç­–ç•¥
        
        // ç®€å•ç¤ºä¾‹ï¼šæ ¹æ®åè§ç¨‹åº¦é€‰æ‹©ç­–ç•¥
        let max_disparity = disparities.values().cloned().fold(0.0, f64::max);
        
        if max_disparity > 0.3 {
            MitigationStrategy::PreProcessing
        } else if max_disparity > 0.1 {
            MitigationStrategy::InProcessing
        } else {
            MitigationStrategy::PostProcessing
        }
    }
}
```

## ğŸ“Š 6. AIä¼¦ç†æ¡ˆä¾‹åˆ†æ

### 6.1 ç®—æ³•åè§æ¡ˆä¾‹

ç®—æ³•åè§åˆ†æå¯å½¢å¼åŒ–ä¸ºï¼š

$$
Bias(A, S, D) = \max_{s_i, s_j \in S} |P(A(x)=1 | S=s_i) - P(A(x)=1 | S=s_j)|
$$

åè§ç¼“è§£ç­–ç•¥å¯è¡¨ç¤ºä¸ºä¼˜åŒ–é—®é¢˜ï¼š

$$
\begin{align}
\min_{\theta} L(A_{\theta}, D) \\
\text{s.t. } Bias(A_{\theta}, S, D) \leq \epsilon
\end{align}
$$

### 6.2 è‡ªåŠ¨é©¾é©¶ä¼¦ç†å†³ç­–

è‡ªåŠ¨é©¾é©¶ä¼¦ç†å†³ç­–å¯å½¢å¼åŒ–ä¸ºï¼š

$$
Decision = \argmax_{a \in Actions} \left( w_S \cdot Safety(a) + w_F \cdot Fairness(a) + w_E \cdot Efficiency(a) \right)
$$

ç‰¹æ´›ä¼¦é—®é¢˜ï¼ˆTrolley Problemï¼‰çš„AIç‰ˆæœ¬ï¼š

$$
\begin{align}
Utilitarian &: \argmin_{a \in Actions} ExpectedCasualties(a) \\
Deontological &: \arg_{a \in Actions} \neg IntentionalHarm(a) \\
Virtue &: \arg_{a \in Actions} Virtuous(a, \{Prudence, Justice, Courage\})
\end{align}
$$

### 6.3 AIéšç§ä¿æŠ¤æ¡ˆä¾‹

AIéšç§æ¨¡å‹å¯å½¢å¼åŒ–ä¸ºï¼š

$$
PrivacyRisk = \sum_{i=1}^{n} P(ReIdentification_i) \times Sensitivity(Data_i)
$$

å·®åˆ†éšç§è§£å†³æ–¹æ¡ˆï¼š

$$
PrivateAI(D) = AI(D) + Noise(\epsilon, \delta)
$$

å…¶ä¸­ $\epsilon$ å’Œ $\delta$ æ§åˆ¶éšç§ä¿æŠ¤çº§åˆ«ã€‚

## ğŸ”— 7. äº¤å‰å¼•ç”¨

- [è§„èŒƒä¼¦ç†å­¦](./01_Normative_Ethics.md) - AIä¼¦ç†å­¦çš„ç†è®ºåŸºç¡€
- [åº”ç”¨ä¼¦ç†å­¦](./03_Applied_Ethics.md) - AIä¼¦ç†å­¦çš„ç‰¹æ®Šåº”ç”¨é¢†åŸŸ
- [å½¢å¼è¯­è¨€ç†è®º](../../03_Formal_Language_Theory/README.md) - å½¢å¼åŒ–è¡¨ç¤ºä¼¦ç†è§„èŒƒ
- [è®¡ç®—ç†è®º](../../04_Computation_Theory/README.md) - å¯è®¡ç®—ä¼¦ç†æ¨¡å‹

## ğŸ“š 8. å‚è€ƒæ–‡çŒ®

1. Floridi, L. & Sanders, J. W. (2004). *On the Morality of Artificial Agents*
2. Wallach, W. & Allen, C. (2009). *Moral Machines: Teaching Robots Right from Wrong*
3. Russell, S. (2019). *Human Compatible: Artificial Intelligence and the Problem of Control*
4. Mittelstadt, B. D. et al. (2016). *The Ethics of Algorithms: Mapping the Debate*
5. Jobin, A., Ienca, M., & Vayena, E. (2019). *The Global Landscape of AI Ethics Guidelines*
6. Barocas, S. & Selbst, A. D. (2016). *Big Data's Disparate Impact*
7. Dwork, C. et al. (2012). *Fairness Through Awareness*
8. Kaplan, A. & Haenlein, M. (2019). *Siri, Siri, in My Hand: Who's the Fairest in the Land?*
9. Floridi, L. (2019). *Establishing the Rules for Building Trustworthy AI*
10. Dignum, V. (2019). *Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way*
